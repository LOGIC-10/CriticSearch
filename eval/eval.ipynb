{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Notebook:\n",
    "## Author: William Diaz\n",
    "### Artificial Agents @ JHU\n",
    "**Evaluation for the Critic Agent Using a modified version of the hotpotQA dataset.**\n",
    "***We drop all context for the questions, instead opting to use the search and reasoning capabilities of each model we evaluate against.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "PERPLEXITY_API_KEY = os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "GPT4O_API_KEY = os.getenv(\"GPT4O_API_KEY\")\n",
    "\n",
    "PERPLEXITY_BASE_URL = \"https://api.perplexity.ai\"\n",
    "GPT4O_BASE_URL = \"https://api.openai.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 566M/566M [01:10<00:00, 8.01MB/s] \n",
      "Downloading data: 100%|██████████| 47.5M/47.5M [00:05<00:00, 8.98MB/s]\n",
      "Downloading data: 100%|██████████| 46.2M/46.2M [00:05<00:00, 8.22MB/s]\n",
      "Generating train split: 100%|██████████| 90447/90447 [00:11<00:00, 7892.51 examples/s] \n",
      "Generating validation split: 100%|██████████| 7405/7405 [00:00<00:00, 8157.17 examples/s] \n",
      "Generating test split: 100%|██████████| 7405/7405 [00:00<00:00, 10007.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"hotpot_qa\", \"fullwiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'question', 'answer', 'type', 'level', 'supporting_facts', 'context'])\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# we will sample from the validation dataset, though this should not matter\n",
    "validation_data = dataset['validation']\n",
    "sample_size = 200\n",
    "random_indices = random.sample(range(len(validation_data)), sample_size)\n",
    "sampled_data = [validation_data[i] for i in random_indices]\n",
    "print(sampled_data[0].keys())\n",
    "print(len(sampled_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output CSV\n",
    "output_file = \"hotpotqa_perplexity_small.csv\"\n",
    "fieldnames = [\n",
    "    \"id\",\n",
    "    \"question\",\n",
    "    \"ground_truth_answer\",\n",
    "    \"perplexity_answer\",\n",
    "    \"binary_success\",\n",
    "    \"context\",\n",
    "    \"type\",\n",
    "    \"level\",\n",
    "    \"supporting_facts\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_backoff_retry(func, max_retries=5, initial_wait=1):\n",
    "    \"\"\"Utility for exponential backoff retries.\"\"\"\n",
    "    wait = initial_wait\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            if i == max_retries - 1:\n",
    "                raise e\n",
    "            time.sleep(wait)\n",
    "            wait *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biographer of John Clare, Jonathan Bate, teaches English Literature at the University of Oxford. Specifically, he is Professor of English Literature at the University of Oxford and also serves as the Provost of Worcester College, Oxford[1][2][3].\n"
     ]
    }
   ],
   "source": [
    "def query_perplexity(question):\n",
    "    \"\"\"Query the perplexity API with a given question.\"\"\"\n",
    "    def do_request():\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {PERPLEXITY_API_KEY}\"\n",
    "        }\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an artificial intelligence assistant and you need to \"\n",
    "                    \"engage in a helpful, detailed, polite conversation with a user. \"\n",
    "                    \"The user will ask you a question. \"\n",
    "                    \"You will break down the question, search online for relevant information \"\n",
    "                    \"and analyze the returned search results, reasoning \"\n",
    "                    \"through ambiguity and providing an accurate fact backed answer to the user.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        json_payload = {\n",
    "            \"model\": \"llama-3.1-sonar-small-128k-online\",\n",
    "            \"messages\": messages\n",
    "        }\n",
    "        response = requests.post(\n",
    "            f\"{PERPLEXITY_BASE_URL}/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=json_payload,\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        resp_json = response.json()\n",
    "        # Assuming response JSON structure: {'choices': [{'message': {'content': 'answer'}}], ...}\n",
    "        # If different, adjust accordingly.\n",
    "        return resp_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    answer = exponential_backoff_retry(do_request)\n",
    "    # Sleep to avoid rate limits\n",
    "    time.sleep(1.3)\n",
    "    return answer\n",
    "\n",
    "\n",
    "query = \"At which university does the biographer of John Clare teach English Literature?\"\n",
    "answer = query_perplexity(question=query)\n",
    "print(answer)\n",
    "# query = \"What is going on with the attempted Coup in Korea?\" # real time question\n",
    "# answer = query_perplexity(question=query)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The evaluator will now compare the answer against the gold:\n",
    "\n",
    "def query_gpt4o_evaluator(question, ground_truth, model_answer):\n",
    "    \"\"\"Query GPT4o for binary evaluation of correctness.\"\"\"\n",
    "    def do_request():\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {GPT4O_API_KEY}\"\n",
    "        }\n",
    "        system_prompt = (\n",
    "            \"You are a reasoning assistant. You are given a question, a ground truth answer, \"\n",
    "            \"and a model's answer. You must determine if the model's answer fully and \"\n",
    "            \"correctly answers the question according to the ground truth. \"\n",
    "            \"If it does, respond with 'True'. If not, respond with 'False'. No explanations, elaborations, or additional information please.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"Question: {question}\\n\"\n",
    "            f\"Ground Truth: {ground_truth}\\n\"\n",
    "            f\"Model Answer: {model_answer}\\n\\n\"\n",
    "            \"Does the model answer match the ground truth fully and correctly?\"\n",
    "        )\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        json_payload = {\n",
    "            \"model\": \"gpt-4o\",  # Replace with actual model name for gpt4o\n",
    "            \"messages\": messages\n",
    "        }\n",
    "        response = requests.post(\n",
    "            f\"{GPT4O_BASE_URL}/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=json_payload,\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        resp_json = response.json()\n",
    "        content = resp_json[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        # Expecting either \"True\" or \"False\"\n",
    "        if content.lower() == \"true\":\n",
    "            return True\n",
    "        elif content.lower() == 'false':\n",
    "            return False\n",
    "        else:\n",
    "            raise ValueError(\"Evaluator returned unexpected content: {content}\")\n",
    "\n",
    "    result = exponential_backoff_retry(do_request)\n",
    "    # A short sleep to avoid hitting rate limit for evaluator as well\n",
    "    time.sleep(0.13)\n",
    "    return result\n",
    "\n",
    "query = \"At which university does the biographer of John Clare teach English Literature?\"\n",
    "answer = \"The biographer of John Clare, Jonathan Bate, teaches English Literature at the University of Oxford. Specifically, he is Professor of English Literature at the University of Oxford and also serves as the Provost of Worcester College, Oxford[1][2][3].\"\n",
    "gold = \"University of Oxford\"\n",
    "query_gpt4o_evaluator(question=query, model_answer=answer, ground_truth=gold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [14:23<00:00,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if there's already a partial file\n",
    "processed_ids = set()\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            processed_ids.add(row[\"id\"])\n",
    "            \n",
    "# write out headers\n",
    "if not os.path.exists(output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "# Main loop\n",
    "with open(output_file, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    for entry in tqdm(sampled_data):\n",
    "        if entry[\"id\"] in processed_ids:\n",
    "            continue\n",
    "\n",
    "        question = entry[\"question\"]\n",
    "        ground_truth_answer = entry[\"answer\"]\n",
    "        context = entry[\"context\"]  # list of (title, [sentences])\n",
    "        supporting_facts = entry[\"supporting_facts\"]\n",
    "        entry_type = entry[\"type\"]\n",
    "        level = entry[\"level\"]\n",
    "        entry_id = entry[\"id\"]\n",
    "\n",
    "        # Query perplexity for answer\n",
    "        perplexity_answer = query_perplexity(question)\n",
    "\n",
    "        # Evaluate correctness with GPT4o\n",
    "        binary_success = query_gpt4o_evaluator(question, ground_truth_answer, perplexity_answer)\n",
    "\n",
    "        # Write result\n",
    "        writer.writerow({\n",
    "            \"id\": entry_id,\n",
    "            \"question\": question,\n",
    "            \"ground_truth_answer\": ground_truth_answer,\n",
    "            \"perplexity_answer\": perplexity_answer,\n",
    "            \"binary_success\": int(binary_success),\n",
    "            \"context\": json.dumps(context),\n",
    "            \"type\": entry_type,\n",
    "            \"level\": level,\n",
    "            \"supporting_facts\": json.dumps(supporting_facts)\n",
    "        })\n",
    "\n",
    "        f.flush()  # Ensure we write after each entry\n",
    "\n",
    "print(\"Evaluation complete. Results saved to\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed. Cleaned CSV saved to perplexity_large_clean.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your input CSV with duplicates\n",
    "input_csv = \"perplexity_large.csv\"\n",
    "\n",
    "# Path to your output CSV after cleaning\n",
    "output_csv = \"perplexity_large_clean.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Drop duplicates based on the 'id' column, keeping the first occurrence\n",
    "df_cleaned = df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# Write the cleaned DataFrame to a new CSV\n",
    "df_cleaned.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Duplicates removed. Cleaned CSV saved to {output_csv}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPLEXTEMPQA attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "\n",
    "data_stream = load_dataset(\"DataScienceUIBK/ComplexTempQA\", split=\"train\", streaming=True)\n",
    "data_comptempqa = list(islice(data_stream, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'question': 'Did the Winter Olympic Games in 1994 which had 1737 participants had a higher number of participants than the sports season in 2006 in Germany which had 32 participants?', 'answer': ['yes'], 'type': '2a', 'rating': 1, 'timeframe': [datetime.datetime(1994, 2, 1, 0, 0), datetime.datetime(2006, 7, 9, 0, 0)], 'question_entity': ['9663', '37285'], 'answer_entity': ['224013'], 'question_country_entity': ['20', '183'], 'is_unnamed': 1, 'answer_country_entity': None}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(data_comptempqa[0])\n",
    "# Prepare output CSV\n",
    "output_file = \"comptempqa_perplexity_small.csv\"\n",
    "fieldnames = [\n",
    "    \"id\",\n",
    "    \"question\",\n",
    "    \"ground_truth_answer\",\n",
    "    \"model_answer\",\n",
    "    \"binary_success\",\n",
    "    \"rating\",\n",
    "    \"timeframe\",\n",
    "    \"type\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [11:52<00:00,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to comptempqa_perplexity_large.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if there's already a partial file\n",
    "processed_ids = set()\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            processed_ids.add(row[\"id\"])\n",
    "\n",
    "print(processed_ids)\n",
    "\n",
    "# write out headers\n",
    "if not os.path.exists(output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "# Main loop\n",
    "with open(output_file, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    for entry in tqdm(data_comptempqa):\n",
    "        if entry[\"id\"] in processed_ids:\n",
    "            continue\n",
    "\n",
    "        question = entry[\"question\"]\n",
    "        ground_truth_answer = entry[\"answer\"]\n",
    "        entry_id = entry[\"id\"]\n",
    "        rating = entry['rating']\n",
    "        timeframe = entry['timeframe']\n",
    "\n",
    "        # Query perplexity for answer\n",
    "        perplexity_answer = query_perplexity(question)\n",
    "\n",
    "        # Evaluate correctness with GPT4o\n",
    "        binary_success = query_gpt4o_evaluator(question, ground_truth_answer, perplexity_answer)\n",
    "\n",
    "        # Write result\n",
    "        writer.writerow({\n",
    "            \"id\": entry_id,\n",
    "            \"question\": question,\n",
    "            \"ground_truth_answer\": ground_truth_answer,\n",
    "            \"model_answer\": perplexity_answer,\n",
    "            \"binary_success\": int(binary_success),\n",
    "            \"rating\": rating,\n",
    "            \"timeframe\": timeframe,\n",
    "        })\n",
    "\n",
    "        f.flush()  # Ensure we write after each entry\n",
    "\n",
    "print(\"Evaluation complete. Results saved to\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIKIQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output CSV\n",
    "output_file = \"wikiqa_perplexity_small.csv\"\n",
    "fieldnames = [\n",
    "    \"id\",\n",
    "    \"question\",\n",
    "    \"model_answer\",\n",
    "    \"ground_truth_answer\",\n",
    "    \"binary_success\",\n",
    "    \"type\",\n",
    "]\n",
    "\n",
    "# Check if there's already a partial file\n",
    "processed_ids = set()\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            processed_ids.add(row[\"id\"])\n",
    "\n",
    "\n",
    "# write out headers\n",
    "if not os.path.exists(output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 6165/6165 [00:00<00:00, 147545.19 examples/s]\n",
      "Generating validation split: 100%|██████████| 2733/2733 [00:00<00:00, 618219.87 examples/s]\n",
      "Generating train split: 100%|██████████| 20360/20360 [00:00<00:00, 1150486.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_temp = load_dataset(\"microsoft/wiki_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question_id', 'question', 'document_title', 'answer', 'label'])\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# we will sample from the validation dataset, though this should not matter\n",
    "validation_data_temp = dataset_temp['validation']\n",
    "sample_size_temp = 200\n",
    "random_indices_temp = random.sample(range(len(validation_data_temp)), sample_size_temp)\n",
    "sampled_data_temp = [validation_data_temp[i] for i in random_indices_temp]\n",
    "print(sampled_data_temp[0].keys())\n",
    "print(len(sampled_data_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 65/200 [09:28<18:53,  8.40s/it]"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "with open(output_file, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    for entry in tqdm(sampled_data_temp):\n",
    "        if entry[\"question_id\"] in processed_ids:\n",
    "            continue\n",
    "\n",
    "        question = entry[\"question\"]\n",
    "        ground_truth_answer = entry[\"answer\"]\n",
    "        entry_id = entry[\"question_id\"]\n",
    "        type = entry[\"label\"]\n",
    "\n",
    "        # Query perplexity for answer\n",
    "        perplexity_answer = query_perplexity(question)\n",
    "\n",
    "        # Evaluate correctness with GPT4o\n",
    "        binary_success = query_gpt4o_evaluator(question, ground_truth_answer, perplexity_answer)\n",
    "\n",
    "        # Write result\n",
    "        writer.writerow({\n",
    "            \"id\": entry_id,\n",
    "            \"question\": question,\n",
    "            \"model_answer\": perplexity_answer,\n",
    "            \"ground_truth_answer\": ground_truth_answer,\n",
    "            \"binary_success\": int(binary_success),\n",
    "            \"type\": type,\n",
    "        })\n",
    "\n",
    "        f.flush()  # Ensure we write after each entry\n",
    "\n",
    "print(\"Evaluation complete. Results saved to\", output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
