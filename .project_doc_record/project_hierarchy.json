{
  "agent_factory/utils.py": [
    {
      "type": "FunctionDef",
      "name": "read_prompt_template",
      "md_content": [
        "**read_prompt_template**: The function of read_prompt_template is to read the contents of a specified file and return it as a string.\n\n**parameters**: The parameters of this Function.\n· file_path: A string representing the path to the file that contains the prompt template to be read.\n\n**Code Description**: The read_prompt_template function is designed to open a file located at the path specified by the file_path parameter. It uses a context manager (the 'with' statement) to ensure that the file is properly opened and closed after its contents are read. Inside the context manager, the file is opened in read mode ('r'), and the entire content of the file is read using the read() method. The contents are then stored in the variable prompt, which is subsequently returned as the output of the function. This function is useful for loading prompt templates or any text data stored in a file format.\n\n**Note**: It is important to ensure that the file specified by file_path exists and is accessible; otherwise, a FileNotFoundError will be raised. Additionally, the function assumes that the file contains text data and is encoded in a format compatible with the default encoding used by Python (usually UTF-8).\n\n**Output Example**: If the file located at the specified file_path contains the text \"Hello, this is a prompt template.\", the function will return the string \"Hello, this is a prompt template.\""
      ],
      "code_start_line": 5,
      "code_end_line": 8,
      "params": [
        "file_path"
      ],
      "have_return": true,
      "code_content": "def read_prompt_template(file_path):\n  with open(file_path, 'r') as file:\n    prompt = file.read()\n  return prompt\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "call_llm",
      "md_content": [
        "**call_llm**: The function of call_llm is to interact with a language model API to generate a response based on provided prompts.\n\n**parameters**: The parameters of this Function.\n· model: A string representing the name of the language model to be used for generating responses.\n· sys_prompt: A string containing the system-level prompt that sets the context for the model.\n· usr_prompt: A string that represents the user-level prompt, which is the specific query or input from the user.\n· config: A dictionary containing configuration settings such as API key, base URL, timeout, maximum retries, temperature, and maximum tokens for the model.\n\n**Code Description**: The call_llm function is designed to facilitate communication with a language model, specifically through the OpenAI API. It begins by initializing an OpenAI client using the provided model name and configuration settings. The configuration includes essential parameters such as the API key, base URL, timeout, and maximum retries, which are retrieved from the config dictionary.\n\nNext, the function constructs a list of messages that includes both the system prompt and the user prompt. This structured format is necessary for the chat completion request to the API. The function then calls the chat completion method of the OpenAI client, passing in the model name, the constructed messages, and additional parameters like temperature and maximum tokens.\n\nUpon receiving the response from the API, the function extracts the content of the first message choice from the response object and returns it. This content represents the model's generated reply based on the provided prompts.\n\nThe call_llm function is invoked within the breakdown_task method of the Manager class in the agent_factory/manager.py file. In this context, it is used to break down a larger task into smaller sub-tasks by rendering a prompt and sending it to the language model for processing. The response from call_llm is then returned as the output of the breakdown_task method, indicating its role in task decomposition and interaction with the language model.\n\n**Note**: When using this function, ensure that the configuration dictionary is properly populated with all necessary keys and values to avoid runtime errors. Additionally, be mindful of the API usage limits and the potential costs associated with calling the OpenAI API.\n\n**Output Example**: A possible return value from the call_llm function could be a string such as \"To break down the task, consider the following steps: 1. Analyze the requirements, 2. Identify key components, 3. Create sub-tasks for each component.\"",
        "**call_llm**: The function of call_llm is to make an API request to a language model service, sending a system and user prompt, and returning the model's response.\n\n**parameters**: The parameters of this function.\n· model: The model name or identifier used for making the request to the language model service.\n· sys_prompt: A string that serves as the system message for the model, providing instructions or context for the conversation.\n· usr_prompt: A string that serves as the user's message or input for the model, to which the model should respond.\n· config: A dictionary containing configuration settings required for the request, including API keys, timeouts, retry settings, temperature, and model-specific settings.\n\n**Code Description**:  \nThe `call_llm` function is responsible for interfacing with a language model API (presumably OpenAI's API). It does so by creating a client instance for the OpenAI service using the configuration values passed in the `config` parameter. The function then constructs a list of messages that includes the system message (`sys_prompt`) and the user message (`usr_prompt`). This list is sent as part of the request to the `chat.completions.create` method of the API client.\n\nThe function retrieves the response from the API and extracts the model's reply from the `choices` list in the response. Specifically, it fetches the content of the message from the first choice in the list, which is assumed to be the relevant response from the model.\n\nKey steps in the process:\n1. An instance of the OpenAI client is created using the provided configuration values, including the API key, base URL, timeout, and retry settings.\n2. A list of messages is constructed, where the system message provides context or instructions, and the user message contains the query or input.\n3. The request is sent to the API, specifying the model, the messages, and other parameters like temperature.\n4. The model's response is extracted from the API's response and returned as the result.\n\nThe `call_llm` function is called within the `chat` method of the `BaseAgent` class in the `agent_factory/agent.py` module. In this context, it is used to send a dynamically rendered prompt to the language model based on the input data. The system and user prompts are provided as part of this interaction, and the function returns the model's response to be processed further or sent back to the user.\n\n**Note**:  \n- The `config` parameter must include valid API keys and any necessary configuration settings like `timeout`, `max_retries`, and `temperature` for the API request to be successful.\n- The function assumes the model will return a response in the form of a list of choices, with the actual message located in `response.choices[0].message`.\n- While the `max_tokens` parameter is mentioned in the code as a potential setting, it is commented out, implying it is either optional or controlled elsewhere in the codebase.\n\n**Output Example**:  \nThe return value of `call_llm` would typically be a string representing the content of the model's response. For example:\n\n```\n\"Sure, here's the information you requested: ... \"\n```"
      ],
      "code_start_line": 11,
      "code_end_line": 34,
      "params": [
        "model",
        "sys_prompt",
        "usr_prompt",
        "config"
      ],
      "have_return": true,
      "code_content": "def call_llm(model, sys_prompt, usr_prompt, config):\n\n    client = OpenAI(\n        api_key=config.get(\"models\").get(model).get(\"api_key\"),\n        base_url=config.get(\"models\").get(model).get(\"base_url\"),\n        timeout=config.get(\"timeout\"),\n        max_retries=config.get(\"max_retries\"),\n    )\n\n    messages = [\n        {\"role\": \"system\", \"content\": sys_prompt},\n        {\"role\": \"user\", \"content\": usr_prompt},\n    ]\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=config.get(\"temperature\"),\n        # max_tokens=config.get(\"models\").get(model).get(\"max_tokens\",\"8192\"),\n    )\n\n    response_message = response.choices[0].message\n\n    return response_message.content",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/agent.py",
        "agent_factory/agent.py/BaseAgent/chat",
        "agent_factory/manager.py"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "agent_factory/agent.py": [
    {
      "type": "ClassDef",
      "name": "BaseAgent",
      "md_content": [
        "**BaseAgent**: The function of BaseAgent is to serve as a foundational class for agents that interact with a language model for chat-based functionalities.\n\n**attributes**: The attributes of this Class.\n· config: A configuration object that is read from a configuration file, containing various settings for the agent's operation.  \n· model: A string that specifies the default model to be used for generating responses, defaulting to \"gpt-4o-mini\".  \n· env: An instance of the Environment class used for loading templates from the file system, specifically for prompt management.  \n· sys_prompt: A string that holds the system prompt, initialized as an empty string, which can be used to set the context for the chat.  \n· repeat_turns: An integer that defines the number of times to repeat the task or interaction, defaulting to 10.\n\n**Code Description**: The BaseAgent class is designed to provide a common interface and functionality for agents that require interaction with a language model. Upon initialization, the class reads configuration settings from an external file, which includes the model to be used and the path to the prompt templates. The class sets up an environment for loading these templates, allowing for dynamic prompt generation based on the data provided during chat interactions.\n\nThe `chat` method is a core function of the BaseAgent class, which facilitates communication with the language model. It takes in a data dictionary and a prompt template, rendering the prompt with the provided data. The rendered prompt is then sent to a function called `call_llm`, which is responsible for interacting with the language model and generating a response based on the system prompt and user prompt.\n\nThe BaseAgent class is utilized by the Manager class, which inherits from it. The Manager class leverages the chat functionality of BaseAgent to break down tasks into sub-tasks. It initializes its own attributes while also inheriting the configuration and model settings from BaseAgent. This relationship allows the Manager to effectively utilize the chat method to interact with the language model for task management purposes.\n\n**Note**: It is essential to ensure that the configuration file is correctly set up and that the necessary template files are available in the specified prompt folder path. The `call_llm` function must also be properly defined elsewhere in the codebase to handle the interaction with the language model.\n\n**Output Example**: An example output from the `chat` method might look like this:\n```\n{\n    \"response\": \"Here are the steps to break down your task: Research the topic, Draft an outline, Write the introduction, Gather references.\",\n    \"status\": \"success\",\n    \"message\": \"Response generated successfully.\"\n}\n```"
      ],
      "code_start_line": 35,
      "code_end_line": 51,
      "params": [],
      "have_return": true,
      "code_content": "class BaseAgent:\n    def __init__(self):\n        self.config = read_config()\n        self.model = self.config.get('default_model', \"gpt-4o-mini\")\n        self.env = Environment(loader=FileSystemLoader(self.config.get('prompt_folder_path')))\n        self.sys_prompt = ''\n        self.repeat_turns = 10\n\n    \n    def chat(self, data, prompt_template):\n        \"\"\"\n        通用的聊天方法，根据传入的data字典适配不同的prompt。\n        \"\"\"\n        rendered_prompt = prompt_template.render(**data)\n        # print(rendered_prompt)\n        response_message = call_llm(model=self.model, sys_prompt=self.sys_prompt, usr_prompt=rendered_prompt, config=self.config)\n        return response_message",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/manager.py",
        "agent_factory/manager.py/Manager"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the BaseAgent class by setting up its configuration and environment.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The __init__ method is a constructor for the BaseAgent class. It is called when an instance of the class is created. The method performs several key operations:\n\n1. It invokes the read_config function to load configuration settings from a YAML file. This function returns a dictionary containing various configuration parameters, which is stored in the instance variable self.config.\n\n2. The method retrieves the value associated with the key 'default_model' from the configuration dictionary. If this key is not present, it defaults to the string \"gpt-4o-mini\". This value is stored in the instance variable self.model, which likely represents the model that the agent will use for its operations.\n\n3. The method initializes an Environment object from the Jinja2 templating library. This object is configured with a loader that points to the directory specified by the 'prompt_folder_path' key in the configuration dictionary. This allows the agent to dynamically load prompt templates from the specified folder.\n\n4. The instance variable self.sys_prompt is initialized as an empty string, which may be used later to store system prompts or messages.\n\n5. Finally, the method sets self.repeat_turns to 10, which could indicate the number of times the agent is allowed to repeat certain actions or interactions during its operation.\n\nOverall, the __init__ method establishes the foundational settings and components necessary for the BaseAgent to function effectively. It relies on the read_config function to ensure that the agent is configured according to the specified settings in the YAML file, thus enabling flexibility and customization in its behavior.\n\n**Note**: It is crucial to ensure that the YAML configuration file is accessible and correctly formatted. Any issues with file access or parsing may result in runtime errors, preventing the BaseAgent from initializing properly."
      ],
      "code_start_line": 36,
      "code_end_line": 41,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.config = read_config()\n        self.model = self.config.get('default_model', \"gpt-4o-mini\")\n        self.env = Environment(loader=FileSystemLoader(self.config.get('prompt_folder_path')))\n        self.sys_prompt = ''\n        self.repeat_turns = 10\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "agent_factory/config.py/read_config"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "chat",
      "md_content": [
        "**chat**: The function of chat is to provide a general chat method that adapts to different prompts based on the input data dictionary.\n\n**parameters**: The parameters of this Function.\n· data: A dictionary containing key-value pairs that are used to render the prompt template dynamically.\n· prompt_template: An object that has a render method, which formats the prompt using the provided data.\n\n**Code Description**: The chat method is a member of the BaseAgent class and is designed to facilitate interactions with a language model by generating a user-specific prompt based on the input data. The method begins by rendering the prompt template using the provided data dictionary, which allows for dynamic customization of the prompt content. This is achieved through the call to `prompt_template.render(**data)`, where the keys in the data dictionary are unpacked as keyword arguments for the render method.\n\nOnce the prompt has been rendered, the method proceeds to call the `call_llm` function. This function is responsible for making an API request to a language model service, sending both a system prompt and the dynamically generated user prompt. The parameters passed to `call_llm` include the model identifier, system prompt, the rendered user prompt, and configuration settings. The response from the `call_llm` function, which contains the model's reply, is then returned as the output of the chat method.\n\nThe chat method is typically invoked by other methods within the BaseAgent class or its subclasses, such as the breakdown_task method in the Manager class. In this context, the breakdown_task method creates a data dictionary that includes the original task and uses the chat method to generate a response from the language model, effectively bridging the gap between task decomposition and intelligent processing.\n\n**Note**: It is essential to ensure that the prompt_template object is properly initialized and that the data dictionary contains the necessary keys expected by the render method to avoid runtime errors. Additionally, the configuration settings passed to the `call_llm` function must be valid and complete for successful API interaction.\n\n**Output Example**: A possible return value from the chat method could be a string such as \"Based on the provided data, here is the response you requested: ...\""
      ],
      "code_start_line": 44,
      "code_end_line": 51,
      "params": [
        "self",
        "data",
        "prompt_template"
      ],
      "have_return": true,
      "code_content": "    def chat(self, data, prompt_template):\n        \"\"\"\n        通用的聊天方法，根据传入的data字典适配不同的prompt。\n        \"\"\"\n        rendered_prompt = prompt_template.render(**data)\n        # print(rendered_prompt)\n        response_message = call_llm(model=self.model, sys_prompt=self.sys_prompt, usr_prompt=rendered_prompt, config=self.config)\n        return response_message",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/manager.py/Manager/breakdown_task"
      ],
      "reference_who": [
        "agent_factory/utils.py/call_llm"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "agent_factory/manager.py": [
    {
      "type": "ClassDef",
      "name": "Manager",
      "md_content": [
        "**Manager**: The function of Manager is to manage tasks by receiving an original task and breaking it down into sub-tasks for further processing.\n\n**attributes**: The attributes of this Class.\n· original_task: A string that holds the original task received by the manager.  \n· sub_tasks: A list that stores the sub-tasks generated from the original task.  \n· config: A configuration object that is read from a configuration file.  \n· model: A string that specifies the default model to be used, defaulting to \"gpt-4o-mini\".  \n· env: An instance of the Environment class used for loading templates from the file system.  \n· sys_prompt: A string that holds the system prompt, initialized as an empty string.  \n· breakdown_prompt: A template loaded from a file named 'manager_break_down.txt' used for breaking down tasks.  \n· reflection_prompt: A placeholder for a reflection prompt, initialized as None.  \n· repeat_turns: An integer that defines the number of times to repeat the task breakdown, defaulting to 10.\n\n**Code Description**: The Manager class is designed to facilitate the management of tasks by allowing the user to input an original task and subsequently breaking it down into smaller, manageable sub-tasks. Upon initialization, the class sets up several attributes, including the original task, a list for sub-tasks, and configuration settings read from an external source. The class utilizes a templating engine to render prompts that guide the task breakdown process. \n\nThe `__init__` method initializes the Manager instance, setting up the necessary attributes and loading the configuration settings. The `receive_task` method allows the user to input an original task, which is stored in the `original_task` attribute. The `breakdown_task` method is responsible for taking the original task and rendering a prompt using the `breakdown_prompt` template. This rendered prompt is then passed to a function called `call_llm`, which presumably interacts with a language model to generate a response based on the task breakdown.\n\n**Note**: It is important to ensure that the configuration file is correctly set up and that the necessary template files are available in the specified prompt folder path. The `call_llm` function must also be properly defined elsewhere in the codebase to handle the interaction with the language model.\n\n**Output Example**: An example output from the `breakdown_task` method might look like this:\n```\n{\n    \"sub_tasks\": [\n        \"Research the topic\",\n        \"Draft an outline\",\n        \"Write the introduction\",\n        \"Gather references\"\n    ],\n    \"status\": \"success\",\n    \"message\": \"Task has been successfully broken down.\"\n}\n```",
        "**Manager**: The function of Manager is to handle task management by receiving original tasks and breaking them down into sub-tasks.\n\n**attributes**: The attributes of this Class.\n· original_task: A string that stores the original task received by the Manager.  \n· sub_tasks: A list that holds the sub-tasks generated from the breakdown of the original task.  \n· breakdown_prompt: A template used for generating prompts related to breaking down tasks, retrieved from the environment.  \n· reflection_prompt: A variable that is initialized as None, intended for future use related to task reflection.\n\n**Code Description**: The Manager class extends the BaseAgent class, inheriting its functionalities and attributes while adding specific capabilities for task management. Upon initialization, the Manager class sets up its own attributes, including `original_task`, which is initialized as an empty string, and `sub_tasks`, which is initialized as an empty list. The `breakdown_prompt` is obtained from the environment's template system, specifically designed for breaking down tasks.\n\nThe primary method of interest in the Manager class is `receive_task`, which accepts a task as an argument and assigns it to the `original_task` attribute. This method is crucial for setting the context for subsequent operations. \n\nAnother significant method is `breakdown_task`, which is responsible for decomposing the original task into smaller, manageable sub-tasks. This method calls `get_data_for_breakdown` to prepare the necessary data, which includes the original task, and then utilizes the inherited `chat` method from the BaseAgent class. The `chat` method interacts with a language model to generate a response based on the breakdown prompt and the provided data.\n\nThe `get_data_for_breakdown` method constructs a dictionary containing the `original_task`, which is then used in the `chat` method to facilitate the breakdown process. This relationship illustrates how the Manager class leverages the capabilities of the BaseAgent class to perform its task management functions effectively.\n\n**Note**: It is important to ensure that the environment is properly set up with the necessary templates for the breakdown prompt. Additionally, the interaction with the language model through the `chat` method relies on the correct implementation of the `call_llm` function, which must be defined elsewhere in the codebase.\n\n**Output Example**: A possible output from the `breakdown_task` method might look like this:\n```\n{\n    \"response\": \"To complete your task, consider the following steps: Define the scope, Research relevant information, Create an outline, Draft the content.\",\n    \"status\": \"success\",\n    \"message\": \"Task breakdown completed successfully.\"\n}\n```"
      ],
      "code_start_line": 14,
      "code_end_line": 39,
      "params": [],
      "have_return": true,
      "code_content": "class Manager(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.original_task = ''\n        self.sub_tasks = []\n        self.breakdown_prompt = self.env.get_template('manager_break_down.txt')\n        self.reflection_prompt = None\n\n\n    def receive_task(self, task):\n        \"\"\"\n        接收原始任务。\n        \"\"\"\n        self.original_task = task\n\n    def breakdown_task(self):\n        \"\"\"\n        将任务拆解成子任务。\n        \"\"\"\n        data = self.get_data_for_breakdown()\n        return self.chat(data, self.breakdown_prompt)\n\n    def get_data_for_breakdown(self):\n        return {\n            'task': self.original_task\n        }\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "agent_factory/agent.py/BaseAgent"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize a new instance of the Manager class, setting up its attributes with default values and loading configuration settings.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this __init__ method.\n\n**Code Description**: The __init__ method is a constructor for the Manager class, which is responsible for initializing the instance attributes when a new Manager object is created. Upon instantiation, the method performs the following actions:\n\n1. It initializes the `original_task` attribute as an empty string. This attribute is likely intended to hold the main task that the Manager will handle.\n2. The `sub_tasks` attribute is initialized as an empty list, which may be used to store any sub-tasks related to the original task.\n3. The `config` attribute is populated by calling the `read_config` function, which reads a configuration file and returns its contents as a dictionary. This function is defined in the agent_factory/config.py file and is crucial for loading the necessary settings for the Manager's operation.\n4. The `model` attribute is set by retrieving the value associated with the key 'default_model' from the `config` dictionary. If this key does not exist, it defaults to the string \"gpt-4o-mini\".\n5. The `env` attribute is initialized as an instance of the Environment class from the Jinja2 library, using a FileSystemLoader that points to the directory specified by the 'prompt_folder_path' key in the `config` dictionary. This setup allows the Manager to load templates for generating prompts.\n6. The `sys_prompt` attribute is initialized as an empty string, which may be used to store a system prompt for the Manager's operations.\n7. The `breakdown_prompt` attribute is assigned a template loaded from the file 'manager_break_down.txt' using the `env` object. This template is likely used for breaking down tasks or generating specific prompts.\n8. The `reflection_prompt` attribute is initialized as None, indicating that it may be set later in the process.\n9. Finally, the `repeat_turns` attribute is set to 10, which may define the number of iterations or turns the Manager will perform in certain operations.\n\nOverall, the __init__ method establishes the foundational state of the Manager object, ensuring that all necessary attributes are initialized and that configuration settings are loaded for subsequent use. The relationship with the `read_config` function is particularly important, as it provides the configuration data that influences the behavior of the Manager.\n\n**Note**: It is essential to ensure that the configuration file specified in the `read_config` function exists and is correctly formatted in YAML. Any issues with the configuration file may lead to errors during the initialization of the Manager instance.",
        "**__init__**: The function of __init__ is to initialize an instance of the Manager class.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The __init__ function is a constructor method that is automatically called when an instance of the Manager class is created. It begins by invoking the constructor of its parent class using `super().__init__()`, ensuring that any initialization defined in the parent class is also executed. \n\nFollowing this, the function initializes several instance variables:\n- `self.original_task` is set to an empty string, which is likely intended to hold the main task that the Manager will handle.\n- `self.sub_tasks` is initialized as an empty list, suggesting that this Manager instance can manage multiple sub-tasks related to the original task.\n- `self.breakdown_prompt` is assigned a template retrieved from the environment using `self.env.get_template('manager_break_down.txt')`. This indicates that the Manager class is likely designed to generate or manipulate prompts based on a predefined template, which could be used for task breakdowns.\n- `self.reflection_prompt` is initialized to `None`, indicating that it may be set later in the code, possibly to hold a prompt related to reflection or review of tasks.\n\nOverall, this constructor sets up the necessary attributes for the Manager class, preparing it for further operations related to task management.\n\n**Note**: It is important to ensure that the parent class's constructor is called to maintain the integrity of the class hierarchy. Additionally, the template file 'manager_break_down.txt' should be present in the expected directory for the code to function correctly."
      ],
      "code_start_line": 15,
      "code_end_line": 20,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super().__init__()\n        self.original_task = ''\n        self.sub_tasks = []\n        self.breakdown_prompt = self.env.get_template('manager_break_down.txt')\n        self.reflection_prompt = None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "receive_task",
      "md_content": [
        "**receive_task**: The function of receive_task is to accept and store an original task.\n\n**parameters**: The parameters of this Function.\n· task: This parameter represents the original task that is being received and stored by the function.\n\n**Code Description**: The receive_task function is designed to accept a single parameter, which is expected to be the original task. When this function is called, it assigns the value of the task parameter to the instance variable self.original_task. This effectively stores the provided task within the instance of the class, allowing it to be accessed later as needed. The function does not perform any validation or processing on the task; it simply stores it for future use.\n\n**Note**: It is important to ensure that the task parameter passed to this function is of the expected type and format, as the function does not include any error handling or type checking. Users of this function should be aware that the stored task can be overwritten if receive_task is called multiple times with different tasks."
      ],
      "code_start_line": 23,
      "code_end_line": 27,
      "params": [
        "self",
        "task"
      ],
      "have_return": false,
      "code_content": "    def receive_task(self, task):\n        \"\"\"\n        接收原始任务。\n        \"\"\"\n        self.original_task = task\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "breakdown_task",
      "md_content": [
        "**breakdown_task**: The function of breakdown_task is to decompose a larger task into smaller sub-tasks.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The breakdown_task method is a member of the Manager class and is responsible for breaking down an original task into sub-tasks. It begins by creating a dictionary named `data`, which contains the key 'task' associated with the value of `self.original_task`. This dictionary is then used to render a prompt through the `self.breakdown_prompt.render(**data)` method call. The rendered prompt is a structured input that will guide the language model in generating a relevant response.\n\nFollowing the prompt rendering, the method invokes the `call_llm` function, passing in several parameters: `self.model`, `self.sys_prompt`, and the `rendered_prompt`. The `call_llm` function is designed to interact with a language model API, specifically to generate a response based on the provided prompts. It initializes an OpenAI client using the model name and configuration settings, constructs a list of messages that includes both the system prompt and the user prompt, and then sends this data to the language model for processing.\n\nThe response from the `call_llm` function is captured in the variable `response_message`, which is then returned as the output of the breakdown_task method. This indicates that the breakdown_task method not only facilitates the decomposition of tasks but also serves as a bridge to the language model, allowing for intelligent processing and generation of sub-tasks based on the original task.\n\n**Note**: Ensure that the `self.original_task` and `self.breakdown_prompt` are properly initialized before calling this method to avoid runtime errors. Additionally, be aware of the API usage limits and the potential costs associated with calling the language model API.\n\n**Output Example**: A possible return value from the breakdown_task method could be a string such as \"The task can be broken down into the following sub-tasks: 1. Research the topic, 2. Draft an outline, 3. Write the introduction.\"",
        "**breakdown_task**: The function of breakdown_task is to decompose a task into subtasks.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The breakdown_task method is a member of the Manager class and is responsible for breaking down a larger task into smaller, manageable subtasks. This method first calls the get_data_for_breakdown method to retrieve the original task data, which is essential for the decomposition process. The data returned is structured as a dictionary, where the key 'task' holds the value of the original task that needs to be broken down.\n\nOnce the data is obtained, the breakdown_task method proceeds to invoke the chat method from the BaseAgent class. This method is designed to interact with a language model by generating a prompt based on the input data. In this case, the breakdown_task method passes the retrieved data and a predefined breakdown prompt to the chat method. The chat method then renders the prompt using the provided data and communicates with the language model to obtain a response, which is expected to contain the subtasks derived from the original task.\n\nThe relationship between breakdown_task and its callees is crucial for the overall functionality of task decomposition. The breakdown_task method relies on get_data_for_breakdown to ensure it has the correct context for the task at hand, and it utilizes the chat method to facilitate the interaction with the language model, effectively bridging the gap between the original task and its decomposition into subtasks.\n\n**Note**: It is important to ensure that the original task is properly initialized within the Manager class before invoking breakdown_task to avoid any runtime errors. Additionally, the breakdown prompt used in the chat method should be appropriately defined to elicit meaningful responses from the language model.\n\n**Output Example**: A possible return value from the breakdown_task method could be a list of subtasks such as [\"Research climate change effects\", \"Draft an outline for the report\", \"Write the introduction section\"]."
      ],
      "code_start_line": 29,
      "code_end_line": 34,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def breakdown_task(self):\n        \"\"\"\n        将任务拆解成子任务。\n        \"\"\"\n        data = self.get_data_for_breakdown()\n        return self.chat(data, self.breakdown_prompt)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "agent_factory/agent.py/BaseAgent/chat",
        "agent_factory/manager.py/Manager/get_data_for_breakdown"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_data_for_breakdown",
      "md_content": [
        "**get_data_for_breakdown**: The function of get_data_for_breakdown is to retrieve the original task data for further processing.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The get_data_for_breakdown method is a member of the Manager class. Its primary role is to return a dictionary containing the original task associated with the instance of the Manager class. Specifically, it constructs a dictionary with a single key-value pair, where the key is 'task' and the value is obtained from the instance variable self.original_task. \n\nThis method is called by the breakdown_task method within the same Manager class. The breakdown_task method utilizes get_data_for_breakdown to gather the necessary data before proceeding to render a prompt for further processing. By calling get_data_for_breakdown, breakdown_task ensures that it has access to the current state of the original task, which is essential for generating meaningful sub-tasks.\n\nThe output of get_data_for_breakdown is directly integrated into the breakdown_task method, which then uses this data to create a structured prompt for a language model. This relationship highlights the importance of get_data_for_breakdown in the overall functionality of task decomposition, as it provides the foundational data needed for subsequent operations.\n\n**Note**: It is crucial to ensure that self.original_task is properly initialized before invoking this method to prevent any runtime errors.\n\n**Output Example**: A possible return value from the get_data_for_breakdown method could be a dictionary such as {'task': 'Write a report on climate change.'}."
      ],
      "code_start_line": 36,
      "code_end_line": 39,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_data_for_breakdown(self):\n        return {\n            'task': self.original_task\n        }\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/manager.py/Manager/breakdown_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "agent_factory/config.py": [
    {
      "type": "FunctionDef",
      "name": "read_config",
      "md_content": [
        "**read_config**: The function of read_config is to read a configuration file in YAML format and return its contents as a Python dictionary.\n\n**parameters**: The parameters of this Function.\n· file_path: A string representing the path to the configuration file. The default value is '../config/config.yaml'.\n\n**Code Description**: The read_config function opens a specified YAML configuration file in read mode and utilizes the `yaml.safe_load` method to parse the contents of the file into a Python dictionary. This function is designed to facilitate the loading of configuration settings that can be used throughout the application. The default file path points to a configuration file located in a 'config' directory, which is one level up from the current directory.\n\nIn the context of its usage, the read_config function is called within the __init__ method of the Manager class located in the agent_factory/manager.py file. During the initialization of a Manager object, the read_config function is invoked without any arguments, which means it will use the default file path to load the configuration settings. The resulting dictionary is stored in the `self.config` attribute of the Manager instance. This configuration dictionary is then used to retrieve various settings, such as the default model and the path to the prompt folder, which are essential for the operation of the Manager class.\n\n**Note**: It is important to ensure that the specified configuration file exists at the given path and is formatted correctly in YAML. Failure to do so will result in an error when attempting to open or parse the file.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```python\n{\n    'default_model': 'gpt-4o-mini',\n    'prompt_folder_path': '/path/to/prompts',\n    'other_setting': 'value'\n}\n```",
        "**read_config**: The function of read_config is to load configuration settings from a specified YAML file.\n\n**parameters**: The parameters of this Function.\n· file_path: A string representing the path to the YAML configuration file. The default value is '../config/config.yaml'.\n\n**Code Description**: The read_config function is designed to read configuration data from a YAML file. It takes a single parameter, file_path, which specifies the location of the configuration file. If no path is provided, it defaults to '../config/config.yaml'. The function opens the specified file in read mode and uses the yaml.safe_load method to parse the contents of the file into a Python dictionary. This dictionary, which contains the configuration settings, is then returned to the caller.\n\nIn the context of the project, the read_config function is called within the __init__ method of the BaseAgent class located in agent_factory/agent.py. When an instance of BaseAgent is created, the read_config function is invoked to load the configuration settings. The resulting configuration dictionary is stored in the instance variable self.config. Subsequently, specific configuration values are accessed, such as the default model name and the path to the prompt folder, which are used to initialize other components of the agent.\n\n**Note**: It is important to ensure that the specified YAML file exists and is correctly formatted, as any issues with file access or parsing could lead to runtime errors.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```yaml\n{\n  'default_model': 'gpt-4o-mini',\n  'prompt_folder_path': '/path/to/prompts',\n  ...\n}\n```"
      ],
      "code_start_line": 3,
      "code_end_line": 6,
      "params": [
        "file_path"
      ],
      "have_return": true,
      "code_content": "def read_config(file_path='../config/config.yaml'):\n  with open(file_path, 'r') as file:\n    config = yaml.safe_load(file)\n  return config\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/agent.py",
        "agent_factory/agent.py/BaseAgent/__init__",
        "agent_factory/manager.py"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ]
}