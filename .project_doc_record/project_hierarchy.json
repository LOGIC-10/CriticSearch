{
  "openai_deepresearch_pipeline.py": [
    {
      "type": "FunctionDef",
      "name": "tavily_search",
      "md_content": [
        "**tavily_search**: The function of tavily_search is to perform a search query using the Tavily API.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the search query to be executed.\n· api_key: A string representing the API key for authenticating with the Tavily service. It defaults to \"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\".\n\n**Code Description**: The tavily_search function initiates a search request to the Tavily API based on the provided query. It first prints the search query to the console for logging purposes. Then, it creates an instance of the TavilyClient using the provided API key. The function calls the search method of the TavilyClient, passing the query and specifying that raw content should be included in the response. After the search request is made, the function introduces a brief pause of 0.1 seconds to manage the request rate. Finally, it returns the search results extracted from the response, specifically targeting the \"results\" key. If no results are found, it returns an empty list.\n\nThis function is called within the process_single_activity function, which processes individual activities. When an activity contains an action of type \"search\", the tavily_search function is invoked with the content of that action as the query. The results from the tavily_search function are then stored in the action dictionary under the \"result\" key. This integration allows for seamless handling of search actions within the broader activity processing workflow.\n\n**Note**: It is important to ensure that the API key used is valid and has the necessary permissions to perform searches. Additionally, the function includes a slight delay after the search request to avoid overwhelming the API with rapid successive calls.\n\n**Output Example**: A possible return value from the tavily_search function could look like this:\n```json\n{\n    \"results\": [\n        {\n            \"title\": \"Who is Leo Messi?\",\n            \"snippet\": \"Lionel Messi is an Argentine professional footballer...\"\n        },\n        {\n            \"title\": \"Lionel Messi - Wikipedia\",\n            \"snippet\": \"Lionel Messi is widely regarded as one of the greatest football players...\"\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 9,
      "code_end_line": 18,
      "params": [
        "query",
        "api_key"
      ],
      "have_return": true,
      "code_content": "def tavily_search(query, api_key=\"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\"):\n    \"\"\"\n    Perform a Tavily search query\n    # Example usage: results = tavily_search(\"Who is Leo Messi?\")\n    \"\"\"\n    print(f\"Searching: {query}\")\n    tavily_client = TavilyClient(api_key=api_key)\n    response = tavily_client.search(query, include_raw_content=True)\n    time.sleep(0.1)  # 添加0.5秒延时\n    return response.get(\"results\", [])\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "openai_deepresearch_pipeline.py/process_single_activity"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "tavily_extract",
      "md_content": [
        "**tavily_extract**: The function of tavily_extract is to extract content from a specified URL using the Tavily API.\n\n**parameters**: The parameters of this Function.\n· url: A string representing the URL from which content is to be extracted.  \n· api_key: An optional string that serves as the API key for authentication with the Tavily service. The default value is \"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\".\n\n**Code Description**: The tavily_extract function is designed to retrieve content from a given URL by utilizing the Tavily API. Upon invocation, it first prints a message indicating the URL being processed. It then creates an instance of the TavilyClient, passing the provided API key for authentication. The function calls the extract method of the TavilyClient with the specified URL, which returns a response containing the extracted content.\n\nAfter a brief pause of 0.1 seconds (to potentially manage rate limits or server load), the function checks the response for results. If results are present, it retrieves the \"raw_content\" from the first result; if no results are found, it returns an empty string. This function is called within the process_single_activity function, which processes individual activities and determines the type of action to perform. Specifically, when the action type is \"browse\", the tavily_extract function is invoked with the content of the activity, allowing for the extraction of relevant information from the specified URL.\n\n**Note**: It is important to ensure that the provided URL is valid and that the API key has the necessary permissions to access the Tavily service. Additionally, users should be aware of any rate limits imposed by the Tavily API to avoid potential errors during extraction.\n\n**Output Example**: An example of the return value from the tavily_extract function could be a string containing the raw content extracted from the specified URL, such as: \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\" If no content is found, the return value would simply be an empty string: \"\"."
      ],
      "code_start_line": 21,
      "code_end_line": 34,
      "params": [
        "url",
        "api_key"
      ],
      "have_return": true,
      "code_content": "def tavily_extract(url, api_key=\"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\"):\n    \"\"\"\n    Extract content from a URL using Tavily\n    # Example usage: content = tavily_extract(\"https://en.wikipedia.org/wiki/Artificial_intelligence\")\n    \"\"\"\n    print(f\"Extracting: {url}\")\n    tavily_client = TavilyClient(api_key=api_key)\n    response = tavily_client.extract(url)\n    time.sleep(0.1)  # 添加延时\n    return (\n        response.get(\"results\", [])[0].get(\"raw_content\", \"\")\n        if response.get(\"results\")\n        else \"\"\n    )\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "openai_deepresearch_pipeline.py/process_single_activity"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "read_json_file",
      "md_content": [
        "**read_json_file**: The function of read_json_file is to read a JSON file from a given file path and return its content as a Python object.\n\n**parameters**: \n· file_path: The path to the JSON file to be read. Default is \"/Users/logic/Documents/CodeSpace/CriticSearch/Deep Research detection_0214.json\".\n\n**Code Description**:  \nThe `read_json_file` function is designed to read a JSON file from a specified location and load its content into a Python object. The function takes a single parameter `file_path`, which defines the location of the JSON file. If no path is provided, it defaults to a specific file path.\n\nThe function operates as follows:\n1. It first attempts to open the file at the provided `file_path` in read mode with UTF-8 encoding using the `open()` function.\n2. If the file is successfully opened, the function proceeds to parse the JSON content using the `json.load()` method. This method converts the JSON data into a Python dictionary or list, depending on the structure of the JSON file.\n3. If no errors occur during this process, the parsed data is returned to the caller.\n\nThe function handles errors gracefully using multiple `except` blocks:\n- If the specified file cannot be found at the provided path, a `FileNotFoundError` is caught, and a message is printed to the console indicating the missing file.\n- If the file is found but the content is not in a valid JSON format, a `json.JSONDecodeError` is raised, and an error message is displayed.\n- Any other unexpected exceptions during the process are captured by a general `Exception` handler, and an error message is printed.\n\nIn each error case, the function returns `None`, signaling that the file could not be successfully read or parsed.\n\n**Note**: \n- Ensure the file path provided is correct and the file exists at the specified location to avoid a `FileNotFoundError`.\n- The function expects the content of the file to be in valid JSON format. If the file contains malformed JSON, a `JSONDecodeError` will be raised.\n- The function does not handle cases where the file is empty or contains non-JSON data other than the expected format.\n  \n**Output Example**:\nIf the file located at `file_path` contains a valid JSON object such as:\n```json\n{\n  \"name\": \"John Doe\",\n  \"age\": 30,\n  \"city\": \"New York\"\n}\n```\nThe function will return the following Python dictionary:\n```python\n{\n  \"name\": \"John Doe\",\n  \"age\": 30,\n  \"city\": \"New York\"\n}\n```\nIf an error occurs, such as the file not being found or containing invalid JSON, the function will return `None`."
      ],
      "code_start_line": 37,
      "code_end_line": 52,
      "params": [
        "file_path"
      ],
      "have_return": true,
      "code_content": "def read_json_file(\n    file_path=\"/Users/logic/Documents/CodeSpace/CriticSearch/Deep Research detection_0214.json\",\n):\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            data = json.load(file)\n            return data\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}\")\n        return None\n    except json.JSONDecodeError:\n        print(f\"Error: Invalid JSON format in {file_path}\")\n        return None\n    except Exception as e:\n        print(f\"Error reading file: {str(e)}\")\n        return None\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_activity",
      "md_content": [
        "**process_activity**: The function of process_activity is to analyze a given text for specific actions related to searching and browsing, extracting relevant information accordingly.\n\n**parameters**: The parameters of this Function.\n· text: A string input that contains the activity description, which may include search and browse actions.\n\n**Code Description**: The process_activity function utilizes regular expressions to identify and extract specific actions from the input text. It defines two patterns: one for detecting search actions that follow the phrase \"Searched for\" and another for detecting browse actions that follow \"Read\" or \"Read more from\". \n\nThe function initializes an empty list called actions to store the identified actions and a variable named thinking to hold the remaining text after processing. It then iterates through the input text to find matches for the search pattern. For each match found, it appends a dictionary containing the action type (\"search\") and the associated content to the actions list, while updating the thinking variable to exclude the processed portion of the text.\n\nSimilarly, the function searches for browse actions using the defined browse pattern. If a valid URL is found (ensuring it does not contain certain characters like brackets), it appends a corresponding dictionary to the actions list and updates the thinking variable accordingly.\n\nFinally, the function returns a dictionary containing the remaining text (thinking) and the list of actions. If no actions were found, it returns None for the action key.\n\nThis function is called by the process_single_activity function, which serves as a helper to process a single activity. The process_single_activity function takes the output of process_activity and further processes each action by performing searches or extracting information based on the action type. This relationship indicates that process_activity is a foundational component that prepares the data for more specific operations in the context of processing activities.\n\n**Note**: It is important to ensure that the input text is formatted correctly to maximize the effectiveness of the regular expressions used in this function. The function assumes that the text follows a specific structure to identify actions accurately.\n\n**Output Example**: A possible return value of the function could look like this:\n{\n    \"thinking\": \"The user was interested in the following topics.\",\n    \"action\": [\n        {\"type\": \"search\", \"content\": \"machine learning\"},\n        {\"type\": \"browse\", \"content\": \"https://example.com/resource\"}\n    ]\n}"
      ],
      "code_start_line": 56,
      "code_end_line": 76,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def process_activity(text):\n    # 定义正则：匹配换行后跟\"Searched for\"和\"Read\"或\"Read more from\"\n    pattern_search = re.compile(r\"\\nSearched for\\s*(.+)\")\n    pattern_browse = re.compile(r\"\\nRead(?: more from)?\\s*(https?://[^\\s\\[\\]()]+)\")\n\n    actions = []\n    thinking = text\n\n    # 查找所有search匹配\n    for match in pattern_search.finditer(text):\n        actions.append({\"type\": \"search\", \"content\": match.group(1).strip()})\n        thinking = text[: match.start()].strip()\n\n    # 查找所有browse匹配\n    for match in pattern_browse.finditer(text):\n        url = match.group(1).strip()\n        if \"[\" not in url and \"]\" not in url and \"(\" not in url and \")\" not in url:\n            actions.append({\"type\": \"browse\", \"content\": url})\n            thinking = text[: match.start()].strip()\n\n    return {\"thinking\": thinking.strip(), \"action\": actions if actions else None}\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "openai_deepresearch_pipeline.py/process_single_activity"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_single_activity",
      "md_content": [
        "**process_single_activity**: The function of process_single_activity is to process a single activity by analyzing its actions and executing corresponding search or browse operations.\n\n**parameters**: The parameters of this Function.\n· activity: A dictionary representing a single activity that may contain actions to be processed.\n\n**Code Description**: The process_single_activity function serves as a helper function designed to handle individual activities by processing their associated actions. It begins by calling the process_activity function, which analyzes the input activity and extracts any actions related to searching or browsing. The output of process_activity is a dictionary that includes both the remaining text (referred to as \"thinking\") and a list of actions.\n\nOnce the actions are identified, the function iterates through each action in the list. For actions of type \"search\", it invokes the tavily_search function, passing the action's content as the search query. This function performs a search using the Tavily API and returns the results, which are then stored in the action dictionary under the \"result\" key. Similarly, for actions of type \"browse\", the tavily_extract function is called with the action's content as the URL. This function extracts content from the specified URL and also stores the result in the action dictionary.\n\nThe processed activity, now enriched with the results of the actions, is returned at the end of the function. This integration allows for a seamless workflow where individual activities can be processed to yield actionable insights based on the specified actions.\n\nThe process_single_activity function is called within the process_activities function, which handles a list of activities. It utilizes a thread pool to process each activity concurrently, thereby improving efficiency. The processed results are collected and returned as part of the overall output.\n\n**Note**: It is essential to ensure that the input activity is correctly formatted and contains valid action types. Additionally, the API key used in tavily_search and tavily_extract must be valid and have the necessary permissions to perform the respective operations.\n\n**Output Example**: A possible return value from the process_single_activity function could look like this:\n```json\n{\n    \"thinking\": \"The user was interested in the following topics.\",\n    \"action\": [\n        {\"type\": \"search\", \"content\": \"machine learning\", \"result\": [{\"title\": \"Machine Learning Overview\", \"snippet\": \"Machine learning is a subset of artificial intelligence...\"}]},\n        {\"type\": \"browse\", \"content\": \"https://example.com/resource\", \"result\": \"Content extracted from the specified URL.\"}\n    ]\n}\n```"
      ],
      "code_start_line": 79,
      "code_end_line": 88,
      "params": [
        "activity"
      ],
      "have_return": true,
      "code_content": "def process_single_activity(activity):\n    \"\"\"Helper function to process a single activity\"\"\"\n    processed = process_activity(activity)\n    if processed.get(\"action\"):\n        for action in processed[\"action\"]:\n            if action[\"type\"] == \"search\":\n                action[\"result\"] = tavily_search(action[\"content\"])\n            elif action[\"type\"] == \"browse\":\n                action[\"result\"] = tavily_extract(action[\"content\"])\n    return processed\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "openai_deepresearch_pipeline.py/process_activities"
      ],
      "reference_who": [
        "openai_deepresearch_pipeline.py/tavily_search",
        "openai_deepresearch_pipeline.py/tavily_extract",
        "openai_deepresearch_pipeline.py/process_activity"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "process_activities",
      "md_content": [
        "**process_activities**: The function of process_activities is to process a list of activities, handling both regular activities and a special \"Deep Research\" item, while utilizing concurrent processing for efficiency.\n\n**parameters**: The parameters of this Function.\n· activities: A list of activities, where each activity can be a dictionary containing either \"Activity\" or \"Deep Research\" keys.\n\n**Code Description**: The process_activities function begins by checking if the input, activities, is a list. If it is not, the function returns the input as is. This ensures that the function can handle unexpected input types gracefully.\n\nThe function initializes an empty list called final_result to store the processed activities and a variable deep_research set to None to temporarily hold any \"Deep Research\" item found during processing.\n\nThe function then iterates over each item in the activities list. If an item is a dictionary and contains the key \"Deep Research,\" it is stored in the deep_research variable, and the iteration continues to the next item without further processing of this item. This allows the function to prioritize the handling of \"Deep Research\" items.\n\nFor items that are dictionaries containing the key \"Activity,\" the function prepares to process the activities listed under this key. It creates an empty list called processed_activities to hold the results of processing each individual activity. The function employs a ThreadPoolExecutor with a maximum of 20 worker threads to process the activities concurrently. This parallel processing is achieved by mapping the process_single_activity function to each activity in the item[\"Activity\"] list. The results are collected into the processed_activities list.\n\nAfter processing, the function updates the original item by replacing its \"Activity\" key with the processed_activities list. The modified item is then appended to the final_result list.\n\nOnce all items have been processed, if a deep_research item was found, it is appended to the final_result list at the end. This ensures that the \"Deep Research\" item is included in the output while maintaining the order of the other activities.\n\nFinally, the function returns the final_result list, which contains all processed activities along with any \"Deep Research\" item.\n\nThe process_single_activity function, which is called within process_activities, is responsible for handling individual activities and their associated actions. It processes each activity to yield actionable insights based on the specified actions, thereby enhancing the overall functionality of process_activities.\n\n**Note**: It is important to ensure that the input activities are formatted correctly and contain valid keys. The function assumes that the \"Activity\" key will always contain a list of activities to be processed.\n\n**Output Example**: A possible return value from the process_activities function could look like this:\n```json\n[\n    {\n        \"Activity\": [\n            {\n                \"thinking\": \"The user was interested in machine learning.\",\n                \"action\": [\n                    {\"type\": \"search\", \"content\": \"machine learning\", \"result\": [{\"title\": \"Machine Learning Overview\", \"snippet\": \"Machine learning is a subset of artificial intelligence...\"}]}\n                ]\n            }\n        ]\n    },\n    {\n        \"Deep Research\": {\n            \"topic\": \"Advanced AI Techniques\",\n            \"details\": \"In-depth analysis of the latest AI methodologies.\"\n        }\n    }\n]\n```"
      ],
      "code_start_line": 92,
      "code_end_line": 122,
      "params": [
        "activities"
      ],
      "have_return": true,
      "code_content": "def process_activities(activities):\n    if not isinstance(activities, list):\n        return activities\n\n    final_result = []\n    deep_research = None\n\n    for item in activities:\n        # 如果是Deep Research项,先保存起来\n        if isinstance(item, dict) and \"Deep Research\" in item:\n            deep_research = item\n            continue\n\n        # 处理其他项(包括Activity)\n        if isinstance(item, dict) and \"Activity\" in item:\n            processed_activities = []\n\n            # 使用线程池并行处理activities\n            with ThreadPoolExecutor(max_workers=20) as executor:\n                processed_activities = list(\n                    executor.map(process_single_activity, item[\"Activity\"])\n                )\n\n            item[\"Activity\"] = processed_activities\n        final_result.append(item)\n\n    # 最后添加Deep Research\n    if deep_research:\n        final_result.append(deep_research)\n\n    return final_result\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "openai_deepresearch_pipeline.py/process_single_activity"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "eval.py": [],
  "tests/test_colorize_message.py": [],
  "tests/test_conversation_format_saving.py": [
    {
      "type": "ClassDef",
      "name": "DummyToolCall",
      "md_content": [
        "**DummyToolCall**: The function of DummyToolCall is to represent a simulated tool call in a chat completion context.\n\n**attributes**: The attributes of this Class.\n· id: A unique identifier for the tool call instance.  \n· name: The name of the function being called.  \n· args: The arguments passed to the function, serialized in JSON format.  \n\n**Code Description**: The DummyToolCall class is a specialized implementation that inherits from the ChatCompletionMessageToolCall class. It is designed to encapsulate the details of a tool call within a chat completion framework. The constructor of DummyToolCall takes three parameters: `id`, `name`, and `args`. The `id` parameter serves as a unique identifier for the tool call, while the `name` parameter specifies the name of the function being invoked. The `args` parameter is expected to be a dictionary containing the arguments for the function, which is then serialized into a JSON string using `json.dumps()`.\n\nWhen an instance of DummyToolCall is created, it calls the constructor of its superclass, ChatCompletionMessageToolCall, passing along the `id`, a fixed `type` of \"function\", and a Function object that is instantiated with the provided `name` and serialized `args`. This structure allows DummyToolCall to be used effectively in scenarios where a tool call needs to be simulated, particularly in testing environments.\n\nThe DummyToolCall class is utilized in the fake_call_llm function, which simulates a call to a language model (LLM). When the `tools` parameter is provided, the function creates an instance of DummyToolCall to represent a simulated search tool call with specific arguments. This allows for testing the behavior of the system when interacting with tools without requiring actual tool execution. If no tools are specified, the function returns a simple chat response, demonstrating the versatility of DummyToolCall in both tool invocation and standard chat interactions.\n\n**Note**: It is important to ensure that the arguments passed to DummyToolCall are properly structured as a dictionary, as they will be serialized into JSON format. Additionally, the id and name parameters should be unique and descriptive to maintain clarity in tool call representations."
      ],
      "code_start_line": 7,
      "code_end_line": 13,
      "params": [],
      "have_return": false,
      "code_content": "class DummyToolCall(ChatCompletionMessageToolCall):\n    def __init__(self, id, name, args):\n        super().__init__(\n            id=id,\n            type=\"function\",\n            function=Function(name=name, arguments=json.dumps(args))\n        )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "tests/test_conversation_format_saving.py/test_conversation_roundtrip/fake_call_llm"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the class with specific attributes.\n\n**parameters**: The parameters of this Function.\n· id: A unique identifier for the instance being created.  \n· name: The name of the function that is being represented.  \n· args: A list of arguments that the function accepts, which will be converted to a JSON string.\n\n**Code Description**: The __init__ function is a constructor that initializes an instance of a class. It takes three parameters: id, name, and args. The id parameter is used to uniquely identify the instance, while the name parameter specifies the name of the function that this instance represents. The args parameter is expected to be a list of arguments that the function can accept. Inside the constructor, the super() function is called to invoke the constructor of the parent class, passing a dictionary that includes the id, a fixed type of \"function\", and a function object created using the Function class. The Function object is initialized with the name provided and the arguments converted to a JSON string using json.dumps. This ensures that the function's arguments are stored in a format that can be easily serialized and deserialized.\n\n**Note**: It is important to ensure that the args parameter is a valid list, as it will be converted to a JSON string. Additionally, the name parameter should be a valid function name to avoid any issues when the function is referenced later in the code."
      ],
      "code_start_line": 8,
      "code_end_line": 13,
      "params": [
        "self",
        "id",
        "name",
        "args"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, id, name, args):\n        super().__init__(\n            id=id,\n            type=\"function\",\n            function=Function(name=name, arguments=json.dumps(args))\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "clear_history_and_stub",
      "md_content": [
        "**clear_history_and_stub**: The function of clear_history_and_stub is to clear the conversation history and stub the template loading and rendering methods to prevent file-related errors during testing.\n\n**parameters**: The parameters of this Function.\n· monkeypatch: An instance of the monkeypatching utility, which allows for modifying or replacing attributes and methods during testing.\n\n**Code Description**: The clear_history_and_stub function is designed to facilitate testing by ensuring a clean state for the conversation history and preventing errors related to template loading and rendering. It begins by invoking the clear_history method of the BaseAgent's conversation_manager, which effectively removes all previous conversation data. This is crucial for tests that require a fresh start without any residual context from prior interactions.\n\nFollowing the clearing of history, the function utilizes the monkeypatch utility to override the load_template and render_template methods of the BaseAgent class. By replacing these methods with lambda functions that return empty strings, the function ensures that any attempts to load or render templates during the test will not result in file-not-found errors. This stubbing is particularly useful in unit tests where the actual template files may not be available or necessary for the test's focus.\n\nThe function employs the yield statement, which allows it to be used as a context manager. After the test code runs, the function again calls clear_history to ensure that any changes made during the test do not affect subsequent tests. This reinforces the importance of maintaining isolation between tests, which is a fundamental principle in unit testing.\n\nThe relationship between clear_history_and_stub and its callees, particularly the clear_history method of the ConversationManager class, emphasizes its role in managing the conversation state during testing. By ensuring that the conversation history is cleared before and after tests, the function helps maintain a consistent and reliable testing environment.\n\n**Note**: It is important to ensure that the monkeypatching is correctly implemented, as improper usage may lead to unexpected behavior in tests. Additionally, developers should be aware that invoking clear_history will permanently delete all existing conversation data, so it should be used judiciously within the testing context."
      ],
      "code_start_line": 16,
      "code_end_line": 27,
      "params": [
        "monkeypatch"
      ],
      "have_return": false,
      "code_content": "def clear_history_and_stub(monkeypatch):\n    # 清空历史\n    BaseAgent.conversation_manager.clear_history()\n    # Stub 模板加载与渲染，避免文件不存在\n    monkeypatch.setattr(\n        BaseAgent, \"load_template\", lambda self, filename, root_folder=None: \"\"\n    )\n    monkeypatch.setattr(\n        BaseAgent, \"render_template\", lambda self, template_str, data: \"\"\n    )\n    yield\n    BaseAgent.conversation_manager.clear_history()\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/models.py/ConversationManager/clear_history"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_conversation_roundtrip",
      "md_content": [
        "**test_conversation_roundtrip**: The function of test_conversation_roundtrip is to validate the roundtrip functionality of the conversation management system by simulating user interactions and tool calls, and ensuring that the conversation history is accurately recorded and retrievable.\n\n**parameters**: The parameters of this Function.\n· monkeypatch: A fixture provided by pytest that allows for modifying or replacing parts of the code during testing.\n· tmp_path: A temporary directory provided by pytest for creating temporary files and directories during the test.\n\n**Code Description**: The test_conversation_roundtrip function is a unit test designed to verify the integrity of the conversation management system within the BaseAgent class. It begins by initializing an instance of BaseAgent, which is responsible for managing user interactions, tool calls, and conversation history.\n\nThe test proceeds with the following steps:\n1. A user message (\"Hello, world!\") is appended to the conversation history using the append_to_history method of the ConversationManager class. This establishes the initial context of the conversation.\n2. A fake tool call to a language model (LLM) is simulated using the monkeypatch fixture. The fake_call_llm function is defined to return either a simulated tool call or a standard response based on the presence of tools. This allows the test to mimic the behavior of external tools without making actual calls.\n3. The agent's search_and_browse method is invoked with an unused prompt, which triggers the simulated tool call. The results of this tool call are then appended to the conversation history using the append_tool_call_result_to_history method, capturing the interaction with the tool.\n4. A subsequent chat interaction is simulated using the chat_with_template method, which generates a response based on a predefined template and data.\n5. The conversation history is serialized using the model_dump method of the ConversationManager, allowing for inspection of the recorded interactions.\n6. Assertions are made to ensure that the conversation history contains the expected elements, including the initial user message, the tool call, the tool result, and the final response from the agent.\n\nThis function serves as a critical test case to ensure that the conversation management system accurately logs and retrieves interactions, maintaining the integrity of the conversation flow. It interacts with various methods of the BaseAgent class, including search_and_browse and chat_with_template, and relies on the ConversationManager for managing the history of interactions.\n\n**Note**: It is important to ensure that the monkeypatching is correctly set up to simulate tool calls without invoking real external dependencies. The test should be run in an isolated environment to prevent side effects on the actual conversation management system.\n\n**Output Example**: A possible appearance of the code's return value when executing the test might look like this:\n```json\n{\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": \"Hello, world!\"},\n    {\"from\": \"function_call\", \"value\": {\"name\": \"search\", \"arguments\": {\"query\": [\"foo\"]}}},\n    {\"from\": \"observation\", \"value\": \"<<fake search result>>\"},\n    {\"from\": \"gpt\", \"value\": \"OK, got it.\"}\n  ],\n  \"tools\": [\"search\"]\n}\n```"
      ],
      "code_start_line": 29,
      "code_end_line": 76,
      "params": [
        "monkeypatch",
        "tmp_path"
      ],
      "have_return": true,
      "code_content": "def test_conversation_roundtrip(monkeypatch, tmp_path):\n    agent = BaseAgent()\n\n    # 1. 用户提问\n    BaseAgent.conversation_manager.append_to_history(\n        role=\"user\", content=\"Hello, world!\"\n    )\n\n    # 2. 模拟工具调用\n    def fake_call_llm(model, usr_prompt, config, tools=None):\n        if tools is not None:\n            # 模拟一次 search 工具调用\n            return SimpleNamespace(\n                tool_calls=[DummyToolCall(\"tc1\", \"search\", {\"query\": [\"foo\"]})],\n                content=None\n            )\n        # 模拟普通聊天回复\n        return SimpleNamespace(tool_calls=None, content=\"OK, got it.\")\n    monkeypatch.setattr(\"criticsearch.base_agent.call_llm\", fake_call_llm)\n\n    # 3. agent 调用工具\n    res = agent.search_and_browse(\"unused prompt\")\n    # 4. 模拟工具执行结果保存\n    BaseAgent.conversation_manager.append_tool_call_result_to_history(\n        tool_call_id=\"tc1\", name=\"search\", content=\"<<fake search result>>\"\n    )\n\n    # 5. 再来一次普通回复\n    reply = agent.chat_with_template(\n        template_name=\"direct_response.txt\",\n        template_data={\"task\": \"dummy\"},\n    )\n\n    # 6. 序列化并检查\n    result = BaseAgent.conversation_manager.model_dump(context={\"sharegpt\": True})\n    assert \"conversations\" in result and \"tools\" in result\n    convs = result[\"conversations\"]\n\n    senders = [c[\"from\"] for c in convs]\n    # 首条应为用户 (human)\n    assert senders[0] == \"human\"\n    # 工具调用以 function_call 出现，并带有 search\n    assert any(c[\"from\"] == \"function_call\" and \"search\" in c[\"value\"] for c in convs)\n    # 工具结果以 observation 出现，包含 fake search result\n    assert any(c[\"from\"] == \"observation\" and \"fake search result\" in c[\"value\"] for c in convs)\n    # 最后一条正常回复以 gpt 出现\n    assert convs[-1][\"from\"] == \"gpt\"\n    assert \"OK, got it.\" in convs[-1][\"value\"]\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/models.py/ConversationManager/append_to_history",
        "src/criticsearch/models.py/ConversationManager/append_tool_call_result_to_history"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "fake_call_llm",
      "md_content": [
        "**fake_call_llm**: The function of fake_call_llm is to simulate a call to a language model (LLM) and return either a simulated tool call or a standard chat response based on the presence of tools.\n\n**parameters**: The parameters of this Function.\n· model: This parameter represents the model being used for the language model call. It is expected to be an object that defines the behavior of the LLM, although it is not directly utilized within the function's logic.  \n· usr_prompt: This parameter is intended to capture the user's prompt or input for the LLM, but it is not utilized in the current implementation of the function.  \n· config: This parameter is used to pass configuration settings that may influence the behavior of the LLM or the simulation, though it is not explicitly referenced in the function.  \n· tools: This optional parameter can be provided to indicate whether a tool call should be simulated. If set to None, the function will return a standard chat response.\n\n**Code Description**: The fake_call_llm function is designed to simulate interactions with a language model in a controlled manner. It takes four parameters: model, usr_prompt, config, and an optional tools parameter. The function checks if the tools parameter is not None. If tools are provided, it simulates a tool call by creating an instance of the DummyToolCall class, which represents a simulated search tool invocation. This instance is initialized with a unique identifier (\"tc1\"), the name of the tool (\"search\"), and a dictionary containing a query argument. The function then returns a SimpleNamespace object that encapsulates the simulated tool calls and sets the content to None.\n\nIf the tools parameter is None, the function bypasses the tool simulation and instead returns a SimpleNamespace object with tool_calls set to None and content set to a simple string response, \"OK, got it.\" This design allows for flexible testing scenarios, enabling developers to evaluate how the system behaves with or without tool interactions.\n\nThe relationship with the DummyToolCall class is significant, as it provides the mechanism for simulating tool calls within the chat completion context. The DummyToolCall class is instantiated when tools are specified, allowing for the representation of a tool call without executing any actual functionality. This is particularly useful in testing environments where the behavior of the system needs to be validated without relying on external tool execution.\n\n**Note**: When using the fake_call_llm function, it is important to ensure that the tools parameter is structured correctly if provided. The function is designed to handle both scenarios—simulating tool calls and returning standard chat responses—making it versatile for various testing needs.\n\n**Output Example**: A possible appearance of the code's return value when tools are provided might look like this:\n```\nSimpleNamespace(tool_calls=[DummyToolCall(\"tc1\", \"search\", {\"query\": [\"foo\"]})], content=None)\n```\nAnd when no tools are provided, the return value would be:\n```\nSimpleNamespace(tool_calls=None, content=\"OK, got it.\")\n```"
      ],
      "code_start_line": 38,
      "code_end_line": 46,
      "params": [
        "model",
        "usr_prompt",
        "config",
        "tools"
      ],
      "have_return": true,
      "code_content": "    def fake_call_llm(model, usr_prompt, config, tools=None):\n        if tools is not None:\n            # 模拟一次 search 工具调用\n            return SimpleNamespace(\n                tool_calls=[DummyToolCall(\"tc1\", \"search\", {\"query\": [\"foo\"]})],\n                content=None\n            )\n        # 模拟普通聊天回复\n        return SimpleNamespace(tool_calls=None, content=\"OK, got it.\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "tests/test_conversation_format_saving.py/DummyToolCall"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "tests/test_tool_result_overwrite.py": [],
  "tests/test_chat_integration.py": [
    {
      "type": "FunctionDef",
      "name": "test_real_model_response",
      "md_content": [
        "**test_real_model_response**: The function of test_real_model_response is to validate the response generated by the BaseAgent when a specific question is posed.\n\n**parameters**: The parameters of this Function.\n· agent: An instance of the BaseAgent class that is responsible for handling the chat interaction.\n· question: A string containing the user's query that will be sent to the agent for processing.\n· response: The output generated by the agent in response to the user's question.\n\n**Code Description**: The test_real_model_response function is designed to test the functionality of the BaseAgent class by simulating a user interaction. It begins by creating an instance of the BaseAgent, which serves as the foundational class for intelligent agents in the project. The function then defines a specific question related to a historical date, which is intended to elicit a structured response in JSON format, including an answer and supporting facts.\n\nThe agent's chat method is invoked with the defined question, which processes the input and generates a response. The response is then printed to the console for visibility. Following this, the function asserts that the response is a non-empty string, ensuring that the agent has successfully generated a meaningful output. This assertion is crucial for validating the agent's performance and ensuring that it meets the expected standards for interaction.\n\nThe relationship with its callees is significant, as the test relies on the BaseAgent's chat method to function correctly. The chat method is responsible for handling the user prompt and generating a response, making it a critical component in the testing process. The test_real_model_response function serves as a unit test, ensuring that the BaseAgent can handle specific queries and return appropriate responses, which is essential for maintaining the integrity of the intelligent agent's capabilities.\n\n**Note**: It is important to ensure that the question posed to the agent is well-formed and relevant to the expected output format. Additionally, the assertion should be carefully considered to accurately reflect the desired characteristics of the response.\n\n**Output Example**: A possible appearance of the code's return value when executing the chat interaction might look like this:\n```json\n{\n  \"answer\": \"In 2008, the global financial crisis began, impacting economies worldwide.\",\n  \"support\": [\n    {\n      \"url\": \"https://example.com/facts/2008-crisis\",\n      \"fact\": \"The crisis was triggered by the collapse of the housing bubble.\"\n    }\n  ]\n}\n```"
      ],
      "code_start_line": 10,
      "code_end_line": 17,
      "params": [],
      "have_return": true,
      "code_content": "def test_real_model_response():\n    agent = BaseAgent()\n    question = 'What happens in 2008-04-17?? return in json with urls and supporting facts like this: {\"answer\":\"XXX\",\"support\":[{\"url\":\"xxx\",\"fact\":\"XXX}]}'\n    response = agent.chat(question)\n    print(\"Integration model response:\", response)\n    # 确保返回一个非空字符串\n    assert isinstance(response, str) and response.strip(), \"Expected a non-empty string\"\n    print(\"Integration model response:\", response)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "tests/test_chat_functionality.py": [
    {
      "type": "ClassDef",
      "name": "DummyMessage",
      "md_content": [
        "**DummyMessage**: The function of DummyMessage is to encapsulate a message content for use in chat functionality.\n\n**attributes**: The attributes of this Class.\n· content: A string that holds the message content.\n\n**Code Description**: The DummyMessage class is a simple data structure designed to store a message's content. It has an initializer method (__init__) that takes a single parameter, 'content', which is expected to be a string. This parameter is assigned to the instance attribute 'self.content', allowing the message content to be accessed later.\n\nThe DummyMessage class is utilized in the context of the function fake_call_llm, which simulates a call to a language model (LLM). In this function, a DummyMessage object is created with the content \"fake response\". This indicates that the DummyMessage class serves as a placeholder for responses that would typically be generated by a language model, facilitating testing and development without requiring actual model calls.\n\nThe relationship between DummyMessage and its caller, fake_call_llm, is that DummyMessage provides a structured way to handle and return a response in a controlled testing environment. By using DummyMessage, developers can ensure that the interface and expected behavior of the chat functionality can be tested without relying on external dependencies.\n\n**Note**: When using the DummyMessage class, ensure that the content provided is a string, as this is the expected data type for the 'content' attribute. This class is primarily intended for testing purposes and should be used in scenarios where a mock response is needed."
      ],
      "code_start_line": 6,
      "code_end_line": 8,
      "params": [],
      "have_return": false,
      "code_content": "class DummyMessage:\n    def __init__(self, content):\n        self.content = content\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "tests/test_chat_functionality.py/patch_call_llm/fake_call_llm"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the attributes of the class with the provided input values.\n\n**parameters**: The parameters of this Function.\n· content: This parameter represents the data that will be assigned to the instance attribute `self.content`.\n\n**Code Description**: \nThe `__init__` function is the constructor of the class. It is automatically called when an object of the class is instantiated. The function takes one parameter, `content`, and assigns its value to the instance variable `self.content`. This allows each object of the class to store a different value for `content` based on the argument passed during instantiation.\n\nThe `self.content` attribute is used to store the value that is passed when creating an object. This attribute can later be accessed and modified by other methods within the class or externally, depending on the class definition.\n\n**Note**: \n- This function does not return any value explicitly as it is a constructor, and its sole purpose is to initialize the object's attributes.\n- It is important to ensure that the `content` parameter passed during object instantiation is of the appropriate type or format expected by the rest of the class functionality."
      ],
      "code_start_line": 7,
      "code_end_line": 8,
      "params": [
        "self",
        "content"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, content):\n        self.content = content\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "patch_call_llm",
      "md_content": [
        "**patch_call_llm**: The function of patch_call_llm is to patch the `call_llm` function references in the `llm_service` and `base_agent` modules, redirecting them to a fake implementation for testing purposes.\n\n**parameters**: The parameters of this function.\n· parameter1: `monkeypatch` (pytest fixture) - This is used to modify or patch parts of the code during testing.\n\n**Code Description**: \nThe `patch_call_llm` function is primarily used in the context of unit testing to mock the behavior of the `call_llm` function in both the `llm_service` and `base_agent` modules. \n\n- It defines a nested function `fake_call_llm`, which simulates the behavior of `call_llm`. The `fake_call_llm` function takes several parameters (`model`, `config`, `usr_prompt`, `tools`, and `messages`), but it doesn't perform any real operations with them. Instead, it simply returns a `DummyMessage` object with a predefined string \"fake response\" as its content. This allows the test to focus on behavior that relies on the `call_llm` function without making actual calls to a language model.\n  \n- The `monkeypatch.setattr` method is then used to override the `call_llm` function in both the `criticsearch.llm_service` and `criticsearch.base_agent` modules. This means that during the test, any call to `call_llm` within these modules will invoke the `fake_call_llm` function, thus preventing real API calls and ensuring that the test environment remains controlled and predictable.\n\n**Note**: \n- This function is typically used in testing scenarios, particularly when interacting with external services such as a language model API. By using `monkeypatch` to substitute a fake version of `call_llm`, tests can simulate various responses and conditions without requiring actual service calls.\n- The `DummyMessage` class must be available in the testing environment, as it is used to return the mocked response. \n\n**Output Example**: \nWhen `patch_call_llm` is invoked, it modifies the `call_llm` function in both the `llm_service` and `base_agent` modules. If a test later calls `call_llm`, it will receive a `DummyMessage` object like the following:\n\n```python\nDummyMessage(content=\"fake response\")\n```"
      ],
      "code_start_line": 11,
      "code_end_line": 17,
      "params": [
        "monkeypatch"
      ],
      "have_return": true,
      "code_content": "def patch_call_llm(monkeypatch):\n    # fake call_llm returns DummyMessage with known content\n    def fake_call_llm(model, config, usr_prompt, tools=None, messages=None):\n        return DummyMessage(content=\"fake response\")\n    # patch both llm_service and base_agent references\n    monkeypatch.setattr(\"criticsearch.llm_service.call_llm\", fake_call_llm)\n    monkeypatch.setattr(\"criticsearch.base_agent.call_llm\", fake_call_llm)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "fake_call_llm",
      "md_content": [
        "**fake_call_llm**: The function of fake_call_llm is to simulate a call to a language model (LLM) and return a mock response encapsulated in a DummyMessage object.\n\n**parameters**: The parameters of this Function.\n· model: Represents the model that would normally process the user input in a real LLM call. In this case, it is not used within the function but is expected as part of the standard function signature.\n· config: Configuration settings that would typically be used to control the behavior of the LLM or the function. This parameter is also not utilized in the function itself but is expected for compatibility or future extensibility.\n· usr_prompt: The user's input or query that would typically be processed by the LLM. While it is passed to the function, it is not used in the current implementation of fake_call_llm.\n· tools (optional): A set of tools or utilities that could be invoked by the LLM. This parameter is also passed to the function but does not influence the function's behavior.\n· messages (optional): A list of messages or dialogue history that could be passed to the LLM. Similar to the other parameters, it is provided to the function but has no effect in the current implementation.\n\n**Code Description**: \nThe function `fake_call_llm` serves as a mock implementation to simulate the behavior of calling a language model (LLM) in a controlled environment, such as for testing or development purposes. Rather than processing the user prompt with an actual LLM model, it directly returns a predefined response encapsulated within a `DummyMessage` object. The content of this response is fixed as the string \"fake response\".\n\nThis function accepts several parameters—model, config, usr_prompt, tools, and messages—which are commonly expected in real LLM calls but are not used in the function's logic. This allows `fake_call_llm` to mimic the signature of a real LLM call while bypassing the need for actual model processing or external resources. \n\nThe primary purpose of this function is to facilitate testing or debugging where a controlled response is required without invoking external dependencies like actual models or services. By using the `DummyMessage` class, the function provides a consistent and predictable mock response for further testing and development.\n\nThe relationship between `fake_call_llm` and its callees, such as the `DummyMessage` class, lies in its use of `DummyMessage` to return a structured response. The function creates a new `DummyMessage` instance with the content \"fake response\" and returns it, ensuring that downstream code can interact with the mock response as if it came from a real LLM.\n\n**Note**: This function is intended purely for testing purposes. In production environments or real LLM interactions, the parameters passed to the function should be used to process the user's input and generate appropriate responses based on model outputs.\n\n**Output Example**: \nThe output of the function will be an instance of the `DummyMessage` class with the content attribute set to \"fake response\". An example of what the returned object might look like is as follows:\n\n```\nDummyMessage(content=\"fake response\")\n```"
      ],
      "code_start_line": 13,
      "code_end_line": 14,
      "params": [
        "model",
        "config",
        "usr_prompt",
        "tools",
        "messages"
      ],
      "have_return": true,
      "code_content": "    def fake_call_llm(model, config, usr_prompt, tools=None, messages=None):\n        return DummyMessage(content=\"fake response\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "tests/test_chat_functionality.py/DummyMessage"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_chat_without_tools_returns_str",
      "md_content": [
        "**test_chat_without_tools_returns_str**: The function of test_chat_without_tools_returns_str is to verify that the chat method of the BaseAgent class returns a string response when called without any tools.\n\n**parameters**: The parameters of this Function.\n· agent: An instance of the BaseAgent class that is used to invoke the chat method.  \n· resp: The response returned from the chat method, which is expected to be a string.\n\n**Code Description**: The test_chat_without_tools_returns_str function is a unit test designed to validate the behavior of the chat method within the BaseAgent class. In this test, an instance of BaseAgent is created, and the chat method is called with a simple user prompt, \"hello\". The function then asserts two conditions: first, that the response (resp) is of type string, and second, that the response matches the expected output, which is \"fake response\".\n\nThis test is crucial for ensuring that the chat method behaves correctly when no tools are utilized during the interaction. The expected behavior is that the method should return a string response, which is a fundamental requirement for any conversational agent. The test checks the integrity of the response type and its content, thereby ensuring that the chat method can handle basic interactions appropriately.\n\nThe chat method itself is responsible for processing user prompts and generating responses, potentially using various tools to enhance the interaction. However, in this specific test case, the focus is on the method's ability to return a valid string response without the involvement of any tools. This highlights the method's core functionality and its ability to operate independently.\n\n**Note**: It is important to ensure that the BaseAgent class is correctly implemented and that the chat method is capable of returning the expected response format. Any changes to the chat method's implementation may require corresponding updates to this test to maintain its validity.\n\n**Output Example**: A possible appearance of the code's return value when executing the chat interaction might look like this:\n\"fake response\""
      ],
      "code_start_line": 19,
      "code_end_line": 23,
      "params": [],
      "have_return": true,
      "code_content": "def test_chat_without_tools_returns_str():\n    agent = BaseAgent()\n    resp = agent.chat(\"hello\")\n    assert isinstance(resp, str)\n    assert resp == \"fake response\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_chat_with_tools_returns_message_object",
      "md_content": [
        "**test_chat_with_tools_returns_message_object**: The function of test_chat_with_tools_returns_message_object is to verify that the chat method of the BaseAgent class returns a message object containing the expected content when invoked with tools.\n\n**parameters**: The parameters of this Function.\n· agent: An instance of the BaseAgent class, which is responsible for managing the chat interaction.  \n· dummy_tools: A list that represents the tools to be used in the chat interaction, which in this case is an empty list.  \n· result: The output returned by the chat method, which is expected to be a message object.\n\n**Code Description**: The test_chat_with_tools_returns_message_object function is a unit test designed to validate the behavior of the chat method within the BaseAgent class when tools are provided. In this test, an instance of BaseAgent is created, and an empty list is passed as the tools parameter to the chat method. The test then asserts that the result returned by the chat method has an attribute named \"content\" and that this content matches the string \"fake response\".\n\nThis function is critical in ensuring that the chat method behaves as expected when interacting with tools, confirming that it returns a structured message object rather than a simple string response. The test checks for the presence of the \"content\" attribute, which is indicative of a properly formed response object. The expected output of \"fake response\" serves as a mock response that the chat method should generate under these conditions.\n\nThe relationship with its callees is established through the invocation of the chat method within the BaseAgent class. The chat method is responsible for processing user prompts and generating responses, potentially utilizing tools to enhance the interaction. This test ensures that the integration of tools does not disrupt the expected output format of the chat method.\n\n**Note**: It is important to ensure that the tools parameter is appropriately defined, even if it is an empty list, to maintain the integrity of the chat method's functionality. Proper testing of the chat method is essential for the overall reliability of the BaseAgent class.\n\n**Output Example**: A possible appearance of the code's return value when executing the chat interaction might look like this:\n```json\n{\n  \"content\": \"fake response\"\n}\n```"
      ],
      "code_start_line": 25,
      "code_end_line": 31,
      "params": [],
      "have_return": true,
      "code_content": "def test_chat_with_tools_returns_message_object():\n    agent = BaseAgent()\n    dummy_tools = []\n    result = agent.chat([\"ignored\"], tools=dummy_tools)\n    # with tools, chat should return the raw message object\n    assert hasattr(result, \"content\")\n    assert result.content == \"fake response\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "tests/test_tavily_extract.py": [
    {
      "type": "ClassDef",
      "name": "DummyResponse",
      "md_content": [
        "**DummyResponse**: The function of DummyResponse is to simulate a response object for testing purposes, providing methods for status code validation and JSON data retrieval.\n\n**attributes**:\n· _data: Stores the data that will be returned as JSON when requested by the `json()` method.  \n· status_code: Represents the HTTP status code for the response.\n\n**Code Description**:  \nThe `DummyResponse` class is a mock response object typically used in unit tests to simulate HTTP responses. It is designed to be a simplified version of an HTTP response object, providing functionality to check for status codes and to return JSON data.\n\n- **__init__(self, data, status_code=200)**: The constructor method initializes a `DummyResponse` instance. It accepts two parameters:\n  - `data`: This is the data that will be returned when the `json()` method is called. It is expected to be in a format that can be serialized as JSON (e.g., a dictionary).\n  - `status_code`: This optional parameter specifies the HTTP status code for the response. By default, it is set to `200`, indicating a successful HTTP response.\n  \n- **raise_for_status(self)**: This method checks if the status code indicates a successful response. If the status code is not in the range of 200 to 299, it raises an `Exception` with a message indicating the status code. This method is typically used to simulate the behavior of a real HTTP response, where an error is raised for non-success status codes (e.g., 4xx or 5xx).\n  \n- **json(self)**: This method returns the `_data` attribute, simulating the behavior of a real HTTP response's `json()` method, which typically deserializes the response body into a Python object (such as a dictionary). In this mock implementation, it simply returns the `_data` directly without actual JSON parsing.\n\nThe `DummyResponse` class is used in several places within the test code to simulate different response scenarios. For instance, in the `test_extract_content_success` test case, an instance of `DummyResponse` is created with a mock dictionary `{\"foo\": \"bar\"}` to simulate a successful response. The `post` method of the `AsyncClient` is patched to return this `DummyResponse` object, and the test verifies that the returned data is correctly handled.\n\nIn the `test_extract_content_retry_on_invalid_json` test case, the `DummyResponse` class is subclassed into `BadResp`, which overrides the `json()` method to raise a `JSONDecodeError`, simulating a scenario where the response body is not valid JSON. The test then verifies that the extractor retries the request when encountering such an error.\n\n**Note**: \n- The `DummyResponse` class is specifically designed for testing purposes and is not intended for production use.\n- The `status_code` can be adjusted to simulate various HTTP response outcomes, such as success (200-299) or client/server errors (400-599).\n- The `raise_for_status()` method is particularly useful in tests where you want to ensure the correct handling of HTTP errors.\n  \n**Output Example**:  \nFor an instance of `DummyResponse` initialized as follows:\n```python\ndummy = DummyResponse({\"foo\": \"bar\"})\n```\nCalling the `json()` method will return:\n```python\n{\"foo\": \"bar\"}\n```  \nIf the status code is set to a non-success code (e.g., 404), calling `raise_for_status()` will raise an exception:\n```python\ndummy = DummyResponse({\"foo\": \"bar\"}, 404)\ndummy.raise_for_status()  # Raises Exception(\"HTTP 404\")\n```"
      ],
      "code_start_line": 7,
      "code_end_line": 18,
      "params": [],
      "have_return": true,
      "code_content": "class DummyResponse:\n    def __init__(self, data, status_code=200):\n        self._data = data\n        self.status_code = status_code\n\n    def raise_for_status(self):\n        if not (200 <= self.status_code < 300):\n            raise Exception(f\"HTTP {self.status_code}\")\n\n    def json(self):\n        # 模拟 JSON 解析\n        return self._data\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "tests/test_tavily_extract.py/test_extract_content_success",
        "tests/test_tavily_extract.py/test_extract_content_retry_on_invalid_json/BadResp",
        "tests/test_tavily_extract.py/test_extract_content_retry_on_invalid_json"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the class with the provided data and an optional status code.\n\n**parameters**: The parameters of this function.\n· data: This parameter represents the data that will be assigned to the instance variable _data. It is required when creating an instance of the class.  \n· status_code: This optional parameter defines the status code for the instance. If not provided, it defaults to 200, which typically indicates a successful operation.\n\n**Code Description**: The __init__ function is the constructor method of the class. When an instance of the class is created, this method is called to initialize the object's state. The function takes two parameters: data and status_code. The data parameter is mandatory, while status_code is optional and defaults to 200 if not specified. The provided data is stored in an instance variable _data, which is intended to hold the main content or response data for the instance. The status_code is stored directly in the instance as a public attribute, indicating the status or result of the operation, with the default value of 200 generally representing a successful response.\n\n**Note**: The __init__ method does not return any value. It simply initializes the instance variables. If the status_code is not provided, the default value is used, ensuring that instances have a meaningful status code even without explicit input."
      ],
      "code_start_line": 8,
      "code_end_line": 10,
      "params": [
        "self",
        "data",
        "status_code"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, data, status_code=200):\n        self._data = data\n        self.status_code = status_code\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "raise_for_status",
      "md_content": [
        "**raise_for_status**: The function of raise_for_status is to check the HTTP response status code and raise an exception if the status code is not in the successful range (200-299).\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `raise_for_status` function is designed to validate the status code of an HTTP response. It checks if the `status_code` attribute of the response object falls within the successful HTTP status code range, specifically between 200 (inclusive) and 300 (exclusive). If the status code is outside this range, the function raises an `Exception` with a message containing the problematic status code. \n\nThis method is often used to enforce error handling for HTTP responses, ensuring that only successful requests (i.e., those with status codes in the 200-299 range) proceed further. If a response indicates failure (such as a 404 or 500 error), the exception is raised, which can be caught or logged for debugging.\n\nThe `raise_for_status` function is called within the context of a test case, as seen in the `test_extract_content_success` function. In this test, the method is patched to simulate the behavior of an HTTP response. The `DummyResponse` class is used to mock a successful response with a status code of 200, ensuring that the `raise_for_status` method will not raise an exception during the test execution. The method helps verify that, under normal circumstances, no exceptions should be raised when the status code is in the successful range.\n\n**Note**: \n- The `raise_for_status` function only raises an exception when the HTTP status code indicates a failure (status codes outside the 200-299 range).\n- It is essential for this function to be invoked in contexts where HTTP response validation is necessary to ensure the reliability of the system by catching erroneous responses early in the process."
      ],
      "code_start_line": 12,
      "code_end_line": 14,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def raise_for_status(self):\n        if not (200 <= self.status_code < 300):\n            raise Exception(f\"HTTP {self.status_code}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "tests/test_tavily_extract.py/test_extract_content_success"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "json",
      "md_content": [
        "**json**: The function of json is to simulate JSON parsing by returning stored data.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The json function is a method that belongs to a class, and its primary purpose is to return the value of the instance variable `_data`. This variable is expected to hold data in a format that resembles JSON, although the function itself does not perform any actual parsing or transformation of the data. Instead, it simply provides access to the raw data stored in `_data`. This function is useful for retrieving the data in its original form, allowing other parts of the program to utilize it as needed.\n\n**Note**: It is important to ensure that the `_data` attribute is properly initialized before calling this function. If `_data` is not set, the function will return `None`, which may lead to errors in subsequent operations that expect valid data.\n\n**Output Example**: A possible appearance of the code's return value could be:\n{\n    \"key1\": \"value1\",\n    \"key2\": \"value2\",\n    \"key3\": [1, 2, 3]\n}"
      ],
      "code_start_line": 16,
      "code_end_line": 18,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def json(self):\n        # 模拟 JSON 解析\n        return self._data\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "DummyClient",
      "md_content": [
        "**DummyClient**: The function of DummyClient is to simulate an HTTP client that can return a mock response when interacting with certain HTTP methods such as POST.\n\n**attributes**: The attributes of this Class.\n· _resp: A response object that is returned by the `post` method.\n\n**Code Description**: \nThe `DummyClient` class is designed to simulate the behavior of an HTTP client. It primarily serves as a mock client to test functionality that would normally rely on making real HTTP requests. This is useful in testing scenarios where the actual HTTP requests are not desired or necessary, and instead, predetermined responses are required.\n\n1. **`__init__(self, resp)`**:\n   - This is the constructor method for the `DummyClient` class. It initializes the object with a response object that will be returned by the `post` method.\n   - **Parameter:**\n     - `resp`: The response object that will be returned whenever the `post` method is called. This can be any object, usually a mock or a predefined response that mimics an actual HTTP response.\n   - The `_resp` attribute is set to the provided `resp` argument, making it accessible throughout the class for later use.\n\n2. **`post(self, url, headers=None, json=None)`**:\n   - This asynchronous method simulates a POST HTTP request. It does not perform an actual HTTP request but instead returns the `_resp` attribute that was set during the initialization.\n   - **Parameters:**\n     - `url`: The URL to which the POST request would be made. This parameter is present for compatibility but does not influence the response.\n     - `headers`: Optional HTTP headers for the request, which are ignored in this method as it's a mock implementation.\n     - `json`: Optional JSON data to be sent with the request. This is also ignored as the method does not interact with any real data.\n   - **Return**: The method returns the `_resp` attribute, which was set when the object was initialized. This mimics the response that would typically come from a real HTTP POST request.\n\n3. **`__aenter__(self)`**:\n   - This is an asynchronous context manager method. It allows instances of `DummyClient` to be used with Python's `async with` syntax, making it behave like a context manager.\n   - **Return**: It returns the instance of the `DummyClient` itself, allowing it to be used inside the `async with` block.\n\n4. **`__aexit__(self, exc_type, exc, tb)`**:\n   - This is the asynchronous counterpart to the `__exit__` method, which is called when exiting an `async with` block. This method does not perform any specific actions but is included for compatibility with asynchronous context management.\n   - **Parameters:**\n     - `exc_type`, `exc`, `tb`: These parameters represent the exception type, the exception itself, and the traceback, respectively. Since no action is needed in this implementation, they are not used.\n\n**Note**: \n- The `DummyClient` class is useful in testing environments where a mock response is needed without actually performing HTTP requests. \n- This class can be used in `async with` blocks to ensure that asynchronous code is properly managed.\n- The `post` method always returns the same response (`_resp`), regardless of the input parameters, making it ideal for predictable, controlled testing scenarios.\n  \n**Output Example**:\nSuppose the `DummyClient` is initialized with a mock response object `{\"status\": \"success\"}`. If the `post` method is called with any URL, headers, or JSON, the output would be:\n\n```json\n{\"status\": \"success\"}\n```"
      ],
      "code_start_line": 20,
      "code_end_line": 31,
      "params": [],
      "have_return": true,
      "code_content": "class DummyClient:\n    def __init__(self, resp):\n        self._resp = resp\n\n    async def post(self, url, headers=None, json=None):\n        return self._resp\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc, tb):\n        pass\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the DummyClient class with a response object.\n\n**parameters**: The parameters of this Function.\n· resp: This parameter represents the response object that will be assigned to the instance variable _resp.\n\n**Code Description**: The __init__ function is a special method in Python, commonly known as a constructor. It is called when an instance of the class is created. In this specific implementation, the __init__ method takes one parameter, resp, which is expected to be an object representing a response. Inside the method, the instance variable _resp is assigned the value of the resp parameter. This allows the instance to store the response object for later use, enabling other methods within the class to access and manipulate this response as needed. The use of the underscore prefix in _resp indicates that this variable is intended for internal use within the class, following the convention of indicating private attributes.\n\n**Note**: It is important to ensure that the resp parameter passed to the __init__ method is of the expected type and structure, as this will directly affect the functionality of the DummyClient instance. Proper validation of the resp object may be necessary in more complex implementations to prevent errors during runtime."
      ],
      "code_start_line": 21,
      "code_end_line": 22,
      "params": [
        "self",
        "resp"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, resp):\n        self._resp = resp\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "post",
      "md_content": [
        "**post**: The function of post is to return a response object.\n\n**parameters**: The parameters of this Function.\n· url: The URL to which the HTTP request is made. This is a required parameter.\n· headers: Optional headers that should be included in the HTTP request. It can be set to None if no headers are needed.\n· json: Optional JSON data to be sent in the body of the HTTP request. It can be set to None if no data needs to be sent.\n\n**Code Description**: The `post` function is an asynchronous method that accepts three parameters: `url`, `headers`, and `json`. It is designed to simulate making an HTTP POST request but doesn't actually send the request or handle any response. Instead, it returns a predefined response object stored in `self._resp`. The parameters `url`, `headers`, and `json` are typically used in making HTTP POST requests, but in this case, they are passed to the function without being utilized within the method's body. The method essentially returns the value of `self._resp`, which is likely an instance attribute holding a mock or predefined response. \n\nThis method can be used in contexts such as testing or simulation where real HTTP requests are not necessary, but the behavior of the code that handles HTTP responses is being tested.\n\n**Note**: \n- The `url`, `headers`, and `json` parameters are not used in this function’s current implementation. If you intend to use the `post` method in a real-world context, ensure that `self._resp` is appropriately set up to reflect a meaningful response.\n- As an asynchronous function, `await` should be used when calling `post` to ensure that it completes before moving on to other operations in the code.\n\n**Output Example**: \nAssuming `self._resp` is a mock response like `{\"status\": \"success\"}`, the return value of the function will be:\n```json\n{\n  \"status\": \"success\"\n}\n```"
      ],
      "code_start_line": 24,
      "code_end_line": 25,
      "params": [
        "self",
        "url",
        "headers",
        "json"
      ],
      "have_return": true,
      "code_content": "    async def post(self, url, headers=None, json=None):\n        return self._resp\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__aenter__",
      "md_content": [
        "**__aenter__**: The function of __aenter__ is to allow an object to be used in an asynchronous context manager.\n\n**parameters**: \n- No parameters.\n\n**Code Description**:  \nThe function **__aenter__** is an asynchronous context manager method. It is used to define behavior when entering a context manager. This function is part of Python’s asynchronous context management protocol, which is intended to be used with `async with` statements.\n\nIn the provided implementation, the function simply returns the instance of the object (`self`). This suggests that the object itself will be used as the context manager. The behavior of this method is typical for cases where the context manager doesn't need to modify or set up any additional resources but simply needs to return the object itself for use within the `async with` block.\n\nThis method must be implemented in classes that are intended to be used with the `async with` statement. Upon entering the `async with` block, the object returned by **__aenter__** will be bound to the variable specified in the `async with` statement.\n\n**Note**:  \n- The function must be implemented as an asynchronous method because it is part of the asynchronous context management protocol.\n- Since the function simply returns `self`, the object remains unchanged during the context manager's entry phase.\n- This method is typically paired with a corresponding **__aexit__** method to define behavior when exiting the context manager.\n\n**Output Example**:  \n```python\nasync with DummyClient() as client:\n    # client will be the same instance of DummyClient as returned by __aenter__\n    assert client is client  # True, because __aenter__ returns self\n```"
      ],
      "code_start_line": 27,
      "code_end_line": 28,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    async def __aenter__(self):\n        return self\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__aexit__",
      "md_content": [
        "**__aexit__**: The function of __aexit__ is to define the exit behavior of an asynchronous context manager.\n\n**parameters**: The parameters of this Function.\n· exc_type: This parameter represents the type of the exception that was raised, if any, within the context block. If no exception occurred, this will be None.  \n· exc: This parameter holds the actual exception instance raised, or None if there was no exception.  \n· tb: This parameter is the traceback object associated with the exception, or None if there was no exception.  \n\n**Code Description**: The __aexit__ function is a special method that is part of the asynchronous context management protocol in Python. It is called when exiting an asynchronous context manager, which is defined using the `async with` statement. The purpose of this method is to allow for cleanup actions to be performed when the context is exited, regardless of whether an exception was raised or not. In this specific implementation, the function body is defined with a `pass` statement, indicating that no specific exit behavior is implemented. This means that when the context is exited, no additional actions will be taken, and any exceptions raised within the context will not be handled or logged by this method.\n\n**Note**: It is important to implement the __aexit__ method appropriately if any cleanup or resource management is required when the asynchronous context is exited. If no cleanup is necessary, the current implementation is sufficient, but developers should be aware that failing to handle exceptions can lead to unhandled exceptions propagating outside the context manager."
      ],
      "code_start_line": 30,
      "code_end_line": 31,
      "params": [
        "self",
        "exc_type",
        "exc",
        "tb"
      ],
      "have_return": false,
      "code_content": "    async def __aexit__(self, exc_type, exc, tb):\n        pass\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_extract_content_success",
      "md_content": [
        "## `test_extract_content_success` Function Documentation\n\n### Overview\nThe `test_extract_content_success` function is a unit test designed to verify the correct functionality of the `extract_content` method in the `TavilyExtract` class. This test specifically checks the behavior when the API responds successfully with a valid JSON object.\n\n### Purpose\nThe primary objective of this test is to ensure that the `TavilyExtract` class correctly processes a successful HTTP response, extracting content and returning it in the expected format.\n\n### Parameters\n- **monkeypatch**: This is a pytest fixture used to modify or replace methods and attributes during the test, allowing the simulation of various scenarios. It is used here to mock the behavior of HTTP requests made by the `AsyncClient` class.\n\n### Test Flow\n1. **Mocking the HTTP Request**:  \n   The test starts by creating a mock response object (`DummyResponse`) with a sample JSON payload `{\"foo\": \"bar\"}`. This is done to simulate a successful HTTP response from the Tavily API.\n\n2. **Patching the `AsyncClient` Methods**:  \n   The `__init__`, `post`, and `raise_for_status` methods of the `AsyncClient` class are patched:\n   - `__init__`: The constructor of `AsyncClient` is overridden to prevent any initialization logic from being executed.\n   - `post`: The `post` method is mocked to return the predefined `DummyResponse` object, simulating the HTTP response.\n   - `raise_for_status`: This method is patched to use the `raise_for_status` method from the `DummyResponse` class, ensuring the correct status code handling.\n\n3. **Extracting Content**:  \n   The `extract_content` method of the `TavilyExtract` class is called with a sample URL `[\"http://example.com\"]`. This triggers the extraction process, during which the mocked response is returned.\n\n4. **Assertions**:  \n   The test verifies the output:\n   - It asserts that the returned data is of type `dict`, confirming the content is properly parsed as a dictionary.\n   - It checks that the value of the key `\"foo\"` in the returned data is `\"bar\"`, validating that the response data is correctly handled.\n\n### Expected Behavior\nThe function expects the following outcomes:\n- The `extract_content` method returns a dictionary.\n- The dictionary contains the correct data as specified in the mock response, namely `{\"foo\": \"bar\"}`.\n\n### Example of Execution\n\n```python\ndummy = DummyResponse({\"foo\": \"bar\"})\n# Simulates a successful response from the HTTP client\nmonkeypatch.setattr(AsyncClient, \"post\", lambda self, url, headers, json: dummy)\n\n# Extracts content from the provided URL\ndata = await extractor.extract_content([\"http://example.com\"])\n\n# Validates the returned data\nassert isinstance(data, dict)\nassert data[\"foo\"] == \"bar\"\n```\n\n### Notes\n- This test ensures that the `TavilyExtract` class can handle successful HTTP responses correctly, extracting and returning the appropriate content.\n- The use of `monkeypatch` ensures that external dependencies (such as actual HTTP requests) are bypassed during testing, providing a controlled environment for verifying functionality.\n  \n### Dependencies\n- `DummyResponse`: A mock response class used to simulate HTTP responses.\n- `AsyncClient`: The HTTP client being patched to simulate the `post` method and response behavior.\n- `TavilyExtract`: The class under test, which interacts with an external API to extract content.\n\n### Conclusion\nThe `test_extract_content_success` function is a crucial unit test for validating the core functionality of content extraction in the `TavilyExtract` class, ensuring that it correctly handles a successful API response and processes the extracted data as expected."
      ],
      "code_start_line": 34,
      "code_end_line": 44,
      "params": [
        "monkeypatch"
      ],
      "have_return": false,
      "code_content": "async def test_extract_content_success(monkeypatch):\n    # 准备一个正确的 JSON 返回值\n    dummy = DummyResponse({\"foo\": \"bar\"})\n    monkeypatch.setattr(AsyncClient, \"__init__\", lambda self, http2=True: None)\n    monkeypatch.setattr(AsyncClient, \"post\", lambda self, url, headers, json: dummy)\n    monkeypatch.setattr(AsyncClient, \"raise_for_status\", dummy.raise_for_status)\n\n    extractor = TavilyExtract(api_key=\"test-key\")\n    data = await extractor.extract_content([\"http://example.com\"])\n    assert isinstance(data, dict)\n    assert data[\"foo\"] == \"bar\"\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "tests/test_tavily_extract.py/DummyResponse",
        "tests/test_tavily_extract.py/DummyResponse/raise_for_status",
        "src/criticsearch/tools/content_scraper/tavily_extract.py/TavilyExtract",
        "src/criticsearch/tools/content_scraper/tavily_extract.py/TavilyExtract/extract_content"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_extract_content_retry_on_invalid_json",
      "md_content": [
        "**test_extract_content_retry_on_invalid_json**: The function of test_extract_content_retry_on_invalid_json is to verify the behavior of the TavilyExtract class when it encounters an invalid JSON response followed by a valid one, ensuring that the extractor retries the request appropriately.\n\n**parameters**: The parameters of this Function.\n· monkeypatch: A fixture provided by pytest that allows for the temporary modification of objects during testing.\n\n**Code Description**: The test_extract_content_retry_on_invalid_json function is an asynchronous test designed to validate the retry mechanism of the TavilyExtract class when it receives an invalid JSON response from the Tavily API. The function utilizes the monkeypatch fixture from pytest to override the behavior of the AsyncClient's post method, simulating different response scenarios.\n\nInitially, a subclass of DummyResponse named BadResp is defined, which simulates a response that raises a JSONDecodeError when its json() method is called. This simulates the first call to the API returning an invalid JSON response. A second instance of DummyResponse, named good, is created to represent a valid response containing a JSON object with an \"ok\" key set to True.\n\nThe fake_post function is defined to control the behavior of the post method. It increments a call count each time it is invoked, returning the bad response on the first call and the good response on the second call. This setup ensures that the extractor will encounter an invalid JSON response first, followed by a valid one.\n\nThe monkeypatch.setattr method is then used to replace the AsyncClient's __init__ method and the post method with the modified versions. This allows the test to control the behavior of the HTTP client without making actual network requests.\n\nAn instance of TavilyExtract is created with a test API key, and the extract_content method is called with a list containing a single URL. The test asserts that the data returned from the extractor matches the expected valid response and that the call count indicates that the post method was called twice, confirming that the retry mechanism functioned as intended.\n\nThis test is crucial for ensuring the robustness of the TavilyExtract class, particularly in handling scenarios where the API may return unexpected or malformed responses. It demonstrates the importance of error handling and retry logic in asynchronous operations, which is essential for maintaining the reliability of API interactions.\n\n**Note**: It is important to ensure that the test environment is properly set up to use the pytest framework and that the necessary dependencies, such as the AsyncClient and TavilyExtract classes, are correctly imported. This test is specifically designed for unit testing and should not be used in production code.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```python\n{\"ok\": True}\n```  \nThe call count would indicate that the post method was invoked twice during the test execution."
      ],
      "code_start_line": 47,
      "code_end_line": 68,
      "params": [
        "monkeypatch"
      ],
      "have_return": true,
      "code_content": "async def test_extract_content_retry_on_invalid_json(monkeypatch):\n    # 第一次返回无效 JSON，第二次返回合法 JSON\n    class BadResp(DummyResponse):\n        def __init__(self):\n            super().__init__(None)\n        def json(self):\n            raise json.JSONDecodeError(\"Expecting value\", \"\", 0)\n    bad = BadResp()\n    good = DummyResponse({\"ok\": True})\n\n    call_count = {\"n\": 0}\n    async def fake_post(self, url, headers=None, json=None):\n        call_count[\"n\"] += 1\n        return bad if call_count[\"n\"] < 2 else good\n\n    monkeypatch.setattr(AsyncClient, \"__init__\", lambda self, http2=True: None)\n    monkeypatch.setattr(AsyncClient, \"post\", fake_post)\n\n    extractor = TavilyExtract(api_key=\"test-key\")\n    data = await extractor.extract_content([\"http://example.com\"])\n    assert data == {\"ok\": True}\n    assert call_count[\"n\"] == 2\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "tests/test_tavily_extract.py/DummyResponse",
        "src/criticsearch/tools/content_scraper/tavily_extract.py/TavilyExtract",
        "src/criticsearch/tools/content_scraper/tavily_extract.py/TavilyExtract/extract_content"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "BadResp",
      "md_content": [
        "**BadResp**: The function of BadResp is to simulate an invalid JSON response for testing purposes.\n\n**attributes**: The attributes of this Class.\n· None\n\n**Code Description**: The `BadResp` class is a subclass of `DummyResponse`, designed specifically for testing scenarios where the response body is not valid JSON. It overrides the `json()` method inherited from `DummyResponse`. When the `json()` method is called on an instance of `BadResp`, it raises a `JSONDecodeError`, simulating a situation where the response cannot be parsed as JSON. This behavior is crucial for testing error handling in code that processes JSON responses, allowing developers to verify that their application correctly retries requests or handles exceptions when faced with invalid JSON data.\n\nThe `BadResp` class does not introduce any new attributes; it relies on the constructor of `DummyResponse`, which is called with `None` as the data parameter. This indicates that there is no valid data to return. The primary purpose of `BadResp` is to facilitate testing by providing a controlled way to trigger JSON decoding errors.\n\nIn the context of the project, `BadResp` is utilized in the `test_extract_content_retry_on_invalid_json` test case. This test case is designed to ensure that the extractor correctly handles scenarios where the response is not valid JSON, thereby validating the robustness of the error handling mechanisms in the application.\n\n**Note**: \n- The `BadResp` class is intended solely for testing purposes and should not be used in production code.\n- It is essential to ensure that the testing framework properly handles the `JSONDecodeError` raised by the `json()` method to verify that the application behaves as expected in the face of invalid JSON responses."
      ],
      "code_start_line": 49,
      "code_end_line": 53,
      "params": [],
      "have_return": false,
      "code_content": "    class BadResp(DummyResponse):\n        def __init__(self):\n            super().__init__(None)\n        def json(self):\n            raise json.JSONDecodeError(\"Expecting value\", \"\", 0)\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "tests/test_tavily_extract.py/DummyResponse"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the class, specifically calling the parent class's __init__ method with a `None` argument.\n\n**parameters**: The function does not accept any parameters beyond `self`, which is standard for instance methods in Python.\n\n**Code Description**:  \nThis method is a constructor, often used to initialize an object when it is created. In this case, the method calls the parent class's `__init__` method using the `super()` function. The `super()` function allows access to methods in a superclass from the current class. By passing `None` as an argument to `super().__init__(None)`, the constructor explicitly sets the initialization behavior of the parent class to be triggered with `None`. The use of `None` as the argument likely suggests that the parent class expects a value or an object, but in this particular implementation, it is being initialized with `None`, indicating that no meaningful argument is passed at this stage.\n\n**Note**:  \n- The use of `super()` ensures that any initialization logic present in the parent class’s `__init__` method is properly executed. \n- The `None` argument passed to `super().__init__(None)` might have specific significance depending on the implementation of the parent class, though it doesn't provide any additional information within this method itself."
      ],
      "code_start_line": 50,
      "code_end_line": 51,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "        def __init__(self):\n            super().__init__(None)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "json",
      "md_content": [
        "**json**: The function of json is to raise a `JSONDecodeError`.\n\n**parameters**: The function does not accept any parameters.\n\n**Code Description**:  \nThe `json` function is a simple function that raises a `JSONDecodeError`. Specifically, it raises this error with the message \"Expecting value\" and specifies an empty string as the document that was being parsed, along with a position of 0. The `JSONDecodeError` is typically used in situations where there is an issue decoding JSON data. In this case, the error suggests that there is an expectation of a value in the JSON input, but none was provided, resulting in an invalid JSON document at the start.\n\nThis function does not perform any actual processing of JSON data. It solely serves to explicitly trigger a `JSONDecodeError` to simulate an error condition, possibly for testing or error handling scenarios where you want to verify how the system behaves when it encounters an invalid JSON input.\n\n**Note**:  \n- The function does not take any parameters and is intended to be used in situations where a `JSONDecodeError` needs to be raised programmatically.\n- The error message \"Expecting value\" corresponds to a common issue when attempting to parse an empty or malformed JSON input. \n- The empty string `\"\"` and position `0` are used to indicate the start of the JSON document as the source of the error."
      ],
      "code_start_line": 52,
      "code_end_line": 53,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "        def json(self):\n            raise json.JSONDecodeError(\"Expecting value\", \"\", 0)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "fake_post",
      "md_content": [
        "**fake_post**: The function of fake_post is to simulate an HTTP POST request, returning different results based on the number of calls made.\n\n**parameters**:\n- url: The URL to which the HTTP POST request is made. It is a required parameter of type string.\n- headers: Optional parameter that specifies the headers to be included in the HTTP request. It defaults to None.\n- json: Optional parameter that specifies the JSON data to be sent with the request. It defaults to None.\n\n**Code Description**:  \nThe `fake_post` function simulates a POST request by accepting the parameters `url`, `headers`, and `json`. When called, it first increments a counter stored in the `call_count[\"n\"]`. This counter tracks the number of times the function has been called. If the counter value is less than 2, the function returns a value referred to as `bad`. Once the function has been called twice or more (i.e., when `call_count[\"n\"]` is 2 or greater), it returns a value referred to as `good`.\n\nThis function is asynchronous, indicated by the `async` keyword, meaning it is expected to be used within an asynchronous context, such as within an `await` statement or another asynchronous function. \n\nThe logic behind the return values (\"bad\" or \"good\") suggests that this function could be used in testing or simulation environments where different responses need to be returned based on the number of attempts or interactions. The function keeps a persistent state across invocations due to the mutable nature of the `call_count` dictionary.\n\n**Note**: \n- The function uses the `call_count` dictionary to track the number of calls made to it. This means that the state of `call_count` should be managed or initialized before use.\n- Since this function is asynchronous, it should be awaited or called within an asynchronous function to ensure proper operation.\n- The behavior of returning \"bad\" on the first call and \"good\" on subsequent calls may be designed for testing purposes where simulating different server responses or handling retry logic is necessary.\n\n**Output Example**:\n- On the first call to `fake_post`: \"bad\"\n- On the second call and subsequent calls to `fake_post`: \"good\""
      ],
      "code_start_line": 58,
      "code_end_line": 60,
      "params": [
        "self",
        "url",
        "headers",
        "json"
      ],
      "have_return": true,
      "code_content": "    async def fake_post(self, url, headers=None, json=None):\n        call_count[\"n\"] += 1\n        return bad if call_count[\"n\"] < 2 else good\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_real_extract_content",
      "md_content": [
        "**test_real_extract_content**: The function of test_real_extract_content is to validate the content extraction functionality of the TavilyExtract class by testing its ability to process a specified URL and return the expected results.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The test_real_extract_content function is an asynchronous test function designed to verify the behavior of the TavilyExtract class's content extraction capabilities. It begins by instantiating the TavilyExtract class with an API key sourced from the project settings. This API key is essential for authenticating requests to the Tavily API.\n\nThe function then defines a list of URLs to be processed, which currently includes a specific URL related to election information. This URL can be replaced with any publicly accessible webpage for testing purposes. The core operation of the function involves calling the extract_content method of the TavilyExtract instance, passing the list of URLs as an argument. This method is responsible for sending requests to the Tavily API and retrieving the content from the specified URLs.\n\nUpon receiving the result from the extract_content method, the function performs several assertions to ensure the output meets the expected criteria. It first checks that the result is of type dictionary, confirming that the response structure is correct. Subsequently, it verifies that the original URL is present in the result dictionary, ensuring that the extraction process has successfully processed the provided URL. Finally, it asserts that the value associated with the URL key in the result is also a dictionary, indicating that the extraction yielded a valid response.\n\nThis function serves as a unit test within the testing framework, ensuring that the TavilyExtract class operates correctly and reliably when extracting content from web pages. It plays a crucial role in maintaining the integrity of the content extraction process by validating that the expected data structure and content are returned.\n\n**Note**: It is important to ensure that the API key used for the TavilyExtract instance is valid and that the URL provided is accessible. This test function is designed to run in an asynchronous context, and proper error handling should be in place to manage any potential issues that may arise during the API request or content extraction process."
      ],
      "code_start_line": 75,
      "code_end_line": 82,
      "params": [],
      "have_return": false,
      "code_content": "async def test_real_extract_content():\n    extractor = TavilyExtract(settings.tavily[\"api_key\"])\n    urls = [\"https://www.electionguide.org/elections/id/4314/\"]  # 可替换为任何公开可访问的网页\n    result = await extractor.extract_content(urls)\n    # 结果应为 dict，且包含每个 URL 的解析结果（非空 dict 或包含 text 字段）\n    assert isinstance(result, dict)\n    assert urls[0] in result\n    assert isinstance(result[urls[0]], dict), \"返回值应为 dict\"\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/content_scraper/tavily_extract.py/TavilyExtract",
        "src/criticsearch/tools/content_scraper/tavily_extract.py/TavilyExtract/extract_content"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "tests/test_function_call_between_models.py": [
    {
      "type": "FunctionDef",
      "name": "test_function_call",
      "md_content": [
        "**test_function_call**: The function of `test_function_call` is to test the behavior of the `call_llm` function when called with different models, ensuring the correct handling of tool calls and function arguments based on model type.\n\n**parameters**: The parameters of this function are as follows:\n- model: A string representing the model name to be tested.\n\n**Code Description**: \nThe `test_function_call` function is designed to verify the behavior of the `call_llm` function when different models are used. It takes a single parameter, `model`, which represents the name of the model to be tested. \n\nThe function performs the following steps:\n1. It first calls the `call_llm` function with the `model`, `usr_prompt`, `config`, and `tools` parameters, where `usr_prompt` is assumed to be defined elsewhere in the code (likely as a list or string) and contains the input message for the model, `config` is a settings dictionary for the model, and `tools` is an optional list of tools available for the model's interaction.\n\n2. The function then proceeds to check the behavior based on the model name. If the model belongs to the \"deepseek_family\" (i.e., \"r1\" or \"reasoner\"), it asserts that no tool calls are returned by the `call_llm` function, raising an `AssertionError` if tool calls are present. This is done using the `pytest.raises(AssertionError)` construct, which ensures that the model should not generate any tool calls.\n\n3. For models other than those in the \"deepseek_family\", such as GPT-4o-mini or similar, the function checks that valid tool calls are returned. If no tool calls are returned, the function will raise an assertion failure with a specific message. Additionally, for each tool call returned, the function verifies that the tool call's function name is `\"get_weather\"` and that the function arguments contain `\"location\"`. These checks ensure that the correct function and arguments are used, validating the model's response and interaction with tools.\n\nThe test ensures the correctness of the `call_llm` function’s integration with various models, verifying both the presence of tool calls and the correctness of the function name and arguments.\n\n**Note**: \n- The test assumes that `messages`, `settings`, and `tools` are predefined elsewhere in the code.\n- The behavior of the test is dependent on the `model` parameter passed to the function, and it distinguishes between models that should not return tool calls (e.g., \"r1\", \"reasoner\") and models that should return valid tool calls.\n- The specific function name checked is `\"get_weather\"`, and it is assumed that all models generating tool calls should utilize this function for weather-related queries.\n- The use of `pytest` for exception handling ensures that expected errors are correctly raised in the event of invalid tool calls.\n\n**Output Example**:\nFor a model such as \"r1\" that belongs to the deepseek family, the expected result would be an AssertionError if any tool calls are present:\n\n```\nAssertionError: Model r1 should not return tool_calls.\n```\n\nFor a model like \"GPT-4o-mini\" that is expected to return tool calls, a correct output would look like:\n\n```\nModel GPT-4o-mini returned empty tool_calls.\nModel GPT-4o-mini returned incorrect function name.\nModel GPT-4o-mini returned incorrect function arguments.\n```\n\nIn these cases, the test would fail, highlighting the specific mismatch in the tool calls, function name, or function arguments."
      ],
      "code_start_line": 38,
      "code_end_line": 67,
      "params": [
        "model"
      ],
      "have_return": true,
      "code_content": "def test_function_call(model):\n    # 调用 call_llm 函数\n    response = call_llm(\n        model=model,\n        usr_prompt=messages,\n        config=settings,\n        tools=tools,\n    )\n\n    # Case 1: DeepSeek model should not return a function call\n    deepseek_family = [\"r1\", \"reasoner\"]\n\n    if any(sub in model.lower() for sub in deepseek_family):\n        with pytest.raises(AssertionError):\n            assert response.tool_calls, f\"Model {model} should not return tool_calls.\"\n\n    # Case 2: Other models (like GPT-4o-mini) should return valid tool calls\n    else:\n        assert response.tool_calls, f\"Model {model} returned empty tool_calls.\"\n\n        # 检查 function 的名称是否正确\n        for tool_call in response.tool_calls:\n            assert tool_call.function.name == \"get_weather\", (\n                f\"Model {model} returned incorrect function name.\"\n            )\n\n            # 检查 function 的参数是否正确\n            assert \"location\" in tool_call.function.arguments, (\n                f\"Model {model} returned incorrect function arguments.\"\n            )\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/llm_service.py/call_llm"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "tests/test_scrape_empty_result.py": [],
  "tests/test_wiki_data_json.py": [
    {
      "type": "FunctionDef",
      "name": "test_all_wiki_json_valid",
      "md_content": [
        "**test_all_wiki_json_valid**: The function of test_all_wiki_json_valid is to validate the JSON files located in the specified directory by attempting to parse them and reporting any failures.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The function begins by initializing an empty list called `failures` to keep track of any JSON files that fail to parse correctly. It then iterates over all JSON files in the `DATA_DIR` directory, which is expected to contain files with a `.json` extension. Each file is read as text using UTF-8 encoding. \n\nBefore attempting to parse the JSON, there is a commented-out section of code that, if uncommented, would remove any lines that start with `//`, which are typically used for comments in JSON files. This indicates that the function may be designed to handle JSON files that could potentially contain such comment lines.\n\nThe function then attempts to load the text as JSON using `json.loads()`. If a `JSONDecodeError` is raised during this process, it means that the file is not valid JSON, and the error message, along with the file name, is appended to the `failures` list.\n\nAfter all files have been processed, the function checks if there are any entries in the `failures` list. If there are, it calls `pytest.fail()` to report the failures, providing a message that lists all the JSON files that could not be parsed along with their respective error messages.\n\n**Note**: It is important to ensure that the `DATA_DIR` variable is correctly defined and points to the directory containing the JSON files. Additionally, if the JSON files may contain comment lines, consider uncommenting the cleaning code to avoid parsing errors. This function is intended to be used in a testing context, specifically with the pytest framework, to ensure that all JSON files are valid before further processing."
      ],
      "code_start_line": 7,
      "code_end_line": 18,
      "params": [],
      "have_return": false,
      "code_content": "def test_all_wiki_json_valid():\n    failures = []\n    for jf in sorted(DATA_DIR.glob(\"*.json\")):\n        text = jf.read_text(encoding=\"utf-8\")\n        # 如果文件前面有注释行，可以先做一次简单清洗：\n        # text = \"\\n\".join(line for line in text.splitlines() if not line.strip().startswith(\"//\"))\n        try:\n            json.loads(text)\n        except json.JSONDecodeError as e:\n            failures.append(f\"{jf.name}: {e}\")\n    if failures:\n        pytest.fail(\"以下 JSON 文件解析失败:\\n\" + \"\\n\".join(failures))",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "tests/test_search_empty_result.py": [],
  "tests/test_taking_notes.py": [
    {
      "type": "FunctionDef",
      "name": "test_extract_notes",
      "md_content": [
        "**test_extract_notes**: The function of test_extract_notes is to validate the functionality of the note extraction process from a given response string.\n\n**parameters**: The parameters of this Function.\n· test_response: A string containing a simulated response that includes notes formatted within <note> tags.\n\n**Code Description**: The test_extract_notes function is designed to test the extract_notes function, which is responsible for extracting a list of notes from a structured response text. In this test, a predefined string, test_response, simulates a response containing two notes, each with an associated citation. The function calls extract_notes with this test_response to retrieve the notes.\n\nThe test then asserts that the length of the returned notes list is equal to 2, confirming that both notes were successfully extracted. Additionally, it checks that the specific content of each note is present in the returned list, ensuring that the extract_notes function correctly identifies and processes the notes as intended.\n\nThis testing function is crucial for verifying the correctness of the extract_notes implementation, ensuring that it behaves as expected when provided with well-formed input. It serves as a unit test, which is a fundamental practice in software development to maintain code quality and reliability.\n\n**Note**: It is important to ensure that the input string, test_response, is formatted correctly according to the expected structure for the extract_notes function to work properly. Any deviations from this format may lead to unexpected results or failures in the assertions."
      ],
      "code_start_line": 6,
      "code_end_line": 17,
      "params": [],
      "have_return": false,
      "code_content": "def test_extract_notes():\n    # 测试基本的note提取功能\n    test_response = \"\"\"\n    <answer>[\n        \"<note>First note content with <citation>http://example1.com</citation></note>\",\n        \"<note>Second note content with <citation>http://example2.com</citation></note>\"\n    ]</answer>\n    \"\"\"\n    notes = extract_notes(test_response)\n    assert len(notes) == 2\n    assert \"First note content with <citation>http://example1.com</citation>\" in notes\n    assert \"Second note content with <citation>http://example2.com</citation>\" in notes\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/utils.py/extract_notes"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_taking_notes_integration",
      "md_content": [
        "**test_taking_notes_integration**: The function of test_taking_notes_integration is to test the complete functionality of the note-taking feature within the BaseAgent class.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The test_taking_notes_integration function is designed to validate the note-taking capabilities of the BaseAgent class. It begins by creating an instance of BaseAgent and assigning a test task to it using the receive_task method. This sets the context for the agent to process the task effectively.\n\nNext, the function simulates search results by defining a string containing relevant information. This string is then passed to the taking_notes method of the agent, which is responsible for extracting notes from the provided search results. After the first call to taking_notes, the function checks the size of the memo attribute to ensure that notes have been successfully added. An assertion is made to confirm that the memo contains entries, indicating that the note-taking process has functioned correctly.\n\nThe function then calls taking_notes again with the same search results to test the agent's ability to handle duplicate notes. It asserts that the size of the memo remains unchanged, confirming that the agent does not add duplicate entries.\n\nFinally, the function iterates through the notes stored in the memo to validate their formatting. It checks that the notes do not contain raw XML-like tags (\"<note>\" and \"</note>\") and ensures that each note includes citation tags (\"<citation>\" and \"</citation>\"). This validation step is crucial for maintaining the integrity and structure of the notes recorded by the agent.\n\nThe test_taking_notes_integration function is integral to the testing suite for the BaseAgent class, ensuring that the note-taking feature operates as intended and adheres to the expected format. It serves as a comprehensive integration test that covers the main functionalities of the note-taking process, including task reception, note extraction, duplicate handling, and content validation.\n\n**Note**: It is essential to ensure that the BaseAgent class is correctly implemented and that the taking_notes method is functioning as expected for this test to yield accurate results. Proper formatting of the search results is also critical for the successful extraction of notes."
      ],
      "code_start_line": 19,
      "code_end_line": 49,
      "params": [],
      "have_return": false,
      "code_content": "def test_taking_notes_integration():\n    # 测试完整的taking notes功能\n    agent = BaseAgent()\n    agent.receive_task(\"Test task\")\n    \n    # 模拟搜索结果\n    test_web_results = \"\"\"\n    Some search results about the topic...\n    Source: http://example1.com\n    Content: Important information 1\n    \n    Source: http://example2.com\n    Content: Important information 2\n    \"\"\"\n    \n    # 第一次添加笔记\n    agent.taking_notes(test_web_results)\n    first_memo_size = len(agent.memo)\n    assert first_memo_size > 0, \"Should have added notes to memo\"\n    \n    # 再次添加相同的笔记\n    agent.taking_notes(test_web_results)\n    second_memo_size = len(agent.memo)\n    assert second_memo_size == first_memo_size, \"Should not add duplicate notes\"\n    \n    # 验证memo中的内容格式\n    for note in agent.memo:\n        assert \"<note>\" not in note, \"Raw note should not contain <note> tags\"\n        assert \"</note>\" not in note, \"Raw note should not contain </note> tags\"\n        assert \"<citation>\" in note, \"Note should contain citation\"\n        assert \"</citation>\" in note, \"Note should contain citation\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/receive_task",
        "src/criticsearch/base_agent.py/BaseAgent/taking_notes"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_empty_notes",
      "md_content": [
        "**test_empty_notes**: The function of test_empty_notes is to verify the behavior of the extract_notes function when provided with an empty notes scenario.\n\n**parameters**: The parameters of this Function.\n· test_response: A string formatted as \"<answer>[]</answer>\", representing a response with no notes.\n\n**Code Description**: The test_empty_notes function is a unit test designed to assess the functionality of the extract_notes function from the utils module. It specifically tests the case where the input response contains no notes. The test sets up a string, test_response, which simulates an API or chat response that includes an empty list of notes within the <answer> tags. The extract_notes function is then called with this test_response as an argument. \n\nThe expected outcome of this test is that the extract_notes function should return an empty list when there are no valid notes present in the response. The assertion statement checks that the length of the notes returned by extract_notes is zero, confirming that the function behaves correctly in this scenario. This unit test is crucial for ensuring the robustness of the extract_notes function, particularly in handling edge cases where no notes are available.\n\nThe test_empty_notes function is part of a broader testing suite that ensures the reliability of the note extraction process, which is essential for applications that rely on structured data extraction from responses. By validating that the extract_notes function can handle empty inputs gracefully, developers can be more confident in the function's overall stability and correctness.\n\n**Note**: It is important to ensure that the input to extract_notes is formatted correctly. In this case, the input is specifically designed to represent an absence of notes, and the test confirms that the function responds appropriately by returning an empty list."
      ],
      "code_start_line": 51,
      "code_end_line": 55,
      "params": [],
      "have_return": false,
      "code_content": "def test_empty_notes():\n    # 测试空的笔记情况\n    test_response = \"<answer>[]</answer>\"\n    notes = extract_notes(test_response)\n    assert len(notes) == 0\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/utils.py/extract_notes"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_malformed_notes",
      "md_content": [
        "**test_malformed_notes**: The function of test_malformed_notes is to test the behavior of the `extract_notes` function when provided with malformed notes in the input response text.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The `test_malformed_notes` function is a unit test designed to validate the behavior of the `extract_notes` function when given malformed or incorrectly formatted notes in the response text. Specifically, it checks the scenario where the input contains incomplete or improperly closed note tags, ensuring that the function behaves as expected in such cases.\n\nIn this function, a malformed response string is created with a broken `<note>` tag structure. The `test_response` contains an opening `<note>` tag without a proper closing counterpart, and another `<note>` tag is incorrectly formatted with an extra `note>` tag instead of a properly closed `</note>`.\n\nThe `extract_notes` function is then called with this malformed `test_response`. As the function is designed to extract only valid notes (those that are well-formed with properly closed `<note>` and `</note>` tags, and contain valid `<citation>` elements), it should return an empty list when encountering the malformed input.\n\nThe `assert` statement checks that the result of `extract_notes` is an empty list (`[]`), which confirms that the function properly handles malformed notes by returning no valid notes.\n\nThe purpose of this test is to verify that the `extract_notes` function does not incorrectly include invalid notes in the output and correctly handles edge cases involving improperly formatted input.\n\n**Note**: This test ensures that the `extract_notes` function adheres to its specification of only returning valid notes, even when faced with malformed or incomplete input."
      ],
      "code_start_line": 57,
      "code_end_line": 66,
      "params": [],
      "have_return": false,
      "code_content": "def test_malformed_notes():\n    # 测试格式不正确的笔记\n    test_response = \"\"\"\n    <answer>[\n        \"<note>Incomplete note\n        \"note>Another incomplete note</note>\"\n    ]</answer>\n    \"\"\"\n    notes = extract_notes(test_response)\n    assert len(notes) == 0\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/utils.py/extract_notes"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "tests/test_utils.py": [
    {
      "type": "FunctionDef",
      "name": "test_single_citation",
      "md_content": [
        "**test_single_citation**: The function of test_single_citation is to validate the functionality of the extract_citations function by testing its ability to correctly extract a URL from a formatted citation in a given text.\n\n**parameters**: The parameters of this Function are as follows:\n· None\n\n**Code Description**: The test_single_citation function is a unit test designed to ensure that the extract_citations function operates correctly when provided with a specific input string containing a citation. In this test, a string variable named `text` is defined, which includes a citation formatted as `<citation>http://example.com</citation>`. The function then asserts that the output of the extract_citations function, when called with this text, matches the expected result, which is a list containing the single URL `[\"http://example.com\"]`.\n\nThis test serves as a basic validation of the extract_citations function's ability to identify and extract URLs enclosed within <citation> tags. It checks that the function can handle a straightforward case where only one citation is present in the input text. The assertion ensures that if the extract_citations function does not return the expected output, the test will fail, indicating a potential issue with the citation extraction logic.\n\nThe test_single_citation function is part of a broader suite of tests that may be implemented to cover various scenarios for the extract_citations function, including cases with multiple citations, malformed citations, and edge cases where no citations are present. By validating the functionality of extract_citations through unit tests like test_single_citation, developers can maintain confidence in the reliability and correctness of the citation extraction process within the application.\n\n**Note**: It is important to ensure that the extract_citations function is properly implemented and that the input text is formatted correctly with <citation> tags for the test to pass successfully. If the extract_citations function fails to extract the citation as expected, it may indicate a bug or an issue with the regular expression used for matching."
      ],
      "code_start_line": 4,
      "code_end_line": 6,
      "params": [],
      "have_return": false,
      "code_content": "def test_single_citation():\n    text = \"This is a test <citation>http://example.com</citation> end.\"\n    assert extract_citations(text) == [\"http://example.com\"]\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/utils.py/extract_citations"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_multiple_citations_and_whitespace",
      "md_content": [
        "**test_multiple_citations_and_whitespace**: The function of test_multiple_citations_and_whitespace is to test the extraction of URLs from text that contains multiple citation tags, accounting for possible variations in whitespace and formatting.\n\n**parameters**: The function does not take any parameters.\n\n**Code Description**:  \nThe function `test_multiple_citations_and_whitespace` is designed to validate the behavior of the `extract_citations` function when dealing with multiple citation tags, ensuring that URLs are correctly extracted from text that may contain varied whitespace and formatting.\n\n- The test function defines a multiline string `text`, which contains two citation tags with URLs inside. The URLs in the citation tags are formatted with different whitespace patterns. One citation has extra spaces before and after the URL, while the other citation spans multiple lines. This tests the robustness of the `extract_citations` function against such irregular formatting.\n  \n- The `assert` statement checks that the result of calling `extract_citations(text)` matches the expected output, which is a list of two URLs: `[\"https://foo.com/path\", \"https://bar.com\"]`. The expected result strips any leading or trailing spaces around the URLs and ensures that URLs are correctly identified despite the variations in formatting and whitespace.\n\nThe `extract_citations` function, called within this test, is responsible for parsing the text and extracting all URLs enclosed within `<citation>...</citation>` tags. It handles different variations of the citation content, such as URLs with extra whitespace or those that span multiple lines. \n\nBy performing this test, the function ensures that the `extract_citations` function can handle citations in diverse formats, returning a clean and consistent list of URLs.\n\n**Note**: This test assumes that the `extract_citations` function has been correctly implemented and is able to handle variations in whitespace and formatting around citation tags. The function will return a list of URLs in the order they appear within the citation tags, after stripping any excess whitespace."
      ],
      "code_start_line": 8,
      "code_end_line": 18,
      "params": [],
      "have_return": false,
      "code_content": "def test_multiple_citations_and_whitespace():\n    text = (\n        \"Here are two:\\n\"\n        \"<citation>  https://foo.com/path  </citation>\\n\"\n        \"and\\n\"\n        \"<citation>\\nhttps://bar.com\\n</citation>\"\n    )\n    assert extract_citations(text) == [\n        \"https://foo.com/path\",\n        \"https://bar.com\"\n    ]\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/utils.py/extract_citations"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_duplicate_citations_are_preserved",
      "md_content": [
        "**test_duplicate_citations_are_preserved**: The function of test_duplicate_citations_are_preserved is to verify that duplicate citations within a given text are preserved in the order they appear when extracted.\n\n**parameters**: The parameters of this Function are as follows:\n· None\n\n**Code Description**: The test_duplicate_citations_are_preserved function is a unit test designed to validate the behavior of the extract_citations function from the utils module. It provides a specific input string containing duplicate citation tags and checks that the output of extract_citations correctly reflects the presence of these duplicates.\n\nThe input text for this test is defined as \"<citation>dup</citation> first, then again <citation>dup</citation>\". This string includes two identical citation tags with the content \"dup\". The purpose of this test is to ensure that the extract_citations function does not remove duplicates but instead retains them in the order they appear in the input text.\n\nThe assertion statement within the function checks that the output of extract_citations when applied to the input text is equal to the list [\"dup\", \"dup\"]. This confirms that the function behaves as expected by preserving duplicate citations rather than filtering them out.\n\nThis test is crucial for maintaining the integrity of citation extraction within the project, as it ensures that all citations, regardless of duplication, are accurately captured. The extract_citations function is utilized in various parts of the project, such as process_section and parse_markdown_to_structure, where accurate citation extraction is essential for proper document processing.\n\n**Note**: It is important to ensure that the input text is formatted correctly with <citation> tags for the extract_citations function to operate effectively. The test specifically demonstrates that the function's current implementation does not deduplicate citations, which is a key aspect of its intended functionality."
      ],
      "code_start_line": 20,
      "code_end_line": 25,
      "params": [],
      "have_return": false,
      "code_content": "def test_duplicate_citations_are_preserved():\n    text = (\n        \"<citation>dup</citation> first, then again <citation>dup</citation>\"\n    )\n    # 目前实现不会去重，只保留出现顺序\n    assert extract_citations(text) == [\"dup\", \"dup\"]\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/utils.py/extract_citations"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_no_citations_returns_empty_list",
      "md_content": [
        "**test_no_citations_returns_empty_list**: The function of test_no_citations_returns_empty_list is to verify that the extract_citations function returns an empty list when no <citation> tags are present in the input text.\n\n**parameters**: The parameters of this Function are as follows:\n· None\n\n**Code Description**: The function test_no_citations_returns_empty_list is a unit test designed to validate the behavior of the extract_citations function when the input text does not contain any <citation> tags. It calls the extract_citations function with a string \"no tags here\" as the argument, which does not include any <citation> tags. The test asserts that the return value of the extract_citations function is an empty list. This ensures that when no citations are present in the input text, the function correctly returns an empty list.\n\nThe test serves as a check to ensure that the extract_citations function behaves as expected in the case of an input that does not include any citations. If the extract_citations function fails to return an empty list in this scenario, the test would fail, signaling that the function's handling of such cases is not working correctly.\n\n**Note**: This test case does not require any parameters to be passed directly to it, as the input text is hardcoded within the test function. It only verifies the scenario where no citation tags are found in the input.\n\n**Output Example**: The expected return value from the extract_citations function when the input text is \"no tags here\" would be:\n```python\n[]\n```"
      ],
      "code_start_line": 27,
      "code_end_line": 28,
      "params": [],
      "have_return": true,
      "code_content": "def test_no_citations_returns_empty_list():\n    assert extract_citations(\"no tags here\") == []",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/utils.py/extract_citations"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "tests/test_multiple_same_role.py": [],
  "src/criticsearch/base_agent.py": [
    {
      "type": "ClassDef",
      "name": "BaseAgent",
      "md_content": [
        "**BaseAgent**: The function of BaseAgent is to serve as a foundational class for implementing an intelligent agent that can manage conversations, perform searches, and interact with various tools.\n\n**attributes**: The attributes of this Class.\n· queryDB: A set to store unique queries made by the agent during its operation.  \n· tool_registry: An instance of ToolRegistry that manages the schemas for tools used by the agent.  \n· user_question: A string that holds the current question posed by the user.  \n· conversation_manager: An instance of ConversationManager responsible for maintaining the history of the conversation.  \n· prompts_dir: A string that specifies the directory path where template prompt files are stored.  \n· citationDB: A list of dictionaries that contains search queries and their corresponding results, specifically those praised by a critic.  \n· search_aggregator: An instance of SearchAggregator that facilitates search operations.  \n· search_aggregator_schema: A schema representation of the search aggregator tool, retrieved from the tool registry.  \n· content_scraper: An instance of ContentScraper that handles web scraping tasks.  \n· content_scraper_schema: A schema representation of the content scraper tool, retrieved from the tool registry.  \n· repeat_turns: An integer that defines the number of times the agent will repeat its search and interaction process.\n\n**Code Description**: The BaseAgent class is designed to provide a structured framework for building intelligent agents that can engage in conversations, perform searches, and utilize various tools effectively. Upon initialization, the class sets up several key components, including the conversation manager, tool registry, and search aggregator. \n\nThe conversation manager is responsible for tracking the history of interactions, while the tool registry allows the agent to manage and retrieve schemas for different tools it may use. The citationDB is specifically designed to store search results that have received positive feedback from a critic, ensuring that the agent can reference high-quality information.\n\nThe class provides several methods for loading templates, rendering them with data, and managing conversations. The `common_chat` method is overloaded to handle different types of user prompts, allowing for flexible interactions. The `update_answer` method enables the agent to refine its responses based on previous answers and feedback from critics.\n\nAdditionally, the `search_and_browse` method integrates both search and web scraping functionalities, allowing the agent to gather information from various sources and present it to the user. The `model_confident` method checks the agent's confidence in its responses, guiding its decision-making process on whether to provide an answer or seek additional information.\n\nThe BaseAgent class is utilized in various parts of the project, including the CriticAgent, which extends its functionality to generate critiques based on the agent's responses. The main function in the project initializes an instance of BaseAgent to handle user tasks, demonstrating its role as a central component in the overall architecture.\n\n**Note**: It is essential to ensure that the tool registry is populated with the necessary schemas for the tools being used, as the agent relies on these schemas for proper functionality. Additionally, the citationDB should be managed carefully to maintain the quality of information referenced by the agent.\n\n**Output Example**: A possible appearance of the code's return value when performing a search might look like this:\n```json\n{\n  \"search_results\": [\n    {\n      \"document_id\": \"12345\",\n      \"url\": \"https://example.com/article\",\n      \"title\": \"Understanding the Challenges Faced by Google in 2019\",\n      \"content\": \"In 2019, Google faced several challenges including...\"\n    }\n  ],\n  \"conversation_history\": [\n    {\"role\": \"user\", \"content\": \"What challenges did Google face in 2019?\"},\n    {\"role\": \"assistant\", \"content\": \"Google faced several challenges including...\"}\n  ]\n}\n```",
        "**BaseAgent**: The function of BaseAgent is to serve as a foundational class for intelligent agents, providing essential functionalities for managing conversations, handling templates, and integrating various tools for search and content scraping.\n\n**attributes**: The attributes of this Class.\n· queryDB: A set to store queries made by the agent, ensuring uniqueness and facilitating tracking of search queries.  \n· tool_registry: An instance of ToolRegistry that manages the schemas for tools used by the agent.  \n· user_question: A string that holds the current question posed by the user.  \n· conversation_manager: An instance of ConversationManager responsible for managing the conversation history.  \n· citationDB: A list of dictionaries that stores search queries and their corresponding results, specifically those praised by critics.  \n· search_aggregator: An instance of SearchAggregator that aggregates search results from various sources.  \n· search_aggregator_schema: A schema for the search aggregator tool, retrieved or created from the tool registry.  \n· content_scraper: An instance of ContentScraper that extracts content from web pages.  \n· content_scraper_schema: A schema for the content scraper tool, retrieved or created from the tool registry.  \n· repeat_turns: An integer that indicates the number of times the agent should repeat its search or response process.\n\n**Code Description**: The BaseAgent class is designed to facilitate the operation of intelligent agents by providing a structured approach to managing user interactions and integrating various tools for enhanced functionality. Upon initialization, the class sets up its environment by determining the directory of the current script and establishing paths for template files. It initializes several key components, including a conversation manager to track the history of interactions, a tool registry to manage schemas for tools, and a citation database to store relevant search results.\n\nThe class includes methods for loading and rendering templates, which are essential for generating dynamic responses based on user input. The `chat_with_template` and `chat_with_tools` methods allow the agent to interact with users and tools by rendering prompts and managing the conversation flow. The `common_chat` method serves as a core function that handles communication with the language model, allowing for flexible interactions based on user prompts and available tools.\n\nAdditionally, the BaseAgent class provides methods for updating answers based on user feedback, checking the model's confidence in its responses, and scraping web content from search results. The `search_and_browse` method integrates search functionality with web scraping, enabling the agent to gather and process information effectively.\n\nThe BaseAgent is utilized by other classes, such as CriticAgent, which extends its capabilities to focus on generating critiques of responses. The CriticAgent leverages the functionalities provided by BaseAgent to manage conversations and interact with the language model, demonstrating the foundational role of BaseAgent in the overall architecture of the project.\n\n**Note**: It is important to ensure that the templates used for rendering responses are correctly formatted and accessible, as the BaseAgent relies on these templates for generating dynamic content. Additionally, proper management of the conversation history and tool schemas is crucial for maintaining the integrity and efficiency of the agent's operations.\n\n**Output Example**: A possible appearance of the code's return value when rendering a template might look like this:\n```json\n{\n  \"response\": \"Based on the search results, Google faced several challenges in 2019, including increased competition and regulatory scrutiny.\"\n}\n```",
        "**BaseAgent**: The function of BaseAgent is to serve as a foundational class for intelligent agents, providing functionalities for managing conversations, executing searches, and interacting with various tools.\n\n**attributes**: The attributes of this Class.\n· queryDB: A set to store unique search queries made by the agent.  \n· tool_registry: An instance of ToolRegistry that manages the schemas for tools used by the agent.  \n· search_aggregator: An instance of SearchAggregator responsible for managing and executing search queries across multiple search engines.  \n· user_question: A string that holds the current question posed by the user.  \n· training_data: A list that stores the training data related to the agent's interactions and responses.  \n· conversation_manager: An instance of ConversationManager that manages the history of conversations.  \n· memo: A set that stores notes extracted from search results to avoid duplication.\n\n**Code Description**: The BaseAgent class is designed to facilitate the operations of intelligent agents by providing essential functionalities such as managing conversation history, executing search queries, and interacting with various tools. Upon initialization, the class sets up several class-level attributes that are shared across all instances, including a query database (queryDB) to keep track of unique queries, a tool registry (tool_registry) for managing tool schemas, and a search aggregator (search_aggregator) for executing searches across different engines.\n\nThe constructor initializes the prompts directory, configuration settings, and citation database, which is structured as a list of dictionaries to store search queries and their corresponding results. The class also sets up schemas for the content scraper and search aggregator tools, ensuring that they are available for use during the agent's operations.\n\nThe BaseAgent class provides various methods for loading templates, rendering them with data, and facilitating chat interactions. The chat method is overloaded to handle different input types and can return responses in various formats, including strings and structured message objects. The agent can also update its answers based on user feedback and search results, check its confidence in responses, and scrape web content from search results.\n\nThe class is utilized by other components in the project, such as the CriticAgent, which extends BaseAgent to focus on generating critiques of responses. The CriticAgent relies on the functionalities provided by BaseAgent to manage conversations and interact with the user. Additionally, the BaseAgent is called within the process_single_task function, which orchestrates the execution of tasks by coordinating interactions between multiple agents, including the search aggregator and verifier.\n\n**Note**: It is crucial to ensure that the tool registry is populated with the necessary schemas for the tools being used, as the functionality of the BaseAgent depends on these schemas. Proper handling of the conversation history and memoization is also important to maintain an efficient and coherent dialogue experience.\n\n**Output Example**: A possible appearance of the code's return value when executing a chat interaction might look like this:\n```json\n{\n  \"response\": {\n    \"content\": \"The capital of France is Paris.\",\n    \"tool_calls\": null\n  }\n}\n```",
        "**BaseAgent**: The function of BaseAgent is to serve as a foundational class for intelligent agents, managing interactions, search functionalities, and conversation history.\n\n**attributes**: The attributes of this Class.\n· queryDB: A set to store unique queries made by the agent.  \n· tool_registry: An instance of ToolRegistry that manages tool schemas.  \n· search_aggregator: An instance of SearchAggregator that handles search queries across multiple engines.  \n· user_question: A string that holds the user's question for the agent.  \n· training_data: A list that stores the training data related to the conversation.  \n· conversation_manager: An instance of ConversationManager that manages conversation history.  \n· memo: A set to store unique notes extracted from search results.\n\n**Code Description**: The BaseAgent class is designed to facilitate the operations of intelligent agents by providing essential functionalities such as managing conversation history, executing searches, and rendering templates for user interactions. Upon initialization, the class sets up various attributes that are crucial for its operations, including a directory for prompts, a configuration object, and instances of other classes that enhance its capabilities.\n\nThe class includes methods for loading templates, rendering them with data, and managing chat interactions. The `chat` method is overloaded to handle different types of user prompts and tool interactions, ensuring that the agent can respond appropriately based on the context. The `load_template` method retrieves template files from the prompts directory, while the `render_template` method formats these templates using provided data.\n\nThe BaseAgent also integrates with the SearchAggregator to perform searches and gather information from multiple sources. It utilizes the `web_scrape_results` method to extract content from search results and the `search_and_browse` method to manage search queries and web scraping tasks. Additionally, the `taking_notes` method allows the agent to extract and store relevant information from search results, enhancing its ability to recall important details during conversations.\n\nThe relationship between BaseAgent and its callers is significant, as it serves as a core component for various workflows within the project. For instance, the ReverseUpgradeWorkflow class utilizes BaseAgent to manage interactions and execute tasks, while the CriticAgent class extends BaseAgent to provide critique functionalities for responses generated by the agent. The integration with the ConversationManager allows for effective tracking of conversation history, ensuring that all interactions are logged and can be referenced later.\n\n**Note**: It is important to ensure that the configuration settings are correctly set up for the BaseAgent to function effectively. The agent's ability to perform searches and manage conversations relies heavily on the proper initialization of its attributes and the availability of necessary templates and tools.\n\n**Output Example**: A possible appearance of the code's return value when executing a chat interaction might look like this:\n```json\n{\n  \"response\": \"Here is the information you requested based on the search results.\",\n  \"notes\": [\n    {\"note\": \"Important detail about the topic.\", \"citation\": \"http://example.com/detail\"},\n    {\"note\": \"Another relevant fact.\", \"citation\": \"http://example.com/fact\"}\n  ]\n}\n```",
        "**BaseAgent**: The function of BaseAgent is to serve as a foundational class for intelligent agents, managing user interactions, search functionalities, and conversation history.\n\n**attributes**: The attributes of this Class.\n· queryDB: A set that stores unique queries made by the agent.  \n· tool_registry: An instance of ToolRegistry that manages the schemas of tools available to the agent.  \n· search_aggregator: An instance of SearchAggregator that handles search queries across multiple search engines.  \n· user_question: A string that holds the current question posed by the user.  \n· training_data: A list that stores data related to the agent's interactions and responses.  \n· conversation_manager: An instance of ConversationManager that manages the conversation history.  \n· memo: A set that stores notes extracted from search results.\n\n**Code Description**: The BaseAgent class is designed to facilitate the operations of intelligent agents by providing essential functionalities such as managing user queries, performing searches, and maintaining conversation history. Upon initialization, the class sets up various attributes, including a query database, tool registry, search aggregator, and conversation manager.\n\nThe constructor initializes the base directory for prompts, configures the agent's settings, and registers tools that the agent can utilize during its operations. It also sets up schemas for the search aggregator and content scraper, allowing the agent to perform searches and scrape web content effectively.\n\nThe class provides several key methods for interaction:\n- `load_template`: Loads a specified template file from the prompts directory, ensuring that the file exists before reading its content.\n- `render_template`: Renders a template string using provided data, facilitating dynamic content generation.\n- `chat_with_template`: A unified method that combines template loading, rendering, and initiating a chat with the agent, returning the response generated by the model.\n- `chat_with_tools`: Similar to chat_with_template but specifically designed for interactions that involve tools.\n- `chat`: Handles user prompts and tool interactions, returning responses based on the provided input and tools.\n- `update_answer`: Updates the agent's response based on user feedback and search results.\n- `model_confident`: Checks the agent's confidence in answering the current question.\n- `web_scrape_results`: Extracts web content from search results using a web scraper.\n- `search_and_browse`: Executes a search using the search aggregator and processes the results.\n\nThe BaseAgent class is utilized by various components within the project, including the CriticAgent, which extends its functionalities to evaluate and critique responses. The ReverseUpgradeWorkflow also relies on BaseAgent to manage interactions and facilitate the generation of progressively difficult questions. Additionally, the class is integral to the process_single_task function, which orchestrates the execution of user-defined tasks by initializing an agent and processing the task through various steps.\n\n**Note**: It is essential to ensure that the tool registry is properly populated with tools before invoking methods that rely on them. The conversation manager should also be managed carefully to maintain a coherent history of interactions. Proper handling of the user_question attribute is crucial for accurate response generation.\n\n**Output Example**: A possible appearance of the code's return value when interacting with the chat method might look like this:\n```json\n{\n  \"answer\": \"The current status of the project is on track.\",\n  \"support\": [\n    {\n      \"url\": \"https://example.com/project-status\",\n      \"fact\": \"The project is progressing as planned.\"\n    }\n  ]\n}\n```"
      ],
      "code_start_line": 21,
      "code_end_line": 340,
      "params": [],
      "have_return": true,
      "code_content": "class BaseAgent:\n    # Class-level attributes, shared across all instances\n    queryDB = set()  # A set to store queries\n    tool_registry = ToolRegistry()  # Registry for tools\n    search_aggregator = SearchAggregator()\n    user_question = \"\"\n    training_data = []\n    conversation_manager = ConversationManager()\n    memo = set()  # A set to store the gist extracted from the search results\n\n    def __init__(self):\n        base_dir = os.path.dirname(\n            os.path.abspath(__file__)\n        )  # Directory of the current script\n        self.prompts_dir = os.path.join(base_dir, \"prompts\")\n        self.config = settings  # 添加配置访问支持\n        # self.env = Environment(loader=FileSystemLoader(self.prompts_dir))\n        self.search_aggregator = SearchAggregator()\n\n        self.search_aggregator_schema = (\n            BaseAgent.tool_registry.get_or_create_tool_schema(\n                self.search_aggregator.search\n            )\n        )\n\n        self.content_scraper = ContentScraper()\n\n        self.content_scraper_schema = BaseAgent.tool_registry.get_or_create_tool_schema(\n            self.content_scraper.scrape\n        )\n\n        BaseAgent.conversation_manager.available_tools = [\n            self.content_scraper_schema,\n            self.search_aggregator_schema,\n        ]\n\n        # 注册笔记工具schema并加入可用工具\n        note_schemas = BaseAgent.tool_registry.get_or_create_tool_schema(\n            note_save, note_retrieve\n        )\n        BaseAgent.conversation_manager.available_tools.extend(note_schemas)\n\n        self.repeat_turns = 10\n\n    def load_template(self, filename, root_folder=None):\n        \"\"\"\n        Loads a template file from the prompts directory.\n\n        :param filename: The name of the template file to load.\n        :return: The content of the file as a string.\n        \"\"\"\n        if root_folder:\n            # If a root folder is provided, use it to construct the file path\n            filepath = os.path.join(root_folder, filename)\n        else:\n            filepath = os.path.join(self.prompts_dir, filename)\n\n        # Ensure the file exists\n        if not os.path.exists(filepath):\n            raise FileNotFoundError(\n                f\"Template file '{filename}' not found in {self.prompts_dir}\"\n            )\n\n        # Read and return the content of the file\n        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n            return file.read()\n\n    def render_template(self, template_str, data):\n        \"\"\"\n        Render a template using string formatting.\n\n        :param template_str: Template content as a string.\n        :param data: Dictionary of variables to replace in the template.\n        :return: Rendered string.\n        \"\"\"\n        template = Template(template_str)\n        return template.render(**data)\n\n    def chat_with_template(\n        self,\n        template_name: str,\n        template_data: dict,\n        model: str = None,\n        check_prompt: bool = False,\n        root_folder: str = None,\n        save_history: bool = True,\n    ) -> str:\n        \"\"\"Unified helper method to handle template rendering and chat calling\n\n        Args:\n            template_name: Name of template file\n            template_data: Data to render template with\n            model: Optional model override\n\n        Returns:\n            Chat response content\n        \"\"\"\n        template = self.load_template(template_name, root_folder=root_folder)\n        rendered_prompt = self.render_template(template, template_data)\n\n        if check_prompt:\n            printer.log(f\"Full Rendered Prompt:\\n{rendered_prompt}\")\n            \n        return self.chat(\n            usr_prompt=rendered_prompt,\n            model=model or settings.default_model,\n            save_history=save_history,\n        )\n\n    def chat_with_tools(\n        self, template_name: str, template_data: dict, tools: List, model: str = None\n    ) -> ChatCompletionMessage:\n        \"\"\"Helper method for chat with tools\"\"\"\n        template = self.load_template(template_name)\n        rendered_prompt = self.render_template(template, template_data)\n        return self.chat(\n            usr_prompt=rendered_prompt,\n            tools=tools,\n            model=model or settings.default_model,\n        )\n\n    @overload\n    def chat(\n        self, usr_prompt: List, tools: None = None\n    ) -> ChatCompletionMessage: ...\n\n    @overload\n    def chat(self, usr_prompt: str, tools: List) -> ChatCompletionMessage: ...\n\n    @overload\n    def chat(self, usr_prompt: str, tools: None = None) -> str: ...\n\n    def chat(\n        self,\n        usr_prompt: str | List,\n        tools: Optional[List] = None,\n        role: str = \"assistant\",\n        model: str = settings.default_model,  # 默认使用配置文件中的默认模型\n        save_history: bool = True,\n    ) -> ChatCompletionMessage | str | None:\n        llm_response = call_llm(\n            model=model,  # 使用传入的model / 默认model\n            usr_prompt=usr_prompt,\n            config=settings,\n            tools=tools,\n        )\n\n        if tools is not None:\n            return llm_response\n\n        if save_history:\n            BaseAgent.conversation_manager.append_to_history(\n                role=role, content=llm_response.content\n            )\n\n        return llm_response.content\n\n    def update_answer(self, query, previous_answer, search_results, critic_feedback):\n        data = {\n            \"query\": query,\n            \"previous_answer\": previous_answer,\n            \"search_results\": search_results,\n            \"critic_feedback\": critic_feedback,\n        }\n\n        agent_update_answer_prompt = self.load_template(\"agent_update_answer.txt\")\n        rendered_prompt = self.render_template(agent_update_answer_prompt, data)\n\n        agent_update_answer_response = self.chat(usr_prompt=rendered_prompt)\n\n        return agent_update_answer_response\n\n    def model_confident(self, query):\n        \"\"\"\n        检查模型是否对当前问题有信心。\n        \"\"\"\n        data = {\"user_question\": query}\n        agent_confidence_prompt = self.load_template(\"agent_confidence.txt\")\n\n        rendered_prompt = self.render_template(agent_confidence_prompt, data)\n        agent_confidence_response = self.chat(usr_prompt=rendered_prompt)\n\n        return agent_confidence_response\n\n    def web_scrape_results(self, search_results: str) -> str | None:\n        \"\"\"Extract web content from search results using web scraper\n\n        Args:\n            search_results: Initial search results to scrape from\n\n        Returns:\n            Scraped web content or None if scraping failed\n        \"\"\"\n        web_scraper_prompt = self.load_template(\"web_scraper.txt\")\n        web_scraper_rendered_prompt = self.render_template(\n            web_scraper_prompt,\n            {\n                \"user_question\": self.user_question,\n                \"initial_search_results\": search_results,\n            },\n        )\n\n        # Interact with the model for web scraping\n        web_scraper_response = self.chat(\n            usr_prompt=web_scraper_rendered_prompt,\n            tools=self.content_scraper_schema,\n        )\n\n        # If no tool calls, return the response immediately\n        if web_scraper_response.tool_calls is None:\n            return web_scraper_response.content\n\n        BaseAgent.conversation_manager.append_tool_call_to_history(\n            web_scraper_response.tool_calls\n        )\n\n        final_web_scraper_results = \"\"\n\n        for tool_call in web_scraper_response.tool_calls:\n            urls = json.loads(tool_call.function.arguments).get(\"urls\", [])\n            web_scraper_results = asyncio.run(self.content_scraper.scrape(urls=urls))\n            BaseAgent.conversation_manager.append_tool_call_result_to_history(\n                tool_call_id=tool_call.id,\n                name=\"scrape\",\n                content=web_scraper_results,\n            )\n            final_web_scraper_results += web_scraper_results\n\n        return final_web_scraper_results\n\n    def search_and_browse(self, rendered_prompt) -> str | None:\n        search_with_tool_response = self.chat(\n            usr_prompt=rendered_prompt, tools=self.search_aggregator_schema\n        )\n\n        printer.log(f\"search_with_tool_response:\\n{search_with_tool_response}\")\n\n        # If no tool calls, return the response immediately\n        if search_with_tool_response.tool_calls is None:\n            return search_with_tool_response.content\n\n        BaseAgent.conversation_manager.append_tool_call_to_history(\n            search_with_tool_response.tool_calls\n        )\n\n        final_search_results = \"\"\n\n        for tool_call in search_with_tool_response.tool_calls:\n            query = json.loads(tool_call.function.arguments).get(\"query\", \"\")\n\n            search_results = asyncio.run(self.search_aggregator.search(query=query))\n\n            time.sleep(0.2)\n\n            BaseAgent.conversation_manager.append_tool_call_result_to_history(\n                tool_call_id=tool_call.id,\n                name=\"search\",\n                content=search_results,\n            )\n\n            BaseAgent.queryDB.update(query)\n\n            final_search_results += f\"{search_results}\"\n\n        return self.web_scrape_results(final_search_results)\n    \n    def extract_and_validate_yaml(self, model_response):\n        match = re.search(r\"```yaml\\n([\\s\\S]*?)\\n```\", model_response, re.DOTALL)\n        if not match:\n            return None  # 如果没有找到匹配的内容，返回None\n\n        model_response = match.group(1).strip()\n        try:\n            # 尝试解析YAML内容\n            parsed_yaml = yaml.safe_load(model_response)\n            return yaml.dump(parsed_yaml, default_flow_style=False)\n\n        except yaml.YAMLError as exc:\n            print(f\"Invalid YAML content: {exc}\")\n            return None\n\n    def receive_task(self, task):\n        \"\"\"\n        接收原始任务。\n        \"\"\"\n        self.original_task = task\n\n    def extract_and_validate_json(self, model_response):\n        # Try to extract JSON data wrapped in ```json``` blocks\n        # and return the parsed JSON content\n        match = re.search(r\"```json\\n([\\s\\S]*?)\\n```\", model_response, re.DOTALL)\n        if match:\n            json_content = match.group(1).strip()\n        else:\n            json_content = model_response.strip()\n\n        try:\n            parsed_json = json.loads(json_content, encoding=\"utf-8\")\n            return parsed_json\n\n        except json.JSONDecodeError as exc:\n            print(f\"Invalid JSON content: {exc}\")\n            return None\n\n    def taking_notes(self, web_results):\n        \"\"\"从搜索结果中提取信息并记录。\"\"\"\n        result = self.chat_with_template(\n            template_name=\"taking_notes.txt\",\n            template_data={\"search_result\": web_results, \"TASK\": self.original_task, \"previous_notes\": self.memo},\n        )\n        notes = extract_notes(result)\n        if isinstance(notes, list) and notes:\n            # 先转换成集合进行自动去重，然后更新到memo中\n            new_notes = set(notes)  # 使用set自动去重\n            printer.rule(\"New notes\"); printer.print(new_notes)\n            self.memo.update(new_notes)\n            return list(new_notes)\n        else:\n            printer.print(\"No new notes.\")\n            return []\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/__init__",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/cli",
        "src/criticsearch/abstract_substitution/abs_workflow.py",
        "src/criticsearch/critic_agent.py",
        "src/criticsearch/critic_agent.py/CriticAgent",
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/generate_content_for_section",
        "src/criticsearch/main.py/_model_action_decision",
        "src/criticsearch/main.py/_action_router",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/instruction_generator.py",
        "src/criticsearch/reportbench/instruction_generator.py/InstructionGenerator/__init__",
        "src/criticsearch/reportbench/process_search_result.py",
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/__init__",
        "src/criticsearch/reportbench/test_llm.py",
        "src/criticsearch/reportbench/test_llm.py/test_llm",
        "src/criticsearch/session.py",
        "src/criticsearch/session.py/Session/__init__",
        "src/criticsearch/tasks_runner.py",
        "src/criticsearch/tasks_runner.py/execute_multiple_tasks",
        "src/criticsearch/tasks_runner.py/execute_from_mapping/run",
        "src/criticsearch/workflow.py",
        "src/criticsearch/workflow.py/WorkflowExecutor/__init__",
        "src/criticsearch/workflow.py/iterate_traj",
        "tests/test_chat_functionality.py",
        "tests/test_chat_functionality.py/test_chat_without_tools_returns_str",
        "tests/test_chat_functionality.py/test_chat_with_tools_returns_message_object",
        "tests/test_chat_integration.py",
        "tests/test_chat_integration.py/test_real_model_response",
        "tests/test_conversation_format_saving.py",
        "tests/test_conversation_format_saving.py/clear_history_and_stub",
        "tests/test_conversation_format_saving.py/test_conversation_roundtrip",
        "tests/test_taking_notes.py",
        "tests/test_taking_notes.py/test_taking_notes_integration"
      ],
      "reference_who": [
        "src/criticsearch/models.py/ConversationManager",
        "src/criticsearch/tools/tool_registry.py/ToolRegistry",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the BaseAgent class, setting up necessary directories, databases, and tools for the agent's operation.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The __init__ method is the constructor for the BaseAgent class. It is responsible for initializing various components that the agent will use during its operation. \n\n1. **Directory Setup**: The method begins by determining the base directory of the current script using `os.path.dirname(os.path.abspath(__file__))`. This directory is essential for locating resources related to the agent, specifically the prompts directory, which is constructed by joining the base directory with the string \"prompts\".\n\n2. **Citation Database Initialization**: The `citationDB` attribute is initialized as a list containing a single dictionary. This dictionary is designed to store search queries as keys and their corresponding results as values. The comment indicates that only search results praised by a critic will be included in this database. Each entry in the citationDB is structured to hold a unique document identifier along with its associated metadata, such as URL, title, and content.\n\n3. **Search Aggregator Setup**: An instance of the `SearchAggregator` class is created and assigned to the `search_aggregator` attribute. This component is responsible for managing search queries across multiple search engines.\n\n4. **Tool Schema Creation**: The method retrieves or creates schemas for the search aggregator and content scraper tools using the `get_or_create_tool_schema` method from the `tool_registry`. This method ensures that the schemas for these tools are registered and available for use in the agent's operations. The schemas are stored in `search_aggregator_schema` and `content_scraper_schema` attributes, respectively.\n\n5. **Content Scraper Initialization**: An instance of the `ContentScraper` class is created and assigned to the `content_scraper` attribute. This component is responsible for scraping content from specified URLs.\n\n6. **Updating Available Tools**: The method updates the `available_tools` attribute of the `conversation_manager` class variable in BaseAgent to include the schemas for both the content scraper and the search aggregator. This allows the agent to utilize these tools during interactions.\n\n7. **Repeat Turns Configuration**: Finally, the `repeat_turns` attribute is initialized to 10, which likely indicates the number of times the agent can repeat a search or interaction before stopping.\n\nThe __init__ method is crucial for setting up the BaseAgent instance with all necessary components and configurations, ensuring that it is ready to perform its intended functions effectively.\n\n**Note**: When using this method, it is important to ensure that the required directories and tools are correctly set up in the environment. Additionally, the proper functioning of the search aggregator and content scraper depends on the availability of their respective configurations and API keys.",
        "**__init__**: The function of __init__ is to initialize the BaseAgent class and set up its necessary components and attributes.\n\n**parameters**: The parameters of this Function.\n· No parameters are required for this function.\n\n**Code Description**: The __init__ method of the BaseAgent class is responsible for setting up the initial state of an instance of the class. It begins by determining the directory of the current script using `os.path.dirname(os.path.abspath(__file__))`, which is essential for locating the prompts directory. The prompts directory is then constructed by joining the base directory with the \"prompts\" folder, allowing the agent to access necessary prompt files.\n\nThe method assigns the `settings` object to the `self.config` attribute, providing access to configuration settings throughout the class. It initializes `self.citationDB`, which is a list containing a dictionary that will store search queries and their corresponding results. This database is designed to hold only those search results that have received positive feedback from a critic, ensuring that only valuable information is retained.\n\nNext, an instance of `SearchAggregator` is created and assigned to `self.search_aggregator`. This component is responsible for managing and aggregating search queries across multiple search engines, thus enhancing the agent's search capabilities. The method then retrieves or creates the tool schema for the search aggregator's search function using the `BaseAgent.tool_registry.get_or_create_tool_schema` method. This ensures that the necessary schema for the search functionality is available for use.\n\nFollowing this, an instance of `ContentScraper` is initialized and assigned to `self.content_scraper`. The content scraper is crucial for extracting content from URLs, which is a key part of the agent's functionality. Similar to the search aggregator, the method retrieves or creates the tool schema for the content scraper's scrape function, ensuring that the agent can effectively utilize this tool.\n\nThe available tools for the conversation manager are then updated to include the schemas for both the content scraper and the search aggregator. This integration allows the agent to utilize these tools during its operations, enhancing its ability to gather and process information.\n\nFinally, the method sets `self.repeat_turns` to 10, which likely indicates the number of times the agent will repeat certain actions or queries during its operation.\n\nOverall, the __init__ method establishes the foundational components of the BaseAgent, enabling it to perform searches, scrape content, and manage its interactions effectively.\n\n**Note**: When using the BaseAgent class, ensure that the necessary configurations are set up correctly, particularly the settings for the search engines and the content scraping service. The proper initialization of the search aggregator and content scraper is crucial for the agent's functionality.",
        "**__init__**: The function of __init__ is to initialize an instance of the BaseAgent class, setting up necessary directories, configurations, and tool schemas for content scraping and search aggregation.\n\n**parameters**: The parameters of this Function.\n· No parameters are required for this function.\n\n**Code Description**: The __init__ method is the constructor for the BaseAgent class. It performs several critical initialization tasks upon the creation of a BaseAgent instance. \n\nFirst, it determines the directory of the current script using `os.path.dirname(os.path.abspath(__file__))`, which allows the agent to locate its resources relative to its file location. It then constructs the path for the prompts directory by joining the base directory with the \"prompts\" subdirectory.\n\nNext, the method assigns the `settings` object to the `self.config` attribute, enabling access to configuration settings throughout the BaseAgent instance. This is crucial for managing various operational parameters and API keys required by the agent.\n\nThe method then initializes an instance of the `SearchAggregator` class, which is responsible for managing search queries across multiple search engines. This instance is stored in the `self.search_aggregator` attribute, allowing the BaseAgent to perform search operations.\n\nFollowing this, the method retrieves or creates a tool schema for the search aggregator's search function using the `get_or_create_tool_schema` method from the `BaseAgent.tool_registry`. This ensures that the necessary schema for the search functionality is available for the agent's operations.\n\nThe `ContentScraper` class is also instantiated, and its instance is assigned to `self.content_scraper`. This class is responsible for scraping content from URLs, and its schema is similarly created and registered using the `get_or_create_tool_schema` method.\n\nThe available tools for the conversation manager are then updated to include the schemas for both the content scraper and the search aggregator. This integration is essential for the agent's ability to utilize these tools during its execution.\n\nFinally, the method sets the `self.repeat_turns` attribute to 10, which likely indicates the number of times the agent will repeat certain actions or queries during its operation.\n\nOverall, the __init__ method establishes the foundational components and configurations necessary for the BaseAgent to function effectively within the broader application context, facilitating both content scraping and search capabilities.\n\n**Note**: It is important to ensure that the settings are correctly configured, particularly the API keys for any external services used by the SearchAggregator and ContentScraper. Additionally, the proper functioning of the tool schemas is critical for the agent's operational capabilities.",
        "**__init__**: The function of __init__ is to initialize an instance of the BaseAgent class, setting up necessary directories, configurations, and tool schemas for content scraping and search aggregation.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The __init__ method is the constructor for the BaseAgent class. It begins by determining the directory of the current script using `os.path.dirname(os.path.abspath(__file__))`. This directory is crucial as it establishes the base path for accessing other resources within the project.\n\nNext, the method sets up the `prompts_dir` attribute by joining the base directory with the \"prompts\" folder. This directory is intended to store prompt templates that the agent may utilize during its operation.\n\nThe `config` attribute is initialized with the settings, which provides access to various configuration parameters required for the agent's functionality. This includes API keys and other necessary configurations.\n\nAn instance of the `SearchAggregator` class is created and assigned to the `search_aggregator` attribute. The `SearchAggregator` is responsible for managing search queries across multiple search engines, allowing the BaseAgent to perform searches effectively.\n\nThe method then retrieves or creates a tool schema for the `search` method of the `search_aggregator` using the `get_or_create_tool_schema` method from the `tool_registry`. This ensures that the search functionality is properly registered and can be utilized by the agent.\n\nFollowing this, an instance of the `ContentScraper` class is created and assigned to the `content_scraper` attribute. The `ContentScraper` is designed to scrape content from provided URLs, utilizing both API-based extraction and fallback web scraping methods.\n\nSimilar to the search aggregator, the method retrieves or creates a tool schema for the `scrape` method of the `content_scraper` and registers it in the tool registry. This registration allows the BaseAgent to use the content scraping functionality seamlessly.\n\nThe available tools for the conversation manager are then updated to include the schemas for both the content scraper and the search aggregator. This integration ensures that the agent can access these tools during its operations.\n\nAdditionally, the method registers note tools schemas for saving and retrieving notes, extending the capabilities of the agent to manage notes effectively.\n\nFinally, the `repeat_turns` attribute is initialized to a default value of 10, which likely indicates the number of times the agent can repeat its actions or queries during its operation.\n\nOverall, the __init__ method establishes the foundational components and configurations necessary for the BaseAgent to function effectively within the project, enabling it to perform searches, scrape content, and manage notes.\n\n**Note**: It is important to ensure that the settings are correctly configured, particularly the API keys for the search engines and content scraping services. The proper initialization of the BaseAgent is crucial for its successful operation within the system."
      ],
      "code_start_line": 31,
      "code_end_line": 63,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        base_dir = os.path.dirname(\n            os.path.abspath(__file__)\n        )  # Directory of the current script\n        self.prompts_dir = os.path.join(base_dir, \"prompts\")\n        self.config = settings  # 添加配置访问支持\n        # self.env = Environment(loader=FileSystemLoader(self.prompts_dir))\n        self.search_aggregator = SearchAggregator()\n\n        self.search_aggregator_schema = (\n            BaseAgent.tool_registry.get_or_create_tool_schema(\n                self.search_aggregator.search\n            )\n        )\n\n        self.content_scraper = ContentScraper()\n\n        self.content_scraper_schema = BaseAgent.tool_registry.get_or_create_tool_schema(\n            self.content_scraper.scrape\n        )\n\n        BaseAgent.conversation_manager.available_tools = [\n            self.content_scraper_schema,\n            self.search_aggregator_schema,\n        ]\n\n        # 注册笔记工具schema并加入可用工具\n        note_schemas = BaseAgent.tool_registry.get_or_create_tool_schema(\n            note_save, note_retrieve\n        )\n        BaseAgent.conversation_manager.available_tools.extend(note_schemas)\n\n        self.repeat_turns = 10\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/tool_registry.py/ToolRegistry/get_or_create_tool_schema",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "load_template",
      "md_content": [
        "## Function: `load_template`\n\n### Overview:\nThe `load_template` function is responsible for loading a template file from a predefined directory and returning its contents as a string. This method is particularly useful for retrieving text-based templates that can be rendered or processed further in the application.\n\n### Method Signature:\n```python\ndef load_template(self, filename)\n```\n\n### Parameters:\n- **`filename`** (str): The name of the template file to be loaded. This should be a valid filename present within the specified prompts directory.\n\n### Returns:\n- **str**: The content of the template file as a string.\n\n### Raises:\n- **FileNotFoundError**: If the specified template file does not exist in the prompts directory, this error is raised with a descriptive message indicating the missing file.\n\n### Detailed Description:\nThe `load_template` function performs the following steps:\n1. It constructs the full file path by combining the predefined prompts directory (`self.prompts_dir`) with the provided `filename`.\n2. It checks if the file exists at the constructed path.\n3. If the file does not exist, it raises a `FileNotFoundError` with a detailed message about the missing file.\n4. If the file is found, it opens the file in read mode, reads its content, and returns the content as a string.\n\nThis function ensures that template files are loaded correctly, and if a template file is missing, the user is informed with a clear error message.\n\n### Example Usage:\n```python\ntemplate_content = self.load_template(\"example_template.txt\")\n```\n\nIn this example, the function loads the content of `example_template.txt` from the predefined directory and stores it in the `template_content` variable for further use.",
        "**load_template**: The function of load_template is to load a template file from the prompts directory.\n\n**parameters**: The parameters of this Function.\n· filename: The name of the template file to load.\n\n**Code Description**: The load_template function is designed to retrieve the content of a specified template file from a predefined directory, referred to as prompts_dir. The function takes a single parameter, filename, which represents the name of the template file to be loaded.\n\nThe function constructs the full file path by joining the prompts_dir with the provided filename using the os.path.join method. It then checks if the constructed file path exists using os.path.exists. If the file does not exist, a FileNotFoundError is raised, providing a clear error message indicating that the specified template file could not be found in the prompts directory.\n\nIf the file exists, the function proceeds to open the file in read mode with UTF-8 encoding. It reads the content of the file and returns it as a string. This functionality is essential for the BaseAgent class, as it allows the agent to dynamically load templates that are used for generating prompts and responses in various interactions.\n\nThe load_template function is called by several other methods within the BaseAgent class, including chat_with_template, chat_with_tools, and update_answer. These methods rely on load_template to fetch the appropriate template files needed for rendering prompts based on user queries and other contextual data. For example, in the chat_with_template method, load_template is used to retrieve the template specified by template_name, which is then rendered with the provided template_data to create a prompt for the chat model.\n\n**Note**: It is crucial to ensure that the filename passed to the load_template function corresponds to an existing template file in the prompts directory. Failure to do so will result in a FileNotFoundError, which will halt the execution of the calling function until the issue is resolved.\n\n**Output Example**: A possible return value from the load_template function could be a string containing the content of the template file, such as:\n```\n\"Hello, {{ user_name }}! How can I assist you today?\"\n```",
        "**load_template**: The function of load_template is to load a template file from the prompts directory.\n\n**parameters**: The parameters of this Function.\n· filename: The name of the template file to load.  \n· root_folder: An optional parameter that specifies a custom root folder from which to load the template file.\n\n**Code Description**: The load_template function is a method within the BaseAgent class that is responsible for loading a specified template file from a designated directory. The function accepts two parameters: `filename`, which is the name of the template file to be loaded, and `root_folder`, which is an optional parameter that allows the caller to specify a different directory from which to load the file.\n\nWhen the function is invoked, it first checks if the `root_folder` parameter is provided. If it is, the function constructs the file path by joining the `root_folder` with the `filename`. If `root_folder` is not provided, it defaults to using the `prompts_dir` attribute of the BaseAgent instance to construct the file path.\n\nThe function then verifies the existence of the constructed file path using `os.path.exists()`. If the file does not exist, it raises a `FileNotFoundError`, providing a clear error message that indicates the missing template file. If the file exists, the function proceeds to open the file in read mode with UTF-8 encoding and returns its content as a string.\n\nThe load_template function is called by several other methods within the BaseAgent class, such as `chat_with_template`, `chat_with_tools`, `update_answer`, and `model_confident`. Each of these methods relies on load_template to retrieve the appropriate template content necessary for generating prompts or responses. For instance, `chat_with_template` uses load_template to load a specified template file that will be rendered with user-provided data, while `update_answer` uses it to load a template that structures the agent's response based on user feedback and search results.\n\n**Note**: It is essential to ensure that the filename provided corresponds to an existing template file in the specified directory. If the file is not found, a FileNotFoundError will be raised, which must be handled appropriately by the calling functions to avoid runtime errors.\n\n**Output Example**: A possible return value from the load_template function could be a string containing the content of the loaded template, such as:\n```\n\"Hello, {user_name}! How can I assist you today?\"\n```",
        "**load_template**: The function of load_template is to load a template file from the prompts directory.\n\n**parameters**: The parameters of this Function.\n· filename: The name of the template file to load.  \n· root_folder: An optional parameter specifying a custom root folder from which to load the template file.\n\n**Code Description**: The load_template function is a method within the BaseAgent class that is responsible for loading a specified template file from a designated directory. The function accepts two parameters: `filename`, which is the name of the template file to be loaded, and an optional `root_folder`, which allows for the specification of a custom directory from which to load the file.\n\nWhen the function is invoked, it first checks if the `root_folder` parameter is provided. If it is, the function constructs the file path by joining the `root_folder` with the `filename`. If `root_folder` is not specified, the function defaults to using the `prompts_dir` attribute of the BaseAgent class to construct the file path.\n\nThe function then checks for the existence of the constructed file path using `os.path.exists()`. If the file does not exist, a FileNotFoundError is raised, indicating that the specified template file could not be found in the expected directory. This ensures that the caller is immediately informed of any issues related to missing template files.\n\nIf the file exists, the function proceeds to open the file in read mode with UTF-8 encoding and reads its content. The content of the file is then returned as a string.\n\nThe load_template function is called by several other methods within the BaseAgent class, including chat_with_template, chat_with_tools, update_answer, model_confident, and web_scrape_results. Each of these methods relies on load_template to retrieve the appropriate template file necessary for generating prompts or responses. For instance, the chat_with_template method uses load_template to load the specified template file before rendering it with data, while the update_answer method uses it to load a specific template for generating an updated response based on user feedback.\n\n**Note**: It is essential to ensure that the `filename` corresponds to an existing template file in the specified directory. If the `root_folder` is provided, it must be a valid path. Additionally, the function will raise a FileNotFoundError if the specified template file cannot be found, which should be handled appropriately by the caller.\n\n**Output Example**: A possible return value from the load_template function could be a string containing the content of the loaded template, such as:\n```\n\"Hello, {{ user_name }}! Welcome to our service.\"\n```"
      ],
      "code_start_line": 65,
      "code_end_line": 86,
      "params": [
        "self",
        "filename",
        "root_folder"
      ],
      "have_return": true,
      "code_content": "    def load_template(self, filename, root_folder=None):\n        \"\"\"\n        Loads a template file from the prompts directory.\n\n        :param filename: The name of the template file to load.\n        :return: The content of the file as a string.\n        \"\"\"\n        if root_folder:\n            # If a root folder is provided, use it to construct the file path\n            filepath = os.path.join(root_folder, filename)\n        else:\n            filepath = os.path.join(self.prompts_dir, filename)\n\n        # Ensure the file exists\n        if not os.path.exists(filepath):\n            raise FileNotFoundError(\n                f\"Template file '{filename}' not found in {self.prompts_dir}\"\n            )\n\n        # Read and return the content of the file\n        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n            return file.read()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_tools",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/critic_agent.py/CriticAgent/__init__",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_factualqa",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section_with_models/process_with_model",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/verify_qa_format"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "render_template",
      "md_content": [
        "**render_template**: The function of render_template is to render a template using string formatting.\n\n**parameters**: The parameters of this Function.\n· template_str: Template content as a string.  \n· data: Dictionary of variables to replace in the template.  \n\n**Code Description**: The render_template function is designed to take a string representation of a template and a dictionary of data, which contains key-value pairs that will be used to replace placeholders in the template. The function utilizes the Template class to create a template object from the provided template string. It then calls the render method on this template object, unpacking the data dictionary to replace the placeholders with the corresponding values. The result is a fully rendered string that incorporates the provided data.\n\nThis function is called by various methods within the project, specifically in the ReportBenchmark and ReportEvaluation classes. For instance, in the attempt method of the ReportBenchmark class, render_template is used to create a prompt for a chat interaction by loading a specific template and populating it with relevant data such as wiki_text and UserQuery. Similarly, in the run_factualqa method, it renders a template for FactualQA evaluation, incorporating user queries and ground truth data. The rendered output is then used to generate responses from a common chat function, which is a critical part of the application's functionality.\n\nIn the ReportEvaluation class, methods like examinees_outline_generation and evaluate_factualqa also leverage render_template to prepare prompts for generating outlines and evaluating factual questions, respectively. Each of these methods relies on the rendered output to facilitate interactions with the chat system, ensuring that the responses are contextually relevant and tailored to the specific queries being addressed.\n\n**Note**: It is important to ensure that the data dictionary passed to the render_template function contains all necessary keys that correspond to placeholders in the template string. Failure to provide the correct keys may result in rendering errors or incomplete output.\n\n**Output Example**: An example of the rendered output might look like this:\n\nIf the template_str is \"Hello, {{ UserQuery }}! Here is your information: {{ wiki_text }}.\" and the data is {\"UserQuery\": \"What is AI?\", \"wiki_text\": \"AI stands for Artificial Intelligence.\"}, the output would be:\n\n\"Hello, What is AI?! Here is your information: AI stands for Artificial Intelligence.\"",
        "**render_template**: The function of render_template is to render a template using string formatting.\n\n**parameters**: The parameters of this Function.\n· template_str: Template content as a string.  \n· data: Dictionary of variables to replace in the template.  \n\n**Code Description**: The render_template function is designed to take a template string and a dictionary of data, and then produce a formatted string by replacing placeholders in the template with corresponding values from the data dictionary. It utilizes the Template class to create a template object from the provided template string. The method then calls the render function on this template object, unpacking the data dictionary to replace the placeholders with actual values.\n\nThis function is called by several other methods within the BaseAgent class, including chat_with_template, chat_with_tools, and update_answer. Each of these methods relies on render_template to generate a prompt that is sent to a conversational model. For instance, in chat_with_template, the method first loads a template based on the template_name provided, then uses render_template to format this template with the provided template_data before passing the rendered prompt to the common_chat method for interaction with the model. Similarly, chat_with_tools and update_answer also use render_template to prepare prompts that incorporate dynamic data, ensuring that the responses generated by the model are contextually relevant and tailored to the specific interaction.\n\nThe render_template function is crucial for maintaining the flexibility and adaptability of the agent's responses, allowing it to generate contextually appropriate prompts based on varying inputs.\n\n**Note**: It is essential to ensure that the template_str provided is correctly formatted and that the data dictionary contains all necessary keys corresponding to the placeholders in the template. Failure to do so may result in runtime errors during the rendering process.\n\n**Output Example**: A possible return value from the render_template function could be a string such as \"Hello, John! Your balance is $100.\" if the template_str was \"Hello, {name}! Your balance is ${balance}.\" and the data dictionary was {\"name\": \"John\", \"balance\": 100}.",
        "**render_template**: The function of render_template is to render a template using string formatting.\n\n**parameters**: The parameters of this Function.\n· template_str: Template content as a string.  \n· data: Dictionary of variables to replace in the template.  \n\n**Code Description**: The render_template function is a method within the BaseAgent class that facilitates the rendering of a template by substituting placeholders in the template string with actual values provided in a dictionary. This function takes two parameters: `template_str`, which contains the template content, and `data`, which is a dictionary mapping variable names to their corresponding values.\n\nThe function begins by creating a Template object from the provided `template_str`. It then calls the `render` method on this Template object, passing the unpacked `data` dictionary as keyword arguments. This process replaces any placeholders in the template with the values from the `data` dictionary, resulting in a fully rendered string that reflects the provided data.\n\nThe render_template function is called by several other methods within the BaseAgent class, including chat_with_template, chat_with_tools, update_answer, and model_confident. Each of these methods relies on render_template to generate contextually relevant prompts for interaction with a conversational model or to refine responses based on user queries and feedback.\n\nFor instance, in the chat_with_template method, render_template is used to format the prompt that will be sent to the conversational model by replacing placeholders with actual data from the `template_data` dictionary. Similarly, in the update_answer method, render_template is employed to create a prompt that incorporates user feedback and search results, ensuring that the agent's response is informed and relevant.\n\nThe render_template function is crucial for maintaining the dynamic nature of the agent's interactions, allowing for the generation of tailored prompts that enhance the quality and relevance of the responses provided by the conversational model.\n\n**Note**: It is essential to ensure that the `template_str` provided is a valid template and that the `data` dictionary contains all necessary keys corresponding to the placeholders in the template. Failure to do so may result in errors during the rendering process.\n\n**Output Example**: A possible return value from the render_template function could be a string such as:\n```\n\"Hello, John! Your balance is $100.\"\n```",
        "**render_template**: The function of render_template is to render a template using string formatting.\n\n**parameters**: The parameters of this Function.\n· template_str: Template content as a string.  \n· data: Dictionary of variables to replace in the template.  \n\n**Code Description**: The render_template function is a method within the BaseAgent class that facilitates the rendering of a template by substituting placeholders with actual values from a provided data dictionary. This function takes two parameters: `template_str`, which is the string content of the template, and `data`, which is a dictionary containing the variables that will replace the placeholders in the template.\n\nUpon invocation, the function first creates a Template object using the provided `template_str`. It then calls the `render` method on this Template object, passing the unpacked `data` dictionary as keyword arguments. This process effectively replaces any placeholders in the template with the corresponding values from the `data` dictionary, resulting in a fully rendered string that reflects the provided context.\n\nThe render_template function is called by other methods within the BaseAgent class, such as chat_with_template and chat_with_tools. These methods rely on render_template to generate prompts that are sent to conversational models. For instance, in the chat_with_template function, the rendered prompt is created by loading a template file and formatting it with user-provided data before sending it to the model for a response. Similarly, the chat_with_tools function uses render_template to prepare prompts that incorporate specific tools into the conversation.\n\nThe render_template function is essential for ensuring that templates are dynamically populated with relevant data, allowing for flexible and context-aware interactions with the conversational model. Its ability to handle various templates and data structures makes it a critical component of the overall functionality of the BaseAgent class.\n\n**Note**: It is important to ensure that the `template_str` is a valid template and that the `data` dictionary contains all necessary keys that correspond to the placeholders in the template. Failure to provide the correct keys may result in rendering errors.\n\n**Output Example**: A possible return value from the render_template function could be a string such as:\n```\n\"Hello, John! Your order number is 12345.\"\n```"
      ],
      "code_start_line": 88,
      "code_end_line": 97,
      "params": [
        "self",
        "template_str",
        "data"
      ],
      "have_return": true,
      "code_content": "    def render_template(self, template_str, data):\n        \"\"\"\n        Render a template using string formatting.\n\n        :param template_str: Template content as a string.\n        :param data: Dictionary of variables to replace in the template.\n        :return: Rendered string.\n        \"\"\"\n        template = Template(template_str)\n        return template.render(**data)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_tools",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_factualqa",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section_with_models/process_with_model",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/verify_qa_format"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "chat_with_template",
      "md_content": [
        "**chat_with_template**: The function of chat_with_template is to provide a unified method for rendering templates and facilitating chat interactions with a conversational model.\n\n**parameters**: The parameters of this Function.\n· template_name: The name of the template file to be used for rendering the prompt.  \n· template_data: A dictionary containing the data that will be used to populate the template.  \n· model: An optional parameter that allows for overriding the default model used for the chat interaction.\n\n**Code Description**: The chat_with_template function is a method within the BaseAgent class that serves as a comprehensive utility for managing interactions with a conversational model through template rendering. This function first loads a specified template file using the load_template method, which retrieves the content of the template from a predefined directory. The template_name parameter is passed to load_template to identify the correct file.\n\nOnce the template is loaded, the function utilizes the render_template method to format the template content by replacing placeholders with actual values from the template_data dictionary. This step is crucial as it ensures that the prompt sent to the conversational model is contextually relevant and tailored to the user's input.\n\nAfter rendering the prompt, chat_with_template calls the common_chat method, passing the rendered prompt along with the specified model (or the default model if none is provided). The common_chat method is responsible for sending the prompt to the conversational model and retrieving the generated response.\n\nThe chat_with_template function is invoked in various contexts within the project, such as in the process_single_task function, where it is used to assess the confidence of the agent and to generate responses based on user queries. For example, it is called to create prompts for evaluating the agent's confidence level and for generating direct responses to user tasks. Additionally, it is utilized in the ReportVerifier class to verify questions against factual data, demonstrating its versatility and importance in ensuring accurate and context-aware interactions with the conversational model.\n\n**Note**: It is essential to ensure that the template_name provided corresponds to an existing template file in the prompts directory, and that the template_data dictionary contains all necessary keys for rendering. Failure to do so may result in errors during the template loading or rendering process.\n\n**Output Example**: A possible return value from the chat_with_template function could be a string such as:\n```\n\"Hello, John! How can I assist you today?\"\n```",
        "**chat_with_template**: The function of chat_with_template is to provide a unified method for rendering templates and interacting with a conversational model.\n\n**parameters**: The parameters of this Function.\n· template_name: The name of the template file to be loaded and rendered.  \n· template_data: A dictionary containing the data to be used for rendering the template.  \n· model: An optional parameter that allows for overriding the default model used for generating responses.  \n· check_prompt: A boolean flag indicating whether to log the full rendered prompt for debugging purposes.  \n· root_folder: An optional parameter specifying a custom root folder from which to load the template file.\n\n**Code Description**: The chat_with_template function is a method within the BaseAgent class designed to streamline the process of loading a template, rendering it with provided data, and then sending the rendered prompt to a conversational model for a response. \n\nWhen invoked, the function first calls the load_template method to retrieve the specified template file. The template_name and root_folder parameters guide this loading process. Once the template is successfully loaded, the function utilizes the render_template method to format the template with the provided template_data, effectively replacing placeholders in the template with actual values.\n\nIf the check_prompt parameter is set to True, the function logs the fully rendered prompt using the log method from the RichPrinter class. This logging is beneficial for debugging and ensuring that the prompt sent to the model is as expected.\n\nFinally, the function calls the chat method to send the rendered prompt to the conversational model. It uses the model parameter to determine which model to utilize for generating the response, defaulting to the model specified in the settings if none is provided.\n\nThe chat_with_template function is called by various methods within the project, including method_choice, query_update, generate_seed, and gpt_search_generate_seed. Each of these methods relies on chat_with_template to generate contextually relevant prompts based on user input or other data, facilitating interactions with the conversational model. For instance, in the method_choice function, chat_with_template is used to determine the appropriate method for processing a question and answer pair, while in the query_update function, it generates prompts that incorporate search results and user feedback.\n\n**Note**: It is essential to ensure that the template_name corresponds to an existing template file in the specified directory. Additionally, the template_data dictionary must contain all necessary keys that match the placeholders in the template to avoid rendering errors.\n\n**Output Example**: A possible return value from the chat_with_template function could be a string containing the response from the conversational model, such as:\n```\n\"Based on the provided information, the recommended method is 'simple abstraction' with the following queries: [query1, query2].\"\n```",
        "**chat_with_template**: The function of chat_with_template is to provide a unified method for rendering templates and interacting with a conversational model to generate responses.\n\n**parameters**: The parameters of this Function.\n· template_name: A string representing the name of the template file to be loaded and rendered.  \n· template_data: A dictionary containing the data to be used for rendering the template.  \n· model: An optional string that allows for overriding the default model used for generating responses.  \n· check_prompt: A boolean flag indicating whether to log the fully rendered prompt for debugging purposes.  \n· root_folder: An optional string specifying a custom root folder from which to load the template file.  \n· save_history: A boolean flag indicating whether to save the conversation history after generating a response.\n\n**Code Description**: The chat_with_template function is a method within the BaseAgent class that serves as a helper for loading, rendering, and processing templates to interact with a conversational model. The function begins by calling the load_template method to retrieve the specified template file, which is essential for generating the prompt that will be sent to the model. The root_folder parameter allows for flexibility in locating the template file, enabling the function to accommodate different directory structures.\n\nOnce the template is loaded, the function utilizes the render_template method to format the template with the provided template_data. This step is crucial as it replaces placeholders in the template with actual values, resulting in a fully rendered prompt that reflects the context of the conversation.\n\nIf the check_prompt parameter is set to True, the function logs the rendered prompt using the printer.log method, which aids in debugging by providing visibility into the exact prompt being sent to the model.\n\nFinally, the function calls the chat method to send the rendered prompt to the conversational model, using either the specified model or the default model defined in the settings. The save_history parameter determines whether the conversation history should be recorded, ensuring that interactions can be tracked for future reference.\n\nThe chat_with_template function is called by various methods within the BaseAgent class, including method_choice, query_update, generate_seed, and gpt_search_query_update. Each of these methods relies on chat_with_template to generate prompts that are contextually relevant and tailored to specific tasks. For instance, method_choice uses chat_with_template to determine the appropriate processing method for a question-answer pair, while query_update utilizes it to refine questions based on search results.\n\n**Note**: It is essential to ensure that the template_name corresponds to an existing template file in the specified directory. The template_data dictionary must contain all necessary keys that correspond to placeholders in the template. Additionally, the function will raise a FileNotFoundError if the specified template file cannot be found, which should be handled appropriately by the caller.\n\n**Output Example**: A possible return value from the chat_with_template function could be a string containing the model's response, such as:\n```\n\"Based on the information provided, here is the response you requested.\"\n```"
      ],
      "code_start_line": 99,
      "code_end_line": 128,
      "params": [
        "self",
        "template_name",
        "template_data",
        "model",
        "check_prompt",
        "root_folder",
        "save_history"
      ],
      "have_return": true,
      "code_content": "    def chat_with_template(\n        self,\n        template_name: str,\n        template_data: dict,\n        model: str = None,\n        check_prompt: bool = False,\n        root_folder: str = None,\n        save_history: bool = True,\n    ) -> str:\n        \"\"\"Unified helper method to handle template rendering and chat calling\n\n        Args:\n            template_name: Name of template file\n            template_data: Data to render template with\n            model: Optional model override\n\n        Returns:\n            Chat response content\n        \"\"\"\n        template = self.load_template(template_name, root_folder=root_folder)\n        rendered_prompt = self.render_template(template, template_data)\n\n        if check_prompt:\n            printer.log(f\"Full Rendered Prompt:\\n{rendered_prompt}\")\n            \n        return self.chat(\n            usr_prompt=rendered_prompt,\n            model=model or settings.default_model,\n            save_history=save_history,\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/method_choice",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/method_choice",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_fuzzy_replacement",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_entity_extraction",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_combination_workflow",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_combination_workflow/process_entity",
        "src/criticsearch/base_agent.py/BaseAgent/taking_notes",
        "src/criticsearch/main.py/_model_action_decision",
        "src/criticsearch/main.py/_action_router",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/verify_section/verify_single_question",
        "tests/test_conversation_format_saving.py/test_conversation_roundtrip"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/rich_output.py/RichPrinter/log"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "chat_with_tools",
      "md_content": [
        "### `chat_with_tools` Method Documentation\n\n#### Description\nThe `chat_with_tools` method is a helper function within the `BaseAgent` class designed to facilitate communication with external tools during a conversation. This method is responsible for loading a template, rendering it with the provided data, and initiating a chat with specified tools using the rendered prompt. It abstracts the process of preparing a prompt from a template and interacting with the tools in a standardized manner.\n\n#### Parameters\n- **template_name** (`str`): The name of the template file to be loaded. This template defines the structure of the prompt to be used in the conversation.\n- **template_data** (`dict`): A dictionary containing the data that will be used to render the template. The dictionary's keys correspond to placeholders in the template, and the values are inserted into the template during the rendering process.\n- **tools** (`List`): A list of tools that will be used in the chat interaction. These tools may include models, external APIs, or other utilities that assist in processing the conversation.\n- **model** (`str`, optional): The model to be used for the chat interaction. If not provided, the method will default to using the model specified in the application's settings.\n\n#### Return Type\n- **ChatCompletionMessage**: The method returns a `ChatCompletionMessage` object that represents the result of the chat interaction. This message encapsulates the response from the tools based on the rendered prompt.\n\n#### Functionality Overview\n1. **Template Loading**: The method first loads the specified template using the `load_template` function. This function retrieves the content of the template file from a predefined directory.\n2. **Template Rendering**: After loading the template, it is rendered with the provided `template_data` using the `render_template` function. This process replaces placeholders in the template with actual data from the `template_data` dictionary.\n3. **Chat Interaction**: Finally, the rendered prompt is passed to the `common_chat` method along with the provided tools and model. The `common_chat` method handles the interaction with the tools, generating the response based on the prompt.\n\n#### Usage Example\n```python\nresponse = agent.chat_with_tools(\n    template_name=\"user_query_template.txt\",\n    template_data={\"user_name\": \"John\", \"query\": \"What is the weather like today?\"},\n    tools=[weather_tool],\n    model=\"gpt-4\"\n)\n```\n\n#### Related Functions\n- **`load_template`**: Loads a template file based on the `template_name` provided.\n- **`render_template`**: Renders the template with the provided data, substituting placeholders with actual values.\n- **`common_chat`**: Handles the chat interaction with the specified tools and model, using the rendered prompt.\n\n#### Notes\n- Ensure that the template file specified in `template_name` exists in the appropriate directory, as the method will raise an error if the file cannot be found.\n- If no model is specified, the method will default to the model set in the application's settings.",
        "**chat_with_tools**: The function of chat_with_tools is to facilitate a conversation with tools by rendering a prompt template and interacting with a conversational model.\n\n**parameters**: The parameters of this Function.\n· template_name: A string representing the name of the template to be loaded for rendering the prompt.  \n· template_data: A dictionary containing the data to be used for populating the template.  \n· tools: A list of tools that may assist in the chat interaction.  \n· model: An optional string specifying the model to be used for generating the response, defaulting to a predefined model if not provided.\n\n**Code Description**: The chat_with_tools function is a method within the BaseAgent class that serves as a helper for engaging in conversations that utilize specific tools. The function begins by loading a template using the load_template method, which retrieves the content of the specified template file from the prompts directory. This is crucial as it allows for dynamic prompt generation based on the context provided by the template name.\n\nOnce the template is loaded, the function proceeds to render the prompt using the render_template method. This method takes the loaded template content and substitutes placeholders with actual values from the template_data dictionary, resulting in a fully formatted prompt that is ready for interaction.\n\nAfter rendering the prompt, the function calls the chat method, passing the rendered prompt along with the tools and the specified model. The chat method is responsible for sending the prompt to the conversational model and generating a response based on the input. This interaction can leverage the tools provided to enhance the response generation process.\n\nThe chat_with_tools function is integral to workflows that require interaction with a conversational model while utilizing external tools. It encapsulates the process of preparing a prompt and managing the conversation, ensuring that the agent can effectively communicate and respond to user queries.\n\n**Note**: It is essential to ensure that the template_name corresponds to an existing template file in the specified directory, and that the template_data dictionary contains all necessary keys for rendering the prompt. If the template file is not found, a FileNotFoundError will be raised. Additionally, the tools parameter should be defined appropriately to maximize the effectiveness of the chat interaction.\n\n**Output Example**: A possible return value from the chat_with_tools function could be a structured response from the conversational model, such as:\n```\n\"Based on the information provided, here are the tools you can use to achieve your goal...\"\n```"
      ],
      "code_start_line": 130,
      "code_end_line": 140,
      "params": [
        "self",
        "template_name",
        "template_data",
        "tools",
        "model"
      ],
      "have_return": true,
      "code_content": "    def chat_with_tools(\n        self, template_name: str, template_data: dict, tools: List, model: str = None\n    ) -> ChatCompletionMessage:\n        \"\"\"Helper method for chat with tools\"\"\"\n        template = self.load_template(template_name)\n        rendered_prompt = self.render_template(template, template_data)\n        return self.chat(\n            usr_prompt=rendered_prompt,\n            tools=tools,\n            model=model or settings.default_model,\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "chat",
      "md_content": [
        "**chat**: The function of chat is to facilitate a conversation with a conversational model by sending a user prompt and optionally utilizing tools.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A list containing the user's prompt that will be sent to the conversational model for processing.  \n· tools: An optional parameter that defaults to None, which can be a list of tools that may assist in the chat interaction.\n\n**Code Description**: The chat function is a method within the BaseAgent class designed to manage interactions with a conversational model. It takes a user prompt as input and can optionally utilize tools to enhance the interaction. The function is expected to return a ChatCompletionMessage, which encapsulates the response generated by the model.\n\nThis function is integral to various workflows within the project, as it serves as the primary means of communication with the conversational model. For instance, it is invoked in the multi_verify method of the ReverseUpgradeWorkflow class, where it is used to validate whether a model can answer a given question. In this context, the chat function is called with a prompt that instructs the model to respond in a specific format, ensuring that the output is structured correctly for further processing.\n\nAdditionally, the chat function is utilized in the evaluate function, where it generates responses based on questions extracted from a JSON file. This demonstrates its versatility in handling different types of prompts and contexts, allowing the agent to interact effectively with the model across various scenarios.\n\nThe chat function is also called in the generate_seed method, where it is used to obtain a unique seed fact that cannot be easily found through a simple search. This highlights its role in generating creative and contextually relevant content based on user-defined prompts.\n\nOverall, the chat function is a critical component of the BaseAgent class, enabling seamless communication with the conversational model and facilitating the execution of complex workflows that rely on dynamic interactions.\n\n**Note**: It is important to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation. Additionally, if tools are provided, they should be compatible with the expected input and output of the chat function to avoid errors during execution.",
        "**chat**: The function of chat is to facilitate a conversation with a conversational model by processing a user prompt and returning a structured response.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: List - A list containing the user prompt that will be sent to the conversational model for processing.\n· tools: None - An optional parameter that may include tools to assist in the chat interaction, though it defaults to None.\n\n**Code Description**: The chat function is a method within the BaseAgent class designed to handle interactions with a conversational model. When invoked, it takes a user prompt (usr_prompt) in the form of a list and an optional tools parameter. The primary purpose of this function is to send the user prompt to the conversational model and retrieve a response.\n\nThe function begins by preparing the prompt for the model, ensuring that it is formatted correctly for processing. It then calls the chat method of the underlying conversational model, passing the prepared prompt along with any specified tools. The response generated by the model is expected to be structured and relevant to the input prompt.\n\nThis function is integral to various workflows within the project, as it serves as the primary means of communication between the user and the conversational model. It is called by several other functions and methods throughout the project, including those responsible for generating content, evaluating answers, and refining responses based on user feedback. For instance, the multi_verify function utilizes the chat method to validate whether a model can answer a specific question encapsulated within a QAItem instance. Similarly, the evaluate function calls chat to assess the correctness of model-generated answers against ground truth data.\n\nThe chat function's ability to interact with the conversational model is crucial for the overall functionality of the BaseAgent, as it enables dynamic and contextually relevant responses based on user input and model capabilities.\n\n**Note**: It is essential to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation. Additionally, the function relies on the proper configuration of the conversational model to generate accurate and meaningful responses.",
        "**chat**: The function of chat is to facilitate a conversation with a conversational model by processing user prompts and generating responses.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A list that contains the user input or prompt to be sent to the conversational model.\n· tools: An optional parameter that can be set to None, which may represent tools that assist in the chat interaction.\n\n**Code Description**: The chat function is a method within the BaseAgent class that serves as a core component for interacting with a conversational model. When invoked, the function takes a user prompt (usr_prompt) and potentially a list of tools to enhance the interaction. The primary purpose of this function is to send the user prompt to the conversational model and retrieve a structured response.\n\nThe function begins by preparing the prompt for the model, ensuring that it adheres to the expected format. It then calls the chat method of the underlying conversational model, passing the prepared prompt and any tools that may be relevant to the interaction. The response generated by the model is expected to be in a structured format, which is then returned to the caller.\n\nThis function is integral to various workflows within the project, as it is called by multiple other methods and classes that require conversational capabilities. For instance, it is utilized in the generate_seed, multi_verify, and evaluate functions, among others. Each of these functions relies on the chat method to obtain responses from the conversational model based on specific prompts related to their respective tasks.\n\nThe chat function's ability to process user inputs and generate responses is crucial for the overall functionality of the intelligent agent, enabling it to engage in meaningful conversations and provide relevant information based on user queries.\n\n**Note**: It is essential to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation. Additionally, the tools parameter should be defined appropriately to maximize the effectiveness of the chat interaction. The function's performance may vary based on the configuration of the conversational model and the quality of the input provided."
      ],
      "code_start_line": 143,
      "code_end_line": 145,
      "params": [
        "self",
        "usr_prompt",
        "tools"
      ],
      "have_return": false,
      "code_content": "    def chat(\n        self, usr_prompt: List, tools: None = None\n    ) -> ChatCompletionMessage: ...\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/evaluate",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/upgrade_question",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/knowledge_validator",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/search_validator",
        "src/criticsearch/abstract_substitution/abs_workflow.py/verify_fuzzy_item",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_workflow.py/evaluate/evaluate_item_worker",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_tools",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main.py/generate_content_for_section",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/instruction_generator.py/InstructionGenerator/generate_instructions/process_file",
        "src/criticsearch/reportbench/process_search_result.py",
        "src/criticsearch/reportbench/report_benchmark.py/safe_chat_and_parse",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_factualqa",
        "src/criticsearch/reportbench/test_llm.py/test_llm",
        "src/criticsearch/workflow.py/run_workflow",
        "src/criticsearch/workflow.py/iterate_traj",
        "tests/test_chat_functionality.py/test_chat_without_tools_returns_str",
        "tests/test_chat_functionality.py/test_chat_with_tools_returns_message_object",
        "tests/test_chat_integration.py/test_real_model_response"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "chat",
      "md_content": [
        "**chat**: The function of chat is to facilitate a conversation with a user by processing a prompt and utilizing specified tools to generate a response.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string representing the user's input prompt that needs to be processed.  \n· tools: A list of tools that may be utilized during the chat interaction to assist in generating the response.\n\n**Code Description**: The chat function is a method within the BaseAgent class that serves as a primary interface for interacting with a conversational model. It takes two parameters: `usr_prompt`, which is the input prompt provided by the user, and `tools`, which is an optional list of tools that can be employed to enhance the response generation process.\n\nThe function begins by preparing the prompt for the conversational model. It may involve loading a specific template or formatting the input prompt to ensure it meets the requirements of the model. Once the prompt is ready, the function communicates with the conversational model, sending the prepared prompt along with any specified tools. The model processes the input and generates a response based on the context provided.\n\nThe chat function is invoked in various parts of the project, demonstrating its versatility and importance in facilitating user interactions. For instance, it is called within the `multi_verify` method of the ReverseUpgradeWorkflow class, where it is used to validate answers against a given question. In this context, the chat function processes prompts that include search results and user queries, allowing the model to generate responses that are relevant and contextually appropriate.\n\nAdditionally, the chat function is utilized in the `evaluate` function, where it is employed to assess the correctness of answers based on a set of predefined criteria. This highlights the function's role in ensuring that the conversational model can provide accurate and reliable responses based on user input and external data.\n\nOverall, the chat function is a critical component of the BaseAgent class, enabling seamless communication between the user and the conversational model while leveraging additional tools to enhance the quality of the interaction.\n\n**Note**: It is essential to ensure that the `usr_prompt` parameter is well-formed and relevant to the context of the conversation. The `tools` parameter should be appropriately defined to maximize the effectiveness of the chat interaction.",
        "**chat**: The function of chat is to facilitate a conversation with a conversational model by processing a user prompt and returning a structured response.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string representing the user's input prompt that will be sent to the conversational model for processing.  \n· tools: A list of tools that may assist in the chat interaction.\n\n**Code Description**: The chat function is a method within the BaseAgent class designed to handle interactions with a conversational model. When invoked, it takes a user prompt (usr_prompt) and an optional list of tools as parameters. The primary purpose of this function is to send the user prompt to the conversational model and retrieve a response.\n\nThe function begins by preparing the prompt for the model, ensuring that it is formatted correctly for processing. It then calls the chat method of the conversational model, passing the prepared prompt and any specified tools. The response generated by the model is expected to be structured, typically in the form of a message object that contains the content of the model's reply.\n\nThe chat function is integral to various workflows within the project, as it is called by multiple other functions and methods that require interaction with the conversational model. For instance, it is utilized in functions such as generate_seed, multi_verify, and evaluate_item_worker, among others. Each of these functions relies on the chat method to obtain responses based on user queries or internal prompts, thereby facilitating the overall functionality of the intelligent agent.\n\nIn the context of the multi_verify function, the chat method is called to validate whether a conversational model can answer a specific question encapsulated within a QAItem instance. The prompt constructed in multi_verify is sent to the chat method, which processes it and returns the model's response. This response is then analyzed to determine if the model can provide a valid answer, contributing to the verification process.\n\n**Note**: It is essential to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation. Additionally, the tools parameter should be defined appropriately to maximize the effectiveness of the chat interaction. The function's ability to return a structured response is critical for maintaining the integrity of the interactions within the project.",
        "**chat**: The function of chat is to facilitate a conversation with a user by processing a prompt and generating a response from a conversational model.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string representing the user input that will be sent to the chat function of the BaseAgent.  \n· tools: A list of tools that may assist in the chat interaction, which can enhance the response generation process.\n\n**Code Description**: The chat function is a method within the BaseAgent class that serves as a core component for interacting with a conversational model. When invoked, the function takes a user prompt (usr_prompt) and an optional list of tools as parameters. The primary purpose of this function is to send the user input to the conversational model and retrieve a structured response.\n\nThe function begins by preparing the prompt for the model, ensuring that it is formatted correctly for processing. If tools are provided, they are incorporated into the interaction, allowing the model to leverage additional functionalities that may enhance the quality of the response. The chat method then calls the underlying model's API, passing the prepared prompt and any specified tools.\n\nUpon receiving the response from the model, the function processes the output to ensure it meets the expected format. This may involve parsing the response to extract relevant information or structuring it in a way that is usable for further operations. The final output is typically a message object that contains the content generated by the model, which can include the answer to the user's query along with any additional context or supporting information.\n\nThe chat function is called by various methods within the BaseAgent class, including chat_with_template and chat_with_tools. These methods rely on the chat function to generate prompts that are contextually relevant and tailored to specific tasks. For instance, chat_with_template uses chat to determine the appropriate response based on a rendered template, while chat_with_tools utilizes it to engage in conversations that require the use of specific tools.\n\n**Note**: It is essential to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation. The function relies on the proper configuration of the conversational model to generate accurate and meaningful responses. Additionally, if tools are provided, they should be defined appropriately to maximize the effectiveness of the chat interaction."
      ],
      "code_start_line": 148,
      "code_end_line": 148,
      "params": [
        "self",
        "usr_prompt",
        "tools"
      ],
      "have_return": false,
      "code_content": "    def chat(self, usr_prompt: str, tools: List) -> ChatCompletionMessage: ...\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/evaluate",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/upgrade_question",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/knowledge_validator",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/search_validator",
        "src/criticsearch/abstract_substitution/abs_workflow.py/verify_fuzzy_item",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_workflow.py/evaluate/evaluate_item_worker",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_tools",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main.py/generate_content_for_section",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/instruction_generator.py/InstructionGenerator/generate_instructions/process_file",
        "src/criticsearch/reportbench/process_search_result.py",
        "src/criticsearch/reportbench/report_benchmark.py/safe_chat_and_parse",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_factualqa",
        "src/criticsearch/reportbench/test_llm.py/test_llm",
        "src/criticsearch/workflow.py/run_workflow",
        "src/criticsearch/workflow.py/iterate_traj",
        "tests/test_chat_functionality.py/test_chat_without_tools_returns_str",
        "tests/test_chat_functionality.py/test_chat_with_tools_returns_message_object",
        "tests/test_chat_integration.py/test_real_model_response"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "chat",
      "md_content": [
        "**chat**: The function of chat is to facilitate a conversation with a user by processing their input prompt and returning a response.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string that represents the user's input prompt that needs to be processed.  \n· tools: An optional parameter that can be set to None, allowing for the inclusion of additional tools for the chat interaction.\n\n**Code Description**: The chat function is a method within the BaseAgent class that serves as a primary interface for interacting with a conversational model. The function takes in a user prompt (usr_prompt) and an optional tools parameter, which can be utilized to enhance the chat experience by integrating external functionalities.\n\nThe function processes the usr_prompt by sending it to a conversational model, which generates a response based on the input. The tools parameter, if provided, allows the function to leverage additional capabilities or resources during the chat interaction. This flexibility enables the chat function to adapt to various contexts and requirements, enhancing the overall user experience.\n\nThe chat function is invoked in several contexts throughout the project, demonstrating its versatility and importance in facilitating communication between the user and the intelligent agent. For instance, it is called within the multi_verify method of the ReverseUpgradeWorkflow class, where it is used to validate the model's ability to answer specific questions. The multi_verify method constructs a prompt based on the user's question and utilizes the chat function to assess the model's response, determining whether the model can provide an accurate answer.\n\nAdditionally, the chat function is employed in the evaluate function, where it is used to generate responses based on questions extracted from a JSON trace. This highlights the function's role in evaluating the performance of the conversational model against predefined ground truth answers.\n\nIn the context of the generate_seed method, the chat function is called to generate a unique seed question that cannot be easily answered through standard search queries. This showcases the function's capability to produce creative and contextually relevant responses based on user input.\n\nOverall, the chat function is a critical component of the BaseAgent class, enabling seamless interactions with the conversational model and supporting various functionalities across the project.\n\n**Note**: It is essential to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation. The tools parameter should be used judiciously to enhance the chat experience without introducing unnecessary complexity.",
        "**chat**: The function of chat is to facilitate a conversation with a conversational model by processing a user prompt and returning the model's response.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: str - The input prompt provided by the user that will be sent to the conversational model for processing.\n· tools: None - This parameter is optional and can be used to specify any tools that may assist in the chat interaction.\n\n**Code Description**: The chat function is a method within the BaseAgent class that serves as a primary interface for interacting with a conversational model. When invoked, the function takes a user-defined prompt (usr_prompt) and processes it to generate a response from the model. The function begins by preparing the prompt for the model, ensuring that it adheres to the expected format required for effective communication with the model.\n\nThe chat method is designed to handle various scenarios, including those where tools are involved in the interaction. If tools are specified, the function integrates them into the conversation, allowing for enhanced responses based on the capabilities of the tools. The function then sends the rendered prompt to the conversational model and retrieves the response generated by the model.\n\nThis method is called by various components within the project, including other methods in the BaseAgent class and higher-level workflows that require conversational capabilities. For instance, it is utilized in functions such as chat_with_template and chat_with_tools, which rely on chat to generate responses based on dynamically rendered prompts. Additionally, the chat method is integral to the operation of the multi_verify function, where it is used to validate the model's ability to answer specific questions based on provided prompts.\n\nThe relationship with its callers is significant, as the chat function acts as a foundational building block for various interactions within the project. It ensures that the conversational model can be effectively engaged, whether for direct user queries or as part of more complex workflows involving multiple steps and tools.\n\n**Note**: It is essential to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation. The function relies on the proper formatting of the prompt to generate meaningful responses from the model. Additionally, the tools parameter should be defined appropriately to maximize the effectiveness of the chat interaction.",
        "**chat**: The function of chat is to facilitate a conversation with a user by processing a prompt and generating a response from a conversational model.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: str - The prompt provided by the user that will be sent to the conversational model for processing.\n· tools: None - An optional parameter that can be used to specify tools that may assist in the chat interaction.\n\n**Code Description**: The chat function is a method within the BaseAgent class that serves as the primary interface for interacting with a conversational model. When invoked, it takes a user-defined prompt (usr_prompt) and sends it to the model to generate a response. The function is designed to handle the intricacies of prompt processing and response generation, ensuring that the interaction is seamless and contextually relevant.\n\nThe function begins by preparing the prompt for the model, which may involve loading a template or rendering the prompt with specific data. Once the prompt is ready, the function calls the chat method of the underlying conversational model, passing the prepared prompt along with any specified tools. The response from the model is then returned, encapsulating the output generated based on the input provided.\n\nThe chat function is integral to various workflows within the project, as it is called by multiple methods across different classes. For instance, it is utilized in the generate_seed function to create seed questions, in the evaluate function to assess model-generated answers, and in the multi_verify function to validate the model's ability to answer specific questions. This highlights the function's versatility and its critical role in facilitating communication between the user and the conversational model.\n\nThe chat function also interacts with other components of the system, such as templates and tools, to enhance the quality of the responses generated. By leveraging these elements, the function ensures that the output is not only accurate but also tailored to the context of the conversation.\n\n**Note**: It is essential to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation. The function relies on the proper configuration of the conversational model to generate accurate and meaningful responses. Additionally, if tools are specified, they should be defined appropriately to maximize the effectiveness of the chat interaction."
      ],
      "code_start_line": 151,
      "code_end_line": 151,
      "params": [
        "self",
        "usr_prompt",
        "tools"
      ],
      "have_return": false,
      "code_content": "    def chat(self, usr_prompt: str, tools: None = None) -> str: ...\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/evaluate",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/upgrade_question",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/knowledge_validator",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/search_validator",
        "src/criticsearch/abstract_substitution/abs_workflow.py/verify_fuzzy_item",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_workflow.py/evaluate/evaluate_item_worker",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_tools",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main.py/generate_content_for_section",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/instruction_generator.py/InstructionGenerator/generate_instructions/process_file",
        "src/criticsearch/reportbench/process_search_result.py",
        "src/criticsearch/reportbench/report_benchmark.py/safe_chat_and_parse",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_factualqa",
        "src/criticsearch/reportbench/test_llm.py/test_llm",
        "src/criticsearch/workflow.py/run_workflow",
        "src/criticsearch/workflow.py/iterate_traj",
        "tests/test_chat_functionality.py/test_chat_without_tools_returns_str",
        "tests/test_chat_functionality.py/test_chat_with_tools_returns_message_object",
        "tests/test_chat_integration.py/test_real_model_response"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "chat",
      "md_content": [
        "**chat**: The function of chat is to facilitate a conversation with a user by processing their prompt and generating a response using a language model.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string or list that contains the user's input prompt for the conversation. This can be a single string or a list of messages.\n· tools: An optional list of tools that may be utilized during the chat session to enhance the response.\n· role: A string indicating the role of the responding agent, defaulting to \"assistant\".\n· model: A string specifying the model to be used for generating the response, defaulting to the model defined in the settings.\n\n**Code Description**: The chat function is a method within the BaseAgent class that orchestrates the interaction between the user and a language model. It begins by calling the call_llm function, passing the necessary parameters such as the model, user prompt, configuration settings, and any tools that may be required. The call_llm function is responsible for communicating with the language model and generating a response based on the provided input.\n\nIf the tools parameter is not None, the function directly returns the response generated by the language model without appending it to the conversation history. However, if tools are not provided, the function appends the generated response to the conversation history using the append_to_history method of the ConversationManager class. This method records the interaction by storing the role of the agent (in this case, \"assistant\") and the content of the response.\n\nThe chat function is called in various contexts throughout the project, including the multi_verify function and the evaluate function. In these instances, it serves as a crucial component for generating responses based on user queries or for validating answers against expected outcomes.\n\nThe call to the call_llm function is essential as it encapsulates the logic for interacting with the language model, while the append_to_history method ensures that the conversation flow is maintained by keeping a record of all interactions.\n\n**Note**: It is important to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation. Additionally, the tools parameter can enhance the functionality of the chat but is not mandatory for the basic operation of the function.\n\n**Output Example**: A possible return value from the chat function could be a string such as:\n\"Hello! How can I assist you today?\"",
        "**chat**: The function of chat is to facilitate a conversation with a user by processing their prompt and generating a response using a conversational model.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string or list representing the user's input prompt that will be sent to the conversational model.\n· tools: An optional list of tools that may assist in the chat interaction.\n· role: A string indicating the role of the entity in the conversation, defaulting to \"assistant\".\n· model: A string specifying the model to be used for generating responses, defaulting to the model defined in the settings.\n· save_history: A boolean flag indicating whether to save the conversation history, defaulting to True.\n\n**Code Description**: The chat function is a method within the BaseAgent class that serves as a primary interface for interacting with a conversational model. When invoked, it takes the user's prompt and processes it to generate a response. The function begins by calling the `call_llm` function, which is responsible for communicating with the large language model (LLM). This function takes several parameters, including the model to be used, the user's prompt, and any tools that may be relevant to the conversation.\n\nIf the tools parameter is provided, the function directly returns the response from the LLM without saving the conversation history. If no tools are specified and the save_history flag is set to True, the function appends the response to the conversation history using the `append_to_history` method from the ConversationManager class. This ensures that all interactions are logged for future reference, maintaining the context of the conversation.\n\nThe chat function is integral to the overall workflow of the BaseAgent, as it enables dynamic interactions between the user and the conversational model. It is called by various methods within the project, including those that require user input or need to generate responses based on specific queries. The ability to save conversation history allows for a more coherent dialogue, enhancing the user experience.\n\n**Note**: It is important to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation. The tools parameter should be defined appropriately to maximize the effectiveness of the chat interaction. Additionally, the function assumes that the model specified is correctly configured and capable of generating relevant responses based on the provided prompt.\n\n**Output Example**: A possible appearance of the code's return value when calling the chat function might look like this:\n\"Based on your input, here is the information you requested: [details].\"",
        "**chat**: The function of chat is to facilitate a conversation with a user by processing their prompt and generating a response using a conversational model.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string or list representing the user's input prompt that will be sent to the conversational model for processing.\n· tools: An optional list of tools that may assist in the chat interaction.\n· role: A string indicating the role of the agent in the conversation, defaulting to \"assistant\".\n· model: A string specifying the model to be used for generating the response, defaulting to the model defined in the settings.\n· save_history: A boolean flag indicating whether to save the conversation history after generating a response, defaulting to True.\n\n**Code Description**: The chat function is a core method within the BaseAgent class that handles user interactions by sending prompts to a conversational model and processing the responses. Upon invocation, the function first calls the `call_llm` method, which interacts with the specified language model (LLM) to generate a response based on the provided user prompt and configuration settings.\n\nThe function checks if any tools are provided. If tools are specified, the function returns the response from the LLM directly without saving the conversation history. If no tools are provided and the `save_history` flag is set to True, the function appends the generated response to the conversation history using the `append_to_history` method from the ConversationManager class. This ensures that all interactions are logged for future reference.\n\nThe chat function is called by various methods within the BaseAgent class, including `chat_with_template`, `chat_with_tools`, and `update_answer`. Each of these methods relies on the chat function to generate contextually relevant responses based on user input and the current state of the conversation. For instance, `chat_with_template` uses the chat function to send a rendered prompt to the model, while `update_answer` utilizes it to refine the agent's response based on user feedback and previous interactions.\n\n**Note**: It is essential to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation. The function relies on the proper configuration of the conversational model to generate accurate and meaningful responses. Additionally, the tools parameter should be defined appropriately to maximize the effectiveness of the chat interaction.\n\n**Output Example**: A possible appearance of the code's return value when calling the chat function might look like this:\n```\n{\n    \"content\": \"Based on the information provided, here is the response you requested.\"\n}\n```"
      ],
      "code_start_line": 153,
      "code_end_line": 176,
      "params": [
        "self",
        "usr_prompt",
        "tools",
        "role",
        "model",
        "save_history"
      ],
      "have_return": true,
      "code_content": "    def chat(\n        self,\n        usr_prompt: str | List,\n        tools: Optional[List] = None,\n        role: str = \"assistant\",\n        model: str = settings.default_model,  # 默认使用配置文件中的默认模型\n        save_history: bool = True,\n    ) -> ChatCompletionMessage | str | None:\n        llm_response = call_llm(\n            model=model,  # 使用传入的model / 默认model\n            usr_prompt=usr_prompt,\n            config=settings,\n            tools=tools,\n        )\n\n        if tools is not None:\n            return llm_response\n\n        if save_history:\n            BaseAgent.conversation_manager.append_to_history(\n                role=role, content=llm_response.content\n            )\n\n        return llm_response.content\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/evaluate",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/upgrade_question",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/knowledge_validator",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/search_validator",
        "src/criticsearch/abstract_substitution/abs_workflow.py/verify_fuzzy_item",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_workflow.py/evaluate/evaluate_item_worker",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_tools",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main.py/generate_content_for_section",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/instruction_generator.py/InstructionGenerator/generate_instructions/process_file",
        "src/criticsearch/reportbench/process_search_result.py",
        "src/criticsearch/reportbench/report_benchmark.py/safe_chat_and_parse",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_factualqa",
        "src/criticsearch/reportbench/test_llm.py/test_llm",
        "src/criticsearch/workflow.py/run_workflow",
        "src/criticsearch/workflow.py/iterate_traj",
        "tests/test_chat_functionality.py/test_chat_without_tools_returns_str",
        "tests/test_chat_functionality.py/test_chat_with_tools_returns_message_object",
        "tests/test_chat_integration.py/test_real_model_response"
      ],
      "reference_who": [
        "src/criticsearch/llm_service.py/call_llm",
        "src/criticsearch/models.py/ConversationManager/append_to_history"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "update_answer",
      "md_content": [
        "**update_answer**: The function of update_answer is to update the agent's response based on a new query, previous answer, search results, and feedback from a critic.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the new question or query that the agent needs to address.  \n· previous_answer: A string containing the previous answer provided by the agent to the query.  \n· search_results: A string or data structure that holds the results obtained from a search operation related to the query.  \n· critic_feedback: A string containing feedback from a critic that may influence the update of the answer.\n\n**Code Description**: The update_answer function is designed to refine the agent's response to a user query by incorporating new information and feedback. The function begins by organizing the input parameters into a dictionary named `data`, which includes the current query, the previous answer, the search results, and the critic's feedback. \n\nNext, the function calls the load_template method to retrieve a specific template file named \"agent_update_answer.txt\". This template is expected to contain a structured format for generating a prompt that will be sent to the conversational model. The retrieved template is then rendered using the render_template method, which replaces placeholders in the template with the actual values from the `data` dictionary. This step ensures that the prompt is contextually relevant and tailored to the current interaction.\n\nAfter rendering the prompt, the function invokes the common_chat method, passing the rendered prompt as an argument. This method facilitates communication with the language model, sending the prompt and receiving a response. The response generated by the common_chat method is then returned as the output of the update_answer function.\n\nThe update_answer function is called within the main function of the project, specifically during the iterative process of refining answers based on user queries and feedback from a critic agent. In the main function, after the initial answer is generated, the update_answer function is invoked to incorporate the latest search results and feedback from the critic. This iterative approach allows the agent to improve its responses over multiple iterations, ensuring that the final answer is well-informed and aligned with user expectations.\n\n**Note**: It is essential to ensure that the parameters passed to the update_answer function are valid and appropriately formatted. The function relies on the successful loading of the template and the proper functioning of the common_chat method to generate an accurate response.\n\n**Output Example**: A possible return value from the update_answer function could be a string such as \"Based on the latest search results and feedback, the updated answer is: [new answer].\" This output reflects the agent's refined response after considering the provided inputs.",
        "**update_answer**: The function of update_answer is to refine the agent's response based on user feedback, previous answers, and search results.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the user's current question or prompt that needs to be addressed.  \n· previous_answer: A string containing the agent's last response to the user's query, which serves as a reference for improvement.  \n· search_results: A string or structured data containing the results obtained from a search operation that may provide additional context or information relevant to the query.  \n· critic_feedback: A string containing feedback from a critic agent, which may include suggestions or evaluations of the previous answer.\n\n**Code Description**: The update_answer function is a method within the BaseAgent class designed to enhance the quality of the agent's responses by integrating new information and feedback. This function takes four parameters: `query`, `previous_answer`, `search_results`, and `critic_feedback`. \n\nThe function begins by organizing these parameters into a dictionary called `data`, which serves as a structured format for the information that will be utilized in generating a new response. The next step involves loading a template for the update process by calling the load_template method with the filename \"agent_update_answer.txt\". This template is crucial as it provides a predefined structure for the prompt that will be sent to the conversational model.\n\nOnce the template is loaded, the function renders it using the render_template method, passing in the `data` dictionary. This step replaces placeholders in the template with actual values from the provided parameters, creating a contextually relevant prompt.\n\nThe rendered prompt is then sent to the common_chat method, which facilitates communication with the conversational model. This method processes the prompt and returns a response based on the updated context, effectively allowing the agent to generate a more informed and relevant answer.\n\nThe update_answer function is called within the main function of the project, specifically during the iterative process of refining the agent's responses based on feedback from a critic agent. After the initial response is generated, the update_answer method is invoked to incorporate new search results and feedback, ensuring that the agent's final output is polished and aligned with user expectations.\n\n**Note**: It is important to ensure that the parameters passed to the update_answer function are well-formed and relevant to the context of the conversation. The effectiveness of the function relies on the quality of the previous answer, search results, and critic feedback provided.\n\n**Output Example**: A possible return value from the update_answer function could be a string such as \"Based on the latest search results and your feedback, the updated answer is: The capital of France is Paris.\"",
        "**update_answer**: The function of update_answer is to update the agent's response based on user feedback, previous answers, and search results.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the user's current question or task that needs to be addressed.  \n· previous_answer: A string containing the agent's last response to the user's query.  \n· search_results: A string or data structure that holds the results obtained from a search operation relevant to the query.  \n· critic_feedback: A string containing feedback from a CriticAgent that evaluates the previous answer and suggests improvements.\n\n**Code Description**: The update_answer function is a method within the BaseAgent class that facilitates the refinement of the agent's response to a user's query. This function takes four parameters: `query`, `previous_answer`, `search_results`, and `critic_feedback`, which collectively inform the agent's updated response.\n\nUpon invocation, the function first constructs a dictionary named `data` that encapsulates the provided parameters. This dictionary serves as the basis for generating a structured prompt that the agent will use to interact with a conversational model.\n\nThe function then calls the `load_template` method to retrieve a specific template file named \"agent_update_answer.txt\". This template is designed to format the prompt that will be sent to the conversational model. The `render_template` method is subsequently called with the loaded template and the `data` dictionary, which replaces placeholders in the template with actual values from the provided parameters. This results in a rendered prompt that is contextually relevant to the user's query and the agent's previous interactions.\n\nNext, the function utilizes the `chat` method to send the rendered prompt to the conversational model, which processes the input and generates a response based on the updated context. The response from the model is then returned as the output of the update_answer function.\n\nThe update_answer function is called within the main function of the project, specifically during the iterative process of refining the agent's responses based on feedback from a CriticAgent. After the agent generates an initial answer, it receives feedback and may need to update its response accordingly. This iterative feedback loop is crucial for enhancing the quality and relevance of the agent's answers, ensuring that they align with user expectations and the latest information retrieved from searches.\n\n**Note**: It is essential to ensure that the parameters passed to the update_answer function are well-defined and relevant to the context of the conversation. The function relies on accurate and meaningful input to produce a refined response that effectively addresses the user's query.\n\n**Output Example**: A possible return value from the update_answer function could be a string such as:\n```\n\"Based on the latest search results and your feedback, here is the updated answer to your question: ...\"\n```"
      ],
      "code_start_line": 178,
      "code_end_line": 191,
      "params": [
        "self",
        "query",
        "previous_answer",
        "search_results",
        "critic_feedback"
      ],
      "have_return": true,
      "code_content": "    def update_answer(self, query, previous_answer, search_results, critic_feedback):\n        data = {\n            \"query\": query,\n            \"previous_answer\": previous_answer,\n            \"search_results\": search_results,\n            \"critic_feedback\": critic_feedback,\n        }\n\n        agent_update_answer_prompt = self.load_template(\"agent_update_answer.txt\")\n        rendered_prompt = self.render_template(agent_update_answer_prompt, data)\n\n        agent_update_answer_response = self.chat(usr_prompt=rendered_prompt)\n\n        return agent_update_answer_response\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "model_confident",
      "md_content": [
        "**model_confident**: The function of model_confident is to check whether the model is confident in its response to the current user query.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the user's question that needs to be evaluated for model confidence.\n\n**Code Description**: The model_confident function is designed to assess the confidence level of the model regarding a specific user query. It begins by constructing a data dictionary that includes the user's question under the key \"user_question\". The function then loads a confidence assessment template from a file named \"agent_confidence.txt\" using the load_template method. This template serves as a structured prompt for the model to evaluate its confidence.\n\nNext, the function renders the loaded template by passing the data dictionary to the render_template method, which replaces any placeholders in the template with the corresponding values from the dictionary. The rendered prompt is then sent to the model through the common_chat method, which facilitates the interaction with the model and retrieves the response.\n\nThe response obtained from common_chat is returned as the output of the model_confident function. This output indicates the model's confidence level regarding the provided query.\n\nThe model_confident function is called within the main function of the project, specifically during the first iteration of a loop that manages multiple interactions with the model. In this context, it is used to determine whether the model is confident enough to provide an answer directly or if further actions, such as searching for additional information, are necessary. If the model is deemed confident, the common_chat function is invoked to obtain the answer. Conversely, if the model lacks confidence, the process involves generating a search prompt and retrieving supplementary data before answering the user.\n\n**Note**: It is essential to ensure that the query parameter passed to the model_confident function is well-formed and relevant to the context of the conversation to receive an accurate confidence assessment.\n\n**Output Example**: A possible return value from the model_confident function could be a string indicating the model's confidence level, such as \"true\" or \"false\", depending on the evaluation of the user query.",
        "**model_confident**: The function of model_confident is to check whether the model is confident in its response to the current user query.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the user's question that needs to be evaluated for the model's confidence.\n\n**Code Description**: The model_confident function is a method within the BaseAgent class that assesses the confidence level of the model regarding a specific user query. It takes a single parameter, query, which is the user's question. The function begins by creating a data dictionary that includes the user's question under the key \"user_question\". \n\nNext, it calls the load_template method to retrieve a template from the prompts directory, specifically the \"agent_confidence.txt\" file. This template is used to formulate a prompt that will be sent to the conversational model. The rendered prompt is generated by passing the loaded template and the data dictionary to the render_template method, which replaces placeholders in the template with actual values from the data dictionary.\n\nOnce the rendered prompt is prepared, the function invokes the common_chat method, passing the rendered prompt as the usr_prompt parameter. This method facilitates communication with the conversational model, sending the prompt and receiving a response that indicates the model's confidence level regarding the user's query.\n\nThe model_confident function is called within the main function of the project, specifically during the first iteration of a loop that processes user tasks. After initializing the task, the model_confident function is invoked to determine if the model is confident enough to provide a direct answer to the user's question. If the model indicates confidence, the common_chat method is called again to retrieve the answer. Conversely, if the model is not confident, the function initiates a search process to gather more information before attempting to answer the query.\n\nThis function plays a critical role in ensuring that the agent only provides answers when it is confident in the information, thereby enhancing the reliability of the responses generated by the conversational model.\n\n**Note**: It is essential to ensure that the query parameter passed to the model_confident function is well-formed and relevant to the context of the conversation. This will help in obtaining an accurate assessment of the model's confidence level.\n\n**Output Example**: A possible return value from the model_confident function could be a string indicating the model's confidence level, such as:\n```\n\"Confidence: true\"\n```",
        "**model_confident**: The function of model_confident is to check whether the model is confident in answering the current query.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the user's question or task that the agent needs to evaluate confidence for.\n\n**Code Description**: The model_confident function is a method within the BaseAgent class that assesses the confidence level of the model in relation to a specific user query. When invoked, the function takes a single parameter, `query`, which is the user's input question. \n\nThe function begins by creating a dictionary named `data`, which contains the user's question under the key \"user_question\". This structured data is then used to prepare a prompt for the model. The function calls the load_template method to retrieve the content of a template file named \"agent_confidence.txt\". This template is designed to format the prompt that will be sent to the conversational model.\n\nOnce the template is loaded, the function utilizes the render_template method to substitute any placeholders in the template with the actual data from the `data` dictionary. This results in a fully rendered prompt that accurately reflects the user's question.\n\nAfter rendering the prompt, the function calls the chat method, passing the rendered prompt as an argument. The chat method is responsible for sending the prompt to the conversational model and receiving the model's response. The response generated by the model indicates the level of confidence it has in answering the user's question.\n\nThe model_confident function is called within the main function of the project, specifically during the first iteration of the conversation process. In this context, it plays a crucial role in determining whether the agent should provide a direct answer to the user's question or initiate a search for additional information. If the model is confident, the agent proceeds to generate an answer directly; if not, it gathers more information to enhance the quality of its response.\n\n**Note**: It is essential to ensure that the query parameter is well-formed and relevant to the context of the conversation. The function relies on the existence of the template file \"agent_confidence.txt\" to generate the prompt, and any issues in loading this template may affect the function's ability to assess confidence accurately.\n\n**Output Example**: A possible return value from the model_confident function could be a string indicating the confidence level, such as:\n```\n\"Confidence: true\"\n```"
      ],
      "code_start_line": 193,
      "code_end_line": 203,
      "params": [
        "self",
        "query"
      ],
      "have_return": true,
      "code_content": "    def model_confident(self, query):\n        \"\"\"\n        检查模型是否对当前问题有信心。\n        \"\"\"\n        data = {\"user_question\": query}\n        agent_confidence_prompt = self.load_template(\"agent_confidence.txt\")\n\n        rendered_prompt = self.render_template(agent_confidence_prompt, data)\n        agent_confidence_response = self.chat(usr_prompt=rendered_prompt)\n\n        return agent_confidence_response\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "web_scrape_results",
      "md_content": [
        "**web_scrape_results**: The function of web_scrape_results is to extract web content from search results using a web scraper.\n\n**parameters**: The parameters of this Function.\n· search_results: A string representing the initial search results to scrape from.\n\n**Code Description**: The web_scrape_results function is a method within the BaseAgent class that facilitates the extraction of web content based on initial search results provided as input. The function begins by loading a template for the web scraper using the load_template method, which retrieves the content of a specified template file from the prompts directory. This template is then rendered with the user question and the initial search results using the render_template method, which formats the template string by replacing placeholders with actual values from the provided data.\n\nFollowing the preparation of the rendered prompt, the function interacts with a conversational model through the common_chat method, sending the rendered prompt along with any necessary tools defined in the content_scraper_schema. The response from this interaction is analyzed to determine if any tool calls were made. If no tool calls are present, the function returns the content of the response directly.\n\nIn cases where tool calls are included in the response, the function appends these calls to the conversation history using the append_tool_call_to_history method. It then initializes an empty string to accumulate the final web scraping results. For each tool call, the function extracts the URLs from the arguments and invokes the content scraper's scrape method asynchronously to retrieve the content from the specified URLs. The results of these scraping operations are logged into the conversation history using the append_tool_call_result_to_history method.\n\nThe web_scrape_results function is called within the search_and_browse method of the BaseAgent class, which orchestrates a two-step process of searching for information and subsequently scraping content based on the search results. This highlights the function's role in enhancing the agent's ability to provide accurate and relevant responses by leveraging real-time data from web sources.\n\n**Note**: It is essential to ensure that the search_results parameter is well-formed and contains relevant queries for the scraping process to function effectively. Additionally, proper error handling should be implemented to manage cases where the scraping operations do not yield results.\n\n**Output Example**: A possible return value from the web_scrape_results function could be a string formatted as follows:\n```\n\"Here are the results from your search: 1. Title: Example Article, URL: http://example.com/article1, Content: This is a summary of the article. 2. Title: Another Example, URL: http://example.com/article2, Content: This is another summary.\"\n```",
        "**web_scrape_results**: The function of web_scrape_results is to extract web content from search results using a web scraper.\n\n**parameters**: The parameters of this Function.\n· search_results: str - Initial search results to scrape from.\n\n**Code Description**: The web_scrape_results function is a method within the BaseAgent class that facilitates the extraction of web content based on provided search results. The function begins by loading a template for the web scraper from a file named \"web_scraper.txt\" using the load_template method. This template serves as a prompt for the web scraping operation.\n\nNext, the function renders the loaded template by substituting placeholders with actual values, specifically the user question and the initial search results. This is achieved through the render_template method, which takes the template string and a dictionary containing the relevant data.\n\nFollowing the preparation of the prompt, the function interacts with a conversational model via the chat method, sending the rendered prompt and specifying any tools that may assist in the scraping process. The response from this interaction is stored in the web_scraper_response variable.\n\nIf the response indicates that no tool calls were made (i.e., tool_calls is None), the function returns the content of the response directly, indicating that scraping was either not necessary or could not be performed.\n\nIn cases where tool calls are present, the function appends these calls to the conversation history using the append_tool_call_to_history method from the ConversationManager class. This ensures that all interactions with external tools are logged for future reference.\n\nThe function then initializes an empty string to accumulate the final web scraping results. It iterates through each tool call, extracting the URLs from the arguments of the tool call. For each set of URLs, it invokes the scrape method from the ContentScraper class asynchronously to perform the actual content scraping. The results from this scraping operation are logged into the conversation history using the append_tool_call_result_to_history method.\n\nFinally, the function concatenates all the scraped results and returns them as a single string. This structured approach allows the function to effectively gather and present web content based on user queries and search results, enhancing the overall response provided by the agent.\n\nThe web_scrape_results function is called within the search_and_browse method, which orchestrates a two-step process: first performing a search based on user input and then scraping web content from the results of that search. This highlights the function's role in ensuring that the agent can provide accurate and relevant information by leveraging real-time data from web sources.\n\n**Note**: It is essential to ensure that the search_results parameter is well-formed and contains valid data. Proper error handling should be implemented to manage cases where the scraping operation does not yield valid results.\n\n**Output Example**: A possible return value from the web_scrape_results function could be a string formatted as follows:\n\"Here are the scraped results: 1. Title: Latest Tech Innovations, Content: Discover the newest advancements in technology. 2. Title: AI in Healthcare, Content: Explore how AI is transforming the healthcare industry.\""
      ],
      "code_start_line": 205,
      "code_end_line": 249,
      "params": [
        "self",
        "search_results"
      ],
      "have_return": true,
      "code_content": "    def web_scrape_results(self, search_results: str) -> str | None:\n        \"\"\"Extract web content from search results using web scraper\n\n        Args:\n            search_results: Initial search results to scrape from\n\n        Returns:\n            Scraped web content or None if scraping failed\n        \"\"\"\n        web_scraper_prompt = self.load_template(\"web_scraper.txt\")\n        web_scraper_rendered_prompt = self.render_template(\n            web_scraper_prompt,\n            {\n                \"user_question\": self.user_question,\n                \"initial_search_results\": search_results,\n            },\n        )\n\n        # Interact with the model for web scraping\n        web_scraper_response = self.chat(\n            usr_prompt=web_scraper_rendered_prompt,\n            tools=self.content_scraper_schema,\n        )\n\n        # If no tool calls, return the response immediately\n        if web_scraper_response.tool_calls is None:\n            return web_scraper_response.content\n\n        BaseAgent.conversation_manager.append_tool_call_to_history(\n            web_scraper_response.tool_calls\n        )\n\n        final_web_scraper_results = \"\"\n\n        for tool_call in web_scraper_response.tool_calls:\n            urls = json.loads(tool_call.function.arguments).get(\"urls\", [])\n            web_scraper_results = asyncio.run(self.content_scraper.scrape(urls=urls))\n            BaseAgent.conversation_manager.append_tool_call_result_to_history(\n                tool_call_id=tool_call.id,\n                name=\"scrape\",\n                content=web_scraper_results,\n            )\n            final_web_scraper_results += web_scraper_results\n\n        return final_web_scraper_results\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/models.py/ConversationManager/append_tool_call_to_history",
        "src/criticsearch/models.py/ConversationManager/append_tool_call_result_to_history",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "search_and_browse",
      "md_content": [
        "**search_and_browse**: The function of search_and_browse is to perform a search and web scraping operation based on a rendered prompt, returning the final results as a string.\n\n**parameters**: The parameters of this Function.\n· rendered_prompt: A string that contains the prompt to be processed for searching and browsing.\n\n**Code Description**: The search_and_browse function is a method within the BaseAgent class that orchestrates a two-step process: searching for information using a search aggregator and then scraping content from the web based on the search results. \n\n1. The function begins by invoking the common_chat method with the provided rendered_prompt and the search_aggregator_schema. This interaction initiates a search operation, and the response is logged for debugging purposes.\n\n2. If the search_with_tool_response indicates that no tool calls were made, the function immediately returns the content of the response. This allows for quick exit when no further action is necessary.\n\n3. If tool calls are present, the function appends these calls to the conversation history using the append_tool_call_to_history method. This ensures that all interactions with tools are recorded for future reference.\n\n4. The function then initializes an empty string to accumulate the final search results. It iterates over each tool call, extracting the query from the tool call's arguments. For each query, it calls the search method of the search_aggregator to perform the search asynchronously.\n\n5. After a brief pause to manage rate limits, the results from each search are appended to the conversation history using append_tool_call_result_to_history, and the query is updated in the query database.\n\n6. Once all search results are collected, the function loads a web scraper template and renders it with the user's question and the initial search results. This rendered prompt is then sent to the common_chat method to interact with the content scraper.\n\n7. Similar to the search step, if the web_scraper_response contains no tool calls, the function returns the content directly. If tool calls are present, they are appended to the conversation history.\n\n8. The function then processes each tool call related to web scraping, extracting URLs from the arguments and calling the scrape method of the content scraper to gather content from these URLs.\n\n9. The results from the scraping operation are also logged into the conversation history, and the final results are concatenated into a single string, which is returned at the end of the function.\n\nThe search_and_browse function is called within the main function of the project, specifically when the agent is not confident in its initial answer and needs to gather additional information through searching and scraping. This highlights its role in enhancing the agent's ability to provide accurate and relevant responses based on real-time data.\n\n**Note**: It is essential to ensure that the rendered_prompt is well-formed and contains relevant queries for both the search and scraping processes to function effectively. Additionally, proper error handling should be implemented to manage cases where the search or scraping operations do not yield results.\n\n**Output Example**: A possible return value of the search_and_browse function could be a string formatted as follows:\n```\n\"Here are the results from your search: 1. Title: Example Article, URL: http://example.com/article1, Content: This is a summary of the article. 2. Title: Another Example, URL: http://example.com/article2, Content: This is another summary.\"\n```",
        "**search_and_browse**: The function of search_and_browse is to perform a search based on a user-provided prompt and subsequently scrape web content from the search results.\n\n**parameters**: The parameters of this Function.\n· rendered_prompt: A string that contains the user prompt formatted for the search operation.\n\n**Code Description**: The search_and_browse function is a method within the BaseAgent class that orchestrates a two-step process: first, it conducts a search using a conversational model based on the provided user prompt, and second, it scrapes web content from the results of that search. \n\nThe function begins by invoking the common_chat method, passing the rendered_prompt and a schema of tools (search_aggregator_schema) that may be utilized during the search interaction. The response from this method, search_with_tool_response, contains the results of the search operation.\n\nIf the search_with_tool_response indicates that no tool calls were made (i.e., tool_calls is None), the function returns the content of the response directly. This allows for immediate feedback to the user without further processing.\n\nIn cases where tool calls are present, the function appends these calls to the conversation history using the append_tool_call_to_history method from the ConversationManager class. This ensures that all interactions with external tools are logged for future reference.\n\nThe function then initializes an empty string, final_search_results, to accumulate the results from each tool call. It iterates through the tool calls, extracting the search query from the tool call's arguments. For each query, it invokes the search method from the SearchAggregator class asynchronously to perform the actual search. The results are awaited, and a brief sleep is introduced to manage rate limits.\n\nAfter obtaining the search results, the function logs the results into the conversation history using the append_tool_call_result_to_history method. It also updates the query database with the executed query using the update method from the queryDB.\n\nFinally, the function compiles all the search results into the final_search_results string and returns the results after processing them through the web_scrape_results method. This method is responsible for extracting relevant web content based on the search results, enhancing the overall response provided to the user.\n\nThe search_and_browse function is called within the process_single_task function, where it is utilized to refine the agent's responses based on user feedback and additional search queries. This highlights its role in ensuring that the agent can provide accurate and relevant information by leveraging real-time data from web sources.\n\n**Note**: It is important to ensure that the rendered_prompt parameter is well-formed and relevant to the context of the search. Proper error handling should be implemented to manage cases where the search operation does not yield valid results.\n\n**Output Example**: A possible return value from the search_and_browse function could be a string formatted as follows:\n```\n\"Here are the results from your search: 1. Title: Latest Tech Innovations, URL: http://example.com/latest-tech, Content: Discover the newest advancements in technology. 2. Title: AI in Healthcare, URL: http://example.com/ai-healthcare, Content: Explore how AI is transforming the healthcare industry.\"\n```",
        "**search_and_browse**: The function of search_and_browse is to perform a search based on a user-provided prompt and retrieve relevant web content.\n\n**parameters**: The parameters of this Function.\n· rendered_prompt: str - A string containing the user prompt that will be sent to the conversational model for processing.\n\n**Code Description**: The search_and_browse function is a method within the BaseAgent class that orchestrates a two-step process: first, it interacts with a conversational model to initiate a search based on the provided user prompt, and second, it processes any tool calls generated during this interaction to gather additional information from the web.\n\nUpon invocation, the function begins by calling the chat method, passing the rendered_prompt and the search_aggregator_schema as tools. This interaction generates a response that may include tool calls for further actions. The response is logged for debugging purposes using the printer.log method.\n\nIf the response contains no tool calls (i.e., tool_calls is None), the function immediately returns the content of the response, indicating that no further action is required. This allows for quick responses when the search does not necessitate additional processing.\n\nIn cases where tool calls are present, the function appends these calls to the conversation history using the append_tool_call_to_history method from the ConversationManager class. This ensures that all interactions with external tools are logged for future reference.\n\nThe function then initializes an empty string to accumulate the final search results. It iterates through each tool call, extracting the search query from the tool call's arguments. For each query, it invokes the search method from the search_aggregator, which performs the actual search operation asynchronously. The results are then logged into the conversation history using the append_tool_call_result_to_history method.\n\nAdditionally, the function updates the query database with the executed query using the queryDB.update method. After processing all tool calls, the function concatenates the results and returns them by calling the web_scrape_results method, which further processes the gathered information to extract relevant web content.\n\nThe search_and_browse function is called by various components within the project, including the generate_content_for_section function, which utilizes it to gather search results based on a specific section title. This highlights its role in enabling real-time search capabilities and content generation based on user queries.\n\n**Note**: It is essential to ensure that the rendered_prompt parameter is well-formed and relevant to the context of the search. Proper error handling should be implemented to manage scenarios where the search operation does not yield valid results.\n\n**Output Example**: A possible return value from the search_and_browse function could be a string formatted as follows:\n\"Here are the search results: 1. Title: Latest Innovations in AI, Content: Discover the advancements in artificial intelligence. 2. Title: The Future of Technology, Content: Explore the upcoming trends in technology.\"",
        "**search_and_browse**: The function of search_and_browse is to perform a search based on a user-provided prompt and retrieve relevant web content.\n\n**parameters**: The parameters of this Function.\n· rendered_prompt: str - The prompt that will be sent to the conversational model for processing, which contains the search query.\n\n**Code Description**: The search_and_browse function is a method within the BaseAgent class that orchestrates the process of searching for information based on a user-defined prompt and subsequently retrieving relevant web content. Upon invocation, the function first calls the chat method, passing the rendered_prompt and the search_aggregator_schema as tools to facilitate the search process. The response from this call is stored in the search_with_tool_response variable.\n\nThe function then logs the response received from the chat method using the printer.log function, which provides visibility into the search results obtained. If the search_with_tool_response indicates that there are no tool calls (i.e., tool_calls is None), the function immediately returns the content of the response, indicating that no further action is required.\n\nIn cases where tool calls are present, the function appends these tool calls to the conversation history using the append_tool_call_to_history method from the ConversationManager class. This ensures that all interactions with external tools are documented for future reference.\n\nThe function initializes an empty string, final_search_results, to accumulate the results from each tool call. It then iterates through the list of tool calls, extracting the search query from each tool call's arguments. For each query, the function invokes the search method of the search_aggregator asynchronously to perform the actual search operation. A brief sleep is introduced to manage the rate of requests.\n\nAfter obtaining the search results, the function logs the results into the conversation history using the append_tool_call_result_to_history method, which captures the results associated with each tool call. Additionally, the query is updated in the queryDB to maintain a record of the searches performed.\n\nFinally, the function concatenates all the search results collected during the iteration and returns the aggregated results by calling the web_scrape_results method, which further processes the search results to extract relevant web content.\n\nThe search_and_browse function is called by higher-level functions such as generate_content_for_section and main, which rely on its capabilities to gather information and generate content based on user queries. This highlights its role as a critical component in the overall workflow of the intelligent agent, enabling it to provide accurate and relevant information by leveraging real-time data from web sources.\n\n**Note**: It is essential to ensure that the rendered_prompt parameter is well-formed and relevant to the context of the search. Proper error handling should be implemented to manage cases where the search operation does not yield valid results.\n\n**Output Example**: A possible appearance of the code's return value when calling the search_and_browse function might look like this:\n\"Here are the search results: 1. Title: Latest Tech Innovations, Content: Discover the newest advancements in technology. 2. Title: AI in Healthcare, Content: Explore how AI is transforming the healthcare industry.\""
      ],
      "code_start_line": 251,
      "code_end_line": 285,
      "params": [
        "self",
        "rendered_prompt"
      ],
      "have_return": true,
      "code_content": "    def search_and_browse(self, rendered_prompt) -> str | None:\n        search_with_tool_response = self.chat(\n            usr_prompt=rendered_prompt, tools=self.search_aggregator_schema\n        )\n\n        printer.log(f\"search_with_tool_response:\\n{search_with_tool_response}\")\n\n        # If no tool calls, return the response immediately\n        if search_with_tool_response.tool_calls is None:\n            return search_with_tool_response.content\n\n        BaseAgent.conversation_manager.append_tool_call_to_history(\n            search_with_tool_response.tool_calls\n        )\n\n        final_search_results = \"\"\n\n        for tool_call in search_with_tool_response.tool_calls:\n            query = json.loads(tool_call.function.arguments).get(\"query\", \"\")\n\n            search_results = asyncio.run(self.search_aggregator.search(query=query))\n\n            time.sleep(0.2)\n\n            BaseAgent.conversation_manager.append_tool_call_result_to_history(\n                tool_call_id=tool_call.id,\n                name=\"search\",\n                content=search_results,\n            )\n\n            BaseAgent.queryDB.update(query)\n\n            final_search_results += f\"{search_results}\"\n\n        return self.web_scrape_results(final_search_results)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/generate_content_for_section",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "tests/test_conversation_format_saving.py/test_conversation_roundtrip"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/models.py/ConversationManager/append_tool_call_to_history",
        "src/criticsearch/models.py/ConversationManager/append_tool_call_result_to_history",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "extract_and_validate_yaml",
      "md_content": [
        "**extract_and_validate_yaml**: The function of extract_and_validate_yaml is to extract YAML content from a string and validate it.\n\n**parameters**: The parameters of this Function.\n· model_response: A string containing the response from a model which may include YAML content wrapped in code block markers.\n\n**Code Description**: \nThe function `extract_and_validate_yaml` is designed to extract YAML data from a given string, `model_response`, and return it in a structured format if valid. It performs the following steps:\n\n1. **Regular Expression Matching**: The function begins by using a regular expression to search for content between triple backticks ` ```yaml ` and ` ``` `, which is expected to be YAML content. The regular expression `r\"```yaml\\n([\\s\\S]*?)\\n```\"` is used to identify the YAML content enclosed within the code block. If no match is found, it returns `None`.\n\n2. **YAML Parsing**: Once the YAML content is extracted, the function attempts to parse it using `yaml.safe_load`. This function safely loads the YAML content into a Python dictionary or data structure. If the content is not valid YAML, a `yaml.YAMLError` is caught and the error message is printed, returning `None`.\n\n3. **Formatting and Returning**: If the YAML content is successfully parsed, it is re-serialized into a YAML formatted string using `yaml.dump`, and the result is returned. This output is presented in a human-readable YAML format, with the default flow style set to `False`.\n\nIn the context of the broader project, this function is typically used in situations where model responses or other content need to be processed for YAML data. For example, in the `critic` method of the `CriticAgent` class, this function is called to extract and validate YAML content from a model response. Similarly, in the `main` function, it is used to process agent confidence data in YAML format.\n\n**Note**: It is important to ensure that the model response contains valid YAML content, as invalid or improperly formatted YAML will cause the function to return `None` and may trigger error handling in the calling code.\n\n**Output Example**: \nGiven a valid `model_response` such as:\n\n```\n```yaml\nconfidence: true\n```\n```\n\nThe function would return:\n\n```\nconfidence: true\n```",
        "**extract_and_validate_yaml**: The function of extract_and_validate_yaml is to extract YAML content from a given string and validate its format.\n\n**parameters**: The parameters of this Function.\n· model_response: A string containing the response from a model, which is expected to include YAML content wrapped in specific delimiters.\n\n**Code Description**: The extract_and_validate_yaml function is a method within the BaseAgent class that processes a string input, searching for YAML content encapsulated within triple backticks (```yaml```). The function utilizes regular expressions to identify and extract the relevant portion of the string. If the expected YAML content is not found, the function returns None, indicating a failure to extract valid content.\n\nOnce the YAML content is extracted, the function attempts to parse it using the yaml.safe_load method. This method is designed to safely parse YAML strings into Python objects. If the parsing is successful, the function returns a formatted YAML string using yaml.dump, which can be utilized for further processing or output. However, if a yaml.YAMLError occurs during parsing, the function catches this exception, prints an error message indicating the invalid content, and returns None.\n\nThe extract_and_validate_yaml function is called by other methods within the project, such as the critic method in the CriticAgent class. In this context, it is used to validate and extract YAML feedback from the model's response after generating a critique based on user input and agent responses. The successful extraction and validation of YAML content are crucial for the flow of information between the CriticAgent and the BaseAgent, as it directly influences the feedback mechanism and the overall interaction quality.\n\n**Note**: It is essential to ensure that the model_response string contains valid YAML content wrapped in the correct delimiters. If the content is not valid YAML, the function will return None, which may disrupt the expected flow of the application.\n\n**Output Example**: A possible return value from the extract_and_validate_yaml function could be a YAML formatted string such as:\n\n```yaml\nfeedback: \"The agent's answer is comprehensive but lacks specific examples.\"\nsuggestions:\n  - \"Include more detailed explanations.\"\n  - \"Provide references to support claims.\"\n```",
        "**extract_and_validate_yaml**: The function of extract_and_validate_yaml is to extract YAML content from a model response and validate its format.\n\n**parameters**: The parameters of this Function.\n· model_response: A string containing the response from the model, which is expected to include YAML content wrapped in specific delimiters.\n\n**Code Description**: The extract_and_validate_yaml function is a method within the BaseAgent class that processes a string input, model_response, to extract and validate YAML content. It utilizes a regular expression to search for a specific pattern that indicates the presence of YAML content, which is expected to be enclosed within triple backticks and preceded by the keyword \"yaml\". If the pattern is not found, the function returns None, indicating that no valid YAML content was present in the input.\n\nUpon successfully finding a match, the function retrieves the matched YAML content, strips any leading or trailing whitespace, and attempts to parse it using the yaml.safe_load method. This method is part of the PyYAML library and is designed to safely parse YAML strings into Python objects. If the parsing is successful, the function then converts the parsed YAML back into a string format using yaml.dump, ensuring that the output is in a human-readable format with default flow style.\n\nIn the event of a parsing error, the function catches the yaml.YAMLError exception, prints an error message indicating that the YAML content is invalid, and returns None. This error handling is crucial for maintaining the robustness of the application, as it prevents the propagation of malformed data.\n\nThe extract_and_validate_yaml function is called by other methods within the project, such as the critic method in the CriticAgent class. In this context, it plays a vital role in processing the model's response after generating a critique based on the user's question and the agent's answer. The output of this function is essential for ensuring that the critique can be formatted and utilized effectively, as it provides structured feedback in YAML format that can influence subsequent actions in the application.\n\n**Note**: It is important to ensure that the model's response contains valid YAML content formatted correctly within the specified delimiters for the extract_and_validate_yaml method to function properly. If the content is not valid, the function will return None, which may disrupt the expected flow of the application.\n\n**Output Example**: A possible return value from the extract_and_validate_yaml function could be a YAML formatted string such as:\n\n```yaml\nfeedback: \"The agent's answer is comprehensive but lacks specific examples.\"\nsuggestions:\n  - \"Include more detailed explanations.\"\n  - \"Provide references to support claims.\"\n```"
      ],
      "code_start_line": 287,
      "code_end_line": 300,
      "params": [
        "self",
        "model_response"
      ],
      "have_return": true,
      "code_content": "    def extract_and_validate_yaml(self, model_response):\n        match = re.search(r\"```yaml\\n([\\s\\S]*?)\\n```\", model_response, re.DOTALL)\n        if not match:\n            return None  # 如果没有找到匹配的内容，返回None\n\n        model_response = match.group(1).strip()\n        try:\n            # 尝试解析YAML内容\n            parsed_yaml = yaml.safe_load(model_response)\n            return yaml.dump(parsed_yaml, default_flow_style=False)\n\n        except yaml.YAMLError as exc:\n            print(f\"Invalid YAML content: {exc}\")\n            return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "receive_task",
      "md_content": [
        "**receive_task**: The function of receive_task is to accept and store the original task provided to the agent.\n\n**parameters**: The parameters of this Function.\n· task: The original task that needs to be received and stored by the agent.\n\n**Code Description**: The receive_task function is a method of the BaseAgent class, designed to receive a task as input and store it in the instance variable original_task. This function is crucial for the operation of the agent, as it allows the agent to keep track of the task it is currently handling. When the function is called, it takes the task parameter and assigns it to the instance variable self.original_task, effectively saving the task for future reference or processing.\n\nIn the context of the project, the receive_task function is invoked by the CriticAgent within the main function of the application. Specifically, after the common agent generates an answer to the user's question, the CriticAgent receives the original task (TASK) using the receive_task method. This step is essential for the CriticAgent to evaluate the performance of the common agent based on the task it was given. By storing the original task, the CriticAgent can provide feedback and suggestions for improvement, thereby enhancing the overall interaction and effectiveness of the agents involved in the task processing.\n\n**Note**: It is important to ensure that the task passed to the receive_task function is well-defined and relevant to the agent's capabilities, as this will directly influence the quality of the agent's performance and the feedback provided by the CriticAgent.",
        "**receive_task**: The function of receive_task is to accept and store the original task provided to the agent.\n\n**parameters**: The parameters of this Function.\n· task: The original task that is being received and stored by the agent.\n\n**Code Description**: The receive_task function is a method defined within the BaseAgent class. Its primary purpose is to accept a task input, which is expected to be a string or a structured object representing the task details. Upon invocation, the function assigns the input task to the instance variable original_task, effectively storing it for later use within the agent's operations.\n\nThis function plays a critical role in the workflow of the BaseAgent, as it serves as the initial point of interaction where the agent receives the task it needs to process. The stored task can later be utilized in various methods of the BaseAgent, such as when generating responses or conducting searches based on the task's requirements.\n\nThe receive_task function is called by the CriticAgent within the process_single_task function located in the src/criticsearch/main.py file. In this context, the CriticAgent is responsible for evaluating the task and the responses generated by the common agent. By invoking receive_task, the CriticAgent ensures that it has access to the original task, which is essential for providing accurate feedback and evaluations based on the task's context.\n\n**Note**: It is important to ensure that the task parameter passed to receive_task is well-structured and relevant to the agent's capabilities, as this will directly influence the effectiveness of the agent's subsequent operations.",
        "**receive_task**: The function of receive_task is to receive and store the original task provided to the agent.\n\n**parameters**: The parameters of this Function.\n· task: A string representing the original task or question that the agent needs to process.\n\n**Code Description**: The receive_task function is a method of the BaseAgent class, designed to accept a task input and store it as an instance variable. When this function is called, it takes the provided task string and assigns it to the instance variable `original_task`. This allows the agent to keep track of the task it is currently working on, which is essential for managing the agent's workflow and ensuring that the correct context is maintained throughout the processing of the task.\n\nThis function is called within the process_single_task function, which is responsible for executing a single task by coordinating interactions with various agents. In the context of process_single_task, the receive_task function is invoked immediately after initializing an instance of BaseAgent. By calling receive_task with the task parameter, the process_single_task function ensures that the agent has the necessary information to begin processing the task effectively.\n\nThe relationship between receive_task and its caller, process_single_task, is crucial for the overall functionality of the task execution process. The receive_task method serves as the initial step in setting up the agent's context, allowing subsequent operations—such as confidence evaluation, information retrieval, and response generation—to be performed with the correct task information in mind.\n\n**Note**: It is important to ensure that the input task is clearly defined and relevant to the agent's capabilities, as this will directly impact the agent's ability to process the task effectively.",
        "**receive_task**: The function of receive_task is to accept and store the original task provided to the agent.\n\n**parameters**: The parameters of this Function.\n· task: A string representing the original task or question that the agent needs to process.\n\n**Code Description**: The receive_task function is a method within the BaseAgent class that is responsible for receiving and storing the original task that the agent will work on. When this function is called, it takes a single parameter, task, which is expected to be a string. The function assigns this task to the instance variable original_task, effectively storing the task for future reference and processing by the agent.\n\nThis function plays a crucial role in the workflow of the BaseAgent, as it initializes the context in which the agent operates. By receiving the task, the agent can subsequently utilize this information to perform various operations, such as searching for relevant data, generating responses, and taking notes.\n\nThe receive_task function is called in several contexts within the project. For instance, it is invoked in the process_single_task function, where an instance of the BaseAgent is created, and the task is passed to it. This sets the stage for the agent to begin its processing workflow, including setting up training data and preparing for further interactions. Additionally, the function is called in the Session class's __init__ method, where it initializes the agent with the user's prompt, ensuring that the agent is ready to handle the task from the outset.\n\nThe relationship between receive_task and its callers is integral to the overall functionality of the system. It establishes the foundational task that the agent will address, influencing subsequent actions and decisions made by the agent throughout its processing lifecycle.\n\n**Note**: It is important to ensure that the input task is well-defined and relevant to the agent's capabilities. Proper initialization of the agent with the task is essential for effective processing and response generation."
      ],
      "code_start_line": 302,
      "code_end_line": 306,
      "params": [
        "self",
        "task"
      ],
      "have_return": false,
      "code_content": "    def receive_task(self, task):\n        \"\"\"\n        接收原始任务。\n        \"\"\"\n        self.original_task = task\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/session.py/Session/__init__",
        "tests/test_taking_notes.py/test_taking_notes_integration"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_and_validate_json",
      "md_content": [
        "**extract_and_validate_json**: The function of extract_and_validate_json is to extract JSON data from a given string and validate its correctness.\n\n**parameters**: The parameters of this Function.\n· model_response: A string that may contain JSON data, potentially wrapped in specific formatting.\n\n**Code Description**: The extract_and_validate_json function is designed to process a string input, model_response, which is expected to contain JSON data. The function first attempts to locate JSON content that is enclosed within ```json``` code blocks using a regular expression. If such a block is found, it extracts the content and removes any leading or trailing whitespace. If no such block is found, it assumes that the entire input string is the JSON content and trims any whitespace accordingly.\n\nOnce the JSON content is identified, the function attempts to parse it using the json.loads method. If the parsing is successful, the parsed JSON object is returned. However, if a json.JSONDecodeError occurs during parsing, indicating that the content is not valid JSON, the function prints an error message detailing the issue and returns None.\n\nThis function is called within the context of the BaseAgent class, specifically in methods such as process_single_task and main. In these methods, extract_and_validate_json is used to validate the structure of outlines generated from user queries or model responses. By ensuring that the outlines are valid JSON, the function plays a crucial role in maintaining the integrity of the data being processed, which is essential for subsequent operations such as flattening the outline and generating content for various sections.\n\n**Note**: It is important to ensure that the input string is formatted correctly to avoid JSON parsing errors. The function will return None if the input does not contain valid JSON, which should be handled appropriately by the calling functions to prevent further issues in the workflow.\n\n**Output Example**: A possible return value of the function could be a dictionary representing the parsed JSON, such as:\n{\n    \"sections\": [\n        {\n            \"title\": \"Introduction\",\n            \"content\": \"This is the introduction section.\"\n        },\n        {\n            \"title\": \"Conclusion\",\n            \"content\": \"This is the conclusion section.\"\n        }\n    ]\n}",
        "### Function Documentation: `extract_and_validate_json`\n\n#### **Function Name:**\n`extract_and_validate_json`\n\n#### **Class:**\n`BaseAgent`\n\n#### **Description:**\nThe `extract_and_validate_json` function is responsible for extracting JSON data from a given model response. The response is expected to potentially contain a block of text wrapped in a ```json``` code block. The function attempts to locate this block, parse it as JSON, and return the parsed data. If no valid JSON block is found or if parsing fails, it returns `None` and prints an error message.\n\n#### **Parameters:**\n- `model_response` (`str`): A string containing the model's response, which may include a JSON block wrapped in triple backticks (```) for JSON. The function looks for this JSON block in the response and attempts to parse it.\n\n#### **Returns:**\n- `dict | None`: The function returns the parsed JSON data as a dictionary if the content inside the JSON block is valid JSON. If the JSON is invalid or no JSON block is found, it returns `None`.\n\n#### **Function Behavior:**\n1. **Extraction of JSON Block:**\n   The function uses a regular expression to search for a JSON block within the `model_response`. It looks for text wrapped in triple backticks followed by the keyword `json` (` ```json\\n ... \\n``` `). If a match is found, the content inside the block is extracted. If no match is found, the entire `model_response` is used as the JSON content.\n   \n2. **JSON Parsing:**\n   The extracted content is then stripped of any leading or trailing whitespace and passed to the `json.loads()` method to parse the string into a Python dictionary. The function handles potential parsing errors by catching `json.JSONDecodeError` exceptions.\n\n3. **Error Handling:**\n   If an error occurs during the JSON parsing process, an error message is printed indicating that the content is invalid JSON, and the function returns `None`.\n\n#### **Example Usage:**\n\n```python\nresponse = \"\"\"\nHere is some text\n```json\n{\n  \"name\": \"John Doe\",\n  \"age\": 30\n}\n```\nMore text here.\n\"\"\"\n\nagent = BaseAgent()\nparsed_json = agent.extract_and_validate_json(response)\nif parsed_json:\n    print(parsed_json)\nelse:\n    print(\"Failed to extract valid JSON.\")\n```\n\n#### **Notes:**\n- The function uses Python’s built-in `re` (regular expression) module to locate the JSON block within the `model_response`.\n- If no JSON block is wrapped in triple backticks, the function assumes that the entire response is the intended JSON content and attempts to parse it directly.\n- In the case of a `json.JSONDecodeError`, the error message indicates the failure reason but does not interrupt the program's flow.\n\n#### **Error Handling:**\nThe function prints the following message if JSON parsing fails:\n```\nInvalid JSON content: <error message>\n```"
      ],
      "code_start_line": 308,
      "code_end_line": 323,
      "params": [
        "self",
        "model_response"
      ],
      "have_return": true,
      "code_content": "    def extract_and_validate_json(self, model_response):\n        # Try to extract JSON data wrapped in ```json``` blocks\n        # and return the parsed JSON content\n        match = re.search(r\"```json\\n([\\s\\S]*?)\\n```\", model_response, re.DOTALL)\n        if match:\n            json_content = match.group(1).strip()\n        else:\n            json_content = model_response.strip()\n\n        try:\n            parsed_json = json.loads(json_content, encoding=\"utf-8\")\n            return parsed_json\n\n        except json.JSONDecodeError as exc:\n            print(f\"Invalid JSON content: {exc}\")\n            return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "taking_notes",
      "md_content": [
        "**taking_notes**: The function of taking_notes is to extract information from search results and record it as notes.\n\n**parameters**: The parameters of this Function.\n· web_results: A string containing the search results from which notes will be extracted.\n\n**Code Description**: The taking_notes function is a method within the BaseAgent class that processes search results to extract and record relevant notes. It begins by invoking the chat_with_template method, which renders a template named \"taking_notes.txt\" using the provided web_results, the original task, and any previous notes stored in the agent's memo. This method facilitates interaction with a conversational model to generate a response based on the search results.\n\nThe response from chat_with_template is then passed to the extract_notes function, which parses the response text to identify and extract notes formatted within specific tags. If the extracted notes are valid and non-empty, they are converted into a set to ensure uniqueness, effectively removing any duplicates. The new notes are then printed to the console using the printer's rule and print methods, enhancing the visibility of the newly added information.\n\nSubsequently, the unique notes are added to the agent's memo, which is a set designed to store all recorded notes. The function returns a list of the newly added notes. If no valid notes are found, a message indicating \"No new notes.\" is printed, and an empty list is returned.\n\nThe taking_notes function is called within the _action_router function and the process_single_task function. In both cases, it is used to record notes after conducting searches or web scraping activities. This integration highlights the function's role in maintaining an organized record of information that can be referenced in subsequent actions or decisions made by the agent.\n\n**Note**: It is crucial to ensure that the web_results parameter contains properly formatted data to facilitate accurate note extraction. The function relies on the correct implementation of the extract_notes function to validate and parse the notes effectively.\n\n**Output Example**: A possible return value from the taking_notes function could be a list of newly added notes, such as:\n```\n[\n    \"Important information 1 with <citation>http://example1.com</citation>\",\n    \"Important information 2 with <citation>http://example2.com</citation>\"\n]\n```",
        "**taking_notes**: The function of taking_notes is to extract information from search results and record it as notes.\n\n**parameters**: The parameters of this Function.\n· web_results: A string containing the search results from which notes will be extracted.\n\n**Code Description**: The taking_notes function is a method within the BaseAgent class that facilitates the extraction and recording of notes from provided search results. Upon invocation, the function first calls the chat_with_template method, passing a template name and a dictionary of template data that includes the search results, the original task, and any previous notes stored in the agent's memo. This interaction with the chat_with_template method is crucial as it generates a response based on the context provided, which is then processed to extract valid notes.\n\nThe function utilizes the extract_notes utility to parse the response obtained from the chat_with_template method. This utility is designed to identify and return a list of notes formatted within specific tags, ensuring that only properly structured notes are included. If the extracted notes are valid and non-empty, they are converted into a set to eliminate duplicates before being added to the agent's memo. The use of a set for this purpose is an efficient way to manage note uniqueness.\n\nThe function then employs the printer.rule and printer.print methods to visually indicate the addition of new notes to the console output, enhancing user feedback. If no new notes are found, a message indicating this is printed instead.\n\nThe taking_notes function is called in various contexts within the project, notably within the _action_router function and the process_single_task function. In these instances, it is used to process search results and web scraping outputs, allowing the agent to maintain an updated record of relevant information that can be referenced in future actions or content generation.\n\n**Note**: It is essential to ensure that the web_results parameter is formatted correctly and contains relevant information for effective note extraction. The function will return an empty list if no valid notes are found, which should be handled appropriately by the caller.\n\n**Output Example**: A possible return value from the taking_notes function could be:\n```\n[\"Important information 1 with <citation>http://example1.com</citation>\",\n \"Important information 2 with <citation>http://example2.com</citation>\"]\n```"
      ],
      "code_start_line": 325,
      "code_end_line": 340,
      "params": [
        "self",
        "web_results"
      ],
      "have_return": true,
      "code_content": "    def taking_notes(self, web_results):\n        \"\"\"从搜索结果中提取信息并记录。\"\"\"\n        result = self.chat_with_template(\n            template_name=\"taking_notes.txt\",\n            template_data={\"search_result\": web_results, \"TASK\": self.original_task, \"previous_notes\": self.memo},\n        )\n        notes = extract_notes(result)\n        if isinstance(notes, list) and notes:\n            # 先转换成集合进行自动去重，然后更新到memo中\n            new_notes = set(notes)  # 使用set自动去重\n            printer.rule(\"New notes\"); printer.print(new_notes)\n            self.memo.update(new_notes)\n            return list(new_notes)\n        else:\n            printer.print(\"No new notes.\")\n            return []\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/_action_router",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/session.py/Session/take_notes",
        "tests/test_taking_notes.py/test_taking_notes_integration"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_notes"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/main.py": [
    {
      "type": "FunctionDef",
      "name": "flatten_outline",
      "md_content": [
        "**flatten_outline**: The function of flatten_outline is to flatten a hierarchical outline structure into a list of sections, each annotated with its depth and path.\n\n**parameters**: The parameters of this Function.\n· section: A dictionary representing a section of the outline, which may contain a title and potentially a list of child sections.\n· depth: An integer indicating the current depth level in the hierarchy, defaulting to 1.\n· path: A list that tracks the path of titles leading to the current section, defaulting to None.\n\n**Code Description**: The flatten_outline function takes a section of an outline and recursively flattens it into a list of dictionaries. Each dictionary in the resulting list contains the following keys: \"path\", \"section\", and \"depth\". The \"path\" key holds a list of titles leading to the current section, the \"section\" key contains the original section data, and the \"depth\" key indicates how deep the section is in the hierarchy.\n\nInitially, if the path is not provided, it is set to an empty list. The function constructs a dictionary for the current section, appending its title to the path and setting its depth. This dictionary is added to a list called flat. If the current section has children, the function iterates over each child, calling itself recursively with the child section, incrementing the depth by one, and updating the path to include the current section's title. The results from these recursive calls are then extended into the flat list.\n\nThis function is called within the process_single_task function, where it is used to flatten an outline generated by a benchmark report. The flattened outline is essential for generating content for each section in parallel, allowing for efficient processing of multiple sections at once. The flattened structure enables the system to maintain the context of each section while generating content, ensuring that the generated report is coherent and well-structured.\n\n**Note**: It is important to ensure that the input section is correctly formatted as a dictionary with the expected keys (\"title\" and \"children\") to avoid errors during execution. The function assumes that the outline follows a specific structure, and deviations from this structure may lead to unexpected results.\n\n**Output Example**: An example output of the flatten_outline function might look like this for a given outline:\n\n```json\n[\n    {\n        \"path\": [\"Introduction\"],\n        \"section\": {\"title\": \"Introduction\", \"children\": [...]},\n        \"depth\": 1\n    },\n    {\n        \"path\": [\"Introduction\", \"Background\"],\n        \"section\": {\"title\": \"Background\", \"children\": [...]},\n        \"depth\": 2\n    },\n    {\n        \"path\": [\"Introduction\", \"Background\", \"Details\"],\n        \"section\": {\"title\": \"Details\", \"children\": []},\n        \"depth\": 3\n    }\n]\n``` \n\nThis output illustrates how the function captures the hierarchical structure of the outline while providing a flat representation that can be easily processed for content generation.",
        "**flatten_outline**: The function of flatten_outline is to flatten a hierarchical outline structure into a list of sections with their respective depth and path information.\n\n**parameters**: The parameters of this Function.\n· parameter1: section - A dictionary representing a section of the outline, which may contain a title and potentially a list of child sections.\n· parameter2: depth - An integer indicating the current depth level in the outline hierarchy. It defaults to 1.\n· parameter3: path - A list that keeps track of the titles of the sections leading up to the current section. It defaults to None and is initialized to an empty list within the function.\n\n**Code Description**: The flatten_outline function takes a section of an outline and recursively processes it to create a flat representation of the outline. The function begins by checking if the path parameter is None; if it is, it initializes it to an empty list. It then constructs a dictionary called current, which contains the current section's title appended to the path, the section itself, and its depth level. This current dictionary is added to a list called flat, which will hold all flattened sections.\n\nIf the section contains any children (i.e., sub-sections), the function iterates over each child and recursively calls flatten_outline, increasing the depth by 1 and updating the path to include the current section's title. The results from these recursive calls are then extended into the flat list. Finally, the function returns the flat list, which contains all sections in a flattened format, along with their respective paths and depth levels.\n\n**Note**: It is important to ensure that the input section has a valid structure, including a title and an optional list of children, to avoid errors during execution. The function is designed to handle outlines of varying depths and complexities.\n\n**Output Example**: \nGiven an input section structured as follows:\n{\n    \"title\": \"Main Section\",\n    \"children\": [\n        {\n            \"title\": \"Subsection 1\",\n            \"children\": []\n        },\n        {\n            \"title\": \"Subsection 2\",\n            \"children\": [\n                {\n                    \"title\": \"Sub-subsection 1\",\n                    \"children\": []\n                }\n            ]\n        }\n    ]\n}\n\nThe output of flatten_outline would be:\n[\n    {\"path\": [\"Main Section\"], \"section\": {\"title\": \"Main Section\", \"children\": [...]}, \"depth\": 1},\n    {\"path\": [\"Main Section\", \"Subsection 1\"], \"section\": {\"title\": \"Subsection 1\", \"children\": []}, \"depth\": 2},\n    {\"path\": [\"Main Section\", \"Subsection 2\"], \"section\": {\"title\": \"Subsection 2\", \"children\": [...]}, \"depth\": 2},\n    {\"path\": [\"Main Section\", \"Subsection 2\", \"Sub-subsection 1\"], \"section\": {\"title\": \"Sub-subsection 1\", \"children\": []}, \"depth\": 3}\n]"
      ],
      "code_start_line": 30,
      "code_end_line": 45,
      "params": [
        "section",
        "depth",
        "path"
      ],
      "have_return": true,
      "code_content": "def flatten_outline(section, depth=1, path=None):\n    # 将 outline_json 展平，记录每个节点及其层级和路径\n    if path is None:\n        path = []\n    current = {\n        \"path\": path + [section.get(\"title\")],\n        \"section\": section,\n        \"depth\": depth,\n    }\n    flat = [current]\n    if \"children\" in section:\n        for child in section[\"children\"]:\n            flat.extend(\n                flatten_outline(child, depth + 1, path + [section.get(\"title\")])\n            )\n    return flat\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "generate_content_for_section",
      "md_content": [
        "**generate_content_for_section**: The function of generate_content_for_section is to generate detailed content for a specific section based on a given task and title.\n\n**parameters**: The parameters of this Function.\n· common_agent: An instance of a class that facilitates interactions with various agents, including searching and chatting functionalities.\n· section: A dictionary containing information about the section, specifically its title.\n· task: A string that represents the overarching topic or task under which the section content is to be generated.\n\n**Code Description**: The generate_content_for_section function is designed to create content for a specific section of a report or document. It begins by extracting the title from the provided section dictionary. Using this title, it formulates a search query that instructs the common_agent to generate relevant search queries related to the title within the context of the specified task. The search query is structured to prompt the agent to gather information that will be used to construct the content.\n\nThe function then calls the common_agent's search_and_browse method with the formulated search query, which retrieves search results. These results are then incorporated into a prompt that instructs the agent to write one or several detailed paragraphs about the section title, ensuring that the content is logically structured and includes citations for any data sourced from the web. The prompt explicitly requests that the output be formatted in plain text without summary sentences, and it emphasizes the importance of including citations in a specified format.\n\nAfter constructing the prompt, the function invokes the common_agent's common_chat method to generate the actual content based on the prompt. The generated paragraph is then printed to the console using the printer's print method, preceded by a horizontal rule that indicates the section for which content has been generated. Finally, the function returns the generated paragraph.\n\nThis function is called within the process_single_task function, which orchestrates the overall task processing. In process_single_task, the generate_content_for_section function is executed in parallel for multiple sections of a report, utilizing a ThreadPoolExecutor to manage concurrent execution. This allows for efficient generation of content across different sections, enhancing the overall performance of the task.\n\n**Note**: When using the generate_content_for_section function, it is crucial to ensure that the common_agent is properly initialized and capable of performing search and chat operations. Additionally, the section parameter must contain a valid title to generate meaningful content.\n\n**Output Example**: A possible return value of the function could be a detailed paragraph such as: \"The Syrian opposition has undergone significant transformations since the onset of the civil war. Various factions have emerged, each with distinct ideologies and objectives. According to <cite>https://example.com/source</cite>, the fragmentation of the opposition has complicated efforts for a unified front against the regime. Furthermore, the international community's involvement has influenced the dynamics on the ground, as noted by <cite>https://example.com/another-source</cite>.\"",
        "**generate_content_for_section**: The function of generate_content_for_section is to generate detailed content for a specified section based on search results related to a given task.\n\n**parameters**: The parameters of this Function.\n· agent: An instance of BaseAgent that facilitates the search and content generation process.  \n· section: A dictionary containing details about the section, specifically the title for which content needs to be generated.  \n· task: A string representing the background topic or task that contextualizes the content generation.\n\n**Code Description**: The generate_content_for_section function is designed to create content for a specific section by leveraging the capabilities of an intelligent agent (BaseAgent). The function begins by extracting the title from the provided section dictionary. It then constructs a search query that asks the agent to generate search queries related to the section title within the context of the specified task. This search query is passed to the agent's search_and_browse method, which performs a search and retrieves relevant web content.\n\nOnce the search results are obtained, the function formulates a prompt that instructs the agent to write one or several detailed paragraphs about the section title, incorporating data and facts from the search results. The prompt explicitly requests that the content be formatted in plain text without summary sentences and emphasizes the need for proper citation of sources using a specified format.\n\nThe agent's chat method is then called with the constructed prompt to generate the actual content. After the content is generated, the function utilizes the printer's rule and print methods to display a header indicating that content has been generated for the specified title, followed by the generated content itself.\n\nThis function is closely related to the BaseAgent class, particularly its search_and_browse and chat methods. The search_and_browse method is responsible for executing the search based on the generated query, while the chat method handles the interaction with the conversational model to produce the final content. The generate_content_for_section function serves as a higher-level orchestration function that combines these capabilities to fulfill the specific requirement of generating section content.\n\n**Note**: It is important to ensure that the agent instance passed to the function is properly initialized and configured to perform searches and generate content effectively. The section dictionary should contain a valid title, and the task string should provide sufficient context for the content generation.\n\n**Output Example**: A possible appearance of the code's return value when executing the function might look like this:\n\"Artificial Intelligence (AI) has significantly transformed various industries. In the context of healthcare, AI technologies are being utilized to enhance diagnostic accuracy and streamline patient care. For instance, machine learning algorithms can analyze medical images with remarkable precision, leading to earlier detection of diseases. <cite>https://www.example.com/ai-healthcare</cite>\""
      ],
      "code_start_line": 48,
      "code_end_line": 61,
      "params": [
        "agent",
        "section",
        "task"
      ],
      "have_return": true,
      "code_content": "def generate_content_for_section(agent: BaseAgent, section, task):\n    # 保持 prompt 不变，仅生成单层章节内容，不递归\n    title = section.get(\"title\")\n    search_query = f\"generate some search queries about '{title}' under the background of this TOPIC/TASK: '{task}'.\"\n    search_results = agent.search_and_browse(search_query)\n    prompt = (\n        f\"Using the following search results:\\n\\n{search_results}\\n\\n\"\n        f\"Write one or several detailed paragraphs with data and facts in a logical way about '{title}' under the background of this TOPIC/TASK: '{task}', formatted in pure text, without summary sentences.\"\n        f\"Please make sure you are always obeying and using '<\\cite>The url link that you used for supporting the previous statement<\\cite>' format in every sentence that you are using data from the web.\"\n    )\n    paragraph = agent.chat(usr_prompt=prompt)\n    printer.rule(f\"Generated content for '{title}'\")\n    printer.print(paragraph)\n    return paragraph\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print"
      ],
      "special_reference_type": [
        true,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "reconstruct_markdown",
      "md_content": [
        "### Function: `reconstruct_markdown`\n\n#### Overview:\nThe `reconstruct_markdown` function generates a Markdown formatted text from an outline structure and a set of flattened content. It uses the provided outline's structure, iterating through each section, and then associates the content from the flat list based on the full path of each section to ensure the correct order and content placement. The result is a well-structured Markdown document that preserves the hierarchical organization of the sections.\n\n#### Parameters:\n- **outline (dict)**: A dictionary representing the hierarchical structure of the document. This outline contains keys such as \"title\" for section titles, and \"children\" for sub-sections. The structure represents the sections and subsections in a nested manner.\n  \n- **flat_contents (list of tuples)**: A list of tuples where each tuple contains an item (a dictionary with a key \"path\" representing the unique path to the section) and its associated content. The content for each section is mapped to its full path, ensuring that the correct content is placed in the respective section.\n\n#### Return:\n- **str**: A string containing the reconstructed Markdown content. The structure follows the outline's hierarchy, with the correct content inserted under each section title. The content is formatted using Markdown syntax, with headings marked by `#` corresponding to the section depth.\n\n#### Description:\n1. **Mapping Content to Paths**:  \n   The function starts by creating a dictionary (`content_map`) that maps a unique path (represented as a tuple of section titles) to its corresponding content. This ensures that each section's content is correctly aligned with the section’s path in the outline.\n\n2. **Recursive Helper Function**:  \n   A recursive helper function (`helper`) is defined to traverse the outline. For each section, it:\n   - Constructs the path by appending the current section's title to the path of its parent.\n   - Generates the appropriate heading for the section based on its depth in the hierarchy (using the number of titles in the path to determine the heading level).\n   - Checks if the section has corresponding content in the `content_map` and adds it to the Markdown output if available.\n   - Recursively processes child sections if they exist.\n\n3. **Result Construction**:  \n   The function starts by adding the title of the root section (if available) to the Markdown output. Then, it iterates through each section in the outline’s children and invokes the helper function to build the full document.\n\n#### Example Usage:\n\n```python\noutline = {\n    \"title\": \"Main Title\",\n    \"children\": [\n        {\n            \"title\": \"Section 1\",\n            \"children\": []\n        },\n        {\n            \"title\": \"Section 2\",\n            \"children\": [\n                {\"title\": \"Subsection 2.1\", \"children\": []}\n            ]\n        }\n    ]\n}\n\nflat_contents = [\n    ({\"path\": [\"Main Title\", \"Section 1\"]}, \"Content for Section 1\"),\n    ({\"path\": [\"Main Title\", \"Section 2\"]}, \"Content for Section 2\"),\n    ({\"path\": [\"Main Title\", \"Section 2\", \"Subsection 2.1\"]}, \"Content for Subsection 2.1\")\n]\n\nmarkdown_text = reconstruct_markdown(outline, flat_contents)\nprint(markdown_text)\n```\n\n#### Output:\n\n```markdown\n# Main Title\n\n## Section 1\n\nContent for Section 1\n\n## Section 2\n\n### Subsection 2.1\n\nContent for Subsection 2.1\n\nContent for Section 2\n```\n\n#### Notes:\n- The function ensures that the Markdown content is structured hierarchically, with section titles prefixed by the appropriate number of `#` characters based on their depth in the outline.\n- The path-based content mapping ensures that even deeply nested sections receive their correct content, avoiding misalignment in the final document.\n",
        "**reconstruct_markdown**: The function of reconstruct_markdown is to generate a final Markdown text based on a flattened content structure and an original outline.\n\n**parameters**: The parameters of this Function.\n· outline: A dictionary representing the hierarchical structure of the document, which includes titles and potentially nested sections (children).\n· flat_contents: A list of tuples, where each tuple contains a dictionary with a \"path\" key and the corresponding content string. The \"path\" key represents the unique path to the content in the outline.\n\n**Code Description**: The reconstruct_markdown function constructs a Markdown representation of a document by combining an outline structure with corresponding content. It begins by creating a mapping of content using the complete path as a key to ensure uniqueness. The function defines a nested helper function that recursively processes each section of the outline. For each section, it generates a Markdown header based on the depth of the section in the hierarchy and appends any associated content if it exists in the content map. The function also handles nested sections by iterating through any children of the current section. The final Markdown text is built up and returned as a single string.\n\nThe process begins by initializing an empty string for the result. If the outline has a title, it adds it as the top-level header in the Markdown format. The function then iterates through the children of the outline, invoking the helper function for each child section, which constructs the Markdown recursively.\n\n**Note**: It is important to ensure that the paths in flat_contents correspond accurately to the structure defined in the outline. Any discrepancies may result in missing content in the final Markdown output.\n\n**Output Example**: \nGiven an outline like:\n{\n    \"title\": \"Document Title\",\n    \"children\": [\n        {\n            \"title\": \"Section 1\",\n            \"children\": []\n        },\n        {\n            \"title\": \"Section 2\",\n            \"children\": [\n                {\n                    \"title\": \"Subsection 2.1\",\n                    \"children\": []\n                }\n            ]\n        }\n    ]\n}\nAnd flat_contents like:\n[\n    ({\"path\": [\"Document Title\", \"Section 1\"]}, \"Content for Section 1.\"),\n    ({\"path\": [\"Document Title\", \"Section 2\", \"Subsection 2.1\"]}, \"Content for Subsection 2.1.\")\n]\nThe output of reconstruct_markdown would be:\n# Document Title\n\n## Section 1\n\nContent for Section 1.\n\n## Section 2\n\n### Subsection 2.1\n\nContent for Subsection 2.1."
      ],
      "code_start_line": 64,
      "code_end_line": 100,
      "params": [
        "outline",
        "flat_contents"
      ],
      "have_return": true,
      "code_content": "def reconstruct_markdown(outline, flat_contents):\n    \"\"\"\n    根据展平后的内容与原 outline 结构，拼接生成最终 Markdown 文本\n    使用完整路径为key确保唯一性\n    \"\"\"\n    # 构建以完整路径为key的映射字典\n    content_map = {}\n    for item, content in flat_contents:\n        path_key = tuple(item[\"path\"])  # 使用完整路径作为key\n        content_map[path_key] = content\n\n    def helper(section, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n\n        # 生成标题（深度就是路径的长度）\n        depth = len(current_path)\n        md = f\"{'#' * depth} {section.get('title')}\\n\\n\"\n\n        # 添加该节的内容（如果有）\n        if path_key in content_map:\n            md += f\"{content_map[path_key]}\\n\\n\"\n\n        # 处理子节点\n        if \"children\" in section:\n            for child in section[\"children\"]:\n                md += helper(child, current_path)\n        return md\n\n    result = \"\"\n    if \"title\" in outline:\n        result += f\"# {outline['title']}\\n\\n\"\n\n    for section in outline.get(\"children\", []):\n        result += helper(section, [outline.get(\"title\")] if \"title\" in outline else [])\n\n    return result\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "helper",
      "md_content": [
        "**helper**: The function of helper is to recursively generate a Markdown representation of a structured document based on the provided section and its children.\n\n**parameters**: The parameters of this Function.\n· parameter1: section - A dictionary representing a section of the document, which includes a title and potentially child sections.\n· parameter2: path - A list that keeps track of the current path of titles leading to the current section, defaulting to an empty list.\n\n**Code Description**: The helper function constructs a Markdown string for a given section of a document. It begins by creating a new path that includes the title of the current section. This path is converted into a tuple, which serves as a key to look up content in a predefined `content_map`. The depth of the current section is determined by the length of the path, which is used to format the section title with the appropriate number of hash symbols for Markdown headers.\n\nIf the current section's path key exists in the `content_map`, the corresponding content is appended to the Markdown string. The function then checks if the section contains any child sections. If so, it iterates through each child and recursively calls itself, appending the resulting Markdown from each child to the current Markdown string. Finally, the complete Markdown string for the section and its children is returned.\n\n**Note**: It is important to ensure that the `content_map` is defined and contains the necessary content for the sections being processed. The function assumes that the structure of the input section is consistent and that each section may have a \"title\" and optionally \"children\".\n\n**Output Example**: An example output for a section with the title \"Introduction\" and a child section titled \"Background\" might look like this:\n\n```\n# Introduction\n\nThis is the content for the Introduction section.\n\n## Background\n\nThis is the content for the Background section.\n```"
      ],
      "code_start_line": 75,
      "code_end_line": 91,
      "params": [
        "section",
        "path"
      ],
      "have_return": true,
      "code_content": "    def helper(section, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n\n        # 生成标题（深度就是路径的长度）\n        depth = len(current_path)\n        md = f\"{'#' * depth} {section.get('title')}\\n\\n\"\n\n        # 添加该节的内容（如果有）\n        if path_key in content_map:\n            md += f\"{content_map[path_key]}\\n\\n\"\n\n        # 处理子节点\n        if \"children\" in section:\n            for child in section[\"children\"]:\n                md += helper(child, current_path)\n        return md\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "create_document_structure",
      "md_content": [
        "**create_document_structure**: The function of create_document_structure is to create a structured document based on an outline and associated content.\n\n**parameters**: The parameters of this Function.\n· outline_json: A JSON object representing the outline structure of the document, including titles and hierarchical relationships.\n· flat_contents: A list of tuples, where each tuple contains an item (with a path) and its corresponding content as a string.\n\n**Code Description**: The create_document_structure function constructs a document structure by utilizing the provided outline and content. It initializes a document dictionary with a title and a level, setting up a place for subsections. The function then creates a mapping of content to paths derived from the flat_contents parameter, allowing for easy retrieval of content based on the section paths.\n\nThe core of the function is the nested process_section function, which recursively processes each section of the outline. It constructs a section_data dictionary that includes the title, level, and paragraphs for each section. If content exists for a given path, it splits the content into paragraphs based on double newlines, ignoring any empty paragraphs. Each paragraph is processed to extract citations, which are stored alongside the paragraph text.\n\nThe function also handles child sections by recursively calling process_section for each child, appending the resulting data to the current section's subsections. Finally, the function iterates through the root sections of the outline, processing each one and appending it to the main document structure before returning the complete document.\n\n**Note**: It is important to ensure that the outline_json is well-structured and that the flat_contents accurately corresponds to the paths defined in the outline. The function assumes that paragraphs are separated by double newlines and that citations can be extracted from the paragraphs.\n\n**Output Example**: A possible appearance of the code's return value could be as follows:\n{\n    \"document\": {\n        \"title\": \"Sample Document Title\",\n        \"level\": 1,\n        \"subsections\": [\n            {\n                \"title\": \"Introduction\",\n                \"level\": 2,\n                \"paragraphs\": [\n                    {\n                        \"text\": \"This is the first paragraph of the introduction.\",\n                        \"citations\": [\"Citation1\", \"Citation2\"]\n                    },\n                    {\n                        \"text\": \"This is the second paragraph of the introduction.\",\n                        \"citations\": []\n                    }\n                ],\n                \"subsections\": []\n            },\n            {\n                \"title\": \"Methodology\",\n                \"level\": 2,\n                \"paragraphs\": [],\n                \"subsections\": []\n            }\n        ]\n    }\n}"
      ],
      "code_start_line": 103,
      "code_end_line": 156,
      "params": [
        "outline_json",
        "flat_contents"
      ],
      "have_return": true,
      "code_content": "def create_document_structure(outline_json, flat_contents):\n    \"\"\"\n    基于outline结构和生成的内容创建文档结构\n    \"\"\"\n    document = {\n        \"document\": {\n            \"title\": outline_json.get(\"title\", \"\"),\n            \"level\": 1,\n            \"subsections\": [],\n        }\n    }\n\n    # 构建路径到内容的映射\n    content_map = {}\n    for item, content in flat_contents:\n        path_key = tuple(item[\"path\"])\n        content_map[path_key] = content\n\n    def process_section(section, depth=1, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n\n        section_data = {\"title\": section.get(\"title\"), \"level\": depth, \"paragraphs\": []}\n\n        # 如果有内容，处理段落\n        if path_key in content_map:\n            content = content_map[path_key]\n            paragraphs = content.split(\"\\n\\n\")  # 假设段落用空行分隔\n            for para in paragraphs:\n                if para.strip():  # 忽略空段落\n                    citations = extract_citations(para)\n                    paragraph_data = {\n                        \"text\": para.strip(),  # 保留原始文本，包括cite标记\n                        \"citations\": citations,\n                    }\n                    section_data[\"paragraphs\"].append(paragraph_data)\n\n        # 处理子节点\n        if \"children\" in section:\n            section_data[\"subsections\"] = []\n            for child in section[\"children\"]:\n                child_data = process_section(child, depth + 1, current_path)\n                section_data[\"subsections\"].append(child_data)\n\n        return section_data\n\n    # 处理根节点下的所有节点\n    for section in outline_json.get(\"children\", []):\n        doc_section = process_section(\n            section, 2, [outline_json.get(\"title\")] if \"title\" in outline_json else []\n        )\n        document[\"document\"][\"subsections\"].append(doc_section)\n\n    return document\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_section",
      "md_content": [
        "**process_section**: The function of process_section is to recursively process a section of a document, extracting its title, paragraphs, and any nested subsections.\n\n**parameters**: The parameters of this Function are as follows:\n· section: A dictionary representing a section of the document, which includes a title and potentially child sections.\n· depth: An integer indicating the current depth of the section in the document hierarchy, defaulting to 1.\n· path: A list that tracks the path of titles leading to the current section, defaulting to an empty list.\n\n**Code Description**: The process_section function begins by constructing the current path of titles by appending the title of the current section to the existing path. This path is then converted into a tuple, which serves as a key to access content from a predefined content_map. The function initializes a dictionary, section_data, to store the title, depth level, and an empty list for paragraphs.\n\nIf the current path key exists in the content_map, the function retrieves the associated content and splits it into paragraphs based on double newlines, assuming that paragraphs are separated by empty lines. Each paragraph is processed to extract citations using the extract_citations function, which identifies URLs enclosed within <cite> tags. Non-empty paragraphs are added to the section_data dictionary, along with their corresponding citations.\n\nThe function then checks if the current section has any child sections. If so, it initializes a list for subsections and recursively calls process_section for each child, incrementing the depth by one and passing the updated path. Each child's processed data is appended to the subsections list within section_data.\n\nFinally, the function returns the constructed section_data dictionary, which encapsulates the processed information of the section, including its title, depth, paragraphs, and any nested subsections.\n\nThis function is integral to the overall structure of the document processing system, as it organizes sections and their content into a structured format. It relies on the extract_citations function to ensure that citations are accurately captured and associated with their respective paragraphs.\n\n**Note**: It is essential that the content_map contains properly formatted entries corresponding to the section titles for the function to retrieve and process content effectively. If the section does not have associated content or if the structure of the input data is incorrect, the function may return incomplete or empty section data.\n\n**Output Example**: An example of the function's return value could be:\n{\n    \"title\": \"Introduction\",\n    \"level\": 1,\n    \"paragraphs\": [\n        {\n            \"text\": \"This is the first paragraph of the introduction.<cite>http://example.com/citation1</cite>\",\n            \"citations\": [\"http://example.com/citation1\"]\n        },\n        {\n            \"text\": \"This is the second paragraph of the introduction.\",\n            \"citations\": []\n        }\n    ],\n    \"subsections\": [\n        {\n            \"title\": \"Background\",\n            \"level\": 2,\n            \"paragraphs\": [],\n            \"subsections\": []\n        }\n    ]\n}"
      ],
      "code_start_line": 121,
      "code_end_line": 147,
      "params": [
        "section",
        "depth",
        "path"
      ],
      "have_return": true,
      "code_content": "    def process_section(section, depth=1, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n\n        section_data = {\"title\": section.get(\"title\"), \"level\": depth, \"paragraphs\": []}\n\n        # 如果有内容，处理段落\n        if path_key in content_map:\n            content = content_map[path_key]\n            paragraphs = content.split(\"\\n\\n\")  # 假设段落用空行分隔\n            for para in paragraphs:\n                if para.strip():  # 忽略空段落\n                    citations = extract_citations(para)\n                    paragraph_data = {\n                        \"text\": para.strip(),  # 保留原始文本，包括cite标记\n                        \"citations\": citations,\n                    }\n                    section_data[\"paragraphs\"].append(paragraph_data)\n\n        # 处理子节点\n        if \"children\" in section:\n            section_data[\"subsections\"] = []\n            for child in section[\"children\"]:\n                child_data = process_section(child, depth + 1, current_path)\n                section_data[\"subsections\"].append(child_data)\n\n        return section_data\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/utils.py/extract_citations"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "parse_markdown_to_structure",
      "md_content": [
        "## Function Documentation: `parse_markdown_to_structure`\n\n### Overview\n\nThe `parse_markdown_to_structure` function is responsible for parsing a given markdown text and converting it into a structured document format. It organizes the markdown content into a hierarchical document structure, which includes titles, subsections, and paragraphs. Citations within paragraphs are also extracted and included in the structure.\n\n### Parameters\n\n- **markdown_text** (str): The input markdown text to be parsed. This should be a string containing the raw markdown content.\n\n### Returns\n\n- **document** (dict): A dictionary representing the parsed document structure. It contains the following keys:\n  - **document**: The root level of the document, which includes:\n    - **title**: The title of the document (empty string initially).\n    - **level**: The level of the document (1 initially).\n    - **subsections**: A list of subsections at this level.\n  - **subsections**: A list of subsections for each heading found in the markdown text, with each subsection containing:\n    - **title**: The title of the subsection.\n    - **level**: The level of the subsection, corresponding to the number of `#` symbols in the markdown heading.\n    - **paragraphs**: A list of paragraphs under the subsection.\n    - **subsections**: Subsections under the current section.\n\n### Description\n\nThe function performs the following tasks:\n\n1. **Initial Setup**: \n   - It splits the input `markdown_text` into lines and initializes a dictionary (`document`) with a structure containing a title, level, and subsections.\n   - It sets up a `section_stack` to keep track of the current section as the parsing progresses.\n\n2. **Processing Lines**: \n   - The function iterates through each line of the markdown text:\n     - **Handling Headings**: \n       - When a line starts with `#`, the function processes it as a heading. The number of `#` symbols in the heading determines the level of the subsection.\n       - If there is any unprocessed paragraph text before encountering a heading, it is added as a paragraph under the current section.\n       - The function adjusts the `section_stack` based on the heading level to ensure that subsections are nested correctly.\n     - **Handling Paragraphs**: \n       - For lines that are not headings, the function accumulates them as paragraph text until a blank line is encountered or a new heading is found. When a paragraph is fully processed, the function extracts any citations within it and adds the paragraph to the current section.\n   \n3. **Finalizing Paragraphs**: \n   - After processing all lines, any remaining paragraph text is finalized and added to the document structure.\n\n4. **Citation Extraction**:\n   - Throughout the parsing process, the `extract_citations` function is called to extract any citations (URLs within `<cite>` tags) from the paragraph text.\n\n### Example\n\n#### Input:\n```markdown\n# Main Title\n\nThis is an introduction paragraph with a citation <cite>http://example.com/citation1</cite>.\n\n## Subsection 1\n\nThis is a paragraph under subsection 1 with another citation <cite>http://example.com/citation2</cite>.\n\n### Subsubsection 1.1\n\nText in the subsubsection.\n```\n\n#### Output:\n```python\n{\n  \"document\": {\n    \"title\": \"\",\n    \"level\": 1,\n    \"subsections\": [\n      {\n        \"title\": \"Main Title\",\n        \"level\": 1,\n        \"subsections\": [\n          {\n            \"title\": \"Subsection 1\",\n            \"level\": 2,\n            \"subsections\": [\n              {\n                \"title\": \"Subsubsection 1.1\",\n                \"level\": 3,\n                \"subsections\": [],\n                \"paragraphs\": [\n                  {\"text\": \"Text in the subsubsection.\", \"citations\": []}\n                ]\n              }\n            ],\n            \"paragraphs\": [\n              {\"text\": \"This is a paragraph under subsection 1 with another citation http://example.com/citation2.\", \"citations\": [\"http://example.com/citation2\"]}\n            ]\n          }\n        ],\n        \"paragraphs\": [\n          {\"text\": \"This is an introduction paragraph with a citation http://example.com/citation1.\", \"citations\": [\"http://example.com/citation1\"]}\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Notes\n\n- The function relies on the proper formatting of markdown headings (e.g., using `#` for titles) and paragraphs (text separated by blank lines).\n- Citations are identified by the `extract_citations` function, which looks for URLs enclosed within `<cite>` tags in the text.\n- The document structure is hierarchical, allowing for nested subsections and paragraphs.",
        "**parse_markdown_to_structure**: The function of parse_markdown_to_structure is to parse markdown text and extract its document structure, including titles, subsections, and paragraphs.\n\n**parameters**: The parameters of this Function are as follows:\n· markdown_text: A string containing the markdown text to be parsed.\n\n**Code Description**: The parse_markdown_to_structure function processes a given markdown text and organizes it into a structured format. It begins by splitting the input markdown text into individual lines. A dictionary named `document` is initialized to hold the overall structure, which includes a title, a level, and subsections. The function maintains a stack called `section_stack` to keep track of the current section being processed, allowing for the hierarchical organization of subsections based on their levels.\n\nAs the function iterates through each line of the markdown text, it checks for non-empty lines. If a line starts with a hash symbol (#), it indicates a title. The function calculates the level of the title by counting the number of leading hash symbols and extracts the title text. If there is any accumulated paragraph text before encountering a new title, it processes that text by extracting citations using the extract_citations function and appending it to the current section's paragraphs.\n\nThe function dynamically adjusts the current section based on the title level, ensuring that subsections are correctly nested within their parent sections. If a line is empty, it signifies a break between paragraphs, prompting the function to process any collected paragraph text.\n\nAt the end of the iteration, the function ensures that any remaining paragraph text is processed and added to the current section. Finally, the structured document is returned.\n\nThe parse_markdown_to_structure function relies on the extract_citations function to identify and extract URLs enclosed within `<citation>` tags from the paragraph text. This integration is essential for maintaining the integrity of citations throughout the document processing workflow, ensuring that all relevant citations are captured and associated with their respective paragraphs.\n\n**Note**: It is important to ensure that the input markdown text is properly formatted, with titles indicated by hash symbols and paragraphs separated by empty lines, for the function to successfully parse and structure the document.\n\n**Output Example**: A possible return value from the parse_markdown_to_structure function could be:\n```python\n{\n    \"document\": {\n        \"title\": \"\",\n        \"level\": 1,\n        \"subsections\": [\n            {\n                \"title\": \"Introduction\",\n                \"level\": 1,\n                \"subsections\": [],\n                \"paragraphs\": [\n                    {\"text\": \"This is the introduction paragraph.\", \"citations\": []}\n                ]\n            },\n            {\n                \"title\": \"Methods\",\n                \"level\": 1,\n                \"subsections\": [\n                    {\n                        \"title\": \"Data Collection\",\n                        \"level\": 2,\n                        \"subsections\": [],\n                        \"paragraphs\": [\n                            {\"text\": \"Data was collected from various sources.\", \"citations\": [\"http://example.com/citation1\"]}\n                        ]\n                    }\n                ],\n                \"paragraphs\": []\n            }\n        ]\n    }\n}\n```"
      ],
      "code_start_line": 159,
      "code_end_line": 225,
      "params": [
        "markdown_text"
      ],
      "have_return": true,
      "code_content": "def parse_markdown_to_structure(markdown_text):\n    \"\"\"从markdown文本解析出文档结构\"\"\"\n    lines = markdown_text.split(\"\\n\")\n    document = {\"document\": {\"title\": \"\", \"level\": 1, \"subsections\": []}}\n\n    current_section = document[\"document\"]\n    section_stack = [current_section]\n    current_level = 1\n    current_text = []\n\n    for line in lines:\n        if line.strip():\n            # 处理标题\n            if line.startswith(\"#\"):\n                # 如果有待处理的段落文本，先处理完\n                if current_text:\n                    paragraph_text = \" \".join(current_text)\n                    if paragraph_text.strip():\n                        citations = extract_citations(paragraph_text)\n                        current_section.setdefault(\"paragraphs\", []).append(\n                            {\"text\": paragraph_text.strip(), \"citations\": citations}\n                        )\n                    current_text = []\n\n                # 处理新标题\n                level = len(line.split()[0])  # 计算#的数量\n                title = \" \".join(line.split()[1:])\n\n                # 根据层级调整当前section\n                while len(section_stack) > 1 and level <= section_stack[-1][\"level\"]:\n                    section_stack.pop()\n\n                new_section = {\n                    \"title\": title,\n                    \"level\": level,\n                    \"subsections\": [],\n                    \"paragraphs\": [],\n                }\n\n                section_stack[-1].setdefault(\"subsections\", []).append(new_section)\n                section_stack.append(new_section)\n                current_section = new_section\n\n            else:\n                # 收集段落文本\n                current_text.append(line)\n        else:\n            # 空行，处理当前段落\n            if current_text:\n                paragraph_text = \" \".join(current_text)\n                if paragraph_text.strip():\n                    citations = extract_citations(paragraph_text)\n                    current_section.setdefault(\"paragraphs\", []).append(\n                        {\"text\": paragraph_text.strip(), \"citations\": citations}\n                    )\n                current_text = []\n\n    # 处理最后一个段落\n    if current_text:\n        paragraph_text = \" \".join(current_text)\n        if paragraph_text.strip():\n            citations = extract_citations(paragraph_text)\n            current_section.setdefault(\"paragraphs\", []).append(\n                {\"text\": paragraph_text.strip(), \"citations\": citations}\n            )\n\n    return document\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/utils.py/extract_citations"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_model_action_decision",
      "md_content": [
        "**_model_action_decision**: The function of _model_action_decision is to determine the next action for an agent based on the provided search results, task, and current section.\n\n**parameters**: The parameters of this Function.\n· agent: An instance of BaseAgent that represents the intelligent agent making the decision.  \n· search_results: A string containing the results of previous searches relevant to the task.  \n· task: A string that describes the current task the agent is working on.  \n· current_section: A string indicating the current section of the task or document being processed.\n\n**Code Description**: The _model_action_decision function is designed to facilitate decision-making for an intelligent agent by leveraging a conversational model. The function begins by invoking the agent's chat_with_template method, which loads a predefined template (\"model_action_decision.txt\") and renders it with the provided search results, task, and current section. This interaction generates a decision from the model, which is expected to include both a thought and a set of actions.\n\nOnce the decision is obtained, the function extracts the thought and actions using the helper functions extract_thought_from_response and extract_actions. It performs validation checks to ensure that the decision and actions are valid; if either is missing, a ValueError is raised, prompting a retry of the decision-making process.\n\nThe function then evaluates the first action from the extracted actions. Depending on the action type—SEARCH, BROWSE, or START_WRITING—the function returns the corresponding thought, action, and any additional data required for that action. For example, if the action is SEARCH, it also extracts queries from the decision response. If the action is BROWSE, it extracts citations, and if the action is START_WRITING, it returns None for the data.\n\nThis function is called by the _action_router function, which orchestrates the overall decision-making process for the agent. The _action_router function utilizes the output of _model_action_decision to determine the next steps, such as executing a new search, browsing web content, or generating written content based on the agent's thought and actions.\n\n**Note**: It is crucial to ensure that the template \"model_action_decision.txt\" exists and is correctly formatted, as the function relies on it to generate the decision. Additionally, the input parameters must be valid and properly structured to avoid exceptions during the decision-making process.\n\n**Output Example**: A possible appearance of the code's return value when executing the function could look like this:\n```python\n(\"I think we should search for more information.\", \"SEARCH\", [\"query1\", \"query2\"])\n```"
      ],
      "code_start_line": 234,
      "code_end_line": 263,
      "params": [
        "agent",
        "search_results",
        "task",
        "current_section"
      ],
      "have_return": true,
      "code_content": "def _model_action_decision(\n    agent: BaseAgent,\n    search_results: str,\n    task: str,\n    current_section: str,\n):\n    decision = agent.chat_with_template(\"model_action_decision.txt\", {\n        \"search_results\": search_results,\n        \"task\": task,\n        \"current_section\": current_section\n    })\n    # 校验返回值\n    thought = extract_thought_from_response(decision)\n    actions = extract_actions(decision)\n    if not decision or not actions:\n        # 抛异常触发重试\n        raise ValueError(f\"Invalid decision (will retry): {decision!r}\")\n    action = next(iter(actions))\n\n    printer.rule(\"Model Decision\")\n    printer.print(decision)\n\n    if action == \"SEARCH\":\n        return thought, action, extract_queries_from_response(decision)\n    if action == \"BROWSE\":\n        return thought, action, extract_citations(decision)\n    if action == \"START_WRITING\":\n        return thought, action, None\n\n    raise ValueError(f\"Unknown action (will retry): {action!r}\")\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/_action_router"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_queries_from_response",
        "src/criticsearch/utils.py/extract_thought_from_response",
        "src/criticsearch/utils.py/extract_citations",
        "src/criticsearch/utils.py/extract_actions"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_action_router",
      "md_content": [
        "**_action_router**: The function of _action_router is to manage the decision-making process of an intelligent agent by determining the next action based on the agent's current state and the results of previous actions.\n\n**parameters**: The parameters of this Function.\n· agent: An instance of BaseAgent that represents the intelligent agent making the decision.  \n· search_results: A string containing the results of previous searches relevant to the task.  \n· task: A string that describes the current task the agent is working on.  \n· current_section: A string indicating the current section of the task or document being processed.  \n· iteration: An integer representing the current iteration of the task processing.  \n· agent_report: A string containing the report generated by the agent so far.  \n· guide_line: A string providing guidelines for the task.  \n· detailed_web_results: An optional string that contains detailed results from web scraping, defaulting to an empty string.\n\n**Code Description**: The _action_router function orchestrates the decision-making process for an intelligent agent by evaluating the agent's current state and determining the next steps based on the actions suggested by the model. It begins by invoking the _model_action_decision function, which assesses the agent's context and returns a thought, action, and any necessary data.\n\nThe function then processes the returned action in a structured manner:\n1. If the action is \"SEARCH\", the function executes a new search using the agent's search aggregator. It appends the new search results to the agent's training data and recursively calls itself with the updated search results to continue the decision-making process.\n2. If the action is \"BROWSE\", the function performs web scraping using the agent's content scraper. It logs the results and also recursively calls itself with the updated search results to determine the next action.\n3. If the action is \"START_WRITING\", the function generates content based on the task, current context, and previous reports. It extracts the thought and answer from the generated content and logs the results in the agent's training data.\n\nThe _action_router function is called within the process_single_task function, which manages the overall task execution by coordinating interactions between multiple agents. It is also invoked recursively to handle the iterative nature of the agent's decision-making process, allowing it to adapt based on new information obtained from searches or web scraping.\n\n**Note**: It is essential to ensure that the agent is properly initialized and that the necessary components, such as the search aggregator and content scraper, are available for the function to operate effectively. The function relies on valid input parameters to avoid exceptions during execution.\n\n**Output Example**: A possible appearance of the code's return value when executing the function could look like this:\n```python\n\"Based on the gathered information, the report concludes that the Syrian opposition has faced significant challenges in recent offensives, with various factions emerging and evolving over time.\"\n```",
        "**_action_router**: The function of _action_router is to orchestrate the decision-making process for an intelligent agent by determining the next actions based on the agent's current state and the results of previous interactions.\n\n**parameters**: The parameters of this Function.\n· agent: An instance of BaseAgent that represents the intelligent agent making the decision.  \n· search_results: A string containing the results of previous searches relevant to the task.  \n· task: A string that describes the current task the agent is working on.  \n· current_section: A string indicating the current section of the task or document being processed.  \n· iteration: An integer representing the current iteration of the task.  \n· agent_report: A string containing the report generated by the agent so far.  \n· guide_line: A string providing guidelines for the current task.  \n· detailed_web_results: An optional string that accumulates detailed results from web scraping, defaulting to an empty string.\n\n**Code Description**: The _action_router function is a critical component of the intelligent agent's workflow, designed to manage and direct the agent's actions based on its decision-making process. The function begins by invoking the _model_action_decision function, which determines the agent's next action based on the provided search results, task, and current section. This function returns three values: thought, action, and data.\n\nThe function then evaluates the action determined by the model. There are three primary actions that the agent can take:\n\n1. **SEARCH**: If the action is to search, the function executes a new search using the agent's search aggregator. The results of this search are appended to the agent's training data, and the agent takes notes based on the new search results. The function then recursively calls itself to process the new search results, allowing the agent to continue its decision-making process.\n\n2. **BROWSE**: If the action is to browse, the function performs web scraping using the agent's content scraper. The results of the web scraping are accumulated with previous results, and the agent again takes notes on the newly scraped content. Similar to the search action, the function recursively calls itself to handle the new information and determine the next steps.\n\n3. **START_WRITING**: If the action is to start writing, the function generates content based on the agent's thought and the context provided. It utilizes the agent's chat_with_template method to create a structured response, which is then processed to extract the thought and answer content. This generated content is printed and logged as part of the agent's training data.\n\nThe _action_router function is called within the process_single_task function, which manages the execution of individual tasks by coordinating interactions between various agents and processing the task to generate a comprehensive report. It is also invoked in the start_writing method of the Session class, where it generates content for specific sections based on the agent's current state and the accumulated search results.\n\n**Note**: It is essential to ensure that the parameters passed to the _action_router function are valid and appropriately structured. The recursive nature of the function requires careful handling of the search results and actions to avoid infinite loops or excessive recursion depth.\n\n**Output Example**: A possible appearance of the code's return value when executing the function could look like this:\n```python\n\"Generated Section Content\"\n```",
        "**_action_router**: The function of _action_router is to manage the decision-making process for an intelligent agent, guiding it through various actions such as searching, browsing, or writing based on the provided context and results.\n\n**parameters**: The parameters of this Function.\n· agent: An instance of BaseAgent that represents the intelligent agent making the decisions.  \n· search_results: A string containing the results of previous searches relevant to the task.  \n· task: A string that describes the current task the agent is working on.  \n· current_section: A string indicating the current section of the task or document being processed.  \n· iteration: An integer representing the current iteration of the task.  \n· agent_report: A string that holds the accumulated report generated by the agent so far.  \n· guide_line: A string providing guidelines for the task at hand.  \n· detailed_web_results: An optional string that contains detailed results from web scraping, defaulting to an empty string if not provided.\n\n**Code Description**: The _action_router function orchestrates the decision-making process for an intelligent agent by determining the next steps based on the agent's thought process, the results of previous searches, and the current task context. It begins by invoking the _model_action_decision function, which assesses the agent's state and generates a thought, action, and any necessary data based on the provided search results, task, and current section.\n\nDepending on the action determined by _model_action_decision, the function can execute one of three primary actions:\n\n1. **SEARCH**: If the action is to search, the function performs a new search using the agent's search aggregator. It appends the new search results to the agent's training data and recursively calls _action_router to process the new results, allowing the agent to make further decisions based on the updated context.\n\n2. **BROWSE**: If the action is to browse, the function utilizes the agent's content scraper to scrape web content from the URLs specified in the data. The results are appended to the detailed web results, and the function again recursively calls _action_router to allow the agent to decide on the next steps based on the newly scraped content.\n\n3. **START_WRITING**: If the action is to start writing, the function generates the final content for the specified section by calling the agent's chat_with_template method. It constructs the content based on the task, current context, guidelines, and previously accumulated notes. The generated content is then logged and returned as the output of the function.\n\nThe _action_router function is called by the process_single_task function, which manages the execution of a single task by initializing the agent and processing the task through various steps. Additionally, it is invoked within the start_writing method of the Session class, which initiates the content generation process for a specific section.\n\n**Note**: It is essential to ensure that the agent is properly initialized and that all input parameters are valid. The function relies on the correct functioning of the _model_action_decision function and the agent's methods for searching, browsing, and writing. Proper error handling should be in place to manage any exceptions that may arise during the execution of these actions.\n\n**Output Example**: A possible appearance of the code's return value when executing the function could look like this:\n```python\n\"The generated content for the specified section is: This is the introduction to the topic.\"\n```"
      ],
      "code_start_line": 266,
      "code_end_line": 329,
      "params": [
        "agent",
        "search_results",
        "task",
        "current_section",
        "iteration",
        "agent_report",
        "guide_line",
        "detailed_web_results"
      ],
      "have_return": true,
      "code_content": "def _action_router(\n    agent: BaseAgent,\n    search_results: str,    \n    task: str,\n    current_section: str,\n    iteration: int,\n    agent_report: str,\n    guide_line: str,\n    detailed_web_results: str = \"\",\n):\n    # 获取模型的行动决策\n    thought, action, data = _model_action_decision(agent, search_results, task, current_section)\n\n    printer.log(f\"data: {data}\")\n\n    # 根据不同的行动类型进行处理\n    if action == \"SEARCH\":\n        # 执行新的搜索\n        new_search_results = asyncio.run(agent.search_aggregator.search(data))\n        agent.training_data.append({\"from\": \"agent\", \"thought\": thought, \"action\": action, \"action_content\": data, \"action_result\": new_search_results[:200]})\n\n        ## 让模型记笔记\n        new_notes = agent.taking_notes(new_search_results); agent.training_data.append({\"from\": \"agent\", \"action\": \"TAKING_NOTES\", \"action_content\": new_notes})\n        # 递归调用自身处理新的搜索结果让模型决定下一步的行动\n        return _action_router(agent, new_search_results, task, current_section,iteration, agent_report, guide_line, detailed_web_results)\n        \n    elif action == \"BROWSE\":\n        # 执行网页爬取\n        web_scraper_results = asyncio.run(\n            agent.content_scraper.scrape(urls=data)\n        )\n        detailed_web_results+='\\n\\n'+web_scraper_results # 防止出现连续的browsing， 要把之前的browsing结果保存下来\n        agent.training_data.append({\"from\": \"agent\", \"thought\": thought, \"action\": action, \"action_content\": data, \"action_result\": web_scraper_results[:200]})\n        ## 让模型记笔记\n        new_notes = agent.taking_notes(web_scraper_results); agent.training_data.append({\"from\": \"agent\", \"action\": \"TAKING_NOTES\", \"action_content\": new_notes})\n        # 递归调用自身处理爬取结果让模型决定下一步的行动\n        return _action_router(\n            agent, search_results + '\\n\\n' + web_scraper_results, task, current_section,\n            iteration, agent_report, guide_line, detailed_web_results\n        )\n        \n    elif action == \"START_WRITING\":\n        # 生成最终的内容\n        section_content = agent.chat_with_template(\n            \"guided_generation_thought.txt\",\n            {\n                \"task\": task,\n                \"context\": \"There is no previous context since this is the first section at the beginning of the article, only the user's question exists\"\n                if iteration == 0\n                else f\"Previous report content written by you is:\\n\\n{agent_report}\",\n                \"guidline\": guide_line,\n                \"search_result\": detailed_web_results,\n                \"memo\": agent.memo, # 每次模型写新的内容的时候都会可以看到之前所有的笔记\n            }\n        )\n\n        writing_thought = extract_thought_from_response(section_content)\n        writing_content = extract_answer_from_response(section_content)\n\n        printer.rule(\"Generated Section Content\")\n        printer.print(section_content)\n        agent.training_data.append({\"from\": \"agent\", \"thought\": writing_thought+'\\n\\n'+thought, \"action\": action, \"action_content\": writing_content, \"citation\": extract_citations(section_content)})\n\n        return writing_content\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/session.py",
        "src/criticsearch/session.py/Session/start_writing"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/taking_notes",
        "src/criticsearch/main.py/_model_action_decision",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_thought_from_response",
        "src/criticsearch/utils.py/extract_answer_from_response",
        "src/criticsearch/utils.py/extract_citations",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "process_single_task",
      "md_content": [
        "**process_single_task**: The function of process_single_task is to execute a single task by interacting with various agents to generate a comprehensive report based on the provided task and specified maximum iterations.\n\n**parameters**: The parameters of this Function.\n· task: A string representing the task or question that the agent needs to process.\n· max_iterations: An integer indicating the maximum number of iterations to perform while processing the task.\n\n**Code Description**: The process_single_task function is designed to manage the execution of a specific task by coordinating interactions between multiple agents, including a common agent, a search aggregator, and a verifier. The function begins by initializing necessary components such as the common agent, search aggregator, and report verifier. It then loads a JSON file containing benchmark data, which is essential for generating the report.\n\nThe function processes the task iteratively, up to the specified maximum number of iterations. In each iteration, it assesses the confidence of the agent in its response and determines whether to generate a direct answer or to conduct a guided search based on the outline of the report. If the agent is confident, it provides a direct response; otherwise, it engages in a guided search to gather relevant information.\n\nDuring the guided search, the function extracts thought processes and queries from the agent's responses, which are then used to perform web searches and scrape results. The generated content for each section of the report is constructed in parallel using a thread pool, allowing for efficient processing of multiple sections simultaneously. The function also incorporates a verification step, where the generated answers are checked against extracted facts to ensure accuracy.\n\nThe process_single_task function is called by the execute_multiple_tasks function, which handles the execution of multiple tasks in a loop. This integration allows for the systematic processing of each task, logging the conversation history for later review.\n\n**Note**: It is important to ensure that the input task is well-defined and that the maximum iterations parameter is set appropriately to balance between thoroughness and efficiency. The function relies on the proper functioning of the agents and the availability of the JSON benchmark file to operate effectively.\n\n**Output Example**: A possible return value from the process_single_task function could be a string summarizing the final report generated, such as:\n```\n\"Based on the gathered information, the report concludes that the Syrian opposition has faced significant challenges in recent offensives, with various factions emerging and evolving over time.\"\n```",
        "**process_single_task**: The function of process_single_task is to execute a single task by coordinating interactions between various agents, processing the task, and generating a comprehensive report.\n\n**parameters**: The parameters of this Function.\n· task: A string representing the task or question that the agent needs to process.\n· file_name: An optional string that specifies the name of the JSON file containing ground truth data. If not provided, a default file name is used.\n\n**Code Description**: The process_single_task function is designed to manage the execution of a specific task by leveraging the capabilities of various components within the system. Upon invocation, it initializes an instance of the BaseAgent class, which serves as the core agent responsible for handling the task. The function begins by receiving the task and setting up the agent's training data, which includes the task itself and initializes an empty memo for storing notes.\n\nThe function then prepares to load a JSON file containing benchmark data, which is essential for generating the report. This is done using the importlib.resources module to ensure that the file is correctly located within the package structure. The ReportBenchmark class is instantiated with the loaded JSON file, and it generates a benchmark item that outlines the structure and content of the report to be created.\n\nAs the task progresses, the function logs the initial user question to the conversation history, ensuring that all interactions are recorded for future reference. A progress bar is initialized to provide visual feedback on the task's completion status.\n\nThe core processing logic involves evaluating the agent's confidence in its responses. If the agent is confident, it generates a direct response to the task. If not, it iteratively processes each section of the report outline, performing searches and extracting relevant information based on the task. The agent interacts with various templates to generate content, take notes, and validate the accuracy of the information gathered.\n\nThroughout the execution, the function utilizes the ReportVerifier class to ensure the factual accuracy of the generated content against the extracted facts. The results are compiled into a final report, which is then saved to a JSON file for future reference.\n\nThe process_single_task function is called from the main function, which serves as the entry point for the application. It is also invoked by the execute_multiple_tasks function to handle multiple tasks in sequence. This integration highlights the function's role in orchestrating the overall task execution workflow, ensuring that each task is processed systematically and efficiently.\n\n**Note**: It is important to ensure that the input task is well-defined and relevant to the agent's capabilities. The function relies on the proper configuration of the agent and the availability of the necessary data files to function effectively. Additionally, the caching mechanism for benchmark results should be managed to optimize performance and avoid redundant computations.",
        "**process_single_task**: The function of process_single_task is to manage the execution of a single task by initializing an agent, processing the task through various steps, and generating a comprehensive report based on the provided input.\n\n**parameters**: The parameters of this Function.\n· task: A string representing the task description that needs to be processed by the agent.  \n· file_name: An optional string that specifies the name of the ground truth data file. If not provided, it defaults to \"2024_Syrian_opposition_offensives.json\".\n\n**Code Description**: The process_single_task function is a critical component of the CriticSearch pipeline, responsible for orchestrating the processing of a user-defined task. Upon invocation, the function initializes an instance of the BaseAgent class, which serves as the core agent for managing interactions and processing tasks. The agent receives the task description through the receive_task method, which sets the context for subsequent operations.\n\nThe function then prepares the training data for the agent, initializing it with the task provided by the user. It also sets up a progress bar to visually indicate the processing status of the task. The progress bar is configured to display the number of completed sections out of the total sections to be processed.\n\nNext, the function retrieves benchmark data from a specified JSON file using the ReportBenchmark class. This class is responsible for generating benchmark items that guide the report generation process. The generate_benchmark_item method is called to create a structured outline based on the input data.\n\nAs the function processes each section of the outline, it evaluates the agent's confidence in generating responses. If the agent is confident, it directly generates an answer. If not, the function engages in a guided search process, where it formulates queries based on the context and retrieves relevant information through the agent's search aggregator. The results of these searches are recorded in the agent's training data.\n\nThe function also incorporates a verification step using the ReportVerifier class, which assesses the factual accuracy of the generated content against the extracted facts. The accuracy scores are appended to the agent's training data, ensuring that the performance of the agent is logged for analysis.\n\nFinally, the function returns the agent's training data, which includes all interactions, generated content, and accuracy assessments. This data can be utilized for further analysis or reporting purposes.\n\nThe process_single_task function is called by the main function, which serves as the entry point for executing the CriticSearch pipeline. It is also invoked by other functions, such as execute_multiple_tasks and run, which handle the execution of multiple tasks or specific task executions, respectively. This highlights the function's role in the broader context of task management and processing within the application.\n\n**Note**: It is essential to ensure that the input task is well-defined and relevant to the capabilities of the agent. The optional file_name parameter should be specified if a specific ground truth data file is required for the task processing. Proper configuration of the environment and availability of necessary resources are crucial for the successful execution of this function.\n\n**Output Example**: A possible appearance of the code's return value when executing the function could look like this:\n```json\n[\n    {\"from\": \"human\", \"value\": \"What is the status of the Syrian opposition?\"},\n    {\"from\": \"agent\", \"thought\": \"The current status is...\", \"action\": \"SEARCH\", \"action_content\": [\"query1\", \"query2\"], \"action_result\": [\"result1\", \"result2\"]},\n    {\"from\": \"verifier\", \"section\": \"Section 1\", \"accuracy\": 0.85},\n    {\"from\": \"agent\", \"final_report\": \"The comprehensive report is...\", \"citation\": [\"http://example.com/citation1\"]}\n]\n```",
        "**process_single_task**: The function of process_single_task is to manage the execution of a single task by initializing an agent, processing the task, and generating a report based on the provided input.\n\n**parameters**: The parameters of this Function.\n· task: A string representing the user task description that needs to be processed.\n· file_name: An optional string that specifies the name of the ground truth data file. If not provided, it defaults to None.\n\n**Code Description**: The process_single_task function is designed to handle the core logic of processing a user-defined task within the CriticSearch pipeline. It begins by initializing an instance of the BaseAgent class, which serves as the foundation for managing interactions, search functionalities, and conversation history. The agent receives the task input through the receive_task method, which stores the original task for future reference.\n\nThe function sets up the agent's training data with the initial task and prepares a memoization structure to store unique notes extracted during the processing. It also initializes a ReportVerifier instance, which is responsible for verifying the factual accuracy of the generated content against a set of extracted facts.\n\nNext, the function determines the path to the JSON file containing ground truth data, defaulting to \"2024_Syrian_opposition_offensives.json\" if no file name is provided. It then reads the JSON file and generates benchmark items using the ReportBenchmark class, which organizes the content into manageable sections for processing.\n\nThe agent's user question is set to the provided task, and the conversation history is updated to include the user's input. A progress bar is initialized to provide visual feedback during the processing of the task.\n\nThe function proceeds to evaluate the agent's confidence in generating a response. If the agent is confident, it generates a direct response. If not, it iterates through the sections of the benchmark outline, performing guided searches and extracting relevant information for each section. The agent's training data is updated with the results of these actions, including search thoughts, queries, and notes taken from the search results.\n\nFinally, the generated report is compiled, and the function returns the training data, which includes the final report and any citations extracted during the process. The process_single_task function is called by the main function, which serves as the entry point for executing the CriticSearch pipeline. This establishes a clear relationship between user input and the underlying task execution logic.\n\n**Note**: It is essential to ensure that the input task is well-defined and relevant to the agent's capabilities. The function relies on the proper initialization of the agent and the availability of the specified JSON file for accurate processing and report generation.\n\n**Output Example**: A possible appearance of the code's return value when executing the function could look like this:\n```json\n[\n    {\"from\": \"human\", \"value\": \"What is the status of the Syrian opposition?\"},\n    {\"from\": \"agent\", \"thought\": \"The current status is...\", \"action\": \"SEARCH\", \"action_content\": [\"query1\", \"query2\"], \"action_result\": [\"result1\", \"result2\"]},\n    {\"from\": \"verifier\", \"section\": \"Section 1\", \"accuracy\": 0.85},\n    {\"from\": \"agent\", \"final_report\": \"The comprehensive report is...\", \"citation\": [\"http://example.com/citation1\"]}\n]\n```"
      ],
      "code_start_line": 332,
      "code_end_line": 441,
      "params": [
        "task",
        "file_name"
      ],
      "have_return": true,
      "code_content": "def process_single_task(task, file_name=None):\n    # Initialize agents\n    agent = BaseAgent()\n    agent.receive_task(task)\n    agent.training_data = [\n        {\"from\": \"human\", \"value\": task},\n    ]\n    agent.memo = set()\n    search_agg = agent.search_aggregator\n    verifier = ReportVerifier(agent)\n\n    package_name = \"criticsearch.reportbench.wiki_data\"\n    file_name = file_name or \"2024_Syrian_opposition_offensives.json\"\n\n    with importlib.resources.files(package_name).joinpath(file_name) as json_file_path:\n        json_file = str(json_file_path)\n\n    benchmark = ReportBenchmark(json_file)\n    outline = benchmark.generate_benchmark_item(max_window_tokens=1, use_cache=True)\n\n    # initialize the task\n    agent.user_question = task\n\n    printer.log(f\"Starting the conversation with task: \\n{task}\")\n\n    BaseAgent.conversation_manager.append_to_history(role=\"user\", content=task)\n\n    # ============ 进度条 =============\n    progress = Progress(\n        TextColumn(\"[bold cyan]Chapter Progress:\"),\n        BarColumn(bar_width=None),\n        # 显示 completed(name)/total\n        TextColumn(\"{task.completed}({task.fields[section_name]})/{task.total}\"),\n        TextColumn(\"{task.percentage:>3.0f}%\"),\n        disable=getattr(settings, \"disable_progress\", False),\n    )\n    with progress:\n        # 初始时用第一个 section 的 name 作为 section_name\n        first_name = outline[0][\"path\"].split(\"->\")[-1].strip().lstrip(\"# \").strip()\n        section_task = progress.add_task(\"processing\", total=len(outline), section_name=first_name)\n\n        printer.rule(\"BEGIN SECTION‑BY‑SECTION GENERATION\")\n\n        # 先判断一次模型信心\n        agent_confident = agent.chat_with_template(\"agent_confidence.txt\", {\"user_question\": task})\n        agent_confident_yaml = agent.extract_and_validate_yaml(agent_confident)\n\n        if agent_confident_yaml is None:\n            printer.log(\"Failed to extract valid YAML content. Defaulting to 'false'.\")\n            agent_confident = False\n        else:\n            agent_confident_dict = yaml.safe_load(agent_confident_yaml)\n            agent_confident = agent_confident_dict.get(\"confidence\", \"true\").lower() == \"true\"\n\n        if agent_confident:\n            agent_answer = agent.chat_with_template(\"direct_response.txt\", {\"task\": task})\n        else:\n            agent_report_sections = []  # Initialize the list to store report sections\n            for item in outline:  # 按wiki的GT大纲走一次滑窗\n                # 提取当前窗口的“名称”，这里直接用 path 文本，或者你也可以进一步 split 拿最后的标题\n                current_name = item[\"path\"].split(\"->\")[-1].strip().lstrip(\"# \").strip()\n                # 更新进度条里的字段\n                progress.update(section_task, section_name=current_name)\n                # 推进“节”计数\n                progress.advance(section_task)\n\n                section_path = item[\"path\"]\n                merged_section_content = item[\"merged_section_window_content\"]\n                extracted_facts = item[\"extracted_facts\"]\n\n                agent_report = \"\\n\".join(agent_report_sections)\n\n                search_thought_and_queries = agent.chat_with_template(\n                    \"guided_search_thought.txt\",\n                    {\n                        \"task\": task,\n                        \"context\": \"There is no previous context since this is the first section at the beginning of the article, only the user's question exists\"\n                        if not agent_report\n                        else f\"Previous text written by the Agent is:\\n\\n{agent_report}\",\n                        \"GroundTruth\": merged_section_content,\n                    },\n                )\n\n                thought_content = extract_thought_from_response(search_thought_and_queries)\n                queries_list = extract_queries_from_response(search_thought_and_queries)\n                printer.print(search_thought_and_queries)\n                search_results = asyncio.run(search_agg.search(queries_list))\n                agent.training_data.append({\"from\": \"agent\", \"thought\": thought_content, \"action\": \"SEARCH\", \"action_content\": queries_list, \"action_result\": search_results})\n                # detailed_web_results = agent.web_scrape_results(search_results)\n                new_notes = agent.taking_notes(search_results); agent.training_data.append({\"from\": \"agent\", \"action\": \"TAKING_NOTES\", \"action_content\": new_notes})\n\n                ## 从这里开始我们提供了详细的网页信息，由模型决定下一步的行动\n                ## 模型会根据上一次的搜索结果，决定下一步的行动\n                answer_content = _action_router(agent, search_results, task, section_path, 0, agent_report, section_path, search_results) # 本次迭代模型生成的这一节（paragraph）内容\n\n                # 将生成的内容添加到agent_report_sections\n                agent_report_sections.append(answer_content)\n\n                # 使用verifier进行factual QA验证，得到这个段落写作的reward分数（也就是准确率）\n                accuracy = verifier.verify_section(answer_content, extracted_facts)\n                agent.training_data.append({\"from\": \"verifier\", \"section\": section_path, \"accuracy\": accuracy})\n\n            # 拼接完整的report\n            agent_answer = \"\\n\".join(agent_report_sections)\n            agent.training_data.append({\n                \"from\": \"agent\",\n                \"final_report\": agent_answer,\n                \"citation\": extract_citations(agent_answer)\n            })\n            return agent.training_data\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/main",
        "src/criticsearch/tasks_runner.py",
        "src/criticsearch/tasks_runner.py/execute_multiple_tasks",
        "src/criticsearch/tasks_runner.py/execute_from_mapping/run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/extract_and_validate_yaml",
        "src/criticsearch/base_agent.py/BaseAgent/receive_task",
        "src/criticsearch/base_agent.py/BaseAgent/taking_notes",
        "src/criticsearch/main.py/_action_router",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_queries_from_response",
        "src/criticsearch/utils.py/extract_thought_from_response",
        "src/criticsearch/utils.py/extract_citations",
        "src/criticsearch/models.py/ConversationManager/append_to_history",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_benchmark_item",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/verify_section"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: The function of main is to serve as the entry point for executing the CriticSearch pipeline, handling user input and managing task processing.\n\n**parameters**: The parameters of this Function.\n· task: A string representing the user task description that needs to be processed.\n· file_name: An optional string that specifies the name of the ground truth data file. If not provided, it defaults to None.\n\n**Code Description**: The main function initializes an argument parser using the argparse module to facilitate command-line interaction. It sets up the parser with a description of the application and defines the required positional argument \"task\" along with an optional argument for the file name, which can be abbreviated with \"-f\". \n\nOnce the arguments are parsed, the function attempts to process the specified task by calling the process_single_task function, passing the task and file_name as parameters. This function is responsible for executing the core logic of the CriticSearch pipeline, which includes managing agents, processing user queries, and generating reports based on the provided task.\n\nIf the task processing is successful and returns a result, the main function prints the result to the console. In the event of an exception during the task processing, the function catches the exception and invokes the print_exception method from the RichPrinter class to log the error message and print the exception details. This ensures that any issues encountered during execution are communicated clearly to the user, and the application exits with a status code of 1 to indicate failure.\n\nThe main function serves as a critical component of the application, orchestrating the flow of data from user input to task execution and error handling. It directly interacts with the process_single_task function, which encapsulates the logic for processing the task, and the print_exception method, which enhances the application's robustness by providing informative error messages.\n\n**Note**: It is essential for users to provide a well-defined task description when invoking the main function. The optional file_name parameter should also be specified if a specific ground truth data file is required for the task processing. Proper usage of command-line arguments is crucial for the successful execution of the CriticSearch pipeline."
      ],
      "code_start_line": 443,
      "code_end_line": 454,
      "params": [],
      "have_return": false,
      "code_content": "def main():\n    parser = argparse.ArgumentParser(description=\"Run CriticSearch pipeline\")\n    parser.add_argument(\"task\", help=\"用户任务描述\")\n    parser.add_argument(\"--file-name\", \"-f\", default=None, help=\"GT 数据文件名\")\n    args = parser.parse_args()\n    try:\n        result = process_single_task(args.task, file_name=args.file_name)\n        if result:\n            print(result)\n    except Exception as e:\n        printer.print_exception(f\"运行失败: {e}\")\n        sys.exit(1)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/rich_output.py/RichPrinter/print_exception"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "src/criticsearch/__init__.py": [],
  "src/criticsearch/main_old_paralell.py": [
    {
      "type": "FunctionDef",
      "name": "flatten_outline",
      "md_content": [
        "**flatten_outline**: The function of flatten_outline is to flatten a hierarchical outline structure into a list of sections, each annotated with its depth and path.\n\n**parameters**: The parameters of this Function.\n· section: A dictionary representing a section of the outline, which may contain a title and potentially children sections.\n· depth: An integer indicating the current depth level in the outline hierarchy, defaulting to 1.\n· path: A list that tracks the path of titles leading to the current section, defaulting to None.\n\n**Code Description**: The flatten_outline function is designed to transform a nested outline structure, typically represented in JSON format, into a flat list. Each entry in this list contains a dictionary with three keys: \"path\", \"section\", and \"depth\". The \"path\" key holds a list of titles leading to the current section, the \"section\" key contains the original section data, and the \"depth\" key indicates how deep the section is within the hierarchy.\n\nThe function begins by checking if the path parameter is None; if so, it initializes it as an empty list. It then constructs a dictionary for the current section, appending the section's title to the path and setting the depth. This dictionary is added to a list called flat, which will ultimately be returned.\n\nIf the current section contains children (indicating that it is not a leaf node), the function iterates over each child section. For each child, it recursively calls flatten_outline, increasing the depth by one and passing the updated path. The results from these recursive calls are extended into the flat list.\n\nThe flatten_outline function is called within the main function of the project, specifically after the outline has been generated from search results. The resulting flat_sections list is then used to generate content for each section in parallel, allowing for efficient processing of potentially large outlines. This integration highlights the function's role in transforming complex hierarchical data into a manageable format for further processing.\n\n**Note**: It is important to ensure that the input section is structured correctly as a JSON object with the expected keys (\"title\" and \"children\") for the function to operate effectively.\n\n**Output Example**: An example return value of flatten_outline might look like this for a given outline:\n[\n    {\"path\": [\"Introduction\"], \"section\": {\"title\": \"Introduction\", \"children\": [...]}, \"depth\": 1},\n    {\"path\": [\"Introduction\", \"Background\"], \"section\": {\"title\": \"Background\", \"children\": [...]}, \"depth\": 2},\n    {\"path\": [\"Introduction\", \"Background\", \"Details\"], \"section\": {\"title\": \"Details\"}, \"depth\": 3}\n]"
      ],
      "code_start_line": 15,
      "code_end_line": 24,
      "params": [
        "section",
        "depth",
        "path"
      ],
      "have_return": true,
      "code_content": "def flatten_outline(section, depth=1, path=None):\n    # 将 outline_json 展平，记录每个节点及其层级和路径\n    if path is None:\n        path = []\n    current = {\"path\": path + [section.get(\"title\")], \"section\": section, \"depth\": depth}\n    flat = [current]\n    if \"children\" in section:\n        for child in section[\"children\"]:\n            flat.extend(flatten_outline(child, depth + 1, path + [section.get(\"title\")]))\n    return flat\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "generate_content_for_section",
      "md_content": [
        "**generate_content_for_section**: The function of generate_content_for_section is to generate detailed textual content for a specified section based on a search query and a given task.\n\n**parameters**: The parameters of this Function.\n· common_agent: An instance of the BaseAgent class that facilitates search and chat functionalities.\n· section: A dictionary containing information about the section, specifically its title.\n· TASK: A string representing the overarching task or topic under which the content is to be generated.\n\n**Code Description**: The generate_content_for_section function is designed to create content for a specific section of a report or document. It begins by extracting the title from the provided section dictionary. Using this title, it formulates a search query that prompts the common_agent to generate relevant search queries related to the title within the context of the specified TASK. The search results obtained from the common_agent's search_and_browse method are then utilized to construct a detailed prompt for generating content.\n\nThis prompt instructs the common_agent to write one or several paragraphs that logically present data and facts about the section's title, ensuring that all information is cited correctly using a specified format. The content is generated through the common_agent's common_chat method, which processes the prompt and returns the generated paragraphs.\n\nThe function is called within a multi-threaded context in the main function of the project. Specifically, it is invoked for each section of a flattened outline generated from initial search results. This allows for parallel content generation across multiple sections, enhancing efficiency and reducing the overall time required to compile the report. The results from each call to generate_content_for_section are collected and later reconstructed into a coherent markdown format.\n\n**Note**: It is important to ensure that the common_agent is properly initialized and that the section parameter contains a valid title to avoid errors during content generation. Additionally, the function's output should be formatted correctly to maintain consistency in citation style.\n\n**Output Example**: A possible appearance of the code's return value could be:\n\"Climate change is a pressing global issue that affects various aspects of life on Earth. According to the Intergovernmental Panel on Climate Change (IPCC), the average global temperature has risen by approximately 1.2 degrees Celsius since the late 19th century<\\cite>https://www.ipcc.ch/report/ar6/wg1/#:~:text=The%20average%20global%20temperature%20has%20risen%20by%20approximately%201.2%20degrees%20Celsius%20since%20the%20late%2019th%20century<\\cite>. This increase has led to more frequent and severe weather events, including hurricanes, droughts, and floods<\\cite>https://www.ncdc.noaa.gov/sotc/global/202012#climate<\\cite>.\""
      ],
      "code_start_line": 26,
      "code_end_line": 39,
      "params": [
        "common_agent",
        "section",
        "TASK"
      ],
      "have_return": true,
      "code_content": "def generate_content_for_section(common_agent, section, TASK):\n    # 保持 prompt 不变，仅生成单层章节内容，不递归\n    title = section.get(\"title\")\n    search_query = f\"generate some search queries about '{title}' under the background of this TOPIC/TASK: '{TASK}'.\"\n    search_results = common_agent.search_and_browse(search_query)\n    prompt = (\n        f\"Using the following search results:\\n\\n{search_results}\\n\\n\"\n        f\"Write one or several detailed paragraphs with data and facts in a logical way about '{title}' under the background of this TOPIC/TASK: '{TASK}', formatted in pure text, without summary sentences.\"\n        f\"Please make sure you are always obeying and using '<\\cite>The url link that you used for supporting the previous statement<\\cite>' format in every sentence that you are using data from the web.\"\n    )\n    paragraph = common_agent.common_chat(usr_prompt=prompt)\n    print(f\"--- Generated content for '{title}' ---\")\n    print(paragraph)\n    return paragraph\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "reconstruct_markdown",
      "md_content": [
        "**reconstruct_markdown**: The function of reconstruct_markdown is to generate a final Markdown document based on a hierarchical outline and corresponding content.\n\n**parameters**: The parameters of this Function.\n- outline: A dictionary that represents the hierarchical structure of the document, including sections and their titles.\n- flat_contents: A list of tuples where each tuple contains a dictionary representing a section and the corresponding content for that section.\n\n**Code Description**: \nThe `reconstruct_markdown` function is responsible for assembling a Markdown document by combining the hierarchical structure of a document (provided in the `outline` parameter) with corresponding content (provided in the `flat_contents` parameter). It follows these steps:\n1. **Mapping Content**: It first creates a dictionary (`content_map`) where each key is the full path of a section (represented as a tuple of strings), and the value is the content associated with that section. This is done by iterating over the `flat_contents` and extracting the path and content for each section.\n   \n2. **Recursive Content Generation**: The function defines a helper function `helper` that is recursively used to generate Markdown for each section of the outline. This function takes a section and the current path (starting as an empty list) as input.\n   - For each section, it combines the title (from the `title` field) with the correct level of Markdown headers (`#` symbols based on the depth of the section in the outline).\n   - If content for the section exists (i.e., its path is found in the `content_map`), the content is appended to the Markdown string for that section.\n   - If the section has child sections (i.e., if `children` exist), the function recursively processes each child section, appending their corresponding Markdown to the current section.\n\n3. **Result Construction**: The `helper` function is initially called for the top-level sections of the document, and the resulting Markdown for all sections is concatenated into the `result` string. If the `outline` contains a title, it is added at the beginning of the Markdown document with the appropriate header (`#`).\n\n4. **Final Markdown Output**: After all sections and their content have been processed, the final Markdown string is returned.\n\nThis function is integral to the document generation pipeline. Specifically, it is used in the context of a broader workflow where an agent gathers search results and creates an outline, followed by generating content for individual sections. Once content is generated in parallel, `reconstruct_markdown` is called to structure that content into a final Markdown document that is later polished and saved.\n\nThe function is invoked within the `main` function as part of a multi-step document generation process. After gathering search results and content for individual sections, `reconstruct_markdown` is used to combine the content and hierarchical structure into a Markdown report, which is further polished and stored.\n\n**Note**: The function relies on the assumption that `flat_contents` contains content for each section as a tuple where the first element is the section's metadata (containing a \"path\") and the second element is the content itself. The path is crucial for matching the content to the correct section in the `outline`. Additionally, the recursive nature of the `helper` function means that deeply nested sections are properly handled by the same logic.\n\n**Output Example**: \nFor an outline like:\n```json\n{\n    \"title\": \"Main Title\",\n    \"children\": [\n        {\n            \"title\": \"Section 1\",\n            \"children\": [\n                {\"title\": \"Subsection 1.1\", \"children\": []},\n                {\"title\": \"Subsection 1.2\", \"children\": []}\n            ]\n        },\n        {\n            \"title\": \"Section 2\",\n            \"children\": []\n        }\n    ]\n}\n```\nand corresponding content for each section, the generated Markdown might look like:\n```\n# Main Title\n\n## Section 1\n\n### Subsection 1.1\n\nContent for Subsection 1.1\n\n### Subsection 1.2\n\nContent for Subsection 1.2\n\n## Section 2\n\nContent for Section 2\n```"
      ],
      "code_start_line": 41,
      "code_end_line": 77,
      "params": [
        "outline",
        "flat_contents"
      ],
      "have_return": true,
      "code_content": "def reconstruct_markdown(outline, flat_contents):\n    \"\"\"\n    根据展平后的内容与原 outline 结构，拼接生成最终 Markdown 文本\n    使用完整路径为key确保唯一性\n    \"\"\"\n    # 构建以完整路径为键的映射字典\n    content_map = {}\n    for item, content in flat_contents:\n        path_key = tuple(item[\"path\"])  # 使用完整路径作为key\n        content_map[path_key] = content\n\n    def helper(section, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n        \n        # 生成标题（深度就是路径的长度）\n        depth = len(current_path)\n        md = f\"{'#' * depth} {section.get('title')}\\n\\n\"\n        \n        # 添加该节的内容（如果有）\n        if path_key in content_map:\n            md += f\"{content_map[path_key]}\\n\\n\"\n            \n        # 处理子节点\n        if \"children\" in section:\n            for child in section[\"children\"]:\n                md += helper(child, current_path)\n        return md\n\n    result = \"\"\n    if \"title\" in outline:\n        result += f\"# {outline['title']}\\n\\n\"\n    \n    for section in outline.get(\"children\", []):\n        result += helper(section, [outline.get(\"title\")] if \"title\" in outline else [])\n    \n    return result\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "helper",
      "md_content": [
        "**helper**: The function of helper is to recursively generate Markdown text for a given section and its nested sub-sections.\n\n**parameters**:\n- parameter1: section (dict) - A dictionary representing a section, containing a title and possibly children, which are nested sections.\n- parameter2: path (list, optional) - A list representing the current path of titles. Default is an empty list, and it is used to track the hierarchy of section titles.\n\n**Code Description**:  \nThe `helper` function is responsible for generating a Markdown representation of a section, including its title and any associated content. It handles nested sections by recursively calling itself for child sections, creating a hierarchical structure based on the depth of the section in the hierarchy. The function performs the following operations:\n\n1. **Initialize Path**: The function begins by constructing a `current_path`, which combines the existing `path` with the title of the current section. This path is used to track the hierarchical level of the section.\n\n2. **Path Key**: It then creates a `path_key`, which is a tuple formed by the `current_path`. This tuple is used to check if there is any associated content for the section in the `content_map`.\n\n3. **Generate Markdown for the Title**: The depth of the section is determined by the length of the `current_path`. The number of hashtags (`#`) in the Markdown title is equal to the depth of the section. The function then constructs the section title in Markdown format (`# Title`).\n\n4. **Include Content**: If there is any content mapped to the `path_key` in the `content_map`, this content is added below the section title. The content is added in Markdown format.\n\n5. **Handle Child Sections**: If the current section has any children (i.e., sub-sections), the function iterates over them and recursively calls itself on each child, appending the generated Markdown to the result.\n\n6. **Return the Result**: After processing the title, content, and children (if any), the function returns the generated Markdown string for the current section and its descendants.\n\n**Note**: \n- The `content_map` must be defined elsewhere in the code, as it is used to retrieve the content associated with each section's path.\n- The function is designed to handle hierarchical section structures where each section can have nested sub-sections. The depth of each section is represented in the generated Markdown by the number of `#` symbols in the title.\n- The `path` parameter allows the function to track the nesting level of each section and ensures that the correct content is included at each level of the hierarchy.\n\n**Output Example**:  \nGiven a sample input where `content_map` contains content for each section and its sub-sections, the output might look like this:\n\nFor a `section` with the structure:\n```python\n{\n    \"title\": \"Introduction\",\n    \"children\": [\n        {\n            \"title\": \"Background\",\n            \"children\": []\n        },\n        {\n            \"title\": \"Objective\",\n            \"children\": []\n        }\n    ]\n}\n```\n\nAnd assuming `content_map` has content for \"Introduction\", \"Background\", and \"Objective\", the generated output could look like:\n\n```\n# Introduction\n\nThis is the content for the Introduction section.\n\n## Background\n\nThis is the content for the Background section.\n\n## Objective\n\nThis is the content for the Objective section.\n```\n\nThis example illustrates how the function constructs the hierarchy using `#` for titles and includes content where available."
      ],
      "code_start_line": 52,
      "code_end_line": 68,
      "params": [
        "section",
        "path"
      ],
      "have_return": true,
      "code_content": "    def helper(section, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n        \n        # 生成标题（深度就是路径的长度）\n        depth = len(current_path)\n        md = f\"{'#' * depth} {section.get('title')}\\n\\n\"\n        \n        # 添加该节的内容（如果有）\n        if path_key in content_map:\n            md += f\"{content_map[path_key]}\\n\\n\"\n            \n        # 处理子节点\n        if \"children\" in section:\n            for child in section[\"children\"]:\n                md += helper(child, current_path)\n        return md\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_citations",
      "md_content": [
        "**extract_citations**: The function of extract_citations is to extract all citations (URLs) enclosed in `<cite>` tags from a given text.\n\n**parameters**:  \n· text: The input string in which citations need to be extracted.\n\n**Code Description**:  \nThe `extract_citations` function is responsible for identifying and extracting URLs that are enclosed within `<cite>` HTML tags in a given text. It uses a regular expression pattern to match these tags and captures the content inside them.\n\nThe function begins by initializing an empty list called `citations`, which will hold the URLs found in the input text. It then defines a regular expression pattern `r'<cite>(.*?)<\\/cite>'`. This pattern looks for text between `<cite>` and `</cite>` tags. The `re.findall()` function is then used to search the provided text for all matches of this pattern. Each match found is a string containing the URL or citation within the `<cite>` tag. The function returns a list of all these matches.\n\nThe function is used within the `process_section` and `parse_markdown_to_structure` functions to handle paragraph-level citation extraction. In `process_section`, it extracts citations from each paragraph of a section, while in `parse_markdown_to_structure`, it performs citation extraction as paragraphs are processed while parsing a markdown text structure. In both cases, the `extract_citations` function helps to collect URLs in the specified `<cite>` tags for further processing or inclusion in the returned document structure.\n\n**Note**: The function does not handle malformed or incomplete HTML tags and assumes that the input text will contain properly formatted `<cite>` tags. The function extracts all citations, regardless of the number or complexity of the tags, and returns them as a list.\n\n**Output Example**:  \nGiven an input text:\n```html\nThis is a reference to <cite>https://example.com</cite> in the text.\nAnother citation: <cite>https://another-example.com</cite>.\n```\n\nThe output would be:\n```python\n['https://example.com', 'https://another-example.com']\n```"
      ],
      "code_start_line": 79,
      "code_end_line": 84,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def extract_citations(text):\n    \"\"\"从文本中提取引用的URLs\"\"\"\n    citations = []\n    pattern = r'<cite>(.*?)<\\/cite>'\n    matches = re.findall(pattern, text)\n    return matches\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/create_document_structure/process_section",
        "src/criticsearch/main_old_paralell.py/parse_markdown_to_structure"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "create_document_structure",
      "md_content": [
        "**create_document_structure**: The function of create_document_structure is to generate a structured document based on a given outline and corresponding content.\n\n**parameters**:\n· outline_json: A JSON object representing the outline of the document, which contains hierarchical information about titles and subsections.\n· flat_contents: A list of tuples where each tuple contains a dictionary with a \"path\" key representing the hierarchical path and a content string associated with that path.\n\n**Code Description**:  \nThe function `create_document_structure` is designed to create a structured document based on an outline and flat content. It takes two inputs: `outline_json` and `flat_contents`.\n\n- `outline_json` is a JSON object that represents the hierarchical structure of the document, including the document title, subsections, and children sections.\n- `flat_contents` is a list of tuples, each of which pairs a dictionary containing the path to a section with the corresponding textual content for that section.\n\nThe function proceeds by first initializing a `document` dictionary with a title (extracted from `outline_json`), a level (set to 1), and an empty list of subsections. The main goal is to map each section in the outline to its respective content and organize this data in a structured way.\n\n1. **Mapping Content to Paths**:  \n   The function creates a `content_map` dictionary where each section's path is mapped to its respective content. The `path` is represented as a tuple derived from the \"path\" key in the input tuples of `flat_contents`.\n\n2. **Processing Sections**:  \n   The function defines an internal function `process_section` which is responsible for recursively processing each section of the outline. Each section is processed based on the depth of its position in the outline, and the content associated with that section is retrieved from `content_map` using the path.\n\n   For each section, the function:\n   - Retrieves the section title and assigns it to a new dictionary.\n   - If content exists for the section (found using its path in `content_map`), it splits the content into paragraphs, each of which is processed further to extract any citations using a helper function `extract_citations`.\n   - If the section contains children (subsections), the function recursively processes each child, appending the result to the `subsections` list for that section.\n\n3. **Document Structure Construction**:  \n   After processing all sections in the root of the outline (under the `children` key of `outline_json`), the function returns the fully constructed document, which consists of a hierarchical structure of titles, paragraphs, citations, and subsections.\n\n**Note**: \n- The content is assumed to be divided into paragraphs by empty lines (`\\n\\n`).\n- Citations within paragraphs are processed by a helper function `extract_citations`, although the implementation of this function is not provided here.\n- The structure of the outline is assumed to be consistent, where each section may have a title and children, and content is mapped by its hierarchical path.\n\n**Output Example**:  \nA possible output of this function could look like the following:\n\n```json\n{\n  \"document\": {\n    \"title\": \"Sample Document Title\",\n    \"level\": 1,\n    \"subsections\": [\n      {\n        \"title\": \"Introduction\",\n        \"level\": 2,\n        \"paragraphs\": [\n          {\n            \"text\": \"This is the introduction paragraph.\\n\\nIt introduces the topic.\",\n            \"citations\": [\"citation1\", \"citation2\"]\n          }\n        ],\n        \"subsections\": []\n      },\n      {\n        \"title\": \"Methodology\",\n        \"level\": 2,\n        \"paragraphs\": [\n          {\n            \"text\": \"In this section, we describe the methods used in the study.\",\n            \"citations\": []\n          }\n        ],\n        \"subsections\": [\n          {\n            \"title\": \"Data Collection\",\n            \"level\": 3,\n            \"paragraphs\": [\n              {\n                \"text\": \"Data was collected through surveys and interviews.\",\n                \"citations\": [\"citation3\"]\n              }\n            ],\n            \"subsections\": []\n          }\n        ]\n      }\n    ]\n  }\n}\n```"
      ],
      "code_start_line": 86,
      "code_end_line": 141,
      "params": [
        "outline_json",
        "flat_contents"
      ],
      "have_return": true,
      "code_content": "def create_document_structure(outline_json, flat_contents):\n    \"\"\"\n    基于outline结构和生成的内容创建文档结构\n    \"\"\"\n    document = {\n        \"document\": {\n            \"title\": outline_json.get(\"title\", \"\"),\n            \"level\": 1,\n            \"subsections\": []\n        }\n    }\n    \n    # 构建路径到内容的映射\n    content_map = {}\n    for item, content in flat_contents:\n        path_key = tuple(item[\"path\"])\n        content_map[path_key] = content\n        \n    def process_section(section, depth=1, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n        \n        section_data = {\n            \"title\": section.get(\"title\"),\n            \"level\": depth,\n            \"paragraphs\": []\n        }\n        \n        # 如果有内容，处理段落\n        if path_key in content_map:\n            content = content_map[path_key]\n            paragraphs = content.split('\\n\\n')  # 假设段落用空行分隔\n            for para in paragraphs:\n                if para.strip():  # 忽略空段落\n                    citations = extract_citations(para)\n                    paragraph_data = {\n                        \"text\": para.strip(),  # 保留原始文本，包括cite标记\n                        \"citations\": citations\n                    }\n                    section_data[\"paragraphs\"].append(paragraph_data)\n        \n        # 处理子节点\n        if \"children\" in section:\n            section_data[\"subsections\"] = []\n            for child in section[\"children\"]:\n                child_data = process_section(child, depth + 1, current_path)\n                section_data[\"subsections\"].append(child_data)\n                \n        return section_data\n    \n    # 处理根节点下的所有节点\n    for section in outline_json.get(\"children\", []):\n        doc_section = process_section(section, 2, [outline_json.get(\"title\")] if \"title\" in outline_json else [])\n        document[\"document\"][\"subsections\"].append(doc_section)\n    \n    return document\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_section",
      "md_content": [
        "**process_section**: The function of process_section is to process a section of content, extract paragraphs and citations, and recursively handle sub-sections.\n\n**parameters**: The parameters of this Function.\n- section: A dictionary representing a section, which contains keys like \"title\", \"children\", and possibly other content.\n- depth: An integer representing the current depth level of the section in a hierarchical structure, default value is 1.\n- path: A list used to track the path taken through the section hierarchy, default value is an empty list.\n\n**Code Description**:  \nThe `process_section` function is responsible for processing a single section within a larger content structure, extracting its relevant details, including paragraphs and citations, and recursively processing any child sections (subsections).\n\n1. **Section Title and Path Tracking**:\n   - The function starts by constructing a list `current_path`, which tracks the path taken to the current section. It appends the current section's title to this path.\n   - The `path_key` is formed as a tuple from the `current_path`. This key is then used to look up content for the section from a global `content_map`.\n\n2. **Paragraph Extraction and Citation Handling**:\n   - If content for the section is available in `content_map` (as indicated by `path_key`), the content is retrieved and split into paragraphs based on two consecutive newline characters (`\\n\\n`).\n   - For each paragraph, any non-empty paragraphs are processed. The function calls `extract_citations`, which extracts citations (URLs enclosed in `<cite>` tags) from the paragraph. The resulting citations are stored alongside the paragraph text.\n\n3. **Handling Subsections**:\n   - If the section contains child sections (subsections), the function creates a list under the key `\"subsections\"`. It then recursively calls `process_section` for each child, incrementing the `depth` by 1 and passing the updated `path`.\n\n4. **Return Value**:\n   - The function returns a dictionary containing:\n     - `\"title\"`: The section's title.\n     - `\"level\"`: The depth level of the section.\n     - `\"paragraphs\"`: A list of dictionaries, each representing a paragraph with associated citation data.\n     - `\"subsections\"`: A list of processed subsections, if any.\n\nThis function is designed to support nested sections, where each section can have multiple levels of sub-sections. It is useful for transforming a hierarchical content structure into a more structured format that includes both textual data and citations, making it easier to process or render in a desired output format.\n\nThe `process_section` function interacts with the `extract_citations` function to collect citations from paragraphs. This makes it particularly suited for handling structured content that includes references marked by `<cite>` tags.\n\n**Note**: \n- The function assumes that the input data is well-formed, with sections containing a \"title\" and optional \"children\".\n- The `content_map` must be pre-defined and should map section paths (as tuples) to corresponding content.\n- The function handles nested sections and citations extraction, making it suitable for documents with complex structures.\n\n**Output Example**:\nGiven the following input:\n```python\nsection = {\n    \"title\": \"Introduction\",\n    \"children\": [\n        {\"title\": \"Background\"},\n        {\"title\": \"Objective\"}\n    ]\n}\n```\nAssuming the `content_map` contains the relevant content for the sections, the output might look like:\n```python\n{\n    \"title\": \"Introduction\",\n    \"level\": 1,\n    \"paragraphs\": [\n        {\"text\": \"This is the introduction paragraph. <cite>https://example.com</cite>\", \"citations\": [\"https://example.com\"]}\n    ],\n    \"subsections\": [\n        {\"title\": \"Background\", \"level\": 2, \"paragraphs\": [], \"subsections\": []},\n        {\"title\": \"Objective\", \"level\": 2, \"paragraphs\": [], \"subsections\": []}\n    ]\n}\n```"
      ],
      "code_start_line": 104,
      "code_end_line": 134,
      "params": [
        "section",
        "depth",
        "path"
      ],
      "have_return": true,
      "code_content": "    def process_section(section, depth=1, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n        \n        section_data = {\n            \"title\": section.get(\"title\"),\n            \"level\": depth,\n            \"paragraphs\": []\n        }\n        \n        # 如果有内容，处理段落\n        if path_key in content_map:\n            content = content_map[path_key]\n            paragraphs = content.split('\\n\\n')  # 假设段落用空行分隔\n            for para in paragraphs:\n                if para.strip():  # 忽略空段落\n                    citations = extract_citations(para)\n                    paragraph_data = {\n                        \"text\": para.strip(),  # 保留原始文本，包括cite标记\n                        \"citations\": citations\n                    }\n                    section_data[\"paragraphs\"].append(paragraph_data)\n        \n        # 处理子节点\n        if \"children\" in section:\n            section_data[\"subsections\"] = []\n            for child in section[\"children\"]:\n                child_data = process_section(child, depth + 1, current_path)\n                section_data[\"subsections\"].append(child_data)\n                \n        return section_data\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/main_old_paralell.py/extract_citations"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "parse_markdown_to_structure",
      "md_content": [
        "### Function Documentation: `parse_markdown_to_structure`\n\n#### Overview:\nThe `parse_markdown_to_structure` function parses a given markdown text and converts it into a structured document format, organizing sections, subsections, and paragraphs based on the markdown syntax. It identifies headers marked with `#` to define sections and handles paragraph content, extracting citations where applicable.\n\n#### Parameters:\n- **markdown_text** (str): The input markdown text to be parsed into a structured document. This text can contain headers (denoted by `#`) and paragraphs, with citations included in `<cite>` tags.\n\n#### Returns:\n- **document** (dict): A dictionary representing the parsed structure of the markdown text. It includes a top-level document object with a title, level, subsections, and paragraphs. Each section in the markdown is represented as a subsection in the structure.\n\n#### Structure of the returned document:\n- **document** (dict):\n  - **title** (str): The title of the document, typically set from the first markdown header.\n  - **level** (int): The level of the current section. This is determined by the number of `#` characters at the start of a header.\n  - **subsections** (list): A list of subsections (if any), with each subsection being a dictionary containing:\n    - **title** (str): The title of the subsection.\n    - **level** (int): The level of the subsection based on its header.\n    - **subsections** (list): Further nested subsections.\n    - **paragraphs** (list): A list of paragraphs in the subsection, each represented as a dictionary with:\n      - **text** (str): The paragraph text.\n      - **citations** (list): A list of citations (URLs) extracted from the paragraph's text, if any.\n\n#### Functionality:\n1. **Splitting the Markdown**: The function first splits the input markdown text by line breaks, processing each line to identify headers and paragraphs.\n2. **Handling Headers**: Headers are identified by lines starting with `#`. The level of the header (i.e., how many `#` characters it contains) determines the depth of the section in the document structure. New sections are created as subsections of the current section. If the header level is less than or equal to the current section's level, the function adjusts the section stack to properly organize the hierarchy.\n3. **Handling Paragraphs**: Text between headers is treated as paragraph content. Paragraphs are collected until an empty line or another header is encountered. Citations within paragraphs, enclosed in `<cite>` tags, are extracted using the `extract_citations` function.\n4. **Final Processing**: After processing all lines, any remaining paragraph text is added to the document structure, ensuring that no content is missed.\n\n#### Example:\nGiven the following markdown input:\n```markdown\n# Title of Document\n## Introduction\nThis is an introduction paragraph with a citation <cite>https://example.com</cite>.\n## Main Content\nThis is the main content, which also contains a citation <cite>https://another-example.com</cite>.\n```\n\nThe function will return a document structure like this:\n```python\n{\n    \"document\": {\n        \"title\": \"Title of Document\",\n        \"level\": 1,\n        \"subsections\": [\n            {\n                \"title\": \"Introduction\",\n                \"level\": 2,\n                \"subsections\": [],\n                \"paragraphs\": [\n                    {\n                        \"text\": \"This is an introduction paragraph with a citation.\",\n                        \"citations\": [\"https://example.com\"]\n                    }\n                ]\n            },\n            {\n                \"title\": \"Main Content\",\n                \"level\": 2,\n                \"subsections\": [],\n                \"paragraphs\": [\n                    {\n                        \"text\": \"This is the main content, which also contains a citation.\",\n                        \"citations\": [\"https://another-example.com\"]\n                    }\n                ]\n            }\n        ]\n    }\n}\n```\n\n#### Dependencies:\n- **extract_citations**: The function relies on `extract_citations` to extract any citations embedded in `<cite>` HTML tags within paragraphs.\n\n#### Usage:\nThe `parse_markdown_to_structure` function is typically used when there's a need to convert a markdown document into a structured format, such as when processing or analyzing documents with hierarchical content. The parsed structure can be used for further processing, such as generating reports or extracting specific information from different sections."
      ],
      "code_start_line": 143,
      "code_end_line": 218,
      "params": [
        "markdown_text"
      ],
      "have_return": true,
      "code_content": "def parse_markdown_to_structure(markdown_text):\n    \"\"\"从markdown文本解析出文档结构\"\"\"\n    lines = markdown_text.split('\\n')\n    document = {\n        \"document\": {\n            \"title\": \"\",\n            \"level\": 1,\n            \"subsections\": []\n        }\n    }\n    \n    current_section = document[\"document\"]\n    section_stack = [current_section]\n    current_level = 1\n    current_text = []\n    \n    for line in lines:\n        if line.strip():\n            # 处理标题\n            if line.startswith('#'):\n                # 如果有待处理的段落文本，先��理完\n                if current_text:\n                    paragraph_text = ' '.join(current_text)\n                    if paragraph_text.strip():\n                        citations = extract_citations(paragraph_text)\n                        current_section.setdefault(\"paragraphs\", []).append({\n                            \"text\": paragraph_text.strip(),\n                            \"citations\": citations\n                        })\n                    current_text = []\n                \n                # 处理新标题\n                level = len(line.split()[0])  # 计算#的数量\n                title = ' '.join(line.split()[1:])\n                \n                # 根据层级调整当前section\n                while len(section_stack) > 1 and level <= section_stack[-1][\"level\"]:\n                    section_stack.pop()\n                \n                new_section = {\n                    \"title\": title,\n                    \"level\": level,\n                    \"subsections\": [],\n                    \"paragraphs\": []\n                }\n                \n                section_stack[-1].setdefault(\"subsections\", []).append(new_section)\n                section_stack.append(new_section)\n                current_section = new_section\n                \n            else:\n                # 收集段落文本\n                current_text.append(line)\n        else:\n            # 空行，处理当前段落\n            if current_text:\n                paragraph_text = ' '.join(current_text)\n                if paragraph_text.strip():\n                    citations = extract_citations(paragraph_text)\n                    current_section.setdefault(\"paragraphs\", []).append({\n                        \"text\": paragraph_text.strip(),\n                        \"citations\": citations\n                    })\n                current_text = []\n    \n    # 处理最后一个段落\n    if current_text:\n        paragraph_text = ' '.join(current_text)\n        if paragraph_text.strip():\n            citations = extract_citations(paragraph_text)\n            current_section.setdefault(\"paragraphs\", []).append({\n                \"text\": paragraph_text.strip(),\n                \"citations\": citations\n            })\n\n    return document\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main"
      ],
      "reference_who": [
        "src/criticsearch/main_old_paralell.py/extract_citations"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: The function of main is to orchestrate the conversation process between a user and an intelligent agent, managing iterations of task execution and response refinement.\n\n**parameters**: The parameters of this Function.\n· TASK: A string representing the user's question or task that the agent is expected to address.\n· MAX_ITERATION: An integer indicating the maximum number of iterations the agent should perform to refine its response.\n\n**Code Description**: The main function serves as the entry point for executing the conversation logic of an intelligent agent. It begins by initializing a common agent instance of the BaseAgent class, which is responsible for handling user interactions, managing conversation history, and integrating various tools for search and content generation.\n\nThe function sets the user question for the agent by assigning the TASK parameter to the agent's user_question attribute. It also configures the logging level based on the settings defined in the project, ensuring that relevant messages are logged throughout the execution.\n\nThe conversation process is structured around a loop that iterates up to MAX_ITERATION times. During each iteration, the function performs several key operations:\n\n1. It checks the agent's confidence in its response to the TASK. If the agent is confident, it retrieves an answer directly using the common_chat method. If not, it initiates a search process to gather more information.\n\n2. In the first iteration, if the agent is not confident, it constructs a search prompt and retrieves initial search results. These results are used to generate an outline for the report, which is then flattened into a list of sections for parallel content generation.\n\n3. The function employs a ThreadPoolExecutor to generate content for each section concurrently, enhancing efficiency. Each section's content is generated based on the search results and the TASK context.\n\n4. After generating content for all sections, the function reconstructs the final Markdown document using the reconstructed_markdown function, which combines the outline structure with the generated content.\n\n5. The generated report is then polished and saved to a Markdown file. Additionally, the document structure is parsed and saved in JSON format for further use.\n\n6. In subsequent iterations, the agent updates its answer based on feedback from a CriticAgent, which evaluates the agent's responses and provides suggestions for improvement. The process continues until either the maximum number of iterations is reached or the CriticAgent indicates that the conversation should stop.\n\nThe main function effectively coordinates the interaction between the user, the intelligent agent, and the CriticAgent, ensuring that responses are refined based on real-time feedback and additional information gathered through searches.\n\n**Note**: It is crucial to ensure that the TASK parameter is well-defined and relevant to the agent's capabilities. The MAX_ITERATION parameter should be set appropriately to balance the need for thoroughness with the efficiency of the conversation process.\n\n**Output Example**: A possible return value from the main function could be a string summarizing the final answer provided by the agent, such as:\n```\n\"The comprehensive report on climate change highlights the significant impacts observed globally, including rising temperatures and extreme weather events.\"\n```",
        "**main**: The function of main is to orchestrate the conversation process between a user and an intelligent agent, managing iterations of response generation, feedback, and content refinement.\n\n**parameters**: The parameters of this Function.\n· TASK: A string representing the user's question or task that the agent needs to address.  \n· MAX_ITERATION: An integer specifying the maximum number of iterations for refining the agent's response.\n\n**Code Description**: The main function serves as the central control point for the interaction between the user and the intelligent agent. It begins by initializing a common agent instance of the BaseAgent class and setting the user's question as the task. The function sets the logging level based on the configuration and logs the start of the conversation.\n\nThe conversation history is updated to include the user's initial question. The function then enters a loop that iterates up to the specified maximum number of iterations. During each iteration, it performs several key actions:\n\n1. **Initial Setup**: On the first iteration, it checks the agent's confidence in answering the task. If the agent is confident, it generates a direct answer. If not, it performs a search to gather information, constructs a report outline, and generates content for each section in parallel using threading.\n\n2. **Content Generation**: The content for each section is generated based on search results, and the responses are collected. The generated content is then polished to create a final report, which is saved in Markdown format.\n\n3. **Feedback Loop**: After generating the initial response, the function evaluates the answer using a CriticAgent. The CriticAgent provides feedback, which may lead to further iterations of content refinement. If the feedback indicates that the process should stop, the function logs the total iterations and returns the final answer.\n\n4. **Subsequent Iterations**: For iterations beyond the first, the function updates the answer based on the previous response and the latest search results, incorporating feedback from the CriticAgent. It continues to refine the answer until the maximum number of iterations is reached or a stopping condition is met.\n\nThe main function integrates various components of the project, including the BaseAgent for generating responses, the CriticAgent for evaluating those responses, and utility functions for content generation and Markdown reconstruction. This orchestration ensures a dynamic and iterative process that enhances the quality of the agent's responses through continuous feedback and refinement.\n\n**Note**: It is essential to ensure that the TASK parameter is well-defined and relevant to the agent's capabilities. The MAX_ITERATION parameter should be set appropriately to balance between thoroughness and efficiency in response generation.\n\n**Output Example**: A possible appearance of the code's return value when executing the main function might look like this:\n```\n\"Based on the gathered information and feedback, here is the final answer to your question: ...\"\n```"
      ],
      "code_start_line": 220,
      "code_end_line": 452,
      "params": [
        "TASK",
        "MAX_ITERATION"
      ],
      "have_return": true,
      "code_content": "def main(TASK, MAX_ITERATION):\n    # Initialize agents\n    common_agent = BaseAgent()\n\n    # initialize the task\n    common_agent.user_question = TASK\n\n    set_logger_level_from_config(log_level=settings.log_level.upper())\n\n    logger.success(f\"Starting the conversation with task: {TASK}\")\n\n    BaseAgent.conversation_manager.append_to_history(role=\"user\", content=TASK)\n\n    for iteration in range(MAX_ITERATION):\n        colorize_message(\n            message_title=f\"ITERATION {iteration + 1}\", color=\"cyan\", style=\"bold\"\n        )\n\n        if iteration == 0:\n            # Initialize search_results as None\n            search_results = None\n\n            # Model confidence check - yellow\n            agent_confident = common_agent.model_confident(TASK)\n            agent_confident_yaml = common_agent.extract_and_validate_yaml(\n                agent_confident\n            )\n\n            if agent_confident_yaml is None:\n                logger.warning(\n                    \"Failed to extract valid YAML content. Defaulting to 'false'.\"\n                )\n                agent_confident = False\n            else:\n                agent_confident_dict = yaml.safe_load(agent_confident_yaml)\n                agent_confident = (\n                    agent_confident_dict.get(\"confidence\", \"true\").lower() == \"true\"\n                )\n\n            if agent_confident:\n                # When confident, only get the answer\n                common_agent_answer = common_agent.chat(usr_prompt=TASK)\n            else:\n                # When not confident, get both answer and search results\n                # 并且第一次全面的搜索后的结果用来构建一个report的结构\n                data = {\n                    \"user_question\": TASK,\n                }\n                initial_search_prompt = common_agent.load_template(\n                    \"planner_agent_initial_search_plan.txt\"\n                )\n                initial_search_rendered_prompt = common_agent.render_template(\n                    initial_search_prompt, data\n                )\n                logger.info(\n                    f\"initial_search_rendered_prompt: {initial_search_rendered_prompt}\"\n                )\n\n                initial_web_result_markdown_text = common_agent.search_and_browse(\n                    initial_search_rendered_prompt\n                )# 这里返回的是模型决定了访问哪些后的网页爬取extract的结果\n\n                logger.info(f\"Initial web result: {initial_web_result_markdown_text}\")\n\n                # Generate report outline based on search results\n                outline_prompt = common_agent.render_template(\n                    common_agent.load_template(\"outline_generation.txt\"),\n                    {\n                        \"user_question\": common_agent.user_question,\n                        \"web_result_markdown_text\": initial_web_result_markdown_text,\n                    },\n                )\n\n                outline = common_agent.chat(\n                    usr_prompt=outline_prompt,\n                )\n\n                # verify the outline is a json string   \n                outline_json = common_agent.extract_and_validate_json(outline)\n                \n                # Flatten the outline to get all sections at all levels\n                flat_sections = flatten_outline(outline_json)\n                \n                # Generate content for all sections in parallel\n                with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n                    future_to_section = {\n                        executor.submit(generate_content_for_section, common_agent, item[\"section\"], TASK): item \n                        for item in flat_sections\n                    }\n                    \n                    # Collect results as they complete\n                    flat_contents = []\n                    for future in concurrent.futures.as_completed(future_to_section):\n                        item = future_to_section[future]\n                        try:\n                            content = future.result()\n                            flat_contents.append((item, content))\n                        except Exception as e:\n                            logger.error(f\"Error generating content for section {item['section'].get('title')}: {e}\")\n                \n                # Reconstruct the markdown with proper hierarchy\n                common_agent_answer = reconstruct_markdown(outline_json, flat_contents)\n                print(common_agent_answer)  # 这里是第一次生成的report,还没有polish\n\n                # 用deepseek润色一下得到正式的version1 report\n                polish_prompt = common_agent.load_template(\"polish_first_version.txt\")\n                polish_rendered_prompt = common_agent.render_template(\n                    polish_prompt,\n                    {\n                        \"task\": TASK,\n                        \"report\": common_agent_answer,\n                    },\n                )\n                common_agent_answer = common_agent.chat(\n                    usr_prompt=polish_rendered_prompt,\n                    model=\"gpt-4o\"\n                )\n                print(common_agent_answer)  # 这里是第一次生成的正式的polished report\n                # 保存到一个md\n                with open(\"first_version_report.md\", \"w\") as f:\n                    f.write(common_agent_answer)\n\n                # Polish后从markdown解析出文档结构\n                document_structure = parse_markdown_to_structure(common_agent_answer)\n                common_agent.document_structure = document_structure\n\n                # 保存到一个json文件\n                with open(\"document_structure.json\", \"w\") as f:\n                    json.dump(document_structure, f, indent=4, ensure_ascii=False)\n \n        else:\n            # 前面根据critc的返回得到了新的网页搜索结果web_result_markdown_text\n            common_agent_answer = common_agent.update_answer(\n                query=TASK,\n                previous_answer=common_agent_answer,\n                search_results=web_result_markdown_text,\n                critic_feedback=critic_agent_response,\n            )\n            time.sleep(0.1)  # hitting rate limits for gpt mini\n\n        # ========================== #\n        ## 这里在if-else结构之外 ##\n        colorize_message(\n            message_title=\"COMMON AGENT ANSWER\",\n            color=\"magenta\",\n            message_content=common_agent_answer,\n        )\n\n        # Critic evaluation - blue\n        critic_agent = CriticAgent()\n        critic_agent.receive_task(TASK)\n        critic_agent.receive_agent_answer(common_agent_answer)\n        critic_agent_response = critic_agent.critic()\n\n        colorize_message(\n            message_title=\"CRITIC_AGENT_RESPONSE\",\n            color=\"blue\",\n            message_content=critic_agent_response,\n        )\n\n        if yaml.safe_load(critic_agent_response).get(\"Stop\", {}).lower() == \"true\":\n            colorize_message(\n                message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n            )\n\n            colorize_message(\n                message_title=\"ALL SEARCH QUERIES\",\n                color=\"black\",\n                message_content=\", \".join(map(str, common_agent.queryDB)),\n            )\n            colorize_message(\n                message_title=\"FINAL ANSWER\",\n                color=\"red\",\n                message_content=common_agent_answer,\n            )\n\n            return f\"\\n{common_agent_answer}\\n\"\n\n        # 根据critic的建议再执行一次搜索和爬虫操作\n        # 先构建rendered_prompt\n        reflection_data = {\n            \"user_question\": TASK,\n            \"previous_answer\": common_agent_answer,\n            \"user_feedback\": critic_agent_response,\n            \"search_history\": common_agent.queryDB,\n        }\n        search_again_prompt = common_agent.render_template(\n            common_agent.load_template(\"planner_agent_with_reflection.txt\"),\n            reflection_data,\n        )\n        try:\n            web_result_markdown_text = common_agent.search_and_browse(\n                search_again_prompt\n            )\n        except:\n            colorize_message(\n                message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n            )\n\n            colorize_message(\n                message_title=\"ALL SEARCH QUERIES\",\n                color=\"black\",\n                message_content=\", \".join(map(str, common_agent.queryDB)),\n            )\n\n            colorize_message(\n                message_title=\"FINAL ANSWER\",\n                color=\"red\",\n                message_content=common_agent_answer,\n            )\n\n            # we run out of searches for now, so we force the agent to give a final answer:\n            return f\"\\n{common_agent_answer}\\n\"\n\n        # Check if reached max iterations\n        if iteration == MAX_ITERATION - 1:\n            colorize_message(\n                message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n            )\n\n            colorize_message(\n                message_title=\"ALL SEARCH QUERIES\",\n                color=\"black\",\n                message_content=\", \".join(map(str, common_agent.queryDB)),\n            )\n\n            colorize_message(\n                message_title=\"FINAL ANSWER\",\n                color=\"red\",\n                message_content=common_agent_answer,\n            )\n\n            return f\"\\n{common_agent_answer}\\n\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/base_agent.py/BaseAgent/extract_and_validate_yaml",
        "src/criticsearch/base_agent.py/BaseAgent/receive_task",
        "src/criticsearch/base_agent.py/BaseAgent/extract_and_validate_json",
        "src/criticsearch/main_old_paralell.py/flatten_outline",
        "src/criticsearch/main_old_paralell.py/generate_content_for_section",
        "src/criticsearch/main_old_paralell.py/reconstruct_markdown",
        "src/criticsearch/main_old_paralell.py/parse_markdown_to_structure",
        "src/criticsearch/critic_agent.py/CriticAgent",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/critic_agent.py/CriticAgent/receive_agent_answer",
        "src/criticsearch/models.py/ConversationManager/append_to_history"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/rich_output.py": [
    {
      "type": "ClassDef",
      "name": "RichPrinter",
      "md_content": [
        "**RichPrinter**: The function of RichPrinter is to provide a set of methods for printing styled messages, handling exception printing, and saving console output to a file.\n\n**attributes**:\n· console: Console  \n· default_title_style: str  \n· default_line_characters: str  \n\n**Code Description**:  \nThe `RichPrinter` class is designed to work with the `Console` class, most likely from the `rich` library, to enable styled and formatted output to the terminal. The class also includes features for printing exceptions and saving console output to a file.\n\n- **console**: This attribute stores an instance of the `Console` class. If no `Console` instance is provided during the initialization of `RichPrinter`, a new `Console` instance is created with `record=True`. The `record` parameter indicates that the console should capture the output for later use or export.\n  \n- **default_title_style**: This string attribute sets the default style for titles printed using the `rule` method. The default style is `[cyan bold]`, which means titles will appear in cyan and bold when printed.\n  \n- **default_line_characters**: This attribute defines the default character used to create a line or separator in printed rules. By default, the character is the equal sign (`=`).\n\nThe class provides the following methods:\n\n1. **__init__(self, console: Console = None)**:  \n   The constructor method initializes the `RichPrinter` object. If a `Console` object is passed as a parameter, it uses that; otherwise, it creates a new `Console` instance with the `record` set to `True`.\n\n2. **rule(self, title: str)**:  \n   This method prints a decorative rule (a line) with a title. The title is printed with the default title style (`[cyan bold]`), and the line separating the title is formed using the default line characters (`=`). It calls the `console.rule` method to render the rule.\n\n3. **log(self, message: str, style: str = None)**:  \n   This method prints a log message using the `console.log` method. The message is printed with an optional style, which defaults to `None` if no style is provided.\n\n4. **print(self, message: str, style: str = None)**:  \n   Similar to the `log` method, this method prints a regular message to the console using the `console.print` method. Again, the message can be styled by passing an optional `style` argument.\n\n5. **print_exception(self, message: str, max_frames: int = 5)**:  \n   This method prints an exception message with a bold red style using the `console.print_exception` method. It also allows controlling the number of stack frames shown when printing the exception, defaulting to 5 frames. Before printing the exception, it uses the `log` method to display the exception message in red, bold.\n\n6. **save_output_to_file(self, file_path: Path = Path(\"output.txt\"))**:  \n   This method saves the output of the `Console` object to a specified file. It creates a new `Console` instance that writes to the file. It exports the current text output captured by the original console and saves it to the given file path. The method ensures that a rule is added at the end of the file to mark that the output has been successfully written.\n\n**Note**:  \n- The `save_output_to_file` method exports the content of the console as plain text. It is important to ensure that the file path provided is accessible and writable.  \n- The default behavior of printing to the console and exporting to a file is meant to be flexible, allowing users to either view the output directly or save it for later inspection."
      ],
      "code_start_line": 7,
      "code_end_line": 45,
      "params": [],
      "have_return": false,
      "code_content": "class RichPrinter:\n    def __init__(self, console: Console = None):\n        # 如果没有传入 Console 对象，默认使用一个新的 Console 实例\n        self.console = console or Console(record=True)\n\n        # 默认样式配置\n        self.default_title_style = \"[cyan bold]\"\n        self.default_line_characters = \"=\"  # 使用 `=` 作为分隔线样式\n\n    def rule(self, title: str):\n        self.console.rule(\n            f\"{self.default_title_style}{title}\",\n            characters=self.default_line_characters,  # 默认使用 `=` 符号的分隔线\n        )\n\n    def log(self, message: str, style: str = None):\n        \"\"\"打印带有样式的日志消息\"\"\"\n        self.console.log(message, style=style)\n\n    def print(self, message: str, style: str = None):\n        \"\"\"打印普通的消息\"\"\"\n        self.console.print(message, style=style)\n\n    def print_exception(self, message: str, max_frames: int = 5):\n        printer.log(f\"{message}\", style=\"bold red\")\n\n        \"\"\"打印异常信息\"\"\"\n        self.console.print_exception(max_frames=max_frames)\n\n    def save_output_to_file(self, file_path: Path = Path(\"output.txt\")):\n        with open(file_path, \"wt\", encoding=\"utf-8\") as report_file:\n            # Redirect Console output to the specified file\n            console_for_export = Console(file=report_file)\n\n            # Export the console's current text output and write it to the file\n            console_for_export.log(self.console.export_text())\n\n            # Add a rule at the end of the output to indicate the file has been generated\n            console_for_export.rule(\"Output file Generated.\")\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the instance of the `RichPrinter` class and configure its attributes, including setting up a default `Console` instance and assigning default styles for the title and line separator.\n\n**parameters**: \n· console: Optional. An instance of the `Console` class. If not provided, a new `Console` object will be created.\n\n**Code Description**: \nThe `__init__` function is the constructor method of the `RichPrinter` class. It initializes the object with two main responsibilities:\n1. It accepts an optional `console` parameter, which is an instance of the `Console` class. If no `console` is passed to the constructor, it defaults to creating a new `Console` instance with the `record=True` parameter, enabling the recording of the output.\n2. It sets up default style and formatting attributes:\n   - `default_title_style`: This is set to \"[cyan bold]\", defining the default style for titles to be cyan and bold.\n   - `default_line_characters`: This is set to `\"=\"`, which specifies that the default character for line separators will be the equals sign (`=`).\n\nThis constructor ensures that the `RichPrinter` class has the necessary setup for its output, including the ability to log or record the output and use consistent styling for its visual presentation.\n\n**Note**: If a custom `Console` instance is passed, it will be used instead of the default one. Additionally, the default style settings can be overridden later in the class, but they serve as initial values for title and line separator styling."
      ],
      "code_start_line": 8,
      "code_end_line": 14,
      "params": [
        "self",
        "console"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, console: Console = None):\n        # 如果没有传入 Console 对象，默认使用一个新的 Console 实例\n        self.console = console or Console(record=True)\n\n        # 默认样式配置\n        self.default_title_style = \"[cyan bold]\"\n        self.default_line_characters = \"=\"  # 使用 `=` 作为分隔线样式\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "rule",
      "md_content": [
        "**rule**: The function of rule is to create a formatted horizontal rule in the console output with a specified title.\n\n**parameters**: The parameters of this Function.\n· title: A string that represents the title to be displayed alongside the rule.\n\n**Code Description**: The rule function is a method defined within the RichPrinter class. Its primary purpose is to generate a visually distinct horizontal rule in the console output, which is useful for separating different sections of output for better readability. The function takes a single parameter, title, which is a string that specifies the text to be displayed as part of the rule. \n\nInternally, the function calls the console's rule method, passing a formatted string that combines a default title style with the provided title. The characters parameter is set to self.default_line_characters, which defaults to the '=' symbol, indicating that the horizontal rule will be represented by this character. This method enhances the visual structure of console output, making it easier for users to identify different sections of information.\n\nThe rule function is invoked in two different contexts within the project. First, it is called in the generate_content_for_section function, where it is used to indicate the generation of content for a specific section based on the title extracted from the section dictionary. This helps to clearly delineate the output related to that section.\n\nSecond, it is also called in the process_single_task function, where it is used multiple times to indicate the start of iterations and to separate different outputs, such as the common agent's answer and the critic agent's response. This consistent use of the rule function throughout the codebase contributes to a well-organized and user-friendly console output.\n\n**Note**: When using the rule function, ensure that the title provided is meaningful and relevant to the context in which it is being used, as this will enhance the clarity of the console output.",
        "**rule**: The function of rule is to create a visual separator in the console output with a specified title.\n\n**parameters**: The parameters of this Function.\n· title: A string that represents the title to be displayed in the rule.\n\n**Code Description**: The rule function is a method of the RichPrinter class that utilizes the console's built-in functionality to generate a visually distinct horizontal line in the console output. This line serves as a separator, enhancing the readability of the output by clearly delineating different sections or important information.\n\nWhen the rule function is called, it takes a single parameter, title, which is a string. This title is formatted using a default style defined within the class, specifically `self.default_title_style`. The function then calls `self.console.rule`, passing the formatted title along with a character to be used for the line. By default, this character is set to `=`.\n\nThe rule function is invoked in several places within the project, particularly in the context of logging and displaying important stages or outputs during the execution of various workflows. For instance, it is called in the `call_llm`, `tavily_extract`, and `fallback_scrape` functions, among others. In these instances, the rule function is used to create headers for sections of output, such as \"LLM Prompt\", \"Tavily Extract URLs\", and \"Fallback Scrape Results\". This consistent use of the rule function helps maintain a structured and organized output format, making it easier for users to follow the flow of information.\n\n**Note**: When using the rule function, ensure that the title provided is concise and relevant to the content that follows. This will maximize the effectiveness of the visual separator in enhancing the clarity of the console output.",
        "**rule**: The function of rule is to print a visual separator in the console output with a specified title.\n\n**parameters**: The parameters of this Function.\n· title: A string that contains the title to be displayed in the rule.\n\n**Code Description**: The rule function is a method within the RichPrinter class that enhances console output by creating a visually distinct separator line with an accompanying title. This function utilizes the console's built-in capabilities to format and display the title in a styled manner, making it easier for users to identify different sections of output in the console.\n\nWhen the rule function is called, it constructs a formatted string that combines the default title style with the provided title. The separator line is created using a default character, which is typically an equals sign (`=`), ensuring that the output is clear and visually appealing. The function then invokes the console's rule method to render this formatted string, effectively creating a visual break in the output.\n\nThe rule function is called by various other functions within the project, such as call_llm, tavily_extract, fallback_scrape, and others. These functions utilize the rule method to enhance the readability of their outputs by clearly delineating different sections of information, such as prompts, results, and error messages. This consistent use of the rule function across the project contributes to a more organized and user-friendly console interface.\n\n**Note**: When using the rule function, ensure that the title provided is concise and relevant to the context of the output. This will help maintain clarity and improve the overall user experience when interacting with the console output.",
        "**rule**: The function of rule is to print a visual separator in the console output with a specified title.\n\n**parameters**: The parameters of this Function.\n· title: A string that contains the title to be displayed in the rule.\n\n**Code Description**: The rule function is a method within the RichPrinter class, designed to enhance console output readability by creating a visual separator. It utilizes the console's built-in rule method to print a formatted line that includes the title provided as an argument. The title is styled using a default title style defined in the class, ensuring consistency in appearance across different outputs.\n\nWhen the rule function is called, it constructs a formatted string that combines the default title style with the provided title. The characters used for the separator line are specified by the default_line_characters attribute, which defaults to the \"=\" symbol. This visual separator serves to delineate sections of output, making it easier for users to follow the flow of information in the console.\n\nThe rule function is invoked in various contexts throughout the project, particularly in functions that require clear demarcation of different stages or outputs. For example, it is called in the call_llm function to print a separator titled \"LLM Prompt\" before displaying the prompt sent to the language model. This usage highlights the importance of the rule function in maintaining a structured and organized console output, which is crucial for debugging and user interaction.\n\n**Note**: When using the rule function, ensure that the title provided is concise and relevant to the context of the output. The visual separator enhances the clarity of console logs, making it easier for users to identify key sections of information."
      ],
      "code_start_line": 16,
      "code_end_line": 20,
      "params": [
        "self",
        "title"
      ],
      "have_return": false,
      "code_content": "    def rule(self, title: str):\n        self.console.rule(\n            f\"{self.default_title_style}{title}\",\n            characters=self.default_line_characters,  # 默认使用 `=` 符号的分隔线\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/call_llm",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/tavily_extract",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/fallback_scrape",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/run",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_query_update",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_run",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/save",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/evaluate",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/main/_batch/run_one",
        "src/criticsearch/abstract_substitution/abs_workflow.py",
        "src/criticsearch/abstract_substitution/abs_workflow.py/call_llm",
        "src/criticsearch/abstract_substitution/abs_workflow.py/tavily_extract",
        "src/criticsearch/abstract_substitution/abs_workflow.py/fallback_scrape",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/run",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/save",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_run",
        "src/criticsearch/abstract_substitution/abs_workflow.py/evaluate",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_fuzzy_replacement",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_entity_extraction",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_combination_workflow",
        "src/criticsearch/abstract_substitution/abs_workflow.py/main",
        "src/criticsearch/abstract_substitution/abs_workflow.py/main/_batch/run_one",
        "src/criticsearch/base_agent.py/BaseAgent/taking_notes",
        "src/criticsearch/main.py/generate_content_for_section",
        "src/criticsearch/main.py/_model_action_decision",
        "src/criticsearch/main.py/_action_router",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/sliding_window_pairing",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_for_folder/_proc",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/_calculate_score"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "log",
      "md_content": [
        "**log**: The function of log is to print a styled log message to the console.\n\n**parameters**: The parameters of this Function.\n· message: str - The log message that will be printed to the console.\n· style: str (optional) - The style to be applied to the log message.\n\n**Code Description**: The log function is a method within the RichPrinter class that is responsible for outputting log messages to the console with optional styling. When invoked, it takes a string message as a mandatory parameter and an optional style parameter that specifies how the message should be formatted when displayed.\n\nThe function utilizes the `self.console.log` method to print the message. This method is part of the Rich library, which allows for enhanced console output, including color and style formatting. The `style` parameter can be used to apply specific visual styles to the message, such as colors or bold formatting, enhancing the readability and visual appeal of the logs.\n\nThe log function is called within other parts of the project, specifically in the `search_and_browse` method of the BaseAgent class and the `process_single_task` function in the main module. In `search_and_browse`, it is used to log the response from a search operation, providing visibility into the internal workings of the search process. In `process_single_task`, it logs the initiation of a conversation with a specific task, allowing developers to trace the flow of execution and understand the context of the operations being performed.\n\nBy integrating the log function into these methods, the RichPrinter class plays a crucial role in maintaining a clear and informative logging system throughout the application. This logging capability is essential for debugging and monitoring the application's behavior, especially when dealing with asynchronous operations and complex workflows.\n\n**Note**: It is important to ensure that the message parameter is a well-formed string, and if a style is provided, it should be a valid style recognized by the Rich library. Proper usage of this function contributes to effective logging practices, aiding in the maintenance and troubleshooting of the application.",
        "**log**: The function of log is to print a styled log message to the console.\n\n**parameters**: The parameters of this Function.\n· message: str - The message to be logged, which contains the content to be printed to the console.  \n· style: str (optional) - An optional parameter that specifies the style in which the message should be printed.\n\n**Code Description**: The log function is a method within the RichPrinter class that is designed to output log messages to the console with optional styling. The function takes two parameters: `message`, which is a string containing the content to be logged, and `style`, which is an optional string that defines the visual style of the output.\n\nWhen invoked, the log function calls `self.console.log(message, style=style)`, which utilizes the console's logging capabilities to print the message. The `style` parameter allows for customization of the message's appearance, enabling developers to highlight important information or differentiate between various types of log messages through color or formatting.\n\nThe log function is called within various contexts in the project, notably in the `evaluate` function found in the abs_exp_1.py file. In this context, the log function is used to print formatted messages that indicate the current question being evaluated, the ground truth answer, and the model's predicted answer. This logging is crucial for tracking the evaluation process and understanding the performance of the model during assessments.\n\nAdditionally, the log function is utilized in other parts of the project, such as within the `chat_with_template` function, where it logs the full rendered prompt when the `check_prompt` parameter is set to True. This feature aids in debugging by providing visibility into the prompts sent to the conversational model.\n\nOverall, the log function serves as an essential tool for maintaining transparency in the application's operations, allowing developers to monitor the flow of execution and capture important events or errors during runtime.\n\n**Note**: It is important to ensure that the `message` parameter is a well-formed string. The `style` parameter should be used judiciously to enhance readability without overwhelming the console output. Proper usage of this function contributes to effective logging practices within the application.",
        "**log**: The function of log is to print a styled log message to the console.\n\n**parameters**: The parameters of this Function.\n· message: A string representing the message to be logged.  \n· style: An optional string that specifies the style to be applied to the log message.\n\n**Code Description**: The log function is designed to output a log message to the console with optional styling. It takes two parameters: `message`, which is the text to be displayed, and `style`, which allows for customization of the message's appearance. The function utilizes the `self.console.log` method to print the message, applying the specified style if provided.\n\nThis function is called within various parts of the project to log important information, such as the evaluation results in the evaluate function and the rendered prompts in the chat_with_template function. For instance, in the evaluate function, the log method is used to print the details of each question being processed, including the ground truth answer and the model's predicted answer. This logging is crucial for tracking the evaluation process and understanding the model's performance.\n\nThe log function enhances the visibility of the application's operations by providing styled output, making it easier for developers and users to follow the flow of information and identify key events in the execution of the program.\n\n**Note**: It is important to ensure that the message parameter is a well-formed string. The style parameter should correspond to valid styling options recognized by the console logging mechanism. Proper usage of this function contributes to effective logging practices and aids in monitoring the application's behavior."
      ],
      "code_start_line": 22,
      "code_end_line": 24,
      "params": [
        "self",
        "message",
        "style"
      ],
      "have_return": false,
      "code_content": "    def log(self, message: str, style: str = None):\n        \"\"\"打印带有样式的日志消息\"\"\"\n        self.console.log(message, style=style)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/evaluate",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/main.py/_action_router",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/models.py/ConversationManager/__init__",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/verify_qa_format",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/_calculate_score",
        "src/criticsearch/rich_output.py/RichPrinter/print_exception",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponseList/ser_model",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "src/criticsearch/tools/tool_registry.py/ToolRegistry/get_or_create_tool_schema",
        "src/criticsearch/tools/tool_registry.py/ToolRegistry/register_tool"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "print",
      "md_content": [
        "**print**: The function of print is to output a message to the console with an optional styling.\n\n**parameters**: The parameters of this Function.\n· message: A string that contains the message to be printed to the console.\n· style: An optional string that specifies the style in which the message should be printed.\n\n**Code Description**: The print function is designed to display a message on the console, utilizing the console's print method. It takes two parameters: 'message', which is a required string that represents the content to be printed, and 'style', which is an optional parameter that allows the user to specify a particular style for the output. If no style is provided, the message will be printed in the default format.\n\nThis function is called within the context of other functions in the project, specifically in `generate_content_for_section` and `process_single_task`. In `generate_content_for_section`, the print function is used to output the generated content for a specific section, ensuring that the user is informed of the progress and results of the content generation process. Similarly, in `process_single_task`, the print function is employed to log various messages, including extracted thought processes, queries, and the final answers generated by the common agent. This highlights the function's role in providing feedback and information to the user throughout the execution of tasks.\n\n**Note**: It is important to ensure that the message parameter is always a string, as passing non-string types may lead to unexpected behavior. Additionally, when using the style parameter, users should be aware of the available styles supported by the console to achieve the desired output appearance.",
        "**print**: The function of print is to display a message to the console with an optional style.\n\n**parameters**: The parameters of this Function.\n· message: A string that contains the message to be printed.\n· style: An optional string that specifies the style in which the message should be printed.\n\n**Code Description**: The print function is a method of the RichPrinter class, which is responsible for outputting messages to the console in a formatted manner. It takes two parameters: `message`, which is a string that represents the content to be displayed, and `style`, which is an optional parameter that allows the user to specify how the message should appear (for example, in bold or a specific color).\n\nInternally, the function utilizes the `self.console.print()` method to render the message on the console. This method is part of the Rich library, which provides advanced formatting options for terminal output. If a style is provided, it is applied to the message, enhancing its visibility or aesthetic appeal.\n\nThe print function is called in various parts of the codebase, particularly within the `call_llm`, `tavily_extract`, `fallback_scrape`, and other functions in the `src/criticsearch/abstract_substitution/abs_exp_1.py` file. For instance, in the `call_llm` function, the print method is used to display the prompt sent to the language model and the raw output received from it. This helps in debugging and understanding the flow of data through the application.\n\nIn the context of the overall application, the print function serves as a crucial tool for logging and displaying information to the user, making it easier to track the progress of operations and the results of various tasks.\n\n**Note**: When using the print function, it is essential to ensure that the message is properly formatted and that the style, if used, is valid according to the Rich library's styling options. This will ensure that the output is both informative and visually appealing.",
        "**print**: The function of print is to display a message to the console with an optional style.\n\n**parameters**: The parameters of this Function.\n· message: A string that contains the message to be printed to the console.\n· style: An optional string that specifies the style to be applied to the printed message.\n\n**Code Description**: The print function is a method defined within the RichPrinter class, which is responsible for outputting messages to the console in a formatted manner. The function takes two parameters: `message`, which is the text to be displayed, and `style`, which is an optional parameter that allows the user to specify how the message should be styled when printed.\n\nWhen the print method is called, it utilizes the console's print functionality to display the provided message. If a style is specified, it applies that style to the message, enhancing the visual presentation of the output. This is particularly useful for distinguishing different types of messages, such as warnings, errors, or informational messages, by using various styles (e.g., bold, italic, colored text).\n\nThe print method is invoked in various parts of the project, particularly in functions that require user feedback or logging of information. For example, it is called in the `call_llm`, `tavily_extract`, and `fallback_scrape` functions to display prompts, results, and other relevant information to the user. By centralizing the printing functionality in the RichPrinter class, the project maintains consistency in how messages are displayed across different components.\n\nThe use of the print method enhances the user experience by providing clear and styled output, making it easier for users to follow the flow of information during the execution of the application.\n\n**Note**: When using the print function, it is important to ensure that the message is properly formatted and that any specified style is valid. If no style is provided, the message will be printed in the default format.",
        "**print**: The function of print is to display a message in the console with optional styling.\n\n**parameters**: The parameters of this Function.\n· message: A string that contains the message to be printed to the console.\n· style: An optional string that specifies the style to be applied to the printed message.\n\n**Code Description**: The print function is a method within the RichPrinter class that facilitates the output of messages to the console. It utilizes the console's print method to display the provided message, applying any specified styling to enhance the visual presentation of the output.\n\nThe function begins by accepting two parameters: `message`, which is a string representing the content to be printed, and `style`, which is an optional parameter that allows the user to define the styling of the output (e.g., bold, italic, colored). If a style is provided, it is passed to the console's print method, which applies the desired formatting to the message before displaying it.\n\nThis print function is called in various contexts throughout the project, particularly in functions that require user feedback or logging of information. For instance, it is used in the call_llm function to print prompts and responses during interactions with the OpenAI chat completion API. Additionally, it is utilized in the tavily_extract and fallback_scrape functions to display URLs being processed and results obtained from web scraping.\n\nThe integration of the print function within these contexts ensures that important information is communicated effectively to the user, enhancing the overall usability of the application. By providing visual feedback through styled output, the print function contributes to a more engaging and informative user experience.\n\n**Note**: When using the print function, it is important to ensure that the message is clear and concise. The optional style parameter should be used judiciously to maintain readability and avoid excessive formatting that could detract from the message's clarity.",
        "**print**: The function of print is to display a message to the console with optional styling.\n\n**parameters**: The parameters of this Function.\n· message: A string that contains the message to be printed to the console.\n· style: An optional string that specifies the style to be applied to the printed message.\n\n**Code Description**: The print function is a method within the RichPrinter class that facilitates the output of messages to the console. It takes two parameters: `message`, which is a string representing the content to be displayed, and `style`, which is an optional parameter that allows the user to specify how the message should be styled when printed.\n\nUpon invocation, the function utilizes the `self.console.print` method to render the message on the console. The `style` parameter, if provided, is passed along to apply the specified formatting to the output. This capability enhances the readability and visual appeal of console outputs, making it easier for users to distinguish between different types of messages, such as warnings, errors, or informational messages.\n\nThe print function is called in various contexts throughout the project, particularly in functions that require user feedback or logging of information. For instance, it is utilized in the `call_llm`, `tavily_extract`, `fallback_scrape`, and other functions to display prompts, results, and status messages. This consistent use of the print method ensures that all console outputs maintain a uniform style and formatting, contributing to a cohesive user experience.\n\nIn the context of its callers, the print function serves as a critical tool for communication between the application and the user, providing real-time feedback on the operations being performed. By allowing for styled outputs, it enhances the clarity of the information presented, which is particularly beneficial in debugging and monitoring scenarios.\n\n**Note**: When using the print function, it is important to ensure that the message content is properly formatted and that any specified styles are valid. The function is designed to handle a variety of message types, but incorrect usage may lead to unexpected output or errors in the console."
      ],
      "code_start_line": 26,
      "code_end_line": 28,
      "params": [
        "self",
        "message",
        "style"
      ],
      "have_return": false,
      "code_content": "    def print(self, message: str, style: str = None):\n        \"\"\"打印普通的消息\"\"\"\n        self.console.print(message, style=style)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/call_llm",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/tavily_extract",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/fallback_scrape",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/run",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_query_update",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_run",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/save",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/evaluate",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/main",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/main/_batch/run_one",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/main/_batch",
        "src/criticsearch/abstract_substitution/abs_workflow.py",
        "src/criticsearch/abstract_substitution/abs_workflow.py/call_llm",
        "src/criticsearch/abstract_substitution/abs_workflow.py/tavily_extract",
        "src/criticsearch/abstract_substitution/abs_workflow.py/fallback_scrape",
        "src/criticsearch/abstract_substitution/abs_workflow.py/verify_fuzzy_item",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/run",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/save",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_run",
        "src/criticsearch/abstract_substitution/abs_workflow.py/evaluate",
        "src/criticsearch/abstract_substitution/abs_workflow.py/evaluate/evaluate_item_worker",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_fuzzy_replacement",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_entity_extraction",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_combination_workflow",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_combination_workflow/process_entity",
        "src/criticsearch/abstract_substitution/abs_workflow.py/main",
        "src/criticsearch/abstract_substitution/abs_workflow.py/main/_combination_batch",
        "src/criticsearch/abstract_substitution/abs_workflow.py/main/_batch/run_one",
        "src/criticsearch/abstract_substitution/abs_workflow.py/main/_batch",
        "src/criticsearch/base_agent.py/BaseAgent/taking_notes",
        "src/criticsearch/main.py/generate_content_for_section",
        "src/criticsearch/main.py/_model_action_decision",
        "src/criticsearch/main.py/_action_router",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_load_from_cache",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/sliding_window_pairing",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section_with_models/process_with_model",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section_with_models",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_benchmark_item",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/verify_qa_format",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_for_folder",
        "src/criticsearch/tools/content_scraper/tavily_extract.py/TavilyExtract/extract_content"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "print_exception",
      "md_content": [
        "**print_exception**: The function of print_exception is to log an error message and print the exception details to the console.\n\n**parameters**: The parameters of this Function.\n· message: str - The message to be logged, indicating the nature of the exception.\n· max_frames: int (optional) - The maximum number of stack frames to display when printing the exception details, defaulting to 5.\n\n**Code Description**: The print_exception function is a method within the RichPrinter class that serves to handle and display exception information in a user-friendly manner. When invoked, it first logs the provided message using the log method, which formats the message in bold red style to draw attention to the error. This is achieved through the call to printer.log(f\"{message}\", style=\"bold red\"). \n\nFollowing the logging of the message, the function utilizes the self.console.print_exception method to print the actual exception details to the console. This method is part of the Rich library, which enhances console output by providing formatted and colored text. The max_frames parameter allows the user to control how many frames of the stack trace are displayed, with a default value of 5, which helps in limiting the amount of information shown while still providing enough context for debugging.\n\nThe print_exception function is called in the critic method of the CriticAgent class when an error occurs during the extraction and validation of YAML content. If the YAML extraction fails, the print_exception function is invoked with a specific message indicating the failure. This integration ensures that any issues encountered during the critique process are logged and displayed, allowing developers to trace errors effectively.\n\nBy providing a clear logging mechanism and formatted output for exceptions, the print_exception function plays a crucial role in maintaining the robustness of the application. It aids in debugging by ensuring that error messages are visible and informative, thereby facilitating easier troubleshooting.\n\n**Note**: It is important to ensure that the message parameter is a well-formed string. The max_frames parameter should be set according to the level of detail required for debugging, keeping in mind that excessive stack trace information may clutter the console output. Proper usage of this function contributes to effective error handling practices within the application.",
        "**print_exception**: The function of print_exception is to log an error message and print the exception details to the console.\n\n**parameters**: The parameters of this Function.\n· message: str - The message to be logged, which contains the content to be printed to the console.  \n· max_frames: int (optional) - The maximum number of stack frames to display when printing the exception, defaulting to 5.\n\n**Code Description**: The print_exception method is a member of the RichPrinter class, designed to handle the logging and display of exception messages in a formatted manner. When invoked, it first calls the log method to print the provided message in bold red style, which serves to highlight the error and draw attention to it. This is particularly useful for debugging and monitoring purposes, as it allows developers to quickly identify issues within the application.\n\nFollowing the logging of the message, the method utilizes the console's print_exception functionality to display the exception details. The max_frames parameter controls how many frames of the stack trace are shown, allowing for a concise view of the error context without overwhelming the console output. By default, it limits the display to five frames, which is typically sufficient for diagnosing most issues.\n\nThe print_exception method is called in various contexts throughout the project, notably within the critic method of the CriticAgent class. In this context, it is used to log an error when the extraction and validation of YAML content from the model's response fails. This ensures that any issues encountered during the critique generation process are communicated clearly, allowing developers to address them promptly.\n\nAdditionally, the print_exception method is invoked in the main function of the project when an exception occurs during the execution of the process_single_task function. This provides a mechanism for gracefully handling errors at the top level of the application, ensuring that users receive informative feedback when something goes wrong.\n\nThe method is also referenced in the write method of the ConversationManager class, where it logs errors encountered during file writing operations. This consistent usage of print_exception across different parts of the codebase contributes to a robust error handling strategy, enhancing the overall reliability of the application.\n\n**Note**: It is important to ensure that the message parameter is a well-formed string. The max_frames parameter should be set according to the desired level of detail in the stack trace. Proper usage of this function aids in effective logging and debugging practices within the application.",
        "**print_exception**: The function of print_exception is to log an error message and print the exception details to the console.\n\n**parameters**: The parameters of this Function.\n· message: str - The message to be logged, which contains the content to be printed to the console.  \n· max_frames: int (optional) - The maximum number of stack frames to display when printing the exception, defaulting to 5.\n\n**Code Description**: The print_exception method is a member of the RichPrinter class, designed to handle the logging and display of exception messages in a visually distinct manner. When invoked, it first calls the log method to print the provided message in bold red style, ensuring that the error is immediately noticeable to the user. This is achieved through the line `printer.log(f\"{message}\", style=\"bold red\")`, which utilizes the logging capabilities of the RichPrinter class.\n\nFollowing the logging of the message, the method proceeds to print the actual exception details using the console's print_exception method, with the parameter max_frames controlling how many frames of the stack trace are displayed. This allows developers to quickly identify the source of the error and understand the context in which it occurred.\n\nThe print_exception method is called in various parts of the project, notably within the critic method of the CriticAgent class and the main function of the main module. In the CriticAgent's critic method, it is used to log errors encountered during the parsing of YAML content from the model's response. If a yaml.YAMLError is raised, the print_exception method is invoked to provide clear feedback about the parsing failure, which is critical for debugging and resolving issues in the critique generation process.\n\nSimilarly, in the main function, print_exception is called when an exception occurs during the processing of user tasks. This ensures that any errors encountered during the execution of the CriticSearch pipeline are logged and communicated to the user, enhancing the application's robustness and user experience.\n\nOverall, the print_exception method serves as an essential tool for error handling within the application, providing a consistent approach to logging and displaying exceptions. Its integration into various components of the project underscores its importance in maintaining transparency and facilitating effective debugging.\n\n**Note**: It is important to ensure that the message parameter is a well-formed string. The max_frames parameter should be set according to the desired level of detail in the stack trace output. Proper usage of this function contributes to effective logging practices and aids in the swift resolution of issues within the application."
      ],
      "code_start_line": 30,
      "code_end_line": 34,
      "params": [
        "self",
        "message",
        "max_frames"
      ],
      "have_return": false,
      "code_content": "    def print_exception(self, message: str, max_frames: int = 5):\n        printer.log(f\"{message}\", style=\"bold red\")\n\n        \"\"\"打印异常信息\"\"\"\n        self.console.print_exception(max_frames=max_frames)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main.py/main",
        "src/criticsearch/models.py/ConversationManager/__init__",
        "src/criticsearch/models.py/ConversationManager/write",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient/search",
        "src/criticsearch/utils.py/extract_and_validate_json"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/log"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "save_output_to_file",
      "md_content": [
        "**save_output_to_file**: The function of save_output_to_file is to save the current console output to a specified file.\n\n**parameters**: The parameters of this Function.\n· file_path: A Path object representing the location of the file where the console output will be saved. The default value is \"output.txt\".\n\n**Code Description**:  \nThe `save_output_to_file` function is responsible for saving the current console output to a file. The function takes an optional parameter, `file_path`, which specifies the path where the output will be stored. If not provided, the output is saved to a default file named \"output.txt\". \n\nThe function works as follows:\n1. It opens the file at the specified `file_path` in write-text mode (`\"wt\"`), ensuring the file is created if it does not already exist. The file is opened with UTF-8 encoding to handle text properly.\n2. A `Console` object is created, with the file (`report_file`) passed as the output stream. This step redirects the console's output to the file.\n3. The function then calls the `export_text` method of the `console` object to retrieve the current console output in text format.\n4. The retrieved text is written to the file using the `log` method of the `Console` object.\n5. Finally, a rule (a separator) is added to the output to indicate that the file has been generated, marking the end of the saved content.\n\nThis function ensures that the console's output is properly saved to a file and that the file is marked as complete.\n\n**Note**: \n- The function opens the file in text mode with UTF-8 encoding, so it is important to ensure the file system supports this encoding.\n- The function automatically creates the file if it does not already exist, but overwrites the file if it already exists, without any prompt for user confirmation. \n- The `console.export_text()` method should be correctly populated with the desired console output for accurate results."
      ],
      "code_start_line": 36,
      "code_end_line": 45,
      "params": [
        "self",
        "file_path"
      ],
      "have_return": false,
      "code_content": "    def save_output_to_file(self, file_path: Path = Path(\"output.txt\")):\n        with open(file_path, \"wt\", encoding=\"utf-8\") as report_file:\n            # Redirect Console output to the specified file\n            console_for_export = Console(file=report_file)\n\n            # Export the console's current text output and write it to the file\n            console_for_export.log(self.console.export_text())\n\n            # Add a rule at the end of the output to indicate the file has been generated\n            console_for_export.rule(\"Output file Generated.\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/__main__.py": [],
  "src/criticsearch/utils.py": [
    {
      "type": "FunctionDef",
      "name": "count_tokens",
      "md_content": [
        "**count_tokens**: The function of count_tokens is to calculate the number of tokens in a given text string based on a specified model.\n\n**parameters**:\n· text: The input text that needs to be tokenized.\n· model: The model to be used for tokenization. Default is \"gpt-4o\". This parameter allows the selection of different models that might affect the tokenization method. Common options include \"gpt-3.5-turbo\", \"gpt-4\", and \"text-embedding-ada-002\".\n\n**Code Description**: \nThe function `count_tokens` is designed to calculate the number of tokens in a provided text based on a selected model for tokenization. The process begins by determining the correct encoding method for the specified model using the `tiktoken` library, which is responsible for breaking the text into tokens.\n\n1. The function first attempts to fetch the appropriate encoding for the specified model using `tiktoken.encoding_for_model(model)`.\n2. If the provided model does not exist or is unsupported (raising a `KeyError`), it defaults to using a fallback encoding, `tiktoken.get_encoding(\"cl100k_base\")`.\n3. The text is then encoded into tokens using the encoding method, and the number of tokens is returned by evaluating the length of the tokenized text.\n\nThe function is used in scenarios where it's necessary to measure the token count of a text, particularly when dealing with AI models like OpenAI's GPT family, which have token limits for inputs. This is especially useful in contexts such as text preprocessing or chunking operations, where token limits need to be respected.\n\nIn the project, `count_tokens` is employed within the `extract_sections` function, which recursively processes sections of a dataset to extract content, titles, and their respective paths. The token count for each section's content is calculated and stored in the resulting data structure. The token count is a crucial piece of information because it helps in ensuring that sections do not exceed certain token limits, particularly when preparing inputs for processing by machine learning models, such as in the case of the `sliding_window_pairing` function.\n\nIn the `sliding_window_pairing` function, the `count_tokens` function is critical for ensuring that each \"window\" of text remains within a specified token limit (`max_token_length`). By calculating the token count for each section, the function can group sections together in a way that ensures the total number of tokens in any given window does not exceed the maximum allowable tokens for the model, making it essential for efficient and valid data processing.\n\n**Note**:\n- The `count_tokens` function depends on the `tiktoken` library, which must be available in the environment.\n- If an unsupported model is passed, the function will use a fallback tokenization model, but results may differ slightly depending on the encoding used.\n- The function is optimized to handle different models, and using an inappropriate model might result in less accurate token counts if the fallback encoding is used.\n\n**Output Example**:\nFor example, when calling `count_tokens(\"Hello, world!\")`, the function will return the token count of the string. Assuming \"Hello\" and \"world!\" are treated as separate tokens, the output might be:\n\n```\n4\n```"
      ],
      "code_start_line": 8,
      "code_end_line": 30,
      "params": [
        "text",
        "model"
      ],
      "have_return": true,
      "code_content": "def count_tokens(text: str, model: str = \"gpt-4o\") -> int:\n    \"\"\"\n    Calculate the number of tokens in a text string.\n    \n    Args:\n        text: The text to tokenize\n        model: The model to use for tokenization (default: \"gpt-3.5-turbo\")\n            Some common options: \"gpt-3.5-turbo\", \"gpt-4\", \"text-embedding-ada-002\"\n    \n    Returns:\n        int: The number of tokens in the text\n    \n    Example:\n        >>> count_tokens(\"Hello, world!\")\n        4\n    \"\"\"\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        # Fall back to cl100k_base encoding if model not found\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n    \n    return len(encoding.encode(text))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/sliding_window_pairing/extract_sections",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/sliding_window_pairing"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_queries_from_response",
      "md_content": [
        "## `extract_queries_from_response` Function\n\n### Description\nThe `extract_queries_from_response` function is designed to extract a list of queries from a given response text. The function supports multiple formats for the input text and processes them to return a list of queries. If no valid queries are found, the function returns an empty list.\n\n### Parameters\n- **response_text** (`str`): The input response text containing potential query information. The response text can be in one of the following formats:\n  - `<queries>[query1, query2]</queries>`\n  - `<queries>List[str] = [query1, query2]</queries>`\n  - `<queries> [query1, query2] </queries>`\n\n### Returns\n- **list**: A list of extracted queries. If no queries are found, an empty list is returned.\n\n### Functionality\n1. The function first strips any leading or trailing whitespace from the input `response_text`.\n2. It then attempts to match the text against two possible patterns:\n   - The first pattern matches a format where queries are wrapped in a `List[str] = [...]` structure.\n   - The second pattern matches the standard array-like format `[query1, query2]`.\n3. If a match is found, the queries are extracted and further processed:\n   - The extracted query string is split based on regular expressions, taking into account commas within quotes to avoid splitting queries incorrectly.\n   - Each part is cleaned of any surrounding quotes and extra whitespace.\n4. The cleaned queries are returned as a list.\n5. If no valid queries are extracted, the function returns an empty list.\n\n### Example\n```python\nresponse_text = \"<queries>[\\\"query1\\\", \\\"query2\\\"]</queries>\"\nqueries = extract_queries_from_response(response_text)\nprint(queries)  # Output: ['query1', 'query2']\n```\n\n### Notes\n- The function supports both single-quoted and double-quoted strings for the queries.\n- It uses regular expressions to identify and extract the query list, ensuring flexibility in handling different formatting variations.",
        "## `extract_queries_from_response` Function Documentation\n\n### Function Signature:\n```python\ndef extract_queries_from_response(response_text: str) -> list:\n```\n\n### Description:\nThe `extract_queries_from_response` function processes a given string (`response_text`), which is expected to contain query data in specific formats, and extracts a list of queries from it. The function uses regular expressions to match and retrieve query data from the response text, handling two potential formats for the query data. After extracting the raw query string, the function further processes it by removing unnecessary characters, such as quotes and extra spaces.\n\n### Parameters:\n- **response_text** (`str`): The input string containing the response text from which queries will be extracted. This string must contain query data wrapped in specific XML-like tags (`<queries>`).\n\n### Returns:\n- **list**: A list of strings, where each string represents a query extracted from the `response_text`. If no queries are found or the input format is invalid, an empty list is returned.\n\n### Functionality:\n1. **Preprocessing**: The function first trims any leading or trailing whitespace from the `response_text`.\n2. **Pattern Matching**: It uses two regular expression patterns to match and extract the query data. The patterns are designed to handle two possible formats:\n   - `<queries>List[str] = [...]</queries>`\n   - `<queries>[...]</queries>`\n3. **Query Extraction**: Once a match is found, the function captures the part of the string that contains the queries. It then splits this string into individual queries, considering the possibility of quotes around query strings.\n4. **Cleaning**: Each extracted query is cleaned by stripping extra spaces and removing any surrounding quotes (either single or double quotes).\n5. **Return**: The function returns a list of cleaned queries. If no queries are found, it returns an empty list.\n\n### Example:\n#### Input:\n```python\nresponse_text = '<queries>[ \"query1\", \"query2\", \"query3\" ]</queries>'\nextract_queries_from_response(response_text)\n```\n\n#### Output:\n```python\n['query1', 'query2', 'query3']\n```\n\n### Usage:\nThis function is typically used to parse response texts from various data sources (e.g., APIs, templates) where queries are embedded in a structured format. It is commonly invoked to extract search or task-related queries for further processing, such as sending them to a search engine or using them in a decision-making process.\n\n### Notes:\n- The function assumes that the queries are enclosed within the `<queries>` tag in the input text.\n- It can handle both `List[str]` and basic array-like representations of the queries within the `<queries>` tag.\n- The function is case-insensitive and allows for flexible formatting within the tags."
      ],
      "code_start_line": 32,
      "code_end_line": 62,
      "params": [
        "response_text"
      ],
      "have_return": true,
      "code_content": "def extract_queries_from_response(response_text: str) -> list:\n    # 预处理文本\n    response_text = response_text.strip()\n    \n    # 匹配两种可能的格式\n    patterns = [\n        # 匹配 <queries>List[str] = [...] </queries> 格式\n        r'<queries>\\s*List\\[str\\]\\s*=\\s*\\[(.*?)\\]\\s*</queries>',\n        # 匹配 <queries>[...] </queries> 格式\n        r'<queries>\\s*\\[(.*?)\\]\\s*</queries>'\n    ]\n    \n    for pattern in patterns:\n        match = re.search(pattern, response_text, re.DOTALL | re.IGNORECASE)\n        if match:\n            queries_str = match.group(1)\n            # 处理查询列表\n            queries = []\n            # 使用正则表达式分割，考虑引号内的逗号\n            parts = re.findall(r'\"([^\"]*?)\"|\\'([^\\']*?)\\'|([^,]+)', queries_str)\n            for part in parts:\n                # part是一个元组，包含三个捕获组，取非空的那个\n                query = next((p.strip() for p in part if p.strip()), '')\n                if query:\n                    # 清理引号和多余空格\n                    query = query.strip('\"\\'').strip()\n                    if query:\n                        queries.append(query)\n            return queries\n            \n    return []\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/_model_action_decision",
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_thought_from_response",
      "md_content": [
        "**extract_thought_from_response**: The function of extract_thought_from_response is to extract the thought content from a given response text formatted with specific tags.\n\n**parameters**: The parameters of this Function.\n· response_text: A string containing the response text that includes thought content formatted with `<\\thought>...<\\thought>`.\n\n**Code Description**: The extract_thought_from_response function is designed to parse a string input, specifically looking for content enclosed within the tags `<thought>` and `</thought>`. It utilizes a regular expression pattern to identify and extract the desired content. The function first compiles a regex pattern that matches any text between the `<thought>` and `</thought>` tags. It then applies this pattern to the provided response_text using the re.search method, which allows for matching across multiple lines due to the re.DOTALL flag. If a match is found, the function returns the extracted thought content, stripped of any leading or trailing whitespace. If no match is found, it returns an empty string.\n\nThis function is called within the process_single_task function located in the src/criticsearch/main.py file. During the execution of process_single_task, after generating a response from an agent, extract_thought_from_response is invoked to retrieve the thought process from the agent's response. This extracted thought content is then logged and can be used for further processing, such as appending to conversation data or for debugging purposes. The function plays a crucial role in ensuring that the thought processes of the agent are captured and utilized effectively within the broader task processing workflow.\n\n**Note**: It is important to ensure that the response_text provided to this function is correctly formatted with the appropriate thought tags; otherwise, the function will not be able to extract any content and will return an empty string.\n\n**Output Example**: Given a response_text of \"Here is my thought: <thought>This is the extracted thought content.</thought>\", the function would return \"This is the extracted thought content.\"",
        "## Function Documentation: `extract_thought_from_response`\n\n### Description:\nThe `extract_thought_from_response` function is responsible for extracting the content of the `<thought>` tag from a given response text. This function identifies the content enclosed within the `<thought>...</thought>` tags and returns it as a string. If no such content is found, the function returns an empty string.\n\n### Parameters:\n- **response_text** (`str`): A string containing the response text in which the `<thought>...</thought>` tag is expected to appear. The content of the `<thought>` tag will be extracted.\n\n### Returns:\n- **str**: The content extracted from within the `<thought>` tag. If no `<thought>` tag is found in the `response_text`, an empty string is returned.\n\n### Example Usage:\n\n```python\nresponse_text = \"<thought>This is the thought content.</thought> Other content.\"\nthought = extract_thought_from_response(response_text)\nprint(thought)  # Output: \"This is the thought content.\"\n```\n\n### Implementation Details:\nThe function uses a regular expression (`<thought>(.*?)</thought>`) to search for the content enclosed within the `<thought>` tags. The `re.DOTALL` flag is used to allow the dot (`.`) to match newline characters, while `re.IGNORECASE` ensures the search is case-insensitive. If a match is found, the content inside the `<thought>` tags is returned after stripping any leading or trailing whitespace. If no match is found, an empty string is returned.\n\n### Code Example:\n\n```python\nimport re\n\ndef extract_thought_from_response(response_text: str) -> str:\n    \"\"\"\n    Extracts the thought content from a response text.\n\n    Args:\n        response_text (str): The response text containing <thought>...</thought> tags.\n\n    Returns:\n        str: The extracted thought content, or an empty string if no thought content is found.\n    \"\"\"\n    thought_pattern = r'<thought>(.*?)</thought>'\n    thought_match = re.search(thought_pattern, response_text, re.DOTALL | re.IGNORECASE)\n    if thought_match:\n        return thought_match.group(1).strip()\n    return \"\"\n``` \n\n### Summary:\nThe `extract_thought_from_response` function is designed to efficiently extract specific thought content from structured response text. It is especially useful when working with formatted responses where certain pieces of information are encapsulated in custom tags."
      ],
      "code_start_line": 64,
      "code_end_line": 78,
      "params": [
        "response_text"
      ],
      "have_return": true,
      "code_content": "def extract_thought_from_response(response_text: str) -> str:\n    \"\"\"\n    从响应文本中提取thought内容\n    \n    Args:\n        response_text: 包含<thought>...</thought>格式的响应文本\n        \n    Returns:\n        str: 提取的thought内容,如果未找到则返回空字符串\n    \"\"\"\n    thought_pattern = r'<thought>(.*?)</thought>'\n    thought_match = re.search(thought_pattern, response_text, re.DOTALL | re.IGNORECASE)\n    if thought_match:\n        return thought_match.group(1).strip()\n    return \"\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/_model_action_decision",
        "src/criticsearch/main.py/_action_router",
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_answer_from_response",
      "md_content": [
        "**extract_answer_from_response**: The function of extract_answer_from_response is to extract the content of the answer from a given response text formatted with specific tags.\n\n**parameters**: The parameters of this Function.\n· response_text: A string containing the response text that includes the answer enclosed within `<answer>` tags.\n\n**Code Description**: The extract_answer_from_response function is designed to parse a string input, specifically looking for content that is encapsulated within `<answer>` tags. It utilizes a regular expression pattern to identify and extract the text that appears between these tags. The function employs the `re.search` method from the `re` module, which searches the input string for the specified pattern. If a match is found, the function retrieves the matched content, trims any leading or trailing whitespace, and returns it as a string. If no match is found, the function returns an empty string.\n\nThis function is called within the process_single_task function located in the src/criticsearch/main.py file. In this context, extract_answer_from_response is utilized to extract the answer content from the section content generated by the common agent during the processing of a task. After generating the section content, the function is invoked to retrieve the answer, which is then stored in the conversation data structure along with other relevant information such as thought content and citations. This integration is crucial for maintaining the flow of information and ensuring that the generated reports contain the necessary answers derived from the agent's responses.\n\n**Note**: It is important to ensure that the response_text provided to this function is correctly formatted with the `<answer>` tags; otherwise, the function will return an empty string, indicating that no answer content was found.\n\n**Output Example**: If the input response_text is \"Here is the answer: <answer>This is the extracted answer.</answer>\", the function will return \"This is the extracted answer.\"",
        "**extract_answer_from_response**: The function of extract_answer_from_response is to extract the content between `<answer>` and `</answer>` tags from a given response text.\n\n**parameters**:  \n· response_text: A string that contains a response with content enclosed in `<answer>` and `</answer>` tags.\n\n**Code Description**:  \nThe function `extract_answer_from_response` is designed to extract the content of an `<answer>` tag from a provided `response_text` string. It uses a regular expression to search for the pattern that matches the content within `<answer>` and `</answer>` tags. If a match is found, it returns the extracted content, stripping any leading or trailing whitespace. If no such content is found, the function returns an empty string.\n\nThe function operates as follows:\n1. It defines a regular expression pattern (`answer_pattern`) to match the content between `<answer>` and `</answer>` tags. The pattern `r'<answer>(.*?)</answer>'` utilizes non-greedy matching to ensure it captures everything between the tags.\n2. The function then uses `re.search` with the `re.DOTALL` and `re.IGNORECASE` flags. The `re.DOTALL` flag allows the dot (`.`) to match newline characters, while `re.IGNORECASE` makes the search case-insensitive.\n3. If a match is found, the function extracts the content using `group(1)`, which refers to the first captured group (the content inside the `<answer>` tags). The `strip()` method is applied to remove any leading or trailing whitespace.\n4. If no match is found, the function returns an empty string.\n\nThis function is used in the project primarily to retrieve the answer portion from a larger response that may contain other content. For example, in `src/criticsearch/main.py/_action_router`, after generating a section of content with `agent.chat_with_template`, the `extract_answer_from_response` function is invoked to extract the answer content from the generated response. The extracted answer is then processed and logged as part of the agent's training data.\n\n**Note**:  \n- The function relies on proper formatting of the input string, specifically the presence of the `<answer>` and `</answer>` tags. If these tags are missing or incorrectly formatted, the function will return an empty string.\n- This function is case-insensitive, meaning it will match tags like `<ANSWER>` and `<answer>` equally.\n- If the response text contains multiple `<answer>` sections, only the content of the first match will be returned.\n\n**Output Example**:  \nFor a response text like:\n\n```html\n<response>\n    <answer>This is the extracted answer.</answer>\n</response>\n```\n\nThe function will return:\n\n```python\n\"This is the extracted answer.\"\n```\n\nIf no `<answer>` tag is found in the response text, the function will return:\n\n```python\n\"\"\n```"
      ],
      "code_start_line": 80,
      "code_end_line": 94,
      "params": [
        "response_text"
      ],
      "have_return": true,
      "code_content": "def extract_answer_from_response(response_text: str) -> str:\n    \"\"\"\n    从响应文本中提取answer内容\n    \n    Args:\n        response_text: 包含<answer>...</answer>格式的响应文本\n        \n    Returns:\n        str: 提取的answer内容,如果未找到则返回空字符串\n    \"\"\"\n    answer_pattern = r'<answer>(.*?)</answer>'\n    answer_match = re.search(answer_pattern, response_text, re.DOTALL | re.IGNORECASE)\n    if answer_match:\n        return answer_match.group(1).strip()\n    return \"\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py",
        "src/criticsearch/abstract_substitution/abs_workflow.py",
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/_action_router",
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/parse_tagged_data_to_table"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_boxed_content",
      "md_content": [
        "### Function: `extract_boxed_content`\n\n#### Purpose:\nThe `extract_boxed_content` function extracts the content enclosed within a LaTeX `\\boxed{}` expression from a given answer string. If the `\\boxed{}` expression is not found, it returns the original answer string.\n\n#### Arguments:\n- **answer** (`str`): A string that may contain content enclosed in a `\\boxed{}` expression. This argument represents the answer text without any tags.\n\n#### Returns:\n- **str**: The content inside the `\\boxed{}` expression if it is found. If no such content is found, the function returns the original `answer` string.\n\n#### Description:\nThis function searches for the LaTeX syntax `\\boxed{}` in the provided `answer` string using a regular expression. If a match is found, it extracts and returns the content inside the `\\boxed{}`. If no match is found, it returns the original input string unchanged.\n\n#### Example Usage:\n```python\nanswer = \"The solution is \\\\boxed{42}\"\nresult = extract_boxed_content(answer)\nprint(result)  # Output: 42\n\nanswer = \"The solution is 42\"\nresult = extract_boxed_content(answer)\nprint(result)  # Output: The solution is 42\n```\n\n#### Notes:\n- The function assumes that the `answer` string might contain LaTeX-style boxed content, and it will only extract content if it is enclosed within `\\boxed{}`.\n- The function uses regular expressions to identify and extract the boxed content efficiently.\n\n---\n\nThis function is primarily used to parse and extract boxed content from responses generated by models or other textual inputs in contexts such as automated evaluation or formatted output validation."
      ],
      "code_start_line": 97,
      "code_end_line": 107,
      "params": [
        "answer"
      ],
      "have_return": true,
      "code_content": "def extract_boxed_content(answer: str) -> str:\n    \"\"\"从答案中提取 boxed 内容\n    \n    Args:\n        answer (str): 包含 boxed 内容的答案字符串，但是不包含answer的标签\n    \n    Returns:\n        str: boxed中的内容,如果没有找到则返回原始答案\n    \"\"\"\n    boxed_match = re.search(r\"\\\\boxed{([^}]+)}\", answer)\n    return boxed_match.group(1).strip() if boxed_match else answer\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py",
        "src/criticsearch/abstract_substitution/abs_workflow.py",
        "src/criticsearch/abstract_substitution/abs_workflow.py/verify_fuzzy_item",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_workflow.py/evaluate/evaluate_item_worker",
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/verify_qa_format",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/parse_tagged_data_to_table"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_citations",
      "md_content": [
        "**extract_citations**: The function of extract_citations is to extract all referenced URLs from a given text and ensure they are unique.\n\n**parameters**: The parameters of this Function.\n· text: A string that contains text formatted with <\\citation>URL<\\citation> tags.\n\n**Code Description**: The extract_citations function is designed to identify and extract URLs that are enclosed within specific citation tags in a provided text. The function takes a single argument, 'text', which is expected to be a string containing one or more citations formatted as <\\citation>URL<\\citation>. \n\nThe function initializes an empty set called 'citations' to store the unique URLs. It then defines a regular expression pattern, `r'citation>(.*?)<'`, which is used to search for matches in the input text. This pattern looks for any substring that starts with 'citation>' and ends with '<', capturing the content in between. \n\nUsing the `re.findall` method, the function searches the input text for all occurrences that match the defined pattern. If any matches are found, they are added to the 'citations' set, which automatically handles duplicates by only retaining unique entries. Finally, the function returns the 'citations' set, which will contain all the extracted URLs. If no citations are found, an empty set is returned.\n\n**Note**: It is important to ensure that the input text is correctly formatted with the specified citation tags for the function to work effectively. The function relies on the presence of these tags to identify and extract URLs.\n\n**Output Example**: An example of the function's return value could be a set containing URLs such as {'http://example.com', 'http://anotherexample.com'} if those URLs were found in the input text. If no URLs are found, the output would be an empty set: set().",
        "**extract_citations**: The function of extract_citations is to extract all unique URLs from a given text that are enclosed within `<citation>` tags.\n\n**parameters**: The parameters of this Function are as follows:\n· text: A string containing text that includes URLs formatted as `<citation>URL</citation>`.\n\n**Code Description**: The extract_citations function is designed to identify and extract URLs from a provided text string that are enclosed within specific XML-like tags, namely `<citation>`. The function begins by defining a regular expression pattern that matches the content within these tags. It utilizes the `re.findall` method to search for all occurrences of this pattern in the input text, returning a list of matches.\n\nIf matches are found, the function takes the first match and removes any newline characters, then attempts to clean and evaluate the string using `ast.literal_eval`. This method is preferred for safely evaluating strings that represent Python literals. If this evaluation fails due to a SyntaxError or ValueError, the function then attempts to replace single quotes with double quotes and uses `json.loads` to parse the cleaned string as JSON. If this also fails, it returns an empty list.\n\nThe extract_citations function is called by other functions within the project, such as `process_section` and `parse_markdown_to_structure`. In `process_section`, it is used to extract citations from paragraphs of text, ensuring that any URLs present are captured and associated with their respective paragraphs. Similarly, in `parse_markdown_to_structure`, it extracts citations from markdown text as it is being parsed into a structured document format. This integration is crucial for maintaining the integrity of citations throughout the document processing workflow.\n\n**Note**: It is important to ensure that the input text is properly formatted with `<citation>` tags for the function to successfully extract URLs. If no valid citations are found, the function will return an empty list.\n\n**Output Example**: A possible return value from the extract_citations function could be:\n```python\n[\"http://example.com/citation1\", \"http://example.com/citation2\"]\n```",
        "**extract_citations**: The function of extract_citations is to extract all unique URLs contained within `<citation>` tags from a given text.\n\n**parameters**: The parameters of this Function are as follows:\n· text: A string representing the input text from which URLs will be extracted.\n\n**Code Description**: The extract_citations function utilizes a regular expression pattern to identify and extract URLs that are enclosed within `<citation>` tags in the provided text. The function begins by defining a regex pattern that matches the content between `<citation>` and `</citation>` tags. It then employs the re.findall method to search through the input text, capturing all matches while ignoring case and allowing for multiline content.\n\nAfter obtaining the matches, the function processes the list of URLs by stripping any leading or trailing whitespace from each URL. It filters out any empty strings that may result from the extraction process. To ensure that the returned list of URLs is unique while preserving the original order, the function converts the list to a dictionary and back to a list. This effectively removes duplicates since dictionary keys must be unique.\n\nThe extract_citations function is called within other functions in the project, specifically in process_section and parse_markdown_to_structure. In process_section, it is used to extract citations from paragraphs of text, allowing the function to associate relevant URLs with their respective paragraphs. Similarly, in parse_markdown_to_structure, it extracts citations from markdown text, ensuring that any URLs present in the document structure are captured and organized accordingly. This integration is crucial for maintaining the integrity of citations throughout the document processing workflow.\n\n**Note**: It is important to ensure that the input text is properly formatted and contains the expected `<citation>` tags for the function to operate effectively. If the tags are missing or incorrectly formatted, the function may return an empty list.\n\n**Output Example**: A possible return value from the extract_citations function could be:\n```python\n[\"http://example.com/citation1\", \"http://example.com/citation2\"]\n```",
        "**extract_citations**: The function of extract_citations is to extract all URLs contained within <citation>...</citation> tags from a given text.\n\n**parameters**: The parameters of this Function are as follows:\n· text: A string containing the text that includes <citation>...</citation> formatted citations.\n\n**Code Description**: The extract_citations function is designed to identify and extract URLs that are enclosed within <citation> tags in the provided text. It utilizes a regular expression pattern to find all occurrences of the <citation>...</citation> format, capturing the content between the tags. The function supports both single URLs and multiple URLs wrapped in a Python list format.\n\nThe function begins by defining a regex pattern that matches the <citation> tags and their content. It then uses the re.findall method to retrieve all matches from the input text, which are stored in the matches list. An empty result list is initialized to hold the extracted URLs.\n\nFor each match found, the function strips any leading or trailing whitespace. If the content is empty, it is skipped. If the content appears to be a list (i.e., it starts with '[' and ends with ']'), the function attempts to parse it using ast.literal_eval, which safely evaluates the string as a Python literal. Each valid URL from the parsed list is added to the result list after ensuring it is a non-empty string. If the parsing fails, the content is treated as a regular string and added to the result list.\n\nIf the content does not represent a list, it is directly appended to the result list. The function ultimately returns a list of extracted citation URLs in the order they appeared in the input text.\n\nThe extract_citations function is called by other functions within the project, such as process_section and parse_markdown_to_structure. In process_section, it is used to extract citations from paragraphs of text, ensuring that any URLs present are captured and associated with their respective paragraphs. In parse_markdown_to_structure, it serves a similar purpose, extracting citations from markdown text as it is parsed into a structured format. This integration is crucial for maintaining the integrity of citations throughout the document processing workflow.\n\n**Note**: It is important to ensure that the input text is properly formatted with <citation> tags for the function to successfully extract URLs. If the input does not contain any <citation> tags, the function will return an empty list.\n\n**Output Example**: A possible return value from the extract_citations function could be:\n```python\n[\"http://example.com\", \"https://foo.com/path\", \"https://bar.com\"]\n```"
      ],
      "code_start_line": 109,
      "code_end_line": 143,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def extract_citations(text: str) -> list:\n    \"\"\"\n    从文本中提取所有 <citation>…</citation> 标签中的 URL，\n    支持单个 URL 或者以 Python 列表形式包裹的多个 URL。\n\n    Args:\n        text: 包含 <citation>...</citation> 格式的文本\n\n    Returns:\n        list[str]: 按出现顺序提取的 citation 内容列表\n    \"\"\"\n    pattern = r'<citation>(.*?)</citation>'\n    matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n\n    result = []\n    for m in matches:\n        content = m.strip()\n        if not content:\n            continue\n        # 如果是列表形式，就解析成 Python list\n        if content.startswith('[') and content.endswith(']'):\n            try:\n                # ast.literal_eval 只会执行字面量解析，比 eval 安全\n                urls = ast.literal_eval(content)\n                # 过滤并添加\n                for u in urls:\n                    if isinstance(u, str) and u.strip():\n                        result.append(u.strip())\n            except (SyntaxError, ValueError):\n                # 解析失败时，退回当作普通字符串处理\n                result.append(content)\n        else:\n            # 单个 URL 直接加入\n            result.append(content)\n    return result\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/create_document_structure/process_section",
        "src/criticsearch/main.py/parse_markdown_to_structure",
        "src/criticsearch/main.py/_model_action_decision",
        "src/criticsearch/main.py/_action_router",
        "src/criticsearch/main.py/process_single_task",
        "tests/test_utils.py",
        "tests/test_utils.py/test_single_citation",
        "tests/test_utils.py/test_multiple_citations_and_whitespace",
        "tests/test_utils.py/test_duplicate_citations_are_preserved",
        "tests/test_utils.py/test_no_citations_returns_empty_list"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_notes",
      "md_content": [
        "**extract_notes**: The function of extract_notes is to extract a list of notes from a given response text formatted in a specific way.\n\n**parameters**: The parameters of this Function.\n· response_text: A string containing the response text that includes notes formatted as <note>...</note> within an <answer>...</answer> structure.\n\n**Code Description**: The extract_notes function is designed to parse a string input, specifically looking for notes encapsulated within <note> tags. It utilizes a regular expression to find all occurrences of the note content in the provided response_text. The function ensures that only well-formed notes are included in the output list. A note is considered valid if it contains both <citation> and </citation> tags, and the counts of these tags must match, indicating that the citation is properly closed. \n\nThe function returns a list of valid notes. If no valid notes are found, it returns an empty list. This function is particularly useful in contexts where responses from a chat or API may contain structured data, and there is a need to extract specific information for further processing or storage.\n\nThe extract_notes function is called within the taking_notes method of the BaseAgent class. In this context, it processes the results obtained from a search operation. The results are passed to extract_notes to retrieve any notes that can be recorded. If valid notes are extracted, they are converted into a set to ensure uniqueness before being added to the agent's memo. This integration highlights the utility of extract_notes in managing and organizing information derived from external sources.\n\n**Note**: It is important to ensure that the response_text is formatted correctly according to the expected structure. Any deviations from the expected format may result in an empty list being returned, as the function is strict about the validity of the notes.\n\n**Output Example**: A possible appearance of the code's return value could be:\n[\n    \"First note content with <citation>http://example1.com</citation>\",\n    \"Second note content with <citation>http://example2.com</citation>\"\n]"
      ],
      "code_start_line": 145,
      "code_end_line": 175,
      "params": [
        "response_text"
      ],
      "have_return": true,
      "code_content": "def extract_notes(response_text: str) -> list:\n    \"\"\"\n    从响应文本中提取notes列表\n    \n    Args:\n        response_text: 包含<answer>[<note>...</note>]</answer>格式的响应文本\n        \n    Returns:\n        list: 提取的notes列表,如果未找到则返回空列表\n        要求返回的格式是：[\n            \"First note content with <citation>http://example1.com</citation>\",\n            \"Second note content with <citation>http://example2.com</citation>\"\n        ]\n    \"\"\"\n    # 只匹配格式完整的note内容\n    note_pattern = r'<note>(.*?)</note>'\n    matches = re.findall(note_pattern, response_text, re.DOTALL | re.IGNORECASE)\n    valid_notes = []\n    \n    for note in matches:\n        note = note.strip()\n        # 验证笔记格式的完整性\n        if (\n            note \n            and \"<citation>\" in note \n            and \"</citation>\" in note\n            and note.count(\"<citation>\") == note.count(\"</citation>\")\n        ):\n            valid_notes.append(note)\n            \n    return valid_notes\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/taking_notes",
        "tests/test_taking_notes.py",
        "tests/test_taking_notes.py/test_extract_notes",
        "tests/test_taking_notes.py/test_empty_notes",
        "tests/test_taking_notes.py/test_malformed_notes"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_actions",
      "md_content": [
        "**extract_actions**: The function of extract_actions is to extract all actions from a given text and ensure they are unique.\n\n**parameters**: The parameters of this Function.\n· text: A string containing text formatted with <action>...</action> tags.\n\n**Code Description**: The extract_actions function is designed to parse a string input, searching for specific patterns that denote actions within the text. It utilizes a regular expression to identify all occurrences of text enclosed within <action> and </action> tags. The function compiles these matches into a set, which inherently removes any duplicate entries, ensuring that the returned collection of actions is unique.\n\nThe function begins by initializing an empty set named 'actions'. It then defines a regular expression pattern that matches any content between the specified action tags. The re.findall method is employed to search through the provided text, with the flags re.DOTALL and re.IGNORECASE allowing for multiline matches and case-insensitive searching, respectively. If any matches are found, they are stripped of leading and trailing whitespace and added to the 'actions' set. Finally, the function returns this set of unique actions. If no actions are found, an empty set is returned.\n\nThe extract_actions function is called within the _model_action_decision function located in the src/criticsearch/main.py file. In this context, it processes the decision made by an agent, which is derived from a chat interaction. After obtaining the decision, the extract_actions function is invoked to retrieve any actions specified in the agent's response. The presence of actions is crucial for the subsequent logic in _model_action_decision, as it determines the flow of the program based on the actions identified (e.g., SEARCH, BROWSE, START_WRITING). If no actions are found, an exception is raised, indicating an invalid decision.\n\n**Note**: It is important to ensure that the input text is correctly formatted with the <action> tags for the function to work effectively. The function is case-insensitive, which allows for flexibility in the input format.\n\n**Output Example**: An example of the output when the input text is \"<action>SEARCH</action><action>BROWSE</action>\" would be a set containing: {'SEARCH', 'BROWSE'}. If the input text does not contain any action tags, the output would be an empty set: set()."
      ],
      "code_start_line": 177,
      "code_end_line": 192,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def extract_actions(text: str) -> set:\n    \"\"\"\n    从文本中提取所有actions并去重\n    \n    Args:\n        text: 包含<action>...</action>格式的文本\n        \n    Returns:\n        set: 提取的action集合,如果未找到则返回空集合\n    \"\"\"\n    actions = set()\n    pattern = r'<action>(.*?)</action>'\n    matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n    if matches:\n        actions.update(match.strip() for match in matches)\n    return actions\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py",
        "src/criticsearch/abstract_substitution/abs_workflow.py",
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/_model_action_decision"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_tag_content",
      "md_content": [
        "## Function Documentation: `extract_tag_content`\n\n### Overview\nThe function `extract_tag_content` is designed to extract the content enclosed within a specified HTML-like tag from a given text. It supports case-insensitive searches and handles multiline text efficiently.\n\n### Function Signature\n```python\ndef extract_tag_content(text: str, tag: str) -> str:\n```\n\n### Parameters\n- **`text`** (`str`): The input text that contains the tag. This text can include any HTML-like elements or other content.\n- **`tag`** (`str`): The name of the tag whose content is to be extracted. For example, if the tag is `<question>`, the value of `tag` would be `\"question\"`.\n\n### Returns\n- **`str`**: The content inside the specified tag. If the tag is not found, an empty string is returned. The extracted content will be stripped of leading and trailing whitespace.\n\n### Functionality\nThe function constructs a regular expression pattern based on the provided `tag` and attempts to find the content within the tag in the `text`. The regular expression is case-insensitive and supports multiline content within the tag.\n\n### Example Usage\n```python\ntext = \"<question>What is the capital of France?</question>\"\ntag = \"question\"\nresult = extract_tag_content(text, tag)\nprint(result)  # Output: \"What is the capital of France?\"\n```\n\n### Error Handling\nIf the specified tag is not present in the `text`, the function returns an empty string.\n\n### Notes\n- The function handles both uppercase and lowercase tag names due to the use of `re.IGNORECASE`.\n- It is optimized to handle multiline content by using the `re.DOTALL` flag.",
        "**extract_tag_content**: The function of extract_tag_content is to extract content enclosed within specified XML-like tags from a given text.\n\n**parameters**: The parameters of this Function.\n· text: A string containing the text that includes the specified tags.\n· tag: A string representing the name of the tag to be extracted (e.g., \"question\", \"answer\").\n\n**Code Description**: The extract_tag_content function is designed to facilitate the extraction of content from text that is formatted with specific tags. It utilizes a regular expression pattern to identify and capture the content that lies between the opening and closing tags of the specified type. The function takes two arguments: 'text', which is the input string containing the tagged content, and 'tag', which specifies the particular tag to search for.\n\nThe function constructs a regular expression pattern dynamically using the provided tag name, allowing it to match both the opening and closing tags in a case-insensitive manner. The re.search method is employed to find the first occurrence of the specified tag in the text. If a match is found, the content between the tags is returned after stripping any leading or trailing whitespace. If no match is found, the function returns an empty string.\n\nThis function is called by several other functions within the project, such as method_choice, query_update, and gpt_search_query_update. In these contexts, extract_tag_content plays a crucial role in processing the output from a conversational model or other sources that return structured data in a tagged format. For instance, in the method_choice function, it extracts the suggested method and associated queries from the model's response, ensuring that the workflow can adapt based on real-time suggestions. Similarly, in query_update and gpt_search_query_update, it retrieves updated questions and evidence from the model's output, which are essential for refining the question-answer pairs being processed.\n\n**Note**: It is important to ensure that the input text is well-formed and contains the specified tags; otherwise, the function may return an empty string, indicating that no content was found for the given tag.\n\n**Output Example**: A possible return value from the extract_tag_content function could be:\n```\n\"How does the process work?\"\n```"
      ],
      "code_start_line": 194,
      "code_end_line": 207,
      "params": [
        "text",
        "tag"
      ],
      "have_return": true,
      "code_content": "def extract_tag_content(text: str, tag: str) -> str:\n    \"\"\"\n    通用的 <tag>…</tag> 提取函数\n    \n    Args:\n        text: 包含指定标签的文本\n        tag: 标签名称 (如 \"question\", \"answer\")\n        \n    Returns:\n        str: 提取的标签内容，如果未找到则返回空字符串\n    \"\"\"\n    pattern = rf'<{tag}>(.*?)</{tag}>'\n    m = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n    return m.group(1).strip() if m else \"\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/method_choice",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/method_choice",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_query_update",
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/parse_tagged_data_to_table",
        "src/criticsearch/workflow.py",
        "src/criticsearch/workflow.py/WorkflowExecutor/step",
        "src/criticsearch/workflow.py/iterate_traj"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_and_validate_json",
      "md_content": [
        "**extract_and_validate_json**: The function of extract_and_validate_json is to extract JSON content from a model response, whether it's wrapped in ```json``` fences or is just raw JSON text, and return the parsed object or None on failure.\n\n**parameters**: The parameters of this Function.\n· model_response: str - The string input representing the model's response that may contain JSON data.\n\n**Code Description**: The extract_and_validate_json function is designed to handle the extraction and validation of JSON data from a given string input, which represents a model's response. The function begins by attempting to identify and remove any Markdown-style fences that may be present in the input string. Specifically, it uses a regular expression pattern to search for content enclosed within ```json``` or ``` any-language ``` fences. If such fences are found, the content within them is extracted; if not, the entire input string is treated as the payload.\n\nOnce the payload is isolated, the function attempts to parse it as JSON using the json.loads method. If this parsing is successful, the resulting Python object is returned. However, if a json.JSONDecodeError occurs, indicating that the content is not valid JSON, the function proceeds to clean the payload by removing any stray backticks and attempts to parse it again. If this second attempt also fails, the function logs an error message using the print_exception method from the RichPrinter class, providing details about the failure and the original model content. Ultimately, if both parsing attempts fail, the function returns None.\n\nThe extract_and_validate_json function is called in various contexts throughout the project. For instance, it is utilized in the query_update and generate_seed methods of the ReverseUpgradeWorkflow class, where it processes responses from the agent's chat_with_template method. In these instances, the function is crucial for ensuring that the responses are valid JSON before further processing, such as extracting updated questions or evidence.\n\nAdditionally, it is referenced in the gpt_search_generate_seed and gpt_search_query_update methods, where it validates the JSON structure of responses related to seed generation and query updates, respectively. The consistent use of extract_and_validate_json across these methods highlights its importance in maintaining the integrity of data handling within the application.\n\n**Note**: It is essential to ensure that the input string (model_response) is well-formed and contains valid JSON data or is formatted correctly to avoid parsing errors. Proper usage of this function aids in effective error handling and data validation practices within the application.\n\n**Output Example**: A possible return value of the function could be a dictionary representing the parsed JSON, such as:\n{\n    \"updated_question\": \"What is the capital of France?\",\n    \"updated_evidence\": [\"Paris is the capital of France.\"]\n} \nOr it could return None if the input was invalid or could not be parsed.",
        "**extract_and_validate_json**: The function of extract_and_validate_json is to extract JSON content from a model response, whether it's wrapped in ```json``` fences or is just raw JSON text, and return the parsed object or None on failure.\n\n**parameters**: The parameters of this Function.\n· model_response: str - A string representing the model's response, which may contain JSON data that needs to be extracted and validated.\n\n**Code Description**: The extract_and_validate_json function is designed to handle the extraction and validation of JSON data from a given model response. The function begins by attempting to identify and remove any Markdown-style fences that may wrap the JSON content. This is achieved using a regular expression pattern that matches the fences and captures the content within them. If the response does not contain such fences, the entire response is treated as the payload.\n\nOnce the potential JSON content is isolated, the function attempts to parse it using the json.loads method. If this parsing is successful, the parsed JSON object is returned. However, if a json.JSONDecodeError occurs, indicating that the content is not valid JSON, the function proceeds to clean the payload by removing any stray backticks and attempts to parse it again.\n\nIf the second parsing attempt fails, the function logs an error message using the print_exception method from the RichPrinter class, which provides detailed feedback about the error encountered, including the original model response. In this case, the function returns None, indicating that the extraction and validation process was unsuccessful.\n\nThe extract_and_validate_json function is called within various parts of the project, notably in the query_update and generate_seed functions of the ReverseUpgradeWorkflow class. In these contexts, it plays a crucial role in ensuring that the responses from the conversational agent are properly formatted as JSON before further processing. For instance, in the query_update function, it is used to extract updated question and evidence data from the model's response, which is essential for refining the QAItem being updated. Similarly, in the generate_seed function, it validates the JSON response that contains the seed question and answer, ensuring that the workflow can proceed with valid data.\n\n**Note**: It is important to ensure that the model_response parameter is well-formed and contains valid JSON data. The function's reliance on the successful execution of the json.loads method means that any issues with the input format can lead to failures in the extraction process.\n\n**Output Example**: A possible return value from the extract_and_validate_json function could be a parsed JSON object structured as follows:\n```json\n{\n    \"updated_question\": \"What is the capital of France?\",\n    \"updated_evidence\": [\"Paris is the capital of France.\", \"It is located in northern central France.\"]\n}\n```"
      ],
      "code_start_line": 209,
      "code_end_line": 230,
      "params": [
        "model_response"
      ],
      "have_return": true,
      "code_content": "def extract_and_validate_json(model_response: str):\n    \"\"\"\n    Extract JSON content from a model response, whether it's wrapped in ```json``` fences\n    or is just raw JSON text. Return the parsed object or None on failure.\n    \"\"\"\n    # 1. Try to strip out any ```json``` fences (or ``` any-language ```)\n    fence_pattern = r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\"\n    m = re.search(fence_pattern, model_response, re.DOTALL | re.IGNORECASE)\n    payload = m.group(1) if m else model_response\n    payload = payload.strip()\n\n    # 2. Attempt to parse as JSON\n    try:\n        return json.loads(payload)\n    except json.JSONDecodeError:\n        # 3. Fallback: remove any stray backticks and retry\n        cleaned = payload.replace(\"```\", \"\").strip()\n        try:\n            return json.loads(cleaned)\n        except json.JSONDecodeError as exc:\n            printer.print_exception(f\"Invalid JSON content after cleanup: {exc}\\n\\n Original model content: {model_response}\")\n            return None\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_fuzzy_replacement",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_entity_extraction",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_combination_workflow",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_combination_workflow/process_entity",
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/safe_chat_and_parse"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print_exception"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "src/criticsearch/llm_service.py": [
    {
      "type": "ClassDef",
      "name": "ModelManager",
      "md_content": [
        "**ModelManager**: The function of ModelManager is to manage model configurations and clients, enabling the creation of OpenAI API clients and retrieval of model-specific configurations.\n\n**attributes**:\n- config: Stores the configuration object containing model configurations and other settings.\n- clients: A dictionary that holds initialized client objects indexed by model names.\n\n**Code Description**:  \nThe `ModelManager` class is designed to handle the management of model configurations and the creation of API clients that interact with OpenAI’s services.\n\n- **`__init__(self, config)`**:  \n  The constructor initializes the `ModelManager` with the given `config` parameter, which contains the settings for the models, such as API keys, URLs, and other relevant configurations. It also initializes an empty `clients` dictionary that will store the client instances corresponding to each model.\n\n- **`get_model_config(self, model_name=None)`**:  \n  This method retrieves the configuration for a specific model. If no model name is provided, it defaults to the first model in the configuration. The function raises an error if no models are found or if the specified model name is not available in the configuration. It returns the configuration dictionary for the selected model.\n\n- **`create_client(self, model_name=None)`**:  \n  This method is responsible for creating and returning a client object. It checks if the client for the specified model already exists in the `clients` dictionary. If not, it retrieves the configuration for the model using `get_model_config`, and initializes an OpenAI client (using the `OpenAI` class, which is not shown in the provided code but is assumed to be a predefined class or function) with the relevant settings like the API key, base URL, timeout, and retry parameters. Once created, the client is stored in the `clients` dictionary for reuse.\n\nFrom a functional perspective, the `ModelManager` class is used by the `call_llm` function in the project. The `call_llm` function interacts with `ModelManager` to:\n1. Instantiate a `ModelManager` object using the provided configuration.\n2. Create a client for the specified model (or default to the first model if no model name is provided).\n3. Retrieve the configuration for the selected model.\n4. Use the configuration and client to make API calls to OpenAI’s services and process the responses.\n\nThe `ModelManager` acts as a central utility that abstracts away the complexity of managing multiple models and their configurations. By using this class, the system can easily handle different model configurations and create clients for various models dynamically.\n\n**Note**:\n- The `config` passed into the `ModelManager` should contain a \"models\" key that maps model names to their respective configurations.\n- If a model is not found in the configuration, a `ValueError` will be raised, so proper error handling should be considered when using this class.\n- The OpenAI client is created with a default timeout of 120 seconds and the possibility to customize retry settings, which are important for dealing with API reliability.\n\n**Output Example**:  \nWhen `create_client` is called with a model name like \"gpt-4\", the output will be an instance of an OpenAI client initialized with that model’s configuration, assuming that the configuration contains the appropriate API key and endpoint. The exact appearance of this client is dependent on the implementation of the OpenAI client, but it will allow making requests to OpenAI's API to process prompts and retrieve responses."
      ],
      "code_start_line": 11,
      "code_end_line": 48,
      "params": [],
      "have_return": true,
      "code_content": "class ModelManager:\n    def __init__(self, config):\n        self.config = config\n        self.clients = {}\n\n    def get_model_config(self, model_name=None):\n        models = self.config.get(\"models\", {})\n        if not models:\n            raise ValueError(\"No models found in configuration.\")\n\n        if model_name is None:\n            model_name = next(iter(models.keys()))\n\n        if model_name not in models:\n            raise ValueError(\n                f\"Model '{model_name}' not found in configuration. Available models: {list(models.keys())}\"\n            )\n\n        model_config = models.get(model_name, {})\n        return model_config\n\n    def create_client(self, model_name=None):\n        if model_name is None:\n            model_name = next(iter(self.config.models.keys()))\n\n        if model_name in self.clients:\n            return self.clients[model_name]\n\n        model_config = self.get_model_config(model_name)\n        client = OpenAI(\n            api_key=model_config.get(\"api_key\"),\n            base_url=model_config.get(\"base_url\", \"https://api.openai.com/v1\"),\n            timeout=self.config.get(\"timeout\", 120),\n            max_retries=self.config.get(\"max_retries\"),\n        )\n\n        self.clients[model_name] = client\n        return client\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/llm_service.py/call_llm"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the class with a configuration and an empty client dictionary.\n\n**parameters**: The parameters of this Function.\n· config: A configuration object or data used to initialize the instance.\n\n**Code Description**: \nThe `__init__` function is the constructor method of the class, used to initialize an object of the class when it is instantiated. \n- The `config` parameter is passed into the constructor and assigned to the instance variable `self.config`. This suggests that the configuration data will be available throughout the lifetime of the object for further use. \n- The `clients` instance variable is initialized as an empty dictionary (`{}`). This implies that the object will eventually store client-related data or mappings in this dictionary during its lifecycle.\n\n**Note**: \n- The `config` parameter should be provided when creating an instance of the class; its structure and contents will depend on the requirements of the class and the overall application.\n- The `clients` dictionary is initialized as empty, and its population will likely occur later in the class methods."
      ],
      "code_start_line": 12,
      "code_end_line": 14,
      "params": [
        "self",
        "config"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, config):\n        self.config = config\n        self.clients = {}\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_model_config",
      "md_content": [
        "**get_model_config**: The function of get_model_config is to retrieve the configuration settings for a specific model from the configuration file.\n\n**parameters**:\n· model_name: A string representing the name of the model. If not provided, the first model in the configuration is selected.\n\n**Code Description**:  \nThe `get_model_config` function is responsible for fetching the configuration for a specified model from the configuration dictionary stored in the `self.config` attribute. \n\n1. The function starts by retrieving the \"models\" section of the configuration using `self.config.get(\"models\", {})`. If no models are found, a `ValueError` is raised with the message \"No models found in configuration.\"\n   \n2. If the `model_name` parameter is not provided, the function selects the first available model by using `next(iter(models.keys()))`.\n\n3. The function checks if the `model_name` exists in the models configuration. If it is not found, a `ValueError` is raised indicating the model is not in the configuration, and it provides a list of available models.\n\n4. If the model is found, the function retrieves its configuration from the `models` dictionary and returns it.\n\nThe function is used by other components in the project to ensure that the correct configuration for a model is retrieved. For example, in the `create_client` method, it is used to get the configuration of a specific model when creating an OpenAI client. It is also utilized in the `call_llm` function, where it is used to get the model configuration to set up parameters for making API calls.\n\n**Note**:  \n- The function raises errors if the models section is missing or if the specified model is not found in the configuration.\n- The function defaults to the first model in the configuration if no model name is provided.\n\n**Output Example**:  \nIf the configuration for the model \"gpt-3\" contains the following settings:\n```json\n{\n  \"models\": {\n    \"gpt-3\": {\n      \"api_key\": \"your-api-key\",\n      \"base_url\": \"https://api.openai.com/v1\",\n      \"temperature\": 0.7,\n      \"max_tokens\": 150\n    }\n  }\n}\n```\nCalling `get_model_config(\"gpt-3\")` would return:\n```python\n{\n  \"api_key\": \"your-api-key\",\n  \"base_url\": \"https://api.openai.com/v1\",\n  \"temperature\": 0.7,\n  \"max_tokens\": 150\n}\n```"
      ],
      "code_start_line": 16,
      "code_end_line": 30,
      "params": [
        "self",
        "model_name"
      ],
      "have_return": true,
      "code_content": "    def get_model_config(self, model_name=None):\n        models = self.config.get(\"models\", {})\n        if not models:\n            raise ValueError(\"No models found in configuration.\")\n\n        if model_name is None:\n            model_name = next(iter(models.keys()))\n\n        if model_name not in models:\n            raise ValueError(\n                f\"Model '{model_name}' not found in configuration. Available models: {list(models.keys())}\"\n            )\n\n        model_config = models.get(model_name, {})\n        return model_config\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/llm_service.py/ModelManager/create_client",
        "src/criticsearch/llm_service.py/call_llm"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "create_client",
      "md_content": [
        "**create_client**: The function of create_client is to create and return an OpenAI client instance for a specified model.\n\n**parameters**: The parameters of this Function.\n· model_name: A string representing the name of the model for which the client is to be created. If not provided, the first model in the configuration is selected.\n\n**Code Description**: The `create_client` function is responsible for instantiating and returning an OpenAI client based on the specified model name. The function first checks if a model name has been provided; if not, it defaults to the first model available in the configuration. \n\nThe function then checks if a client for the specified model already exists in the `self.clients` dictionary. If it does, the existing client is returned, avoiding the overhead of creating a new instance. \n\nIf the client does not exist, the function retrieves the model's configuration using the `get_model_config` method. This method fetches the necessary API key, base URL, timeout, and maximum retries for the model from the configuration settings. \n\nAn instance of the OpenAI client is then created using the retrieved configuration parameters. This instance is stored in the `self.clients` dictionary for future use and is returned to the caller.\n\nThe `create_client` function is called by the `call_llm` function, which is responsible for making API calls to the OpenAI service. In `call_llm`, the `create_client` function is invoked to obtain the client needed to interact with the OpenAI API, ensuring that the correct configuration is used for the specified model. This establishes a direct relationship where `call_llm` relies on `create_client` to provide the necessary client instance for executing its operations.\n\n**Note**: It is important to ensure that the model name provided is valid and exists in the configuration. If the model name is not found, the `get_model_config` method will raise an error, which will propagate back to the caller, potentially affecting the execution of the `call_llm` function.\n\n**Output Example**: If the model name \"gpt-3\" is specified and the configuration is correctly set, the function would return an instance of the OpenAI client configured for \"gpt-3\". The returned client would be capable of making API calls to the OpenAI service with the specified parameters."
      ],
      "code_start_line": 32,
      "code_end_line": 48,
      "params": [
        "self",
        "model_name"
      ],
      "have_return": true,
      "code_content": "    def create_client(self, model_name=None):\n        if model_name is None:\n            model_name = next(iter(self.config.models.keys()))\n\n        if model_name in self.clients:\n            return self.clients[model_name]\n\n        model_config = self.get_model_config(model_name)\n        client = OpenAI(\n            api_key=model_config.get(\"api_key\"),\n            base_url=model_config.get(\"base_url\", \"https://api.openai.com/v1\"),\n            timeout=self.config.get(\"timeout\", 120),\n            max_retries=self.config.get(\"max_retries\"),\n        )\n\n        self.clients[model_name] = client\n        return client\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/llm_service.py/call_llm"
      ],
      "reference_who": [
        "src/criticsearch/llm_service.py/ModelManager/get_model_config"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "call_llm",
      "md_content": [
        "## `call_llm` Function Documentation\n\n### Purpose:\nThe `call_llm` function interacts with a language model API to generate responses based on a user-provided prompt. It handles communication with the OpenAI API or a similar service, utilizing a model configuration and client provided through the `ModelManager`.\n\n### Parameters:\n- **model** (`str`):  \n  The name of the model to be used for generating the response (e.g., `\"gpt-4\"`). It is passed to the `ModelManager` to create the appropriate client.\n  \n- **usr_prompt** (`str | Iterable[ChatCompletionMessageParam]`):  \n  The user prompt to send to the model. It can either be a single string message or an iterable of `ChatCompletionMessageParam` objects. The prompt is the basis of the model's response.\n\n- **config** (`dict`):  \n  A configuration dictionary used to initialize the `ModelManager`. It includes settings related to model configurations such as API keys, timeout settings, and retry parameters.\n\n- **tools** (`List | None`, optional):  \n  A list of tools that can be used by the model during the interaction. This is an optional parameter. If not provided, the function defaults to `None`.\n\n### Returns:\n- **ChatCompletionMessage**:  \n  The generated response message from the model. This object contains the model's output based on the provided prompt.\n\n### Functionality:\n1. **Initialize ModelManager**:  \n   A `ModelManager` instance is created using the provided configuration (`config`). This instance is responsible for managing model-specific settings and clients.\n\n2. **Create Model Client**:  \n   The `create_client` method of `ModelManager` is used to create a client for the specified model. If a client for the model does not already exist, it is initialized and stored.\n\n3. **Configure Prompt**:  \n   The `usr_prompt` is processed. If it is a string, it is converted into a message format. If it is already an iterable of `ChatCompletionMessageParam`, it is used as-is.\n\n4. **API Request**:  \n   The function sends the prompt to the OpenAI API (or a compatible service) via the client, using settings such as `temperature` and `max_tokens` from the model's configuration. If tools are provided, they are passed along with the request.\n\n5. **Handle Response**:  \n   The function extracts the response message from the API's result. If the model supports tools, it handles the response accordingly. If not, it reattempts the request without tools.\n\n6. **Error Handling**:  \n   The function raises specific exceptions in case of errors, including:\n   - `APIConnectionError`: If there is an issue connecting to the API.\n   - `ValueError`: If there is an issue with the configuration or model settings.\n   - `BadRequestError`: If the model does not support tools (in which case, a retry without tools is performed).\n\n### Example Usage:\n```python\nresponse = call_llm(\n    model=\"gpt-4\",\n    usr_prompt=\"What is the capital of France?\",\n    config=config_dict\n)\n```\n\n### Notes:\n- The `model` parameter should correspond to a valid model name as specified in the configuration.\n- The `config` dictionary must include all necessary settings for model initialization, including API keys and any model-specific options.\n- The `tools` parameter is optional, and should only be used if supported by the model. If the model does not support tools, the function will handle this by retrying without them.\n- Proper error handling should be implemented to manage exceptions such as connection failures or invalid configurations.",
        "# Documentation for `call_llm` Function\n\n## Overview\n\nThe `call_llm` function is responsible for interacting with a large language model (LLM) to generate chat completions based on a given user prompt and associated configuration. It leverages a model manager to initialize and configure the client for communication with the model, handling the conversation flow and settings efficiently.\n\n## Parameters\n\n- **model** (`str`): The identifier for the model to be used for generating completions. This specifies the particular LLM that will be interacted with.\n\n- **config** (`dict`): A dictionary containing the configuration settings required for setting up the model manager and client. It typically includes parameters like API keys, model settings, and other configuration options necessary for initializing the model.\n\n- **usr_prompt** (`str | Iterable[ChatCompletionMessageParam]`, optional): The prompt or series of messages to be sent to the model. This can be:\n  - A string representing a user query or prompt (default: `\"Hello, how can I help you?\"`).\n  - An iterable of `ChatCompletionMessageParam` objects, if a more structured set of messages is provided. If not provided, the default prompt is used.\n\n- **tools** (`List`, optional): A list of tools or external services that may be used during the chat session. This parameter is optional and can be set to `None` if no tools are needed.\n\n- **messages** (`Iterable[ChatCompletionMessageParam]`, optional): An optional iterable of pre-existing messages. If this is `None`, the function will construct the messages from the `usr_prompt`.\n\n## Return Type\n\n- **ChatCompletionMessage**: The function returns a `ChatCompletionMessage` object, which contains the response generated by the LLM. This includes the model's completion based on the provided input messages and settings.\n\n## Functionality\n\n1. **Model Manager Initialization**: The function starts by creating a `ModelManager` instance using the provided configuration (`config`). This object is responsible for managing the model and its interactions.\n\n2. **Client Creation**: The function then creates a client using the `model_manager.create_client(model)` method, which sets up the client for the specific model passed as the `model` parameter.\n\n3. **Model Configuration**: The configuration for the model is retrieved through the `model_manager.get_model_config(model)` method. This configuration includes settings like temperature, which influence the behavior of the model during completion generation.\n\n4. **Message Setup**: If the `messages` parameter is not provided, the function checks the type of `usr_prompt`. If it's a string, it constructs a message list with the prompt as a `ChatCompletionUserMessageParam` with the role \"user\". If `usr_prompt` is already an iterable of messages, it uses them directly.\n\n5. **Chat Completion Request**: The function then makes a request to the model through the `client.chat.completions.create` method, passing the model, the constructed messages, and other relevant settings, such as the temperature.\n\n6. **Error Handling**: The function includes a `try-except` block, ensuring that any errors during the communication with the model are handled appropriately.\n\n## Usage Example\n\n```python\nconfig = {\n    \"api_key\": \"your-api-key\",\n    \"temperature\": 0.7,\n    # other configuration settings\n}\n\nresponse = call_llm(\n    model=\"gpt-3.5-turbo\",\n    config=config,\n    usr_prompt=\"What is the weather like today?\",\n    tools=None,\n    messages=None\n)\n\nprint(response)\n```\n\nIn this example, the `call_llm` function interacts with the GPT-3.5 model, generating a completion based on the prompt `\"What is the weather like today?\"`.\n\n## Notes\n\n- The function assumes that the configuration provided contains the necessary settings for the model to function correctly.\n- The `usr_prompt` can either be a simple string or a structured set of messages, giving flexibility in how user interactions are provided.\n- The `tools` parameter allows for additional functionality to be included in the session but is not directly utilized in the current implementation of the function."
      ],
      "code_start_line": 51,
      "code_end_line": 96,
      "params": [
        "model",
        "config",
        "usr_prompt",
        "tools",
        "messages"
      ],
      "have_return": true,
      "code_content": "def call_llm(\n    model,\n    config,\n    usr_prompt: str | Iterable[ChatCompletionMessageParam] = \"Hello, how can I help you?\",\n    tools: List | None = None,\n    messages = None\n) -> ChatCompletionMessage:\n    model_manager = ModelManager(config)\n    client = model_manager.create_client(model)\n\n    # 从 ModelManager 获取配置\n    model_config = model_manager.get_model_config(model)\n\n    if messages is None:\n        if isinstance(usr_prompt, str):\n            messages = [ChatCompletionUserMessageParam(content=usr_prompt, role=\"user\")]\n        else:\n            messages = usr_prompt\n\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            temperature=model_config.get(\"temperature\", None),\n            max_tokens=model_config.get(\"max_tokens\", None),\n            tools=tools,  # type: ignore\n        )\n\n        response_message = response.choices[0].message\n        return response_message\n\n    except APIConnectionError as e:\n        raise RuntimeError(f\"Failed to connect to OpenAI API: {e}\")\n    except ValueError as e:\n        raise ValueError(f\"Error in configuration or model: {e}\")\n    except BadRequestError:\n        # Some model like gemini-2.0-flash-thinking-exp may not support tools and raise BadRequestError\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            temperature=model_config.get(\"temperature\", None),\n            max_tokens=model_config.get(\"max_tokens\", None),\n        )\n\n        response_message = response.choices[0].message\n        return response_message\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/reportbench/process_search_result.py",
        "src/criticsearch/reportbench/test_llm.py",
        "src/criticsearch/reportbench/test_llm.py/test_llm_call",
        "tests/test_chat_functionality.py",
        "tests/test_function_call_between_models.py",
        "tests/test_function_call_between_models.py/test_function_call",
        "tests/test_multiple_same_role.py"
      ],
      "reference_who": [
        "src/criticsearch/llm_service.py/ModelManager",
        "src/criticsearch/llm_service.py/ModelManager/get_model_config",
        "src/criticsearch/llm_service.py/ModelManager/create_client"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/main_old_version.py": [
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: The function of main is to orchestrate the conversation process between a user and an intelligent agent, managing iterations of task handling, confidence evaluation, information retrieval, and response refinement.\n\n**parameters**: The parameters of this Function.\n· TASK: A string representing the task or question posed by the user that the agent needs to address.\n· MAX_ITERATION: An integer specifying the maximum number of iterations the agent should perform while refining its response.\n\n**Code Description**: The main function serves as the entry point for processing user tasks within the intelligent agent framework. It begins by initializing a BaseAgent instance, which is responsible for managing the conversation and handling user queries. The user question is assigned to the agent's `user_question` attribute, and the logging level is set based on the configuration.\n\nThe function logs the start of the conversation and appends the initial user question to the conversation history. It then enters a loop that iterates up to the specified `MAX_ITERATION`. In each iteration, the function performs several critical tasks:\n\n1. **Confidence Evaluation**: In the first iteration, the agent checks its confidence in answering the user's question by invoking the `model_confident` method. This method assesses whether the agent is confident enough to provide a direct answer or if it needs to gather more information.\n\n2. **Information Retrieval**: If the agent is not confident, it constructs a search prompt using a template and performs a search operation through the `search_and_browse` method. This method interacts with a search aggregator to retrieve relevant information from the web, which is then used to formulate a more informed response.\n\n3. **Response Generation**: The agent generates a response based on the gathered information or directly if it is confident. This is done through the `common_chat` method, which communicates with the language model to produce a response.\n\n4. **Critique and Refinement**: After generating a response, the function instantiates a CriticAgent, which evaluates the agent's answer. The critique is processed, and if the feedback indicates that the conversation should stop (as determined by the YAML response), the function logs the total iterations and the final answer before returning it.\n\n5. **Iterative Improvement**: If the critique suggests further refinement, the agent constructs a new search prompt based on the feedback and previous answers, repeating the search and response generation process until the maximum iterations are reached or a stopping condition is met.\n\nThroughout this process, the function utilizes various methods from the BaseAgent class, including `load_template`, `render_template`, and `update_answer`, to manage the conversation flow and enhance the quality of responses based on user feedback and search results.\n\n**Note**: It is crucial to ensure that the TASK parameter is well-defined and relevant to the agent's capabilities. The MAX_ITERATION parameter should be set appropriately to balance between thoroughness and efficiency in response generation.\n\n**Output Example**: A possible return value from the main function could be a string such as:\n```\n\"The capital of France is Paris, based on the latest information gathered from reliable sources.\"\n```",
        "**main**: The function of main is to orchestrate a conversation process involving a user-defined task and a specified number of iterations to refine the agent's response.\n\n**parameters**: The parameters of this Function.\n· TASK: A string representing the task or question posed by the user that the agent needs to address.\n· MAX_ITERATION: An integer indicating the maximum number of iterations the conversation will undergo to refine the agent's response.\n\n**Code Description**: The main function serves as the central control point for managing the interaction between a user and an intelligent agent. It begins by initializing an instance of the BaseAgent class, which is responsible for handling the conversation and processing user input. The user question is assigned to the agent's `user_question` attribute, and the logging level is set based on the configuration.\n\nThe function logs the start of the conversation with the provided task and appends the initial user question to the conversation history using the `append_to_history` method from the ConversationManager class. This ensures that all interactions are recorded for future reference.\n\nThe main function then enters a loop that iterates up to the specified `MAX_ITERATION`. During each iteration, it performs the following key operations:\n\n1. It checks the agent's confidence in answering the user's question using the `model_confident` method. If the agent is confident, it directly generates an answer using the `chat` method. If not, it initiates a search for additional information by rendering a search prompt and calling the `search_and_browse` method to gather relevant web content.\n\n2. After obtaining the answer, the function logs the response and evaluates it using an instance of the CriticAgent. The CriticAgent assesses the quality of the agent's answer and provides feedback, which may include suggestions for improvement.\n\n3. If the CriticAgent's response indicates that the conversation should stop (as determined by checking the \"Stop\" field in the YAML response), the function logs the total iterations completed, the search queries made, and the final answer before returning the answer to the user.\n\n4. If the conversation continues, the function constructs a new search prompt based on the CriticAgent's feedback and performs another search to refine the answer further.\n\nThe relationship between the main function and its callees is crucial for the iterative improvement of the agent's responses. The main function relies on the functionalities provided by the BaseAgent and CriticAgent classes to manage conversations, evaluate responses, and gather information from external sources.\n\n**Note**: It is essential to ensure that the TASK parameter is well-defined and relevant to the agent's capabilities. The MAX_ITERATION parameter should be set appropriately to balance the need for thoroughness with the efficiency of the conversation process.\n\n**Output Example**: A possible appearance of the code's return value when executing the main function might look like this:\n```\n\"Based on the latest search results and your feedback, here is the updated answer to your question: ...\"\n```"
      ],
      "code_start_line": 11,
      "code_end_line": 185,
      "params": [
        "TASK",
        "MAX_ITERATION"
      ],
      "have_return": true,
      "code_content": "def main(TASK, MAX_ITERATION):\n    # Initialize agents\n    common_agent = BaseAgent()\n\n    # initialize the task\n    common_agent.user_question = TASK\n\n    set_logger_level_from_config(log_level=settings.log_level.upper())\n\n    logger.success(f\"Starting the conversation with task: {TASK}\")\n\n    BaseAgent.conversation_manager.append_to_history(role=\"user\", content=TASK)\n\n    for iteration in range(MAX_ITERATION):\n        colorize_message(\n            message_title=f\"ITERATION {iteration + 1}\", color=\"cyan\", style=\"bold\"\n        )\n\n        if iteration == 0:\n            # Initialize search_results as None\n            search_results = None\n\n            # Model confidence check - yellow\n            agent_confident = common_agent.model_confident(TASK)\n            agent_confident_yaml = common_agent.extract_and_validate_yaml(\n                agent_confident\n            )\n\n            if agent_confident_yaml is None:\n                logger.warning(\n                    \"Failed to extract valid YAML content. Defaulting to 'false'.\"\n                )\n                agent_confident = False\n            else:\n                agent_confident_dict = yaml.safe_load(agent_confident_yaml)\n                agent_confident = (\n                    agent_confident_dict.get(\"confidence\", \"true\").lower() == \"true\"\n                )\n\n            if agent_confident:\n                # When confident, only get the answer\n                common_agent_answer = common_agent.chat(usr_prompt=TASK)\n            else:\n                # When not confident, start searching information\n                data = {\n                    \"user_question\": TASK,\n                }\n                initial_search_prompt = common_agent.load_template(\n                    \"planner_agent_initial_search_plan.txt\"\n                )\n                initial_search_rendered_prompt = common_agent.render_template(\n                    initial_search_prompt, data\n                )\n                logger.info(\n                    f\"initial_search_rendered_prompt: {initial_search_rendered_prompt}\"\n                )\n\n                initial_web_result_markdown_text = common_agent.search_and_browse(\n                    initial_search_rendered_prompt\n                )\n                logger.info(f\"Initial web result: {initial_web_result_markdown_text}\")\n\n                rag_based_answer_prompt = common_agent.render_template(\n                    common_agent.load_template(\"rag_based_answer.txt\"),\n                    {\n                        \"user_question\": common_agent.user_question,\n                        \"web_result_markdown_text\": initial_web_result_markdown_text,\n                    },\n                )\n\n                common_agent_answer = common_agent.chat(\n                    usr_prompt=rag_based_answer_prompt,\n                )\n\n        else:\n            # 前面根据critc的返回得到了新的网页搜索结果web_result_markdown_text\n            common_agent_answer = common_agent.update_answer(\n                query=TASK,\n                previous_answer=common_agent_answer,\n                search_results=web_result_markdown_text,\n                critic_feedback=critic_agent_response,\n            )\n            time.sleep(0.5)  # hitting rate limits for gpt mini\n\n        colorize_message(\n            message_title=\"COMMON AGENT ANSWER\",\n            color=\"magenta\",\n            message_content=common_agent_answer,\n        )\n\n        # Critic evaluation - blue\n        critic_agent = CriticAgent()\n        critic_agent.receive_task(TASK)\n        critic_agent.receive_agent_answer(common_agent_answer)\n        critic_agent_response = critic_agent.critic()\n\n        colorize_message(\n            message_title=\"CRITIC_AGENT_RESPONSE\",\n            color=\"blue\",\n            message_content=critic_agent_response,\n        )\n\n        if yaml.safe_load(critic_agent_response).get(\"Stop\", {}).lower() == \"true\":\n            colorize_message(\n                message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n            )\n\n            colorize_message(\n                message_title=\"ALL SEARCH QUERIES\",\n                color=\"black\",\n                message_content=\", \".join(map(str, common_agent.queryDB)),\n            )\n            colorize_message(\n                message_title=\"FINAL ANSWER\",\n                color=\"red\",\n                message_content=common_agent_answer,\n            )\n\n            return f\"\\n{common_agent_answer}\\n\"\n\n        # 根据critic的建议再执行一次搜索和爬虫操作\n        # 先构建rendered_prompt\n        reflection_data = {\n            \"user_question\": TASK,\n            \"previous_answer\": common_agent_answer,\n            \"user_feedback\": critic_agent_response,\n            \"search_history\": common_agent.queryDB,\n        }\n        search_again_prompt = common_agent.render_template(\n            common_agent.load_template(\"planner_agent_with_reflection.txt\"),\n            reflection_data,\n        )\n        try:\n            web_result_markdown_text = common_agent.search_and_browse(\n                search_again_prompt\n            )\n        except:\n            colorize_message(\n                message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n            )\n\n            colorize_message(\n                message_title=\"ALL SEARCH QUERIES\",\n                color=\"black\",\n                message_content=\", \".join(map(str, common_agent.queryDB)),\n            )\n\n            colorize_message(\n                message_title=\"FINAL ANSWER\",\n                color=\"red\",\n                message_content=common_agent_answer,\n            )\n\n            # we run out of searches for now, so we force the agent to give a final answer:\n            return f\"\\n{common_agent_answer}\\n\"\n\n        # Check if reached max iterations\n        if iteration == MAX_ITERATION - 1:\n            colorize_message(\n                message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n            )\n\n            colorize_message(\n                message_title=\"ALL SEARCH QUERIES\",\n                color=\"black\",\n                message_content=\", \".join(map(str, common_agent.queryDB)),\n            )\n\n            colorize_message(\n                message_title=\"FINAL ANSWER\",\n                color=\"red\",\n                message_content=common_agent_answer,\n            )\n\n            return f\"\\n{common_agent_answer}\\n\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/base_agent.py/BaseAgent/extract_and_validate_yaml",
        "src/criticsearch/base_agent.py/BaseAgent/receive_task",
        "src/criticsearch/critic_agent.py/CriticAgent",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/critic_agent.py/CriticAgent/receive_agent_answer",
        "src/criticsearch/models.py/ConversationManager/append_to_history"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/config.py": [],
  "src/criticsearch/critic_agent.py": [
    {
      "type": "ClassDef",
      "name": "CriticAgent",
      "md_content": [
        "**CriticAgent**: The function of CriticAgent is to generate critiques based on the responses provided by an intelligent agent.\n\n**attributes**: The attributes of this Class.\n· original_task: A string that holds the original task or question received from the user.  \n· critic_prompt: A string that contains the template for generating critiques, loaded from a specified template file.\n\n**Code Description**: The CriticAgent class extends the BaseAgent class, inheriting its foundational capabilities while specializing in the evaluation of responses generated by other agents. Upon initialization, the CriticAgent sets up its original task as an empty string and loads a critique template from a file named \"critic_agent.txt\". \n\nThe primary method of the CriticAgent is `critic`, which is responsible for generating critiques. This method first gathers the necessary data for critique generation by calling `get_data_for_critic`, which compiles the original task and the agent's answer into a dictionary. The `critic` method then renders the critique prompt using this data and interacts with a common chat interface to obtain a response from the model. \n\nThe response is expected to be in YAML format, which is validated and extracted using the `extract_and_validate_yaml` method. If the response contains invalid YAML, an error message is printed, and the method returns None. \n\nAdditionally, the CriticAgent has a method called `receive_agent_answer`, which allows it to store the answer provided by the agent for later critique. The `get_data_for_critic` method is a helper function that prepares the data needed for the critique by returning a dictionary containing the original task and the agent's answer.\n\nThe CriticAgent is utilized within the main function of the project, where it receives the task and the agent's answer, generates a critique, and evaluates whether to continue the interaction based on the critique's content. This interaction is part of a loop that allows for iterative refinement of the agent's responses based on feedback from the CriticAgent.\n\n**Note**: It is important to ensure that the template file \"critic_agent.txt\" is correctly formatted and accessible, as the CriticAgent relies on this template to generate critiques. Additionally, proper handling of YAML responses is crucial to avoid runtime errors during critique validation.\n\n**Output Example**: A possible appearance of the code's return value when generating a critique might look like this:\n```yaml\nCritique:\n  feedback: \"The answer provided lacks depth and does not address the user's question adequately.\"\n  suggestions:\n    - \"Include more detailed examples.\"\n    - \"Clarify the main points.\"\n  Stop: false\n```",
        "**CriticAgent**: The function of CriticAgent is to generate critiques of responses based on user input and agent answers.\n\n**attributes**: The attributes of this Class.\n· original_task: A string that holds the original task or question received from the user.  \n· critic_prompt: A string that contains the loaded template for generating critiques, sourced from a file named \"critic_agent.txt\".  \n· agent_answer: A string that stores the answer provided by the agent, which will be critiqued.\n\n**Code Description**: The CriticAgent class extends the BaseAgent class, inheriting its functionalities while focusing specifically on the critique generation aspect of the agent's operations. Upon initialization, the CriticAgent sets up its environment by calling the constructor of the BaseAgent class through `super().__init__()`. It initializes the `original_task` attribute to an empty string and loads the critique template from the specified file, which will be used to format the critique responses.\n\nThe main functionality of the CriticAgent is encapsulated in the `critic` method. This method is responsible for generating critiques based on the agent's answer to the original task. It first retrieves the necessary data for critique generation by calling the `get_data_for_critic` method, which compiles the `original_task` and `agent_answer` into a dictionary. The method then renders the critique prompt using the `render_template` method, passing in the data retrieved. The rendered prompt is sent to the language model via the `common_chat` method, which handles the interaction with the model.\n\nThe response from the model is expected to be in YAML format, which is validated and extracted using the `extract_and_validate_yaml` method. If the response contains valid YAML, it is returned; otherwise, an error message is printed, and the method returns None.\n\nThe CriticAgent also includes the `receive_agent_answer` method, which allows it to store the agent's answer for later critique. The `get_data_for_critic` method is a helper function that prepares the data needed for generating critiques by returning a dictionary containing the `original_task` and `agent_answer`.\n\nThe CriticAgent is utilized within the broader context of the project, specifically in the `process_single_task` function and the main execution flow of the application. It receives the original task and the agent's answer, critiques the response, and provides feedback that can be used to refine the agent's output. This interaction is crucial for improving the quality of the responses generated by the agent, as it allows for iterative refinement based on critiques.\n\n**Note**: It is important to ensure that the template file \"critic_agent.txt\" is correctly formatted and accessible, as the CriticAgent relies on this template for generating critiques. Additionally, the agent's answer must be provided before calling the `critic` method to ensure that the critique is based on the most recent response.\n\n**Output Example**: A possible appearance of the code's return value when generating a critique might look like this:\n```yaml\ncritique:\n  feedback: \"The response lacks depth and does not address the user's question adequately.\"\n  suggestions:\n    - \"Provide more detailed explanations.\"\n    - \"Include relevant examples to support the claims.\"\n```",
        "**CriticAgent**: The function of CriticAgent is to generate critiques of responses provided by an intelligent agent, facilitating the refinement of those responses based on feedback.\n\n**attributes**: The attributes of this Class.\n· original_task: A string that holds the original task or question received from the user.  \n· critic_prompt: A string that contains the template for generating critiques, loaded from an external file.\n\n**Code Description**: The CriticAgent class extends the BaseAgent class and is specifically designed to evaluate and critique the responses generated by intelligent agents. Upon initialization, it sets up two key attributes: `original_task`, which is initialized as an empty string, and `critic_prompt`, which loads a template from a file named \"critic_agent.txt\". This template is used to format the critiques generated by the agent.\n\nThe primary method of the CriticAgent is `critic()`, which is responsible for generating critiques. This method first retrieves data necessary for the critique by calling `get_data_for_critic()`, which compiles the `original_task` and the `agent_answer` into a dictionary. It then renders the `critic_prompt` template with this data to create a formatted prompt for the critique. The rendered prompt is sent to a chat method, which interacts with a language model to obtain a response.\n\nThe method also includes error handling for YAML parsing. If the response from the model is not valid YAML, an exception is caught, and an error message is printed, returning `None` to indicate failure in generating a valid critique.\n\nThe `receive_agent_answer(agent_answer)` method allows the CriticAgent to store the answer provided by the intelligent agent for later evaluation. The `get_data_for_critic()` method constructs a dictionary containing the original task and the agent's answer, which is essential for generating contextually relevant critiques.\n\nThe CriticAgent is utilized within the main function of the project, where it receives the user's task and the agent's answer. After the agent generates a response, the CriticAgent evaluates this response and provides feedback, which can lead to further iterations of response refinement. This feedback loop is crucial for improving the quality of the agent's answers based on real-time critiques.\n\n**Note**: It is important to ensure that the `critic_prompt` template is correctly formatted and accessible, as it directly influences the quality of the critiques generated. Proper handling of YAML responses is also essential to avoid runtime errors during critique generation.\n\n**Output Example**: A possible appearance of the code's return value when executing the critique might look like this:\n```yaml\nCritique:\n  feedback: \"The response lacks detail regarding the historical context.\"\n  suggestions:\n    - \"Include specific examples to support the claims.\"\n    - \"Consider addressing potential counterarguments.\"\n  Stop: false\n```"
      ],
      "code_start_line": 7,
      "code_end_line": 34,
      "params": [],
      "have_return": true,
      "code_content": "class CriticAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.original_task = \"\"\n        self.critic_prompt = self.load_template(\"critic_agent.txt\")\n\n    def critic(self):\n        \"\"\"\n        生成评论。\n        \"\"\"\n        data = self.get_data_for_critic()\n\n        rendered_prompt = self.render_template(self.critic_prompt, data)\n        model_response = self.chat(usr_prompt=rendered_prompt, role=\"critic\")\n\n        try:\n            formatted_yaml = self.extract_and_validate_yaml(model_response)\n            return formatted_yaml\n\n        except yaml.YAMLError:\n            printer.print_exception(f\"Invalid YAML content.\")\n            return None\n\n    def receive_agent_answer(self, agent_answer):\n        self.agent_answer = agent_answer\n\n    def get_data_for_critic(self):\n        return {\"user_question\": self.original_task, \"agent_answer\": self.agent_answer}\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py",
        "src/criticsearch/main_old_paralell.py",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the `CriticAgent` class by calling the parent class's constructor and setting up the initial values for the object's attributes.\n\n**parameters**: The __init__ method does not take any additional parameters besides `self`, which refers to the instance of the class being created.\n\n**Code Description**: \nThe `__init__` method is the constructor for the `CriticAgent` class. It first invokes the constructor of its parent class, `BaseAgent`, using the `super().__init__()` call. This ensures that any initialization logic defined in the parent class is executed, allowing the `CriticAgent` class to inherit and initialize any attributes or methods defined in `BaseAgent`.\n\nAfter calling the parent constructor, the method initializes two attributes specific to the `CriticAgent` class:\n1. `self.original_task`: This attribute is set to an empty string. It is likely intended to hold information about the original task associated with the `CriticAgent`, although its exact usage is not defined within this method.\n   \n2. `self.critic_prompt`: This attribute is set by calling the `load_template` method with the argument `\"critic_agent.txt\"`. The `load_template` method, which is inherited from the `BaseAgent` class, loads the content of a template file named \"critic_agent.txt\" from a specified directory (likely defined in the parent class). This template content is then assigned to the `critic_prompt` attribute, which would presumably be used later in the `CriticAgent` for processing tasks related to the critic agent's functionality.\n\nThe `__init__` method ensures that when an instance of `CriticAgent` is created, the necessary setup for its attributes is completed, and the class is ready for further operations.\n\n**Note**: The `load_template` method is responsible for handling the file reading operation. If the file `critic_agent.txt` is missing or cannot be found in the specified prompts directory, it will raise a `FileNotFoundError`, ensuring that users are informed of any missing template files."
      ],
      "code_start_line": 8,
      "code_end_line": 11,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super().__init__()\n        self.original_task = \"\"\n        self.critic_prompt = self.load_template(\"critic_agent.txt\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "critic",
      "md_content": [
        "**critic**: The function of critic is to generate a review based on user input and agent responses.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The critic function is a method within the CriticAgent class that is responsible for generating a review or critique based on the data it retrieves and processes. The function operates as follows:\n\n1. **Data Retrieval**: It begins by calling the `get_data_for_critic` method, which returns a dictionary containing the user's question and the agent's answer. This data is essential for generating a contextually relevant critique.\n\n2. **Template Rendering**: The function then utilizes the `render_template` method to create a prompt for the model. This method takes the `critic_prompt` (a predefined template) and the data retrieved in the previous step, formatting it into a string that can be understood by the model.\n\n3. **Model Interaction**: The rendered prompt is sent to the `common_chat` method, which facilitates a conversation with the model. This method processes the user prompt and returns a response from the model, which is expected to contain the critique or review.\n\n4. **YAML Extraction and Validation**: After receiving the model's response, the function attempts to extract and validate any YAML content using the `extract_and_validate_yaml` method. This method checks for valid YAML formatting and returns the parsed content if successful.\n\n5. **Error Handling**: If the YAML extraction fails (for instance, if the content is not valid YAML), the function catches the `yaml.YAMLError` exception, prints an error message indicating the issue, and returns `None`.\n\nThe critic function is invoked within the `main` function of the project, specifically after the common agent has provided an answer to the user's question. The response from the critic function is then used to evaluate the agent's answer, potentially influencing subsequent iterations of the conversation. This highlights the function's role in providing feedback and improving the overall interaction quality between the user and the agent.\n\n**Note**: It is crucial to ensure that the `critic_prompt` is properly defined and that the data returned by `get_data_for_critic` is valid. If the model response does not contain valid YAML, the function will return `None`, which may affect the flow of the application.\n\n**Output Example**: A possible return value from the critic function could be a YAML formatted string such as:\n\n```yaml\nfeedback: \"The agent's answer is comprehensive but lacks specific examples.\"\nsuggestions:\n  - \"Include more detailed explanations.\"\n  - \"Provide references to support claims.\"\n```",
        "**critic**: The function of critic is to generate a critique based on the user's question and the agent's answer.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The critic function is a method within the CriticAgent class that is responsible for generating a critique of the agent's response to a user's question. It begins by retrieving relevant data through the get_data_for_critic method, which returns a dictionary containing the user's original question and the agent's answer. This data is then used to create a rendered prompt by calling the render_template method, which formats the critique prompt using the specified template and the retrieved data.\n\nNext, the function sends the rendered prompt to a conversational model using the common_chat method. This method facilitates communication with the model, allowing it to process the prompt and generate a response. The response from the model is expected to contain YAML content, which is then extracted and validated using the extract_and_validate_yaml method. This method ensures that the YAML content is correctly formatted and can be parsed into a usable structure.\n\nIf the extraction and validation of the YAML content are successful, the function returns the formatted YAML. However, if there is an error during this process, such as invalid YAML content, the function catches the exception and logs an error message using the print_exception method from the RichPrinter class. In this case, the function returns None, indicating that the critique could not be generated due to an issue with the model's response.\n\nThe critic function is called within the main function of the project, where it is used to evaluate the agent's answer after it has been generated. The critique provided by the critic function can influence subsequent actions, such as whether to stop the iteration process or to perform additional searches based on the feedback received.\n\n**Note**: It is essential that the model's response contains valid YAML content wrapped in the appropriate delimiters for the extract_and_validate_yaml method to function correctly. If the content is not valid, the function will return None, which may disrupt the expected flow of the application.\n\n**Output Example**: A possible return value from the critic function could be a YAML formatted string such as:\n\n```yaml\nfeedback: \"The agent's answer is comprehensive but lacks specific examples.\"\nsuggestions:\n  - \"Include more detailed explanations.\"\n  - \"Provide references to support claims.\"\n```",
        "**critic**: The function of critic is to generate a critique based on the user's question and the agent's answer.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The critic function is a method within the CriticAgent class that is responsible for generating a critique of the agent's response to a user's question. The function begins by calling the get_data_for_critic method, which retrieves the necessary data for generating the critique. This data is structured as a dictionary containing the user's question and the agent's answer.\n\nNext, the function uses the render_template method to create a prompt for the model by incorporating the retrieved data into a predefined template (critic_prompt). This rendered prompt is then sent to the chat method, which facilitates a conversation with a conversational model, specifically designed to generate critiques.\n\nUpon receiving the model's response, the function attempts to extract and validate any YAML content present in the response using the extract_and_validate_yaml method. This method utilizes regular expressions to identify YAML content enclosed within specific delimiters and parses it into a structured format. If the parsing is successful, the formatted YAML is returned as the output of the critic function.\n\nIn the event of a parsing error, the function catches the yaml.YAMLError exception and logs an error message using the print_exception method from the RichPrinter class. This ensures that any issues encountered during the critique generation process are communicated clearly, allowing for prompt resolution.\n\nThe critic function is invoked within the main function of the project, where it plays a crucial role in the iterative process of refining the agent's responses. After the agent generates an answer to the user's question, the critique is obtained through the critic function, which evaluates the quality of the response and provides suggestions for improvement. This feedback loop is essential for enhancing the overall performance and accuracy of the intelligent agent.\n\n**Note**: It is important to ensure that the critic_prompt is properly defined and that the data returned by get_data_for_critic is accurate. Any issues with the model's response or the YAML extraction process may result in the function returning None, which could disrupt the expected flow of the application.\n\n**Output Example**: A possible return value from the critic function could be a YAML formatted string such as:\n\n```yaml\nfeedback: \"The agent's answer is comprehensive but lacks specific examples.\"\nsuggestions:\n  - \"Include more detailed explanations.\"\n  - \"Provide references to support claims.\"\n```"
      ],
      "code_start_line": 13,
      "code_end_line": 28,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def critic(self):\n        \"\"\"\n        生成评论。\n        \"\"\"\n        data = self.get_data_for_critic()\n\n        rendered_prompt = self.render_template(self.critic_prompt, data)\n        model_response = self.chat(usr_prompt=rendered_prompt, role=\"critic\")\n\n        try:\n            formatted_yaml = self.extract_and_validate_yaml(model_response)\n            return formatted_yaml\n\n        except yaml.YAMLError:\n            printer.print_exception(f\"Invalid YAML content.\")\n            return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/extract_and_validate_yaml",
        "src/criticsearch/rich_output.py/RichPrinter/print_exception",
        "src/criticsearch/critic_agent.py/CriticAgent/get_data_for_critic"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "receive_agent_answer",
      "md_content": [
        "**receive_agent_answer**: The function of receive_agent_answer is to store the answer provided by an agent for further processing.\n\n**parameters**: The parameters of this Function.\n· agent_answer: This parameter represents the answer received from the agent, which will be stored in the instance variable.\n\n**Code Description**: The receive_agent_answer function is a method of the CriticAgent class. Its primary role is to accept an answer from an agent and assign it to the instance variable self.agent_answer. This function is crucial in the context of the CriticAgent's operations, as it allows the agent to receive and retain feedback or responses from other agents involved in the conversation or task execution.\n\nIn the broader context of the project, this function is called within the main function located in src/criticsearch/main.py. During the execution of the main function, after the common agent generates an answer to a user question, the CriticAgent is instantiated. The receive_agent_answer method is then invoked with the common agent's answer as an argument. This step is essential for the CriticAgent to evaluate the quality of the answer provided by the common agent. The stored answer can later be used for further analysis or feedback generation, which is part of the CriticAgent's role in the overall system.\n\n**Note**: It is important to ensure that the agent_answer parameter passed to this function is valid and represents a meaningful response from the agent to avoid any issues in subsequent evaluations or processing.",
        "**receive_agent_answer**: The function of receive_agent_answer is to store the answer provided by an agent for further processing.\n\n**parameters**: The parameters of this Function.\n· agent_answer: This parameter represents the answer received from the agent, which will be stored in the instance variable for later use.\n\n**Code Description**: The receive_agent_answer function is a method defined within the CriticAgent class. Its primary role is to accept an answer from an agent and assign it to an instance variable named agent_answer. This allows the CriticAgent to retain the agent's response for subsequent evaluations or actions. \n\nThis function is invoked within the process_single_task function found in the src/criticsearch/main.py file. In this context, after the common_agent generates an answer to a given task, the CriticAgent receives this answer through the receive_agent_answer method. This step is crucial as it enables the CriticAgent to assess the quality of the answer provided by the common_agent, facilitating a feedback loop where the CriticAgent can evaluate and potentially improve the response based on its own criteria.\n\nThe relationship between receive_agent_answer and its caller, process_single_task, is integral to the overall functionality of the system. The CriticAgent acts as a reviewer of the answers generated by the common_agent, and the receive_agent_answer method serves as the mechanism through which the CriticAgent acquires the necessary information to perform its evaluation.\n\n**Note**: It is important to ensure that the agent_answer passed to this function is valid and relevant to the task at hand, as this will directly impact the effectiveness of the CriticAgent's evaluation process.",
        "**receive_agent_answer**: The function of receive_agent_answer is to store the response received from an agent for further processing or evaluation.\n\n**parameters**: The parameters of this Function.\n· agent_answer: The response provided by the agent, which is intended to be stored for subsequent use.\n\n**Code Description**: The receive_agent_answer function is a straightforward method within the CriticAgent class. Its primary role is to accept an input parameter, agent_answer, which represents the response generated by an intelligent agent during a conversation or task execution. Upon invocation, the function assigns the value of agent_answer to the instance variable self.agent_answer. This action effectively stores the agent's response within the CriticAgent instance, allowing it to be accessed later for evaluation or critique.\n\nThis function is called within the main function of the project, specifically in the src/criticsearch/main_old_paralell.py and src/criticsearch/main_old_version.py files. In these contexts, the CriticAgent is instantiated to assess the quality of responses generated by the common agent. After the common agent produces an answer to the user's task, the receive_agent_answer function is invoked to capture this answer. The CriticAgent then processes the stored response to provide feedback, which may influence subsequent iterations of the conversation process.\n\nThe relationship between receive_agent_answer and its callers is crucial for the iterative improvement of the agent's responses. By storing the agent's answers, the CriticAgent can evaluate them against user expectations or predefined criteria, thereby facilitating a feedback loop that enhances the overall performance of the intelligent agent.\n\n**Note**: It is important to ensure that the agent_answer parameter is well-formed and relevant to the context of the conversation, as this will directly impact the effectiveness of the critique and subsequent iterations of response generation."
      ],
      "code_start_line": 30,
      "code_end_line": 31,
      "params": [
        "self",
        "agent_answer"
      ],
      "have_return": false,
      "code_content": "    def receive_agent_answer(self, agent_answer):\n        self.agent_answer = agent_answer\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_data_for_critic",
      "md_content": [
        "**get_data_for_critic**: The function of get_data_for_critic is to return a dictionary containing the user's question and the agent's answer.\n\n**parameters**: This function does not accept any parameters.\n\n**Code Description**:  \nThe `get_data_for_critic` function is a method of a class, which retrieves specific data related to the user's original task and the agent's response. It returns a dictionary with two key-value pairs: the first key is `\"user_question\"`, which corresponds to the `original_task` attribute of the class, and the second key is `\"agent_answer\"`, which corresponds to the `agent_answer` attribute of the class. This method is designed to package these two pieces of information into a format suitable for further processing or analysis.\n\nThis function is invoked by the `critic` method within the same class. The `critic` method uses the data returned by `get_data_for_critic` to render a prompt for a model, which is then used to generate a response. The data returned by `get_data_for_critic` is passed to a template-rendering function (`render_template`), where it is incorporated into a template prompt. The rendered prompt is subsequently sent to a model via the `common_chat` method, and the result is further processed. The `get_data_for_critic` function thus plays a key role in providing necessary input data for the comment generation process within the `critic` method.\n\n**Note**: This method relies on the class having attributes `original_task` and `agent_answer` set properly, as these are used to populate the returned dictionary. If either of these attributes is not initialized or is set to `None`, the function will return a dictionary with missing or invalid values.\n\n**Output Example**:  \nAn example output from `get_data_for_critic` could look as follows:\n\n```json\n{\n  \"user_question\": \"What are the benefits of using AI in healthcare?\",\n  \"agent_answer\": \"AI in healthcare can help with diagnostics, treatment plans, and personalized medicine.\"\n}\n```"
      ],
      "code_start_line": 33,
      "code_end_line": 34,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_data_for_critic(self):\n        return {\"user_question\": self.original_task, \"agent_answer\": self.agent_answer}\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/critic_agent.py/CriticAgent/critic"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/workflow.py": [
    {
      "type": "ClassDef",
      "name": "WorkflowExecutor",
      "md_content": [
        "**WorkflowExecutor**: The function of WorkflowExecutor is to manage and execute a series of actions in a workflow, interacting with tools and generating responses based on the user's query and defined actions.\n\n**attributes**:\n· user_query: A string that contains the user's input query, which is used as the basis for generating the workflow.\n· agent: An instance of the `BaseAgent` class responsible for managing tools and performing actions.\n· registry: The registry of tools available to the agent, used to register and invoke tools.\n· history: A list of interactions that keeps track of the conversation and system responses during the workflow execution.\n· _step_count: A counter to track the number of steps executed in the workflow.\n· _traj: A list that stores the trajectory of actions and rewards throughout the workflow.\n· _last_obs: The last observation generated by the workflow execution.\n\n**Code Description**:  \nThe `WorkflowExecutor` class is designed to facilitate the execution of a structured workflow that interacts with various tools to process the user's query. Upon initialization, the `WorkflowExecutor` takes a `user_query` as input, which sets the context for the workflow execution. The class performs the following functions:\n\n- **Initialization**:\n  - The `__init__` method creates an instance of the `BaseAgent` class and retrieves the tool registry (`self.registry`) to manage available tools.\n  - A unique session ID is generated using `uuid.uuid4()` and set using `set_session()` to track the session for note-taking and reporting purposes.\n  - The tools used in the workflow are registered, including functions such as `search`, `scrape`, note-taking (`taking_notes`), and retrieving notes (`retrieve_notes`).\n  - A schema for each tool is generated by querying the tool registry, which defines the available tools and their arguments.\n  - A system prompt is loaded and rendered using a template from a file, incorporating the available tools and the user query into the prompt.\n  - The history is initialized to track the conversation between the system and user, including the system's initial prompt and the user query.\n  - The step counter (`_step_count`) and trajectory list (`_traj`) are initialized to track the execution progress.\n\n- **Step Execution**:\n  - The `step` method is responsible for executing a single step in the workflow, processing an action and interacting with the tools. This method receives an action, processes it, and returns a tuple containing the observation, reward, completion status, and additional information.\n  - The action is evaluated to determine if it involves a tool invocation. If the action includes a tool call (identified via an XML tag), the tool's name and arguments are extracted, and the tool is invoked through the registry.\n  - The result of the tool invocation is returned as an observation, and the trajectory of actions and rewards is updated.\n  - If the action does not involve a tool invocation (i.e., it is the final answer), the workflow ends with the final response being returned, along with a reward.\n  - Error handling is implemented to return an error response if the tool invocation or argument parsing fails.\n\nThe `WorkflowExecutor` class is utilized in the `run_workflow` and `iterate_traj` functions. In `run_workflow`, the executor is instantiated with the user's query, and the workflow progresses by repeatedly calling the `step` method until the workflow is completed. In `iterate_traj`, the executor is used to process sections of a report, iterating through each section and evaluating its factual accuracy.\n\n**Note**:  \n- Ensure that the user query provided to the `WorkflowExecutor` is well-defined, as it influences the entire workflow execution.\n- The tool registry and system prompt are critical components for successful execution, as they define the tools available and the context in which the workflow operates.\n- The class maintains a history of interactions and tracks the trajectory of actions, which can be useful for debugging or analyzing the workflow.\n- The class performs error handling during tool invocation and argument parsing, ensuring that invalid actions are managed gracefully.\n\n**Output Example**:  \nA typical output of the `step` method could look like this:\n```\n{\n    \"observation\": \"<tool_use_result><name>search</name><result>{'found_results': 5}</result></tool_use_result>\",\n    \"reward\": 0.0,\n    \"done\": False,\n    \"info\": {}\n}\n```\nThis output includes the tool result (`observation`), the reward (`reward`), a flag indicating whether the process is complete (`done`), and any additional information (`info`)."
      ],
      "code_start_line": 15,
      "code_end_line": 103,
      "params": [],
      "have_return": true,
      "code_content": "class WorkflowExecutor:\n    def __init__(self, user_query: str):\n        # Initialize agent and registry\n        self.agent = BaseAgent()\n        self.registry = self.agent.tool_registry\n\n        # 生成唯一 session_id 用于笔记绑定\n        session_id = str(uuid.uuid4())\n        set_session(session_id)\n\n        # 注册工具并生成 schema\n        tool_funcs = [\n            self.agent.search_aggregator.search,\n            self.agent.content_scraper.scrape,\n            taking_notes,\n            retrieve_notes,\n        ]\n        schemas = []\n        for func in tool_funcs:\n            schemas.extend(self.registry.get_or_create_tool_schema(func))\n\n        # 加载并渲染 system prompt\n        tpl_path = Path(self.agent.prompts_dir) / \"tool_use.txt\"\n        tpl_str = tpl_path.read_text(encoding=\"utf-8\")\n        system_prompt = Template(tpl_str).render(\n            AVAILABLE_TOOLS=json.dumps(schemas),\n            USER_QUERY=user_query,\n        )\n\n        # 初始化对话历史、步数与轨迹\n        self.history = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_query}\n        ]\n        self._step_count = 0\n        self._traj = []\n\n\n    def step(self, action: str) -> Tuple[str, float, bool, Dict]:\n        \"\"\"\n        执行一次 action：记录、工具解析与调用，并返回 (observation, reward, done, info)\n        \"\"\"\n        self._step_count += 1\n        self.history.append({\"role\": \"assistant\", \"content\": action})\n\n        tool_xml = extract_tag_content(action, \"tool_use\")\n        if not tool_xml:\n            # 最终回答\n            obs = action\n            # evaluate reports for reward\n            reward = 1 # TODO: 这里需要根据reportbenchmark当前section的extracted_facts考试来得到真实的acc（程序悖论）\n            done = True\n        else:\n            # 解析工具调用\n            tool_name = extract_tag_content(tool_xml, \"name\")\n            arg_str = extract_tag_content(tool_xml, \"arguments\") or \"{}\"\n            try:\n                args = json.loads(arg_str)\n            except json.JSONDecodeError:\n                error_xml = (\n                    f\"<tool_use_result><name>{tool_name}</name>\"\n                    f\"<error>arguments_not_json</error></tool_use_result>\"\n                )\n                self.history.append({\"role\": \"user\", \"content\": error_xml})\n                return error_xml, self.agent.cfg.invalid_penalty, False, {}\n\n            # 执行工具\n            try:\n                result = self.registry.invoke_tool(tool_name, args)\n                result_xml = (\n                    f\"<tool_use_result><name>{tool_name}</name>\"\n                    f\"<result>{json.dumps(result, ensure_ascii=False)}</result>\"\n                    f\"</tool_use_result>\"\n                )\n                self.history.append({\"role\": \"user\", \"content\": result_xml})\n                obs, reward, done = result, 0.0, False\n            except Exception as exc:\n                error_xml = (\n                    f\"<tool_use_result><name>{tool_name}</name>\"\n                    f\"<error>{str(exc)}</error></tool_use_result>\"\n                )\n                self.history.append({\"role\": \"user\", \"content\": error_xml})\n                return error_xml, self.agent.cfg.invalid_penalty, False, {}\n\n        # 记录轨迹\n        self._traj.append({\"a\": action, \"r\": reward})\n        obs_str = json.dumps(obs, ensure_ascii=False) if isinstance(obs, (dict, list)) else str(obs)\n        self._last_obs = obs_str\n        return self._last_obs, reward, done, {}\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/workflow.py/run_workflow",
        "src/criticsearch/workflow.py/iterate_traj"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the WorkflowExecutor class, setting up the agent, registry, session, and conversation history.\n\n**parameters**: The parameters of this Function.\n· user_query: str - A string representing the user's query that will be processed by the WorkflowExecutor.\n\n**Code Description**: The __init__ method is the constructor for the WorkflowExecutor class. It is responsible for initializing various components necessary for the execution of workflows related to user queries. Upon instantiation, the method performs several key actions:\n\n1. **Agent and Registry Initialization**: The method creates an instance of the BaseAgent class, which serves as the foundational class for intelligent agents. This instance is assigned to the `self.agent` attribute, and its tool registry is accessed through `self.registry`.\n\n2. **Session Management**: A unique session identifier is generated using `uuid.uuid4()`, which ensures that each session is distinct. This session ID is then set using the `set_session` function, allowing for the binding of notes and other session-specific data to the current workflow.\n\n3. **Tool Registration and Schema Generation**: The method defines a list of tool functions that the agent can utilize, including search aggregation, content scraping, note-taking, and note retrieval functions. For each of these functions, the method retrieves or creates their corresponding schemas by calling `self.registry.get_or_create_tool_schema(func)`. This ensures that the tools are properly registered and ready for use in the workflow.\n\n4. **System Prompt Loading and Rendering**: The method loads a system prompt template from a specified file path and renders it using the user query and the available tool schemas. This rendered prompt is stored in the `system_prompt` variable and is included in the conversation history.\n\n5. **Conversation History Initialization**: The method initializes the conversation history with the system prompt and the user's query. It also sets the step count to zero and initializes an empty trajectory list, which may be used to track the execution steps of the workflow.\n\nThe __init__ method establishes the foundational setup for the WorkflowExecutor, ensuring that all necessary components are in place for processing user queries effectively. It integrates various functionalities from the BaseAgent class and other tools, enabling a seamless workflow execution process.\n\n**Note**: It is important to ensure that the user_query parameter is valid and properly formatted, as it directly influences the rendering of the system prompt and the subsequent workflow execution. Additionally, the tool registry must be populated with the necessary tools before invoking methods that rely on them."
      ],
      "code_start_line": 16,
      "code_end_line": 50,
      "params": [
        "self",
        "user_query"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, user_query: str):\n        # Initialize agent and registry\n        self.agent = BaseAgent()\n        self.registry = self.agent.tool_registry\n\n        # 生成唯一 session_id 用于笔记绑定\n        session_id = str(uuid.uuid4())\n        set_session(session_id)\n\n        # 注册工具并生成 schema\n        tool_funcs = [\n            self.agent.search_aggregator.search,\n            self.agent.content_scraper.scrape,\n            taking_notes,\n            retrieve_notes,\n        ]\n        schemas = []\n        for func in tool_funcs:\n            schemas.extend(self.registry.get_or_create_tool_schema(func))\n\n        # 加载并渲染 system prompt\n        tpl_path = Path(self.agent.prompts_dir) / \"tool_use.txt\"\n        tpl_str = tpl_path.read_text(encoding=\"utf-8\")\n        system_prompt = Template(tpl_str).render(\n            AVAILABLE_TOOLS=json.dumps(schemas),\n            USER_QUERY=user_query,\n        )\n\n        # 初始化对话历史、步数与轨迹\n        self.history = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_query}\n        ]\n        self._step_count = 0\n        self._traj = []\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/tools/note_manager.py/set_session",
        "src/criticsearch/tools/note_manager.py/taking_notes",
        "src/criticsearch/tools/note_manager.py/retrieve_notes",
        "src/criticsearch/tools/tool_registry.py/ToolRegistry/get_or_create_tool_schema",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "step",
      "md_content": [
        "**step**: The function of step is to execute a specified action, record the process, and return the resulting observation, reward, completion status, and additional information.\n\n**parameters**: The parameters of this Function.\n· action: A string representing the action to be executed, which may involve tool usage or provide a final answer.\n\n**Code Description**: The step function is a core component of the WorkflowExecutor class, responsible for processing an action provided as input. Upon invocation, it increments the step count and appends the action to the history log. The function then attempts to extract any tool usage information from the action string using the extract_tag_content function. If no tool usage is detected, the function treats the action as a final answer, assigns a reward of 1, and sets the done flag to True, indicating the completion of the workflow.\n\nIf tool usage is identified, the function extracts the tool's name and its arguments. It attempts to parse the arguments from a JSON string. If parsing fails, an error message is generated and returned, along with an invalid penalty as the reward. If the arguments are successfully parsed, the function invokes the specified tool using the ToolRegistry's invoke_tool method. The result of the tool invocation is formatted into an XML structure and logged in the history. The function then returns the result, a reward of 0.0, and a done flag set to False, indicating that the workflow is still ongoing.\n\nThe step function is called within the run_workflow function, which orchestrates the overall workflow by continuously prompting the assistant for actions until a final answer is obtained. Additionally, it is utilized in the iterate_traj function, where it processes actions in the context of evaluating sections of a report against extracted facts.\n\n**Note**: It is crucial to ensure that the action string is well-formed and contains valid tool usage information when applicable. If the tool name is not registered in the ToolRegistry, a KeyError will be raised during the invocation process. Furthermore, the arguments must be correctly formatted as a JSON string to avoid parsing errors.\n\n**Output Example**: A possible return value from the step function could be:\n```\n(\"Final answer content\", 1.0, True, {})\n```"
      ],
      "code_start_line": 53,
      "code_end_line": 103,
      "params": [
        "self",
        "action"
      ],
      "have_return": true,
      "code_content": "    def step(self, action: str) -> Tuple[str, float, bool, Dict]:\n        \"\"\"\n        执行一次 action：记录、工具解析与调用，并返回 (observation, reward, done, info)\n        \"\"\"\n        self._step_count += 1\n        self.history.append({\"role\": \"assistant\", \"content\": action})\n\n        tool_xml = extract_tag_content(action, \"tool_use\")\n        if not tool_xml:\n            # 最终回答\n            obs = action\n            # evaluate reports for reward\n            reward = 1 # TODO: 这里需要根据reportbenchmark当前section的extracted_facts考试来得到真实的acc（程序悖论）\n            done = True\n        else:\n            # 解析工具调用\n            tool_name = extract_tag_content(tool_xml, \"name\")\n            arg_str = extract_tag_content(tool_xml, \"arguments\") or \"{}\"\n            try:\n                args = json.loads(arg_str)\n            except json.JSONDecodeError:\n                error_xml = (\n                    f\"<tool_use_result><name>{tool_name}</name>\"\n                    f\"<error>arguments_not_json</error></tool_use_result>\"\n                )\n                self.history.append({\"role\": \"user\", \"content\": error_xml})\n                return error_xml, self.agent.cfg.invalid_penalty, False, {}\n\n            # 执行工具\n            try:\n                result = self.registry.invoke_tool(tool_name, args)\n                result_xml = (\n                    f\"<tool_use_result><name>{tool_name}</name>\"\n                    f\"<result>{json.dumps(result, ensure_ascii=False)}</result>\"\n                    f\"</tool_use_result>\"\n                )\n                self.history.append({\"role\": \"user\", \"content\": result_xml})\n                obs, reward, done = result, 0.0, False\n            except Exception as exc:\n                error_xml = (\n                    f\"<tool_use_result><name>{tool_name}</name>\"\n                    f\"<error>{str(exc)}</error></tool_use_result>\"\n                )\n                self.history.append({\"role\": \"user\", \"content\": error_xml})\n                return error_xml, self.agent.cfg.invalid_penalty, False, {}\n\n        # 记录轨迹\n        self._traj.append({\"a\": action, \"r\": reward})\n        obs_str = json.dumps(obs, ensure_ascii=False) if isinstance(obs, (dict, list)) else str(obs)\n        self._last_obs = obs_str\n        return self._last_obs, reward, done, {}\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/workflow.py/run_workflow",
        "src/criticsearch/workflow.py/iterate_traj"
      ],
      "reference_who": [
        "src/criticsearch/utils.py/extract_tag_content",
        "src/criticsearch/tools/tool_registry.py/ToolRegistry/invoke_tool"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "run_workflow",
      "md_content": [
        "**run_workflow**: The function of run_workflow is to execute a workflow based on a user query, utilizing a WorkflowExecutor to manage the interaction and actions until a final response is obtained.\n\n**parameters**: The parameters of this Function.\n· user_query: A string that represents the user's input query, which serves as the basis for the workflow execution.\n\n**Code Description**: The run_workflow function is designed to facilitate the execution of a structured workflow that processes a user query. Upon invocation, it initializes an instance of the WorkflowExecutor class, passing the user_query as an argument. The WorkflowExecutor is responsible for managing the workflow, including the interaction with tools and the generation of responses based on the user's input.\n\nThe function enters a continuous loop where it interacts with the WorkflowExecutor to obtain the next action suggested by the assistant. This is achieved by calling the chat method of the agent within the WorkflowExecutor, which processes the current history of interactions and generates a response. The response is then passed to the step method of the WorkflowExecutor, which executes the action and returns an observation, reward, completion status, and additional information.\n\nThe loop continues until the step method indicates that the workflow is complete by returning a done flag set to True. At this point, the function exits the loop and returns the history of interactions maintained by the WorkflowExecutor. This history includes all exchanges between the user and the assistant throughout the workflow, providing a comprehensive record of the process.\n\nThe run_workflow function is called within the main function of the project, where it can handle either a single user query or multiple queries concurrently. In the case of multiple queries, the function utilizes a ThreadPoolExecutor to manage concurrent execution, allowing for efficient processing of multiple workflows in parallel.\n\n**Note**: It is essential to ensure that the user_query parameter is well-defined, as it directly influences the workflow execution. The WorkflowExecutor maintains a history of interactions, which can be useful for debugging or analyzing the workflow's performance.\n\n**Output Example**: A possible appearance of the code's return value when calling the run_workflow function might look like this:\n```\n[\n    {\"role\": \"system\", \"content\": \"System prompt based on user query.\"},\n    {\"role\": \"user\", \"content\": \"User's input query.\"},\n    {\"role\": \"assistant\", \"content\": \"Assistant's response based on the query.\"},\n    ...\n]\n```"
      ],
      "code_start_line": 106,
      "code_end_line": 116,
      "params": [
        "user_query"
      ],
      "have_return": true,
      "code_content": "def run_workflow(user_query: str) -> list[dict]:\n    runner = WorkflowExecutor(user_query)\n\n    while True:\n        # Assistant suggests next action或最终回答\n        response = runner.agent.chat(usr_prompt=runner.history)\n        obs, reward, done, _ = runner.step(response)\n        if done:\n            break\n\n    return runner.history\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/workflow.py/main"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/workflow.py/WorkflowExecutor",
        "src/criticsearch/workflow.py/WorkflowExecutor/step"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "iterate_traj",
      "md_content": [
        "**iterate_traj**: The function of iterate_traj is to evaluate a single section's factual accuracy as reward.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The iterate_traj function is designed to evaluate the factual accuracy of various sections of a report by generating content based on user prompts and verifying it against extracted facts. The function begins by initializing an instance of the BaseAgent class, which serves as the core component for managing interactions and processing user queries.\n\nThe function then attempts to load a JSON mapping file that contains user prompts associated with specific sections of the report. If the mapping file cannot be loaded, the function issues a warning and returns a default reward of 0.0. The mapping is essential as it provides the context for generating content for each section.\n\nNext, the function iterates over the mapping items, where each item consists of a filename and a corresponding user prompt. For each section, it checks if the associated JSON file exists in the benchmark directory. If the file is missing, a warning is printed, and the function continues to the next section.\n\nFor each valid section, the function creates an instance of the ReportBenchmark class, passing the path to the JSON file and the user prompt. This instance is responsible for generating benchmark items, which include the section content and extracted facts. The benchmark items are generated using the generate_benchmark_item method, which may utilize caching to improve performance.\n\nThe function then constructs a full prompt for the section generation, which includes the user prompt, the section title, and any previously generated content for context. This prompt is passed to an instance of the WorkflowExecutor class, which manages the execution of actions and interactions with the agent.\n\nWithin a loop, the function engages in a conversation with the agent, sending the constructed prompt and receiving responses until a final answer is obtained. The response is processed to extract the section content, which is then verified against the extracted facts using the ReportVerifier class. The verification process evaluates the accuracy of the generated content and assigns a reward based on the results.\n\nFinally, the function yields the trajectory of actions and rewards for each section, allowing for further analysis or processing. This iterative approach enables the evaluation of multiple sections in a structured manner, providing insights into the factual accuracy of the generated content.\n\n**Note**: It is important to ensure that the mapping file and associated JSON files are correctly formatted and accessible. The function relies on the successful execution of the ReportBenchmark and ReportVerifier classes to generate and verify content accurately.\n\n**Output Example**: A possible appearance of the code's return value when iterating through sections might look like this:\n```json\n[\n    {\n        \"full_prompt\": \"User prompt...\\nNow, based on the background information above, do not generate a complete report, but instead generate the content of the section I am requesting. The section you need to generate currently is: Section Title\\n...\",\n        \"section_content\": \"Generated content for the section...\",\n        \"section_traj\": [...],\n        \"section_reward\": 0.85\n    },\n    ...\n]\n```"
      ],
      "code_start_line": 118,
      "code_end_line": 189,
      "params": [],
      "have_return": true,
      "code_content": "def iterate_traj():\n    \"\"\"\n    Evaluate a single section's factual accuracy as reward.\n    \"\"\"\n    agent = BaseAgent()\n    # For per-section evaluation, load mapping to find current JSON and prompt\n    mapping_path = Path(__file__).parent / \"reportbench\" / \"instruction_mapping.json\"\n    try:\n        mapping = json.loads(mapping_path.read_text(encoding=\"utf-8\"))\n    except Exception:\n        print(f\"[WARN] Cannot load instruction mapping from {mapping_path}\")\n        return 0.0\n    # find JSON file matching this user_query\n    benchmark_dir = Path(__file__).parent / \"reportbench\" / \"wiki_data\"\n    for fname, user_prompt in mapping.items(): # 我在这里遍历构建的所有的user prompt来运行不同的section trajectory\n        \n        candidate = benchmark_dir / fname\n        if not candidate.exists():\n            print(f\"[WARN] JSON file {candidate} not found\")\n            continue\n\n        # generate benchmark items for sections\n        bench = ReportBenchmark(str(candidate), user_query=user_prompt)\n        benchmark_items = bench.generate_benchmark_item(use_cache=True) #  benchmark_items是一个json list\n\n        # 从这里开始构建 背景信息+section信息+之前写作段落信息的 context供模型参考来写当前的section，然后进行verify得到当前section的reward\n        # 1) 背景信息 就是 user_query\n        # 2) section信息 就是 section的title，section内容要模型自己写\n        # 3) 之前写作段落信息 就是之前answer tag里面的内容，所有的拼接起来\n\n        report = ''\n        trajectory_list = []\n\n        for section in benchmark_items:\n            verifier = ReportVerifier(agent)\n            section_title :str = section[\"path\"]\n            section_extracted_facts : List[dict] = section[\"extracted_facts\"] # 有了模型的section answer和这里的extracted_facts就可以调用verifier来验证了\n            \n            # 调用step函数来得到section answer\n            full_prompt = (\n                user_prompt + \"\\n\" +\n                f\"Now, based on the background information above, do not generate a complete report, but instead generate the content of the section I am requesting. The section you need to generate currently is: {section_title}\\n\" +\n                (f\"Here is the content of the previous section you wrote for you reference: {report}\\n You should keep writing the current section based on what you have already wrote\" if section != benchmark_items[0] else \"\")+\n                \"Use the tools immediately in your answer, no spamming, and do not use the tools in the middle of the answer. \"\n\n            )\n\n            runner = WorkflowExecutor(full_prompt)\n            # from IPython import embed; embed()\n        \n            while True:\n                # Assistant suggests next action或最终回答\n                response = runner.agent.chat(usr_prompt=runner.history)\n                obs, reward, done, _ = runner.step(response)\n                if done:\n                    section_content = extract_tag_content(obs,\"answer\")\n                    break   \n            \n            section_reward = verifier.verify_section(section_content, section_extracted_facts)\n\n            # 逐渐拼接每个section的内容作为前文写作的背景\n            report += f\"Section: {section_title}\\n\"\n            report += f\"\\n{section_content}\\n\"\n\n            trajectory_list.append({\n                \"full_prompt\": full_prompt,\n                \"section_content\": section_content,\n                \"section_traj\": runner._traj,\n                \"section_reward\": section_reward,\n            })\n            # 每产生一个 section 的轨迹就立刻产出\n            yield trajectory_list\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/utils.py/extract_tag_content",
        "src/criticsearch/workflow.py/WorkflowExecutor",
        "src/criticsearch/workflow.py/WorkflowExecutor/step",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_benchmark_item",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/verify_section"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: The function of main is to execute a workflow based on user queries, allowing for both single and concurrent processing.\n\n**parameters**: The parameters of this Function.\n· query: A string representing a single user query to process.\n· queries: A list of strings representing multiple user queries to process concurrently.\n· workers: An integer specifying the maximum number of concurrent workers.\n· count: An integer indicating the number of identical runs to perform if a single query is provided.\n\n**Code Description**: The main function serves as the entry point for executing workflows based on user input. It utilizes the argparse library to parse command-line arguments, allowing users to specify either a single query or multiple queries for processing. The function requires at least one of the query options to be provided.\n\nUpon parsing the arguments, the function determines the list of queries to be processed. If multiple queries are specified, it uses a ThreadPoolExecutor to manage concurrent execution of the workflows. The maximum number of workers is limited by either the user-specified value or the number of queries, whichever is smaller.\n\nFor each query, the function submits a task to the executor, which calls the run_workflow function. This function is responsible for executing the workflow based on the provided user query. The results of each workflow execution are collected in a dictionary, where the keys are the original queries and the values are the corresponding histories of interactions.\n\nOnce all futures are completed, the function outputs the complete history for each query in a formatted JSON structure. Additionally, it extracts and displays the final responses from the assistant for each query, providing a summary comparison of the results.\n\nIn the case where only a single query is provided, the function directly calls run_workflow and prints the resulting history.\n\nThe relationship with the run_workflow function is crucial, as it handles the core logic of executing the workflow based on user queries. The main function orchestrates the input handling and manages the concurrent execution of multiple workflows, leveraging the capabilities of run_workflow to obtain the necessary results.\n\n**Note**: It is important to ensure that the command-line arguments are correctly specified when invoking the main function, as improper usage may lead to errors or unexpected behavior. The function is designed to handle both single and multiple queries efficiently, making it versatile for various use cases."
      ],
      "code_start_line": 191,
      "code_end_line": 225,
      "params": [],
      "have_return": false,
      "code_content": "def main():\n    parser = argparse.ArgumentParser(description=\"Run XML-based tool use workflow\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--query\", \"-q\", help=\"Single user query to process\")\n    group.add_argument(\"--queries\", \"-Q\", nargs=\"+\", help=\"Multiple user queries to process concurrently\")\n    parser.add_argument(\"--workers\", \"-w\", type=int, default=1, help=\"Max concurrent workers\")\n    parser.add_argument(\"--count\", \"-c\", type=int, default=1, help=\"Number of identical runs (ignored if --queries)\")\n    args = parser.parse_args()\n\n    # 决定要并发的 queries 列表\n    if args.queries:\n        queries = args.queries\n    else:\n        queries = [args.query] * args.count\n\n    if len(queries) > 1:\n        max_workers = min(args.workers, len(queries))\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # future -> 原始 query\n            futures = {executor.submit(run_workflow, q): q for q in queries}\n            all_histories: dict[str, list] = {}\n            for fut in as_completed(futures):\n                q = futures[fut]\n                all_histories[q] = fut.result()\n\n        # 1) 输出每个 query 的完整 history\n        print(json.dumps(all_histories, ensure_ascii=False, indent=2))\n        # 2) 收集每个 query 的最后一次 assistant 回答\n        final_answers = {q: history[-1][\"content\"] for q, history in all_histories.items()}\n        print(\"===== 最终模型回答对比 =====\")\n        print(json.dumps(final_answers, ensure_ascii=False, indent=2))\n\n    else:\n        history = run_workflow(queries[0])\n        print(json.dumps(history, ensure_ascii=False, indent=2))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/workflow.py/run_workflow"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "src/criticsearch/models.py": [
    {
      "type": "ClassDef",
      "name": "HistoryItem",
      "md_content": [
        "**HistoryItem**: The function of HistoryItem is to represent an individual entry in a conversation history.\n\n**attributes**: The attributes of this Class.\n· role: Literal[\"user\", \"assistant\", \"tool\", \"critic\"]  \n   This attribute defines the role of the entity in the conversation, such as \"user\", \"assistant\", \"tool\", or \"critic\". The role helps categorize the message within the conversation history.  \n· content: Optional[str]  \n   This attribute holds the actual content of the message. It is optional and may be `None` if not provided.  \n· tool_calls: Optional[List[ChatCompletionMessageToolCall]]  \n   This attribute stores a list of tool calls associated with the message. It is optional and only used when the message involves invoking tools.  \n· tool_call_id: Optional[str]  \n   This attribute holds a unique identifier for a tool call. It is optional and is used to track the specific tool call in case the message involves a tool.  \n· name: Optional[str]  \n   This attribute is used to store the name associated with the history item. It is optional and can be used to provide additional context about the message.\n\n**Code Description**:  \nThe `HistoryItem` class is a data model that encapsulates a single entry within a conversation. It has a `role` attribute that defines who or what is contributing to the conversation, such as the user, assistant, tool, or critic. The `content` attribute contains the message's text, and it can be optional if no message content is provided. The `tool_calls` attribute, also optional, is used when a message involves calling a tool or a function, storing details about the tool calls made. Similarly, the `tool_call_id` serves as a unique identifier for each tool call, providing a way to trace or reference specific tool interactions. Finally, the `name` attribute can store a custom name for the entry, providing further context if required.\n\nThis class plays a vital role in managing conversation history, particularly in the context of applications where various entities interact through a series of messages. It integrates directly with the `ConversationManager` class, which manages the overall conversation history and tools used within it. Specifically, the `HistoryItem` class is used when appending new entries to the conversation history, either through regular messages or tool calls, ensuring that each entry is well-defined with relevant context (such as role, content, tool calls, etc.).\n\nIn the `ConversationManager` class, the `HistoryItem` is instantiated and appended to the `history` attribute whenever a new message is added to the conversation. For instance, the `append_to_history` method in `ConversationManager` creates a new `HistoryItem` with the given role and content, and then adds it to the history list. The `serialize_history` method, which handles the serialization of the conversation history, processes these `HistoryItem` instances to ensure that the history is accurately saved, potentially excluding any `None` values during serialization.\n\nFurthermore, in specific cases where tool calls are involved, the `HistoryItem` can hold a list of `tool_calls` and a `tool_call_id`, providing a way to capture the specific details of any tools or functions invoked during the conversation.\n\n**Note**:  \n- The `content` attribute can be omitted for certain roles, such as \"tool\" or \"critic\", where the primary focus is on the interaction rather than the message content.  \n- The use of `tool_calls` and `tool_call_id` is essential when the conversation involves automated tools or functions, ensuring that these interactions are tracked and referenced correctly.  \n- The optional nature of many attributes (like `content`, `tool_calls`, `tool_call_id`, and `name`) allows for flexibility, enabling this class to accommodate various types of messages and interactions in a conversation history."
      ],
      "code_start_line": 21,
      "code_end_line": 26,
      "params": [],
      "have_return": false,
      "code_content": "class HistoryItem(BaseModel):\n    role: Literal[\"user\", \"assistant\", \"tool\", \"critic\"]\n    content: Optional[str] = None\n    tool_calls: Optional[List[ChatCompletionMessageToolCall]] = None\n    tool_call_id: Optional[str] = None\n    name: Optional[str] = None\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/models.py/ConversationManager",
        "src/criticsearch/models.py/ConversationManager/serialize_history",
        "src/criticsearch/models.py/ConversationManager/append_to_history"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "ConversationManager",
      "md_content": [
        "**ConversationManager**: The function of ConversationManager is to manage and maintain the conversation history, including the ability to serialize this history, append new messages, and handle tool calls within the conversation.\n\n**attributes**: The attributes of this Class.\n· history: List[HistoryItem]  \n   This attribute stores the conversation history as a list of HistoryItem instances, which represent individual entries in the conversation.\n\n· max_history_length: int  \n   This attribute defines the maximum number of entries to retain in the conversation history, limiting the size of the history to the most recent entries.\n\n· available_tools: List  \n   This attribute holds a list of tools that can be utilized during the conversation, allowing for integration of various functionalities.\n\n· save_path: Path  \n   This attribute specifies the file path where the conversation history will be saved in JSON Lines format.\n\n· delete_on_init: bool  \n   This attribute is a flag that indicates whether to delete the existing conversation history file upon initialization of the ConversationManager.\n\n**Code Description**: The ConversationManager class is designed to facilitate the management of conversation history in applications that involve interactions between users and automated systems. It extends from BaseModel, allowing it to leverage data modeling capabilities. Upon initialization, the class checks if the specified save_path exists and deletes it if the delete_on_init flag is set to True. This ensures that any previous conversation history is cleared when a new instance is created.\n\nThe class provides methods to append messages to the history, including the append_to_history method, which allows adding new entries with specified roles (user, assistant, tool, or critic) and optional content. It also includes specialized methods for handling tool calls, such as append_tool_call_to_history and append_tool_call_result_to_history, which ensure that interactions with tools are accurately recorded in the history.\n\nSerialization of the conversation history is handled through the serialize_history method, which prepares the history for saving by excluding any None values and limiting the output to the most recent entries based on max_history_length. Additionally, the custom_serialize method allows for context-specific serialization, particularly for formats like ShareGPT, transforming the history into a structured format suitable for sharing.\n\nThe write method is responsible for saving the conversation history to the specified file, handling both the creation of the file and the appending of new data to existing content. The _auto_save method ensures that the most recent entry in the history is automatically saved after each update.\n\nThe ConversationManager is utilized within the BaseAgent class, where it serves as a central component for managing conversation interactions. The BaseAgent class creates an instance of ConversationManager, allowing it to append messages and tool calls to the history as part of its operations. This integration ensures that all interactions, whether user prompts or tool responses, are systematically recorded, providing a comprehensive log of the conversation flow.\n\n**Note**: It is important to ensure that the max_history_length is set appropriately to avoid excessive memory usage, especially in long-running applications. The delete_on_init flag should be used with caution, as it will permanently remove any existing conversation history upon initialization.\n\n**Output Example**: A possible appearance of the code's return value when serializing the conversation history might look like this:\n```json\n{\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": \"Hello, how can I help you?\"},\n    {\"from\": \"function_call\", \"value\": \"{\\\"name\\\": \\\"search\\\", \\\"arguments\\\": {\\\"query\\\": \\\"latest news\\\"}}\"},\n    {\"from\": \"observation\", \"value\": \"{\\\"result\\\": \\\"Here are the latest news articles...\\\"}\"}\n  ],\n  \"tools\": \"[{\\\"name\\\": \\\"search\\\", \\\"description\\\": \\\"Search for information\\\"}]\"\n}\n```",
        "**ConversationManager**: The function of ConversationManager is to manage the conversation history, including storing, serializing, and manipulating entries of conversation.\n\n**attributes**: The attributes of this Class.\n· history: List[HistoryItem]  \n   This attribute holds the conversation history as a list of HistoryItem instances, representing individual entries in the conversation.  \n· max_history_length: int  \n   This attribute sets a limit on the number of entries that can be stored in the conversation history, with a default value of 10.  \n· available_tools: List  \n   This attribute maintains a list of tools that are available for use within the conversation context.  \n· save_path: Path  \n   This attribute specifies the file path where the conversation history will be saved, defaulting to \"conversation_history.jsonl\".  \n· delete_on_init: bool  \n   This attribute is a flag that indicates whether to delete the existing conversation history file upon initialization of the ConversationManager instance.\n\n**Code Description**: The ConversationManager class is designed to facilitate the management of conversation history in applications that involve interactive dialogue, such as chatbots or virtual assistants. Upon initialization, the class checks if the specified save path for the conversation history file exists and, if the delete_on_init flag is set to True, deletes the file to start fresh. This ensures that each instance of ConversationManager can begin with a clean slate if desired.\n\nThe class provides several key methods for handling conversation history. The `serialize_history` method is responsible for serializing the history entries, ensuring that only the most recent entries, up to the defined max_history_length, are included in the output. This is particularly useful for maintaining a manageable size of the history data.\n\nThe `custom_serialize` method allows for tailored serialization logic based on the context, such as transforming the history into a specific format for external systems like ShareGPT. This method processes each HistoryItem, mapping roles to specific formats and handling tool calls appropriately.\n\nThe `write` method is used to save the conversation history to a specified file path, creating the necessary directories if they do not exist. It reads existing data from the file, appends new entries, and writes the updated history back to the file in JSON format.\n\nThe `_auto_save` method automatically saves the latest entry to the specified save path after each update, ensuring that the conversation history is consistently backed up.\n\nThe `append_to_history` method allows for adding new messages to the conversation history, while `append_tool_call_to_history` and `append_tool_call_result_to_history` specifically handle entries related to tool calls, ensuring that all interactions are accurately recorded.\n\nThe `clear_history` method provides functionality to clear the entire conversation history, resetting the state of the ConversationManager.\n\nThe ConversationManager is utilized within the BaseAgent class, which serves as a foundational class for intelligent agents. The conversation_manager attribute in BaseAgent is an instance of ConversationManager, allowing the agent to maintain a record of interactions with users and tools. This integration enables the agent to append messages and tool calls to the history, facilitating a coherent dialogue experience.\n\n**Note**: It is important to manage the max_history_length attribute to prevent excessive memory usage and ensure that only relevant conversation entries are retained. The delete_on_init flag should be used cautiously, as it will permanently remove any existing conversation history upon initialization. Proper handling of the save_path is also crucial to avoid file access issues.\n\n**Output Example**: A possible appearance of the code's return value when serializing the conversation history might look like this:\n```json\n{\n  \"history\": [\n    {\"role\": \"user\", \"content\": \"Hello, how can I help you?\"},\n    {\"role\": \"assistant\", \"content\": \"I need assistance with my order.\"}\n  ],\n  \"tools\": []\n}\n```"
      ],
      "code_start_line": 30,
      "code_end_line": 191,
      "params": [],
      "have_return": true,
      "code_content": "class ConversationManager(BaseModel):\n    history: List[HistoryItem] = []\n    max_history_length: int = 10  # Limit for conversation history\n    available_tools: List = []\n    save_path: Path = Path(\"conversation_history.jsonl\")\n    delete_on_init: bool = True  # Flag to delete file on initialization\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        if self.delete_on_init and self.save_path.exists():\n            try:\n                self.save_path.unlink(missing_ok=True)  # Delete the file if it exists\n                printer.log(f\"Deleted existing file: {self.save_path}\")\n            except Exception:\n                printer.print_exception(f\"Failed to delete file {self.save_path}\")\n                raise\n        # Set the flag to False to avoid further deletions\n        self.delete_on_init = False\n\n    @field_serializer(\"history\")\n    def serialize_history(self, history: List[HistoryItem]):\n        serialized_history = []\n        for item in history[-self.max_history_length :]:\n            serialized_history.append(item.model_dump(exclude_none=True))\n        return serialized_history\n\n    @model_serializer(mode=\"wrap\")\n    def custom_serialize(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ):\n        \"\"\"\n        Custom serialization logic that handles different contexts, such as 'sharegpt'.\n        \"\"\"\n        # Perform default serialization\n        result = handler(self)\n\n        result = result[\"history\"]\n\n        if info.context and info.context.get(\"sharegpt\"):\n            # Transform history into ShareGPT format\n            # TODO: human 和 observation 必须出现在奇数位置，gpt 和 function 必须出现在偶数位置\n            conversations = []\n            for item in self.history:\n                role_mapping = {\n                    \"user\": \"human\",\n                    \"assistant\": \"function_call\",\n                    \"tool\": \"observation\",\n                    \"critic\": \"critic\",\n                }\n\n                role = role_mapping.get(item.role, item.role)\n                value = item.content if item.content else None\n\n                # For tool calls, include the tool_call_id or arguments\n                if role == \"function_call\":\n                    if item.tool_calls:\n                        for tool_call in item.tool_calls:\n                            value = json.dumps(\n                                {\n                                    \"name\": tool_call.function.name,\n                                    \"arguments\": tool_call.function.arguments,\n                                },\n                                ensure_ascii=True,\n                            )\n\n                            conversations.append({\"from\": role, \"value\": value})\n                    else:\n                        conversations.append({\"from\": \"gpt\", \"value\": value})\n\n                elif role == \"observation\":\n                    conversations.append(\n                        {\"from\": role, \"value\": json.dumps(value, ensure_ascii=True)}\n                    )\n\n                else:\n                    conversations.append({\"from\": role, \"value\": value})\n\n            # Build final output structure\n            result = {\n                \"conversations\": conversations,\n                \"tools\": json.dumps(self.available_tools, ensure_ascii=True),\n            }\n\n        return result\n\n    def write(self, data: dict, path: Path | str):\n        if isinstance(path, str):\n            path = Path(path)\n\n        try:\n            # Create parent directories if they do not exist\n            path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Check if the file exists and read existing data\n            if path.exists():\n                with path.open(\"r\", encoding=\"utf-8\") as f:\n                    try:\n                        # Try to parse the existing JSON data (expecting a list at the top level)\n                        existing_data = json.load(f)\n                        if not isinstance(existing_data, list):\n                            existing_data = []  # Ensure it's a list\n                    except json.JSONDecodeError:\n                        # If the file is empty or corrupt, start with an empty list\n                        existing_data = []\n            else:\n                existing_data = []\n\n            # Append the new data to the existing array\n            existing_data.append(data)\n\n            # Write the updated array back to the file\n            with path.open(\"w\", encoding=\"utf-8\") as f:\n                json.dump(existing_data, f, ensure_ascii=True, indent=2)\n\n        except Exception:\n            printer.print_exception(f\"Failed to write to {path}\")\n            raise\n\n    def _auto_save(self):\n        \"\"\"Auto save after each update if save_path is set\"\"\"\n        if settings.save_sharegpt:\n            self.write(\n                path=self.save_path, data=self.history[-1].model_dump(exclude_none=True)\n            )\n\n    def append_to_history(\n        self,\n        role: Literal[\"user\", \"assistant\", \"tool\", \"critic\"],\n        content: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Add a new message to the conversation history.\n        \"\"\"\n        self.history.append(HistoryItem(role=role, content=content, **kwargs))\n        self._auto_save()\n\n    def append_tool_call_to_history(\n        self,\n        tool_calls: List[ChatCompletionMessageToolCall],\n        content: Optional[str] = None,\n    ):\n        \"\"\"\n        Add a tool call entry to the conversation history.\n        \"\"\"\n        self.append_to_history(role=\"assistant\", tool_calls=tool_calls, content=content)\n\n    def append_tool_call_result_to_history(\n        self, tool_call_id: str, name: str, content: str\n    ):\n        \"\"\"\n        Add a tool call result to the conversation history.\n        \"\"\"\n        self.append_to_history(\n            role=\"tool\", tool_call_id=tool_call_id, name=name, content=content\n        )\n\n    def clear_history(self):\n        \"\"\"\n        Clear the entire conversation history.\n        \"\"\"\n        self.history.clear()\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py",
        "src/criticsearch/base_agent.py/BaseAgent"
      ],
      "reference_who": [
        "src/criticsearch/models.py/HistoryItem"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the object and manage file deletion if certain conditions are met.\n\n**parameters**: \n· data: This parameter is passed as keyword arguments (**kwargs**) to initialize the base class.\n\n**Code Description**: \nThe `__init__` function in this case is a constructor that serves to initialize an object. It first calls the constructor of the superclass using `super().__init__(**data)` to ensure that any initialization defined in the parent class is executed. The keyword arguments passed to this constructor are forwarded to the parent class.\n\nThe function then checks the condition `if self.delete_on_init and self.save_path.exists()`. If the `delete_on_init` flag is set to `True` and a file at the path specified by `self.save_path` exists, the function attempts to delete the file using `self.save_path.unlink()`. If the file is successfully deleted, a log message is created with the information that the file was deleted. If the deletion operation fails (e.g., due to insufficient permissions or other issues), the function catches the exception and logs an error message containing the exception details.\n\nAfter attempting the deletion, the function sets `self.delete_on_init = False` to ensure that no further deletion attempts are made in subsequent invocations of the object. This is done to prevent repeated deletions of the same file unless explicitly required elsewhere in the class.\n\n**Note**: \n- The method assumes that `self.save_path` is a valid file path object that supports `.exists()` and `.unlink()` methods.\n- It is important that the `delete_on_init` flag is properly managed to avoid unintended file deletion during object initialization.\n- The use of `logger` for logging information and errors is crucial for debugging and understanding the state of the file deletion process.",
        "**__init__**: The function of __init__ is to initialize an instance of the class and manage the deletion of an existing file if certain conditions are met.\n\n**parameters**: The parameters of this Function.\n· data: dict - A variable-length keyword argument dictionary that contains initialization data for the instance.\n\n**Code Description**: The __init__ method is a constructor that is called when an instance of the class is created. It first invokes the superclass's __init__ method using `super().__init__(**data)`, which allows the parent class to initialize its attributes with the provided data. This is essential for ensuring that the instance inherits and properly initializes any attributes defined in the parent class.\n\nFollowing the superclass initialization, the method checks the `delete_on_init` attribute. If this attribute is set to True and the `save_path` (presumably a Path object) exists, the method attempts to delete the file at that path using `unlink(missing_ok=True)`. This operation removes the file if it exists, and if successful, it logs a message indicating the deletion using the `printer.log` method. The `printer.log` method is designed to print styled log messages to the console, enhancing visibility into the application's operations.\n\nIf an exception occurs during the file deletion process, the method catches the exception and calls `printer.print_exception`, which logs an error message and prints the exception details to the console. This ensures that any issues encountered during the deletion process are communicated clearly to the developer or user.\n\nAfter attempting to delete the file, the method sets the `delete_on_init` attribute to False. This action prevents further deletions from occurring during the lifecycle of the instance, ensuring that the file will not be deleted again unless explicitly handled.\n\nThe relationship with the `printer.log` and `printer.print_exception` methods is significant, as they provide a robust logging mechanism that aids in debugging and monitoring the application's behavior. The logging of both successful deletions and exceptions contributes to a clear understanding of the instance's initialization process and any potential issues that may arise.\n\n**Note**: It is important to ensure that the `delete_on_init` attribute is appropriately set before creating an instance of this class. Additionally, the `save_path` should be a valid Path object that points to a file location, as the deletion operation relies on its existence. Proper usage of this constructor contributes to effective resource management and error handling within the application."
      ],
      "code_start_line": 37,
      "code_end_line": 47,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, **data):\n        super().__init__(**data)\n        if self.delete_on_init and self.save_path.exists():\n            try:\n                self.save_path.unlink(missing_ok=True)  # Delete the file if it exists\n                printer.log(f\"Deleted existing file: {self.save_path}\")\n            except Exception:\n                printer.print_exception(f\"Failed to delete file {self.save_path}\")\n                raise\n        # Set the flag to False to avoid further deletions\n        self.delete_on_init = False\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/rich_output.py/RichPrinter/print_exception"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "serialize_history",
      "md_content": [
        "**serialize_history**: The function of serialize_history is to convert a list of conversation history items into a serialized format, excluding any attributes that are set to None.\n\n**parameters**: The parameters of this Function.\n· history: List[HistoryItem]  \n   This parameter represents a list of `HistoryItem` objects that encapsulate individual entries in a conversation history.\n\n**Code Description**: The `serialize_history` method is designed to process a list of `HistoryItem` instances, which represent entries in a conversation. The method takes a single parameter, `history`, which is expected to be a list of `HistoryItem` objects. The function first initializes an empty list called `serialized_history` to hold the serialized representations of the history items.\n\nThe method then iterates over the last `max_history_length` number of items from the `history` list. This is achieved by slicing the list with `history[-self.max_history_length :]`, which ensures that only the most recent entries are considered for serialization. For each `HistoryItem` in this sliced list, the method calls the `model_dump` method on the item, passing `exclude_none=True` as an argument. This call to `model_dump` is responsible for converting the `HistoryItem` into a dictionary format while omitting any attributes that have a value of None.\n\nFinally, the method returns the `serialized_history` list, which contains the serialized representations of the selected `HistoryItem` instances. This serialized data can be useful for saving conversation history in a structured format, such as for storage in a database or for transmission over a network.\n\nThe `serialize_history` method is closely related to the `HistoryItem` class, which defines the structure of each entry in the conversation history. By utilizing the `model_dump` method from the `HistoryItem`, this function ensures that the serialization process respects the attributes defined in the `HistoryItem` class, providing a consistent and accurate representation of the conversation history.\n\n**Note**: It is important to ensure that the `max_history_length` attribute is defined within the class that contains the `serialize_history` method, as it dictates how many of the most recent history items will be serialized. The method is particularly useful in scenarios where only a limited portion of the conversation history is needed, such as when displaying recent interactions to a user or when preparing data for logging purposes.\n\n**Output Example**: A possible appearance of the code's return value could be as follows:\n```json\n[\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the weather like today?\",\n        \"tool_calls\": null,\n        \"tool_call_id\": null,\n        \"name\": null\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"The weather is sunny with a high of 75°F.\",\n        \"tool_calls\": null,\n        \"tool_call_id\": null,\n        \"name\": null\n    }\n]\n```"
      ],
      "code_start_line": 50,
      "code_end_line": 54,
      "params": [
        "self",
        "history"
      ],
      "have_return": true,
      "code_content": "    def serialize_history(self, history: List[HistoryItem]):\n        serialized_history = []\n        for item in history[-self.max_history_length :]:\n            serialized_history.append(item.model_dump(exclude_none=True))\n        return serialized_history\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/models.py/HistoryItem"
      ],
      "special_reference_type": [
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "custom_serialize",
      "md_content": [
        "**custom_serialize**: The function of custom_serialize is to apply custom serialization logic to handle different contexts, particularly focusing on transforming the history into a ShareGPT format when specified.\n\n**parameters**:\n· handler: A SerializerFunctionWrapHandler responsible for performing default serialization of the object.\n· info: A SerializationInfo object containing the context information, which can determine whether the ShareGPT format should be applied.\n\n**Code Description**:  \nThe `custom_serialize` function begins by invoking the `handler` to perform default serialization on the object. This results in the `result` variable, which contains the serialized data, including the `history` attribute.\n\nIf the `info` parameter contains a context with a key \"sharegpt\", the function processes the history attribute further to format it for the ShareGPT use case. The logic involves iterating over each item in the `history` and remapping the roles using a predefined `role_mapping` dictionary. The roles are mapped from:\n- `\"user\"` to `\"human\"`\n- `\"assistant\"` to `\"function_call\"`\n- `\"tool\"` to `\"observation\"`\n- `\"critic\"` to `\"critic\"`\n\nFor each item, the corresponding value (either content or tool call data) is extracted. If the role is `\"function_call\"`, the function checks if tool calls are associated with the item and formats them as JSON, including the function name and its arguments. If no tool calls are present, the value is directly assigned with the content associated with the \"gpt\" role.\n\nFor `\"observation\"` roles, the content is serialized as a JSON string.\n\nAfter processing all items in the history, the `conversations` list is populated with the formatted role-value pairs. Finally, the `result` object is updated to include the conversations and a serialized version of the `available_tools` attribute. This final output is then returned.\n\n**Note**:  \n- The `result[\"history\"]` is assumed to be a list that contains historical interactions with objects that have `role` and `content` attributes.\n- The handling of tool calls in the `\"function_call\"` role relies on the presence of a `tool_calls` attribute, which is expected to be a list of tool call objects.\n- The format of the final output is specifically structured to accommodate ShareGPT's data format, which includes the key `\"conversations\"` and `\"tools\"`.\n\n**Output Example**:  \nHere is a mock-up of what the final output might look like when the `sharegpt` context is provided:\n\n```json\n{\n  \"conversations\": [\n    {\n      \"from\": \"human\",\n      \"value\": \"Hello, how are you?\"\n    },\n    {\n      \"from\": \"gpt\",\n      \"value\": \"I'm doing well, thank you for asking!\"\n    },\n    {\n      \"from\": \"function_call\",\n      \"value\": \"{\\\"name\\\": \\\"weatherApi.getForecast\\\", \\\"arguments\\\": {\\\"location\\\": \\\"New York\\\"}}\"\n    },\n    {\n      \"from\": \"observation\",\n      \"value\": \"{\\\"temperature\\\": 72, \\\"condition\\\": \\\"Sunny\\\"}\"\n    }\n  ],\n  \"tools\": \"{\\\"weatherApi\\\": {\\\"name\\\": \\\"weatherApi\\\", \\\"description\\\": \\\"Provides weather data\\\"}}\"\n}\n```"
      ],
      "code_start_line": 57,
      "code_end_line": 113,
      "params": [
        "self",
        "handler",
        "info"
      ],
      "have_return": true,
      "code_content": "    def custom_serialize(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ):\n        \"\"\"\n        Custom serialization logic that handles different contexts, such as 'sharegpt'.\n        \"\"\"\n        # Perform default serialization\n        result = handler(self)\n\n        result = result[\"history\"]\n\n        if info.context and info.context.get(\"sharegpt\"):\n            # Transform history into ShareGPT format\n            # TODO: human 和 observation 必须出现在奇数位置，gpt 和 function 必须出现在偶数位置\n            conversations = []\n            for item in self.history:\n                role_mapping = {\n                    \"user\": \"human\",\n                    \"assistant\": \"function_call\",\n                    \"tool\": \"observation\",\n                    \"critic\": \"critic\",\n                }\n\n                role = role_mapping.get(item.role, item.role)\n                value = item.content if item.content else None\n\n                # For tool calls, include the tool_call_id or arguments\n                if role == \"function_call\":\n                    if item.tool_calls:\n                        for tool_call in item.tool_calls:\n                            value = json.dumps(\n                                {\n                                    \"name\": tool_call.function.name,\n                                    \"arguments\": tool_call.function.arguments,\n                                },\n                                ensure_ascii=True,\n                            )\n\n                            conversations.append({\"from\": role, \"value\": value})\n                    else:\n                        conversations.append({\"from\": \"gpt\", \"value\": value})\n\n                elif role == \"observation\":\n                    conversations.append(\n                        {\"from\": role, \"value\": json.dumps(value, ensure_ascii=True)}\n                    )\n\n                else:\n                    conversations.append({\"from\": role, \"value\": value})\n\n            # Build final output structure\n            result = {\n                \"conversations\": conversations,\n                \"tools\": json.dumps(self.available_tools, ensure_ascii=True),\n            }\n\n        return result\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "write",
      "md_content": [
        "**write**: The function of write is to append data to a JSON file, creating the necessary directories and handling potential issues with existing file data.\n\n**parameters**: The parameters of this Function.\n· data: dict - A dictionary containing the data to be written to the file.\n· path: Path | str - The path (as a string or Path object) to the file where the data will be saved.\n\n**Code Description**: The `write` function is responsible for managing the process of appending data to a specified JSON file. The function first ensures that the provided file path is a `Path` object, converting it from a string if necessary. It then verifies that the parent directories of the file exist, creating them if needed. If the file already exists, it attempts to read the existing data and ensures that it is a list (or initializes it as an empty list if the data is invalid or absent). The new data is then appended to this list, and the updated list is written back to the file in a JSON format, ensuring that the data is properly formatted (with indentation for readability).\n\nThe function handles potential errors that might occur during these operations, such as issues with file access or invalid data formats. In such cases, it logs the error message and raises the exception.\n\nFrom a project perspective, the `write` function is utilized by other parts of the code to persist conversation history or similar data. For instance, it is called by the `_auto_save` method in the `ConversationManager` class, where it is used to save the most recent conversation history to a file, if the `save_path` attribute is set. Additionally, the `write` function is called in the `run_tasks` function, which processes a list of tasks and logs conversation data after each task is completed.\n\n**Note**: When using this function, ensure that the path provided is correct, and be aware that it appends data to the file. This means that the file will grow in size over time as more data is added. Also, if the file contains corrupt or non-JSON data, the function will start with an empty list, potentially overwriting previous content.",
        "**write**: The function of write is to append a new data entry to a specified JSON file, ensuring that the file and its parent directories exist.\n\n**parameters**: The parameters of this Function.\n· data: dict - The data to be appended to the JSON file, expected to be in dictionary format.  \n· path: Path | str - The file path where the data will be written, which can be provided as a string or a Path object.\n\n**Code Description**: The write method is a function within the ConversationManager class that facilitates the process of saving conversation data to a JSON file. It begins by checking the type of the `path` parameter; if it is a string, it converts it to a Path object for consistency in file handling.\n\nThe method then attempts to create any necessary parent directories for the specified file path using the `mkdir` method with the `parents=True` and `exist_ok=True` flags. This ensures that the directory structure is in place before any file operations are attempted.\n\nNext, the method checks if the file at the specified path already exists. If it does, it opens the file in read mode and attempts to load the existing data as JSON. The method expects the data to be a list at the top level; if the data is not a list or if a JSONDecodeError occurs (indicating that the file is empty or corrupt), it initializes `existing_data` as an empty list.\n\nAfter ensuring that `existing_data` is a list, the method appends the new `data` entry to this list. It then opens the file in write mode and writes the updated list back to the file using `json.dump`, ensuring that the output is formatted correctly with indentation for readability.\n\nIf any exceptions occur during this process, the method captures them and utilizes the `print_exception` method from the RichPrinter class to log the error message and print the exception details to the console. This error handling mechanism is crucial for debugging and maintaining the robustness of the application.\n\nThe write method is called by the `_auto_save` method within the same class, which is responsible for automatically saving the most recent entry in the conversation history whenever a new message is added. The `_auto_save` method retrieves the last entry from the `history` list and prepares it for saving by invoking the `model_dump` method to exclude any None values. It then calls the write method with the prepared data and the specified save path.\n\nAdditionally, the write method is also invoked by the `execute_multiple_tasks` function in the tasks_runner module. This function processes a list of tasks and, if the save option is enabled, it retrieves the conversation data from the BaseAgent's conversation manager and saves it using the write method.\n\n**Note**: It is essential to ensure that the `path` parameter is valid and that the data being passed is in the correct dictionary format. If the file path is invalid or if the data is not structured as expected, the write operation may fail, leading to exceptions being raised. Proper error handling and validation of inputs are recommended to ensure smooth operation.",
        "**write**: The function of write is to append data to a specified file in JSON format, creating necessary directories and handling existing data appropriately.\n\n**parameters**: The parameters of this Function.\n· data: dict - The data to be appended to the file, expected to be in dictionary format.  \n· path: Path | str - The file path where the data will be written, which can be provided as a string or a Path object.\n\n**Code Description**: The write method is designed to manage the process of saving data to a file in a structured manner. It first checks the type of the provided path, converting it to a Path object if it is given as a string. This ensures compatibility with the subsequent file operations.\n\nThe method then attempts to create any necessary parent directories for the specified file path using the `mkdir` method, with the `parents=True` and `exist_ok=True` options to avoid errors if the directories already exist. \n\nNext, the method checks if the file already exists. If it does, it attempts to read the existing data. The method expects the existing data to be in JSON format, specifically a list at the top level. If the file is empty or contains corrupt data, it initializes an empty list to hold the data. This is crucial for ensuring that the write operation can proceed without errors.\n\nAfter reading the existing data, the method appends the new data (provided as a dictionary) to this list. It then writes the updated list back to the file in JSON format, ensuring that the output is properly formatted with indentation for readability.\n\nIn the event of any exceptions during these operations, the method calls the `print_exception` function from the RichPrinter class to log the error message and print the exception details to the console. This provides a clear mechanism for error reporting, aiding in debugging and monitoring the application's behavior.\n\nThe write method is called by the `_auto_save` function within the ConversationManager class. The `_auto_save` function is responsible for automatically saving the most recent conversation history entry whenever a new entry is added, provided that a valid save path is configured and the appropriate settings are enabled. This creates a seamless experience for users, as their conversation history is preserved without requiring manual intervention.\n\n**Note**: \n- The data parameter must be a well-formed dictionary to ensure successful serialization to JSON format.\n- The path parameter should point to a valid file location, and the necessary permissions must be in place for writing to that location.\n- Proper error handling is implemented to manage potential issues during file operations, ensuring that users receive informative feedback in case of failures."
      ],
      "code_start_line": 115,
      "code_end_line": 146,
      "params": [
        "self",
        "data",
        "path"
      ],
      "have_return": false,
      "code_content": "    def write(self, data: dict, path: Path | str):\n        if isinstance(path, str):\n            path = Path(path)\n\n        try:\n            # Create parent directories if they do not exist\n            path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Check if the file exists and read existing data\n            if path.exists():\n                with path.open(\"r\", encoding=\"utf-8\") as f:\n                    try:\n                        # Try to parse the existing JSON data (expecting a list at the top level)\n                        existing_data = json.load(f)\n                        if not isinstance(existing_data, list):\n                            existing_data = []  # Ensure it's a list\n                    except json.JSONDecodeError:\n                        # If the file is empty or corrupt, start with an empty list\n                        existing_data = []\n            else:\n                existing_data = []\n\n            # Append the new data to the existing array\n            existing_data.append(data)\n\n            # Write the updated array back to the file\n            with path.open(\"w\", encoding=\"utf-8\") as f:\n                json.dump(existing_data, f, ensure_ascii=True, indent=2)\n\n        except Exception:\n            printer.print_exception(f\"Failed to write to {path}\")\n            raise\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/models.py/ConversationManager/_auto_save"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print_exception"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_auto_save",
      "md_content": [
        "**_auto_save**: The function of _auto_save is to automatically save the most recent entry in the conversation history to a specified file path, provided the save path is set.\n\n**parameters**:  \n- None\n\n**Code Description**: The `_auto_save` method is a function within the `ConversationManager` class that is responsible for saving the most recent conversation history to a file, specifically the last entry in the `history` list. The method checks whether the `save_path` attribute is defined, and if it is, it uses the `write` method to persist the most recent entry in the conversation history. This entry is accessed using `self.history[-1]`, which fetches the last item in the `history` list, and then the `model_dump(exclude_none=True)` method is invoked on this entry to prepare it for saving. This method ensures that no `None` values are included in the saved data. The resulting data is then passed to the `write` method, which handles the task of appending this data to a file at the location specified by the `save_path`.\n\nThe `_auto_save` function is called automatically whenever a new message is added to the conversation history. Specifically, it is invoked by the `append_to_history` method, which adds new conversation entries to the `history` list. Once a new message is appended to the history, `_auto_save` is triggered to ensure that the updated history is saved immediately. This process facilitates the continuous preservation of the conversation data without requiring manual intervention from the user.\n\nThe `write` method, which is invoked by `_auto_save`, is responsible for appending data to a JSON file. It manages the creation of necessary directories, reads any existing data from the file, and ensures that the data is in the correct format before appending new content. This integration ensures that the conversation history is consistently updated and saved to disk.\n\n**Note**: It is important to ensure that the `save_path` attribute is set correctly in order for the `_auto_save` function to work as expected. If the `save_path` is not provided or is invalid, no data will be saved. Additionally, the data being saved is the most recent entry in the `history` list, meaning only the latest message or event is preserved during each save operation.",
        "**_auto_save**: The function of _auto_save is to automatically save the most recent conversation history entry if a valid save path is configured.\n\n**parameters**:  \n- None.\n\n**Code Description**:  \nThe `_auto_save` function is responsible for ensuring that the latest conversation history entry is saved to a specified file path after each update, provided that a valid save path (`self.save_path`) is configured and that the `settings.save_sharegpt` is enabled.\n\nThe function first checks the `settings.save_sharegpt` flag. If this flag is `True`, the function proceeds to save the data. It does so by calling the `write` method of the `ConversationManager` class. The data to be saved is retrieved by accessing the last entry in the `self.history` list, which stores the conversation history. The most recent history entry is passed through the `model_dump` method, which serializes it while excluding any `None` values. This ensures that only meaningful data is saved. The data, along with the path specified by `self.save_path`, is then passed to the `write` method.\n\nThe `write` method, which is called within `_auto_save`, handles the task of appending the provided data to the specified file. It ensures that the file's parent directories are created if they do not already exist, reads the existing file data if available, appends the new data, and writes the updated content back to the file in JSON format.\n\nThe `_auto_save` function is called indirectly from the `append_to_history` method, which is responsible for adding new messages to the conversation history. After a new history item is appended, `_auto_save` is invoked to automatically save the updated history to the file. This creates an automated mechanism for persisting the conversation's progress after each new update.\n\n**Note**:  \n- The function will only save the conversation history if the `settings.save_sharegpt` flag is set to `True` and if a valid `save_path` is provided.  \n- The `self.history[-1]` is expected to be a valid entry in the conversation history, and it must be serializable by the `model_dump` method.  \n- If `self.save_path` is not properly set or if the `settings.save_sharegpt` flag is `False`, the function will not perform any saving operations.  \n- Proper configuration of the `save_path` and `settings.save_sharegpt` is necessary for the `_auto_save` function to operate correctly."
      ],
      "code_start_line": 148,
      "code_end_line": 153,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def _auto_save(self):\n        \"\"\"Auto save after each update if save_path is set\"\"\"\n        if settings.save_sharegpt:\n            self.write(\n                path=self.save_path, data=self.history[-1].model_dump(exclude_none=True)\n            )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/models.py/ConversationManager/append_to_history"
      ],
      "reference_who": [
        "src/criticsearch/models.py/ConversationManager/write"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "append_to_history",
      "md_content": [
        "### Function: `append_to_history`\n\n**Purpose**:  \nThe `append_to_history` function is designed to add a new message to the conversation history in a `ConversationManager` instance.\n\n**Parameters**:\n- `role` (Literal[\"user\", \"assistant\", \"tool\", \"critic\"]):  \n  Specifies the role of the entity contributing to the conversation. The value can be one of the following:\n  - `\"user\"`: Represents the user initiating the message.\n  - `\"assistant\"`: Represents the assistant responding to the user.\n  - `\"tool\"`: Represents an automated tool interacting within the conversation.\n  - `\"critic\"`: Represents a critic providing feedback or evaluation.\n\n- `content` (Optional[str]):  \n  The actual content of the message. This is an optional parameter and can be set to `None` if no content is provided.\n\n- `**kwargs`:  \n  Additional optional keyword arguments that can be passed to the `HistoryItem` constructor. These arguments can be used for providing extra context, such as tool calls or identifiers.\n\n**Functionality**:  \nThis method creates a new entry in the conversation history by instantiating a `HistoryItem` object with the provided `role`, `content`, and any additional keyword arguments. The newly created `HistoryItem` is appended to the `history` attribute, which is a list that stores the conversation history. After adding the new history item, the method triggers the `_auto_save` function to automatically save the updated conversation history.\n\n**Usage**:  \nThis function is used to maintain and update the flow of a conversation by systematically adding new messages from different participants (user, assistant, tool, or critic) and ensuring that the history is saved for future reference. It plays a crucial role in tracking the progression of conversations over time.\n\n**Example**:  \n```python\nconversation_manager.append_to_history(role=\"user\", content=\"Hello, how can I assist you today?\")\n```\n\nIn this example, a new message from the user is added to the conversation history. The content of the message is \"Hello, how can I assist you today?\" and the role is set as `\"user\"`.\n\n**Related Methods**:  \n- `_auto_save`: Automatically saves the most recent conversation entry to a file after the message is added to history.",
        "**append_to_history**: The function of append_to_history is to add a new message to the conversation history.\n\n**parameters**: The parameters of this Function.\n· role: Literal[\"user\", \"assistant\", \"tool\", \"critic\"] - This parameter defines the role of the entity contributing to the conversation, which can be one of the following: \"user\", \"assistant\", \"tool\", or \"critic\".\n· content: Optional[str] - This parameter holds the actual content of the message. It is optional and may be `None` if not provided.\n· kwargs: Additional keyword arguments that can be passed to provide more context or information related to the message being appended.\n\n**Code Description**: The append_to_history function is a method within the ConversationManager class that is responsible for appending a new entry to the conversation history. When invoked, it creates a new instance of the HistoryItem class, which represents an individual entry in the conversation history. The function takes in the role of the entity sending the message, the content of the message, and any additional keyword arguments that may be relevant.\n\nUpon execution, the function appends the newly created HistoryItem instance to the history attribute of the ConversationManager. This history attribute is a list that maintains the sequence of messages exchanged during the conversation. After appending the new message, the function calls the _auto_save method to ensure that the updated conversation history is saved automatically, provided that a valid save path is configured.\n\nThe append_to_history function is called in various contexts throughout the project. For instance, it is invoked within the common_chat function of the BaseAgent class, where it appends the model's response to the conversation history after processing a user prompt. Additionally, it is utilized in the process_single_task function, where user queries and responses from the agent are logged into the conversation history. This systematic logging of messages is crucial for maintaining a coherent and traceable dialogue flow, allowing for effective analysis and debugging of the conversation interactions.\n\nIn summary, append_to_history plays a vital role in managing the conversation history by ensuring that each message is accurately recorded along with its associated role and content. This functionality is essential for applications that rely on conversational agents, as it provides a structured way to track interactions over time.\n\n**Note**: It is important to ensure that the role parameter is correctly specified as one of the allowed literals (\"user\", \"assistant\", \"tool\", \"critic\") to avoid errors. The content parameter can be omitted if there is no message content to log, but it should be provided whenever applicable to maintain a complete conversation history. The use of additional keyword arguments (kwargs) allows for flexibility in capturing various aspects of the conversation, enhancing the richness of the logged data.",
        "**append_to_history**: The function of append_to_history is to add a new message to the conversation history.\n\n**parameters**: The parameters of this Function.\n· role: Literal[\"user\", \"assistant\", \"tool\", \"critic\"]  \nThis parameter defines the role of the entity contributing to the conversation, such as \"user\", \"assistant\", \"tool\", or \"critic\".  \n· content: Optional[str]  \nThis parameter holds the actual content of the message. It is optional and may be `None` if not provided.  \n· kwargs: Additional keyword arguments that can be passed to include more context or information related to the message.\n\n**Code Description**: The append_to_history function is a method within the ConversationManager class that is responsible for recording new entries in the conversation history. When invoked, it creates a new instance of the HistoryItem class, which encapsulates the details of the message being added, including the role of the sender and the content of the message. The function appends this HistoryItem instance to the history attribute, which is a list that maintains the sequence of messages exchanged during the conversation.\n\nThe function also calls the _auto_save method after appending the new message to the history. This ensures that the latest entry is automatically saved to a specified file path if the configuration allows for it. The _auto_save method checks if the settings permit saving and then writes the most recent history entry to the designated file, ensuring that the conversation history is preserved for future reference.\n\nThe append_to_history function is called in various contexts within the project, particularly in the chat method of the BaseAgent class. In this context, when the assistant generates a response to a user prompt, it records the interaction by calling append_to_history with the role set to \"assistant\" and the content being the response generated by the assistant. This systematic logging of messages is crucial for maintaining a coherent dialogue flow and for later analysis of the conversation.\n\nAdditionally, the function is also utilized in the process_single_task function, where it records the initial user question as part of the conversation history. This ensures that all interactions, both from the user and the assistant, are captured and can be reviewed or analyzed later.\n\n**Note**: \n- The content parameter can be omitted for certain roles, such as \"tool\" or \"critic\", where the primary focus may not be on the message content.\n- The use of kwargs allows for flexibility in adding additional context or information to the message being recorded.\n- Proper configuration of the save path and settings is necessary for the _auto_save function to operate correctly, ensuring that conversation history is preserved.",
        "**append_to_history**: The function of append_to_history is to add a new message to the conversation history.\n\n**parameters**: The parameters of this Function.\n· role: Literal[\"user\", \"assistant\", \"tool\", \"critic\"] - This parameter defines the role of the entity contributing to the conversation, such as \"user\", \"assistant\", \"tool\", or \"critic\".\n· content: Optional[str] - This parameter holds the actual content of the message. It is optional and may be `None` if not provided.\n· kwargs: Additional keyword arguments that can be passed to include more context or information related to the message.\n\n**Code Description**: The append_to_history method is a crucial function within the ConversationManager class, responsible for managing the conversation history in a structured manner. When invoked, it creates a new instance of the HistoryItem class, which encapsulates the details of the message being added to the history. The role of the contributor is specified through the role parameter, while the content parameter contains the actual message text. The method also allows for additional attributes to be included through kwargs, providing flexibility in what information can be recorded.\n\nUpon execution, the method appends the newly created HistoryItem to the history list maintained by the ConversationManager. This ensures that all interactions within the conversation are logged sequentially, preserving the context and flow of dialogue. Following the addition of the new message, the _auto_save method is called to automatically save the updated conversation history if the appropriate conditions are met, such as having a valid save path configured.\n\nThe append_to_history method is called in various contexts throughout the project. For instance, it is utilized in the chat method of the BaseAgent class, where the assistant's responses are recorded after generating a reply to the user's prompt. Additionally, it is invoked in the process_single_task function, where the user's initial question is logged into the conversation history. This systematic logging of messages is essential for maintaining a coherent dialogue and for later analysis or retrieval of conversation data.\n\n**Note**: It is important to ensure that the role parameter accurately reflects the contributor's identity in the conversation. The content parameter can be omitted for certain roles, such as \"tool\" or \"critic\", where the primary focus may be on the interaction rather than the message content. The use of kwargs allows for additional contextual information to be included, enhancing the richness of the conversation history. Proper configuration of the save path is necessary for the _auto_save functionality to operate effectively, ensuring that conversation history is preserved after each update."
      ],
      "code_start_line": 155,
      "code_end_line": 165,
      "params": [
        "self",
        "role",
        "content"
      ],
      "have_return": false,
      "code_content": "    def append_to_history(\n        self,\n        role: Literal[\"user\", \"assistant\", \"tool\", \"critic\"],\n        content: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Add a new message to the conversation history.\n        \"\"\"\n        self.history.append(HistoryItem(role=role, content=content, **kwargs))\n        self._auto_save()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/models.py/ConversationManager/append_tool_call_to_history",
        "src/criticsearch/models.py/ConversationManager/append_tool_call_result_to_history",
        "tests/test_conversation_format_saving.py/test_conversation_roundtrip"
      ],
      "reference_who": [
        "src/criticsearch/models.py/HistoryItem",
        "src/criticsearch/models.py/ConversationManager/_auto_save"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "append_tool_call_to_history",
      "md_content": [
        "**append_tool_call_to_history**: The function of append_tool_call_to_history is to add a tool call entry to the conversation history.\n\n**parameters**: The parameters of this Function.\n· tool_calls: List[ChatCompletionMessageToolCall] - A list of tool call entries that are to be added to the conversation history.\n· content: Optional[str] - An optional string that represents additional content related to the tool calls. If not provided, it defaults to None.\n\n**Code Description**: The append_tool_call_to_history function is a method within the ConversationManager class that is responsible for appending tool call entries to the conversation history. It takes two parameters: a list of tool calls and an optional content string. The primary purpose of this function is to maintain a record of interactions that involve automated tools within the conversation flow.\n\nWhen invoked, the function calls another method, append_to_history, passing along the role of \"assistant\", the list of tool calls, and any additional content. The append_to_history method is responsible for creating a new entry in the conversation history, which is stored as a list of HistoryItem objects. This ensures that every interaction involving tools is documented, allowing for a comprehensive view of the conversation's progression.\n\nThe append_tool_call_to_history function is called within the search_and_browse method of the BaseAgent class. In this context, it is used to log the tool calls generated during a search operation. If the search_with_tool_response contains tool calls, the function is invoked to append these calls to the conversation history. This is crucial for tracking the actions taken by the assistant and the tools it interacts with, ensuring that the conversation history remains accurate and up-to-date.\n\nIn summary, append_tool_call_to_history plays a vital role in maintaining the integrity of the conversation history by systematically recording tool interactions, which can be referenced later for analysis or debugging.\n\n**Note**: It is important to ensure that the tool_calls parameter is always provided as a list of ChatCompletionMessageToolCall objects to avoid errors during the history appending process. Additionally, the content parameter is optional and can be omitted if there is no additional information to record.",
        "**append_tool_call_to_history**: The function of append_tool_call_to_history is to add a tool call entry to the conversation history.\n\n**parameters**:\n- tool_calls: List[ChatCompletionMessageToolCall] - A list of tool calls to be appended to the conversation history.\n- content: Optional[str] - Optional content to include with the tool calls. This can be `None` if not required.\n\n**Code Description**:  \nThe append_tool_call_to_history function is a method within the ConversationManager class, designed to record tool call entries into the conversation history. It accepts two main parameters: `tool_calls`, which is a list of tool call objects, and an optional `content` parameter. The purpose of this function is to ensure that any tool call interaction made by the assistant is captured and stored in the conversation history for future reference.\n\nUpon invocation, the function delegates the task of appending the tool calls to the history by invoking the `append_to_history` method. It passes along the role as \"assistant\", indicating that the tool calls are being made by the assistant in the context of the conversation. The `tool_calls` are passed as-is, and any additional `content` provided will be included with the entry if it is not `None`. \n\nThis method ensures that every interaction with external tools or services is systematically logged into the conversation history, which is crucial for maintaining a coherent dialogue flow and for later analysis. The `append_to_history` method, which is called inside this function, further appends the tool call details into the conversation history and ensures that the information is saved if auto-save functionality is enabled.\n\nThe `append_tool_call_to_history` method plays a crucial role in contexts where the assistant interacts with external tools to gather or process data. For example, when a web scraping tool is called to extract content based on user queries or search results, the tool calls related to this action will be captured through this method, allowing the conversation history to retain full visibility of tool usage.\n\n**Reference Relationships in the Project**:\n- The `append_tool_call_to_history` function is called in methods such as `web_scrape_results` and `search_and_browse`, which involve web scraping and search operations. In both methods, the tool calls generated during these processes are passed to `append_tool_call_to_history` to ensure they are logged into the conversation history.\n- Specifically, in the `search_and_browse` function, after initiating a search through the common chat interface, the tool calls (if any) are appended to the history using this function. Similarly, after web scraping, any tool calls related to the scraping process are logged in the same manner.\n\n**Note**: \n- It is important to ensure that the `tool_calls` parameter is provided as a valid list of tool call objects. The content parameter is optional and should be included when additional context or information needs to be logged along with the tool calls.\n- This function ensures that all interactions with tools are properly recorded, which is essential for tracking the flow of the conversation, debugging, and improving the system’s response accuracy."
      ],
      "code_start_line": 167,
      "code_end_line": 175,
      "params": [
        "self",
        "tool_calls",
        "content"
      ],
      "have_return": false,
      "code_content": "    def append_tool_call_to_history(\n        self,\n        tool_calls: List[ChatCompletionMessageToolCall],\n        content: Optional[str] = None,\n    ):\n        \"\"\"\n        Add a tool call entry to the conversation history.\n        \"\"\"\n        self.append_to_history(role=\"assistant\", tool_calls=tool_calls, content=content)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse"
      ],
      "reference_who": [
        "src/criticsearch/models.py/ConversationManager/append_to_history"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "append_tool_call_result_to_history",
      "md_content": [
        "**append_tool_call_result_to_history**: The function of `append_tool_call_result_to_history` is to add the result of a tool call to the conversation history.\n\n**parameters**: The parameters of this function are:\n- `tool_call_id`: A string representing the unique identifier of the tool call.\n- `name`: A string representing the name of the tool involved in the call.\n- `content`: A string representing the result or content generated from the tool call.\n\n**Code Description**: \nThe `append_tool_call_result_to_history` function is designed to add the result of a tool call to the conversation history in a structured manner. This is achieved by calling the `append_to_history` method, which is responsible for adding an entry to the conversation history. The `role` parameter in `append_to_history` is set to `\"tool\"`, signifying that the message is coming from an automated tool. Along with this, the `tool_call_id`, `name`, and `content` are passed as keyword arguments to ensure that the specific details of the tool call are included in the history entry.\n\nThis function is called within the `search_and_browse` method of the `BaseAgent` class, which handles the interaction between the agent and external tools. Specifically, after making a tool call to search or scrape data, the results are passed to `append_tool_call_result_to_history` to be logged into the conversation history. This is crucial for maintaining a structured log of tool activities in the ongoing conversation.\n\nWhen tool calls occur during the search and web scraping processes, `append_tool_call_result_to_history` ensures that the relevant tool call results are captured and saved in the conversation history. This allows for easy tracking of the results generated by tools like search aggregators and content scrapers.\n\nIn summary, this function is used to log the results of tool calls in the conversation history, providing a complete and organized record of the interactions involving automated tools.\n\n**Note**: The content parameter passed to this function should contain the result of the tool's operation (such as search results or web scraping data) and should be a string that accurately reflects the output generated by the tool.",
        "**append_tool_call_result_to_history**: The function of append_tool_call_result_to_history is to add the result of a tool call to the conversation history.\n\n**parameters**: The parameters of this Function.\n· tool_call_id: str - This parameter represents the unique identifier of the tool call being recorded in the conversation history.\n· name: str - This parameter specifies the name of the tool that was called.\n· content: str - This parameter contains the result or output generated by the tool call.\n\n**Code Description**: The append_tool_call_result_to_history function is a method within the ConversationManager class that facilitates the logging of results from tool calls into the conversation history. When this function is invoked, it takes three parameters: tool_call_id, name, and content. These parameters are essential for accurately documenting the interaction with the tool.\n\nThe function operates by calling the append_to_history method, which is responsible for appending a new entry to the conversation history. In this context, the role is set to \"tool,\" indicating that the entry pertains to a tool's output. The tool_call_id, name, and content are passed as arguments to provide a comprehensive record of the tool's activity.\n\nThe append_to_history method, which is invoked within append_tool_call_result_to_history, plays a critical role in maintaining the integrity of the conversation history. It ensures that each tool interaction is logged systematically, allowing for effective tracking and analysis of the conversation flow. The history attribute of the ConversationManager class is a list that stores all entries, and each entry is represented by an instance of the HistoryItem class.\n\nThis function is called in various contexts, specifically within the web_scrape_results and search_and_browse methods of the BaseAgent class. In web_scrape_results, after executing a tool call for web scraping, the results are logged using append_tool_call_result_to_history. Similarly, in search_and_browse, the function is used to log the results of search queries processed by the agent. This systematic logging is crucial for maintaining a coherent dialogue and ensuring that all interactions with tools are recorded for future reference.\n\n**Note**: It is important to ensure that the parameters provided to the function are accurate and relevant. The tool_call_id should correspond to a valid tool call, the name should reflect the tool's identity, and the content should contain the actual results generated by the tool. Proper usage of this function contributes to a well-structured conversation history, which is essential for applications relying on conversational agents.",
        "## Function Documentation: `append_tool_call_result_to_history`\n\n### Purpose:\nThe function `append_tool_call_result_to_history` is responsible for adding the results of a tool call to the conversation history. This allows the tracking of tool-related interactions within the conversation flow.\n\n### Parameters:\n- `tool_call_id` (str): A unique identifier for the tool call being logged. This ID is essential for tracking the specific tool call within the conversation history.\n- `name` (str): The name of the tool that was invoked. This helps to identify which tool was used during the conversation.\n- `content` (str): The result or output from the tool call that is being appended to the conversation history.\n\n### Functionality:\nThe `append_tool_call_result_to_history` function is invoked when there is a need to log the result of an external tool call. It utilizes the `append_to_history` method from the `ConversationManager` class to append the tool call result to the conversation history. The function passes the role as `\"tool\"` to signify that the content originates from a tool interaction. The parameters `tool_call_id`, `name`, and `content` are also forwarded to ensure the tool call’s relevant details are accurately recorded.\n\n### Key Considerations:\n- The `append_to_history` method handles the actual insertion of the data into the history. It creates an entry with the provided tool call details and ensures the conversation history is maintained properly.\n- This function is part of a structured approach for logging tool interactions, preserving the integrity and flow of the conversation.\n\n### Usage:\nThis function is primarily used when a tool call has been made within the conversation, and the results need to be documented for future reference or analysis. It plays a critical role in keeping a comprehensive record of all external tool interactions.\n\n### Example:\n\n```python\nconversation_manager.append_tool_call_result_to_history(\n    tool_call_id=\"12345\",\n    name=\"web_scraper\",\n    content=\"Successfully scraped content from the specified URLs.\"\n)\n```\n\nIn this example, a tool call result from the \"web_scraper\" tool is added to the conversation history, with a unique tool call ID and the content describing the success of the scraping operation.\n\n### Conclusion:\nThe `append_tool_call_result_to_history` function is an essential component for maintaining a complete and accurate conversation history, especially when external tools are involved in generating or processing content. By leveraging the `append_to_history` method, it ensures that tool-related interactions are properly logged and stored for future use."
      ],
      "code_start_line": 177,
      "code_end_line": 185,
      "params": [
        "self",
        "tool_call_id",
        "name",
        "content"
      ],
      "have_return": false,
      "code_content": "    def append_tool_call_result_to_history(\n        self, tool_call_id: str, name: str, content: str\n    ):\n        \"\"\"\n        Add a tool call result to the conversation history.\n        \"\"\"\n        self.append_to_history(\n            role=\"tool\", tool_call_id=tool_call_id, name=name, content=content\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "tests/test_conversation_format_saving.py/test_conversation_roundtrip"
      ],
      "reference_who": [
        "src/criticsearch/models.py/ConversationManager/append_to_history"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "clear_history",
      "md_content": [
        "**clear_history**: The function of clear_history is to remove all entries from the conversation history.\n\n**parameters**: The clear_history function does not take any parameters.\n\n**Code Description**: The clear_history function is a method that belongs to a class, presumably related to managing conversations. When invoked, this method performs a single action: it clears the entire conversation history stored in the object's history attribute. The history attribute is expected to be a data structure that supports the clear operation, such as a list or a similar collection. By calling self.history.clear(), the method effectively empties this collection, ensuring that no previous conversation data remains accessible. This functionality is particularly useful in scenarios where a fresh start is required, such as resetting a chat interface or preparing for a new session without any prior context.\n\n**Note**: It is important to be aware that once the clear_history method is executed, all conversation data will be permanently deleted and cannot be recovered. Therefore, it is advisable to use this method with caution, especially in applications where conversation history may be needed for reference or analysis.",
        "**clear_history**: The function of clear_history is to clear the entire conversation history.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: The clear_history function is a method of the ConversationManager class that is responsible for clearing all stored conversation history. When invoked, it calls the clear method on the history attribute, which is expected to be a data structure (such as a list or dictionary) that holds the records of past interactions. This action effectively resets the conversation state, allowing for a fresh start without any previous context.\n\nThe clear_history function is utilized in the execute_multiple_tasks function and the run function within the tasks_runner module. In both cases, it is called at the beginning of the task execution process. This ensures that any previous conversation history is removed before processing new tasks. Specifically, in execute_multiple_tasks, clear_history is called in a loop for each task, ensuring that each task starts with a clean slate. Similarly, in the run function, clear_history is invoked before processing a single task, maintaining consistency in the handling of conversation history.\n\nBy clearing the history at the start of task execution, the system can avoid potential confusion or errors that might arise from residual data from prior interactions. This is particularly important in scenarios where tasks may be related but should not carry over context from previous executions.\n\n**Note**: It is essential to understand that invoking clear_history will permanently remove all previous conversation data. Therefore, if there is a need to retain any part of the conversation history for future reference, appropriate measures should be taken to save that data before calling this function.",
        "**clear_history**: The function of clear_history is to clear the entire conversation history.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: The clear_history function is a method of the ConversationManager class, which is responsible for managing the conversation history within the application. When invoked, this method calls the clear method on the history attribute of the ConversationManager instance. This action effectively removes all stored conversation data, ensuring that subsequent tasks or interactions begin with a clean slate.\n\nThe clear_history method is utilized in various parts of the project, particularly in the execute_multiple_tasks and run functions found in the tasks_runner module. In execute_multiple_tasks, clear_history is called at the beginning of the function to ensure that each writing task is executed without any influence from previous tasks. This is crucial for maintaining the integrity of the task execution process, as it prevents any residual context from affecting the outcomes of new tasks.\n\nSimilarly, in the run function, clear_history is invoked to reset the conversation state before processing a specific task. This guarantees that each task is handled independently, allowing for accurate and isolated execution of the task logic.\n\nThe relationship between clear_history and its callers highlights its role as a foundational step in the workflow of task execution. By ensuring that the conversation history is cleared before each task, the application can maintain a consistent and reliable environment for processing user instructions.\n\n**Note**: It is important to recognize that invoking clear_history will permanently delete all existing conversation data. Users should ensure that any necessary information is saved or backed up before calling this method, as the action cannot be undone."
      ],
      "code_start_line": 187,
      "code_end_line": 191,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def clear_history(self):\n        \"\"\"\n        Clear the entire conversation history.\n        \"\"\"\n        self.history.clear()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tasks_runner.py/execute_multiple_tasks",
        "src/criticsearch/tasks_runner.py/execute_from_mapping/run",
        "tests/test_conversation_format_saving.py/clear_history_and_stub"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tasks_runner.py": [
    {
      "type": "FunctionDef",
      "name": "execute_multiple_tasks",
      "md_content": [
        "**execute_multiple_tasks**: The function of execute_multiple_tasks is to execute a series of tasks iteratively, process them, and log the conversation history.\n\n**parameters**: The parameters of this Function.\n· tasks: list - A list of task strings (questions) that need to be processed.  \n· max_iterations: int - The maximum number of iterations allowed for each task, defaulting to 10.  \n· output_file: Path | str - The file path where the conversation history will be saved in ShareGPT format, defaulting to \"conversation_history_sharegpt.jsonl\".\n\n**Code Description**: The execute_multiple_tasks function is designed to handle the execution of multiple tasks in a systematic manner. It takes a list of tasks and processes each one individually by calling the process_single_task function. This function is responsible for executing a single task, which involves interacting with various agents to generate a comprehensive report based on the provided task and specified maximum iterations.\n\nDuring the execution of each task, the function checks if the setting to save conversation history is enabled. If it is, the function retrieves the conversation data from the BaseAgent's conversation manager and saves it to the specified output file using the write method. This ensures that the conversation history is logged for future reference.\n\nThe execute_multiple_tasks function is called by the start_task_execution function, which serves as the entry point for executing predefined tasks. In this context, start_task_execution initializes a list of tasks and specifies the maximum number of iterations before invoking execute_multiple_tasks to process the tasks.\n\nThe function relies on the proper functioning of the process_single_task function, which manages the execution of each individual task, and the BaseAgent class, which provides the necessary infrastructure for managing conversations and interactions with agents. Additionally, the write method from the ConversationManager class is utilized to handle the saving of conversation data.\n\n**Note**: It is important to ensure that the tasks provided are well-defined and that the output file path is valid. The function's behavior is contingent upon the settings for saving conversation history, and proper error handling should be in place to manage any exceptions that may arise during the execution process.",
        "**execute_multiple_tasks**: The function of execute_multiple_tasks is to sequentially execute multiple writing tasks and save the conversation history for each task.\n\n**parameters**: The parameters of this Function.\n· tasks (list[str]): A list of writing instructions to be executed.\n· output_file (Path|str): (Unused) Retained for compatibility with the previous version of the interface.\n· file_name (str|None): If specified, it will be used as a prefix; otherwise, a prefix will be generated from the task text.\n· conv_dir (Path|str): The directory where conversation history files will be saved, created automatically if necessary.\n\n**Code Description**: The execute_multiple_tasks function is designed to handle the execution of multiple writing tasks in a sequential manner. Initially, it clears the conversation history using the clear_history method from the BaseAgent's conversation_manager. This ensures that each task starts with a clean slate, preventing any residual context from previous tasks from affecting the current execution.\n\nFor each task in the provided list, the function calls process_single_task, which is responsible for executing the individual task and managing the interactions between various agents. After processing each task, the function generates a timestamp and constructs a filename for saving the conversation history. The filename is derived from either the provided file_name parameter or the task text, ensuring that it is unique and informative.\n\nThe conversation history is then dumped into a JSON file within the specified conv_dir directory. The function utilizes the model_dump method of the conversation_manager to retrieve the current conversation state, which is then saved in a structured format. The output file is named using the format `<prefix>_<timestamp>_conversation.json`, where `<prefix>` is determined based on the task or the provided file_name.\n\nThis function is called by the start_task_execution function, which serves as the entry point for executing predefined tasks. The start_task_execution function prepares the list of tasks and invokes execute_multiple_tasks, passing the necessary parameters. This relationship highlights the role of execute_multiple_tasks in managing the iterative execution of tasks and the logging of conversation history.\n\n**Note**: It is important to ensure that the tasks provided to this function are well-defined and relevant to the capabilities of the underlying agents. Additionally, users should be aware that invoking this function will overwrite any existing conversation history in the specified directory, as the clear_history method is called at the beginning of each task execution.",
        "**execute_multiple_tasks**: The function of execute_multiple_tasks is to sequentially execute multiple writing tasks and save the conversation history for each task.\n\n**parameters**: The parameters of this Function are as follows:\n· tasks (list[str]): A list of writing instructions to be executed.\n· output_file (Path|str): An optional parameter that is not used in the current implementation but retained for compatibility with previous versions of the interface.\n· file_name (str|None): An optional prefix for the output file name; if not specified, the prefix is generated from the task text.\n· conv_dir (Path|str): The directory where conversation history files will be saved, which is automatically created if it does not exist.\n\n**Code Description**: The execute_multiple_tasks function is designed to handle the execution of a series of writing tasks in a sequential manner. It begins by ensuring that the specified conversation directory exists, creating it if necessary. For each task in the provided list, the function performs the following steps:\n\n1. It clears the conversation history using the clear_history method from the ConversationManager class. This ensures that each task starts with a clean slate, preventing any residual context from previous tasks from affecting the current execution.\n\n2. The function then calls the process_single_task function, passing the current task and the optional file_name parameter. This function is responsible for managing the execution of a single task, including initializing an agent, processing the task, and generating a report based on the task's requirements.\n\n3. If the process_single_task function encounters any errors, such as a FileNotFoundError or JSONDecodeError, it catches these exceptions, logs an error message, and continues to the next task without terminating the execution process.\n\n4. Upon successful completion of a task, the function generates a timestamp and constructs a filename for the conversation history based on the specified file_name or the task text. It then saves the conversation history in JSON format to the designated directory.\n\n5. Throughout the execution, informative messages are printed to the console to indicate the progress of the task execution and the saving of conversation histories.\n\nThe execute_multiple_tasks function is called by the start_task_execution function, which serves as the command-line interface (CLI) entry point for initiating task execution. This establishes a clear relationship between the user input from the command line and the underlying task execution logic. The start_task_execution function parses command-line arguments and determines whether to execute tasks directly or from a mapping file, ultimately delegating the execution of tasks to execute_multiple_tasks when appropriate.\n\n**Note**: It is important to ensure that the tasks provided are valid and well-defined. Additionally, users should be aware that invoking clear_history will permanently delete all existing conversation data, so any necessary information should be backed up before calling this method. The output_file parameter is currently not utilized in the function, and its presence is primarily for backward compatibility."
      ],
      "code_start_line": 13,
      "code_end_line": 49,
      "params": [
        "tasks",
        "output_file",
        "file_name",
        "conv_dir"
      ],
      "have_return": false,
      "code_content": "def execute_multiple_tasks(\n    tasks: list,\n    output_file: Path | str = \"conversation_history_sharegpt.jsonl\",\n    file_name: str = None,\n    conv_dir: Path | str = \"conversation_histories\",\n):\n    \"\"\"\n    顺序执行多个写作任务，并为每个任务保存对话历史。\n\n    先清空会话管理器，然后调用 process_single_task 完成单个任务。\n    完成后将对话历史保存到 conv_dir/<前缀>_<timestamp>_conversation.json。\n\n    Args:\n        tasks (list[str]): 待执行的写作指令列表。\n        output_file (Path|str): （未使用）保留以兼容上一版接口。\n        file_name (str|None): 如果指定，则将其作为前缀；否则使用任务文本生成前缀。\n        conv_dir (Path|str): 保存对话历史文件的目录，按需自动创建。\n\n    Returns:\n        None\n    \"\"\"\n    conv_dir = Path(conv_dir)\n    conv_dir.mkdir(parents=True, exist_ok=True)\n\n    for task in tasks:\n        try:\n            BaseAgent.conversation_manager.clear_history()\n            records = process_single_task(task, file_name=file_name)\n        except (FileNotFoundError, json.JSONDecodeError) as e:\n            print(f\"[ERROR] skip task `{task}` due to invalid JSON file `{file_name}`: {e}\")\n            continue\n        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        prefix = Path(file_name).stem if file_name else task.replace(\" \", \"_\")[:50]\n        out_path = conv_dir / f\"{prefix}_{ts}_conversation.json\"\n        with out_path.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(records, f, ensure_ascii=False, indent=2)\n        print(f\"[INFO] saved conversation to {out_path}\")\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tasks_runner.py/start_task_execution"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/models.py/ConversationManager/clear_history"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "execute_from_mapping",
      "md_content": [
        "**execute_from_mapping**: The function of execute_from_mapping is to batch schedule writing tasks from an instruction mapping file, supporting both sequential and concurrent execution.\n\n**parameters**: The parameters of this Function are as follows:\n· mapping_file: Path|str - The path to the instruction_mapping.json file.\n· concurrent: bool - A flag indicating whether to execute all tasks concurrently.\n· workers: int - The maximum number of threads to use in concurrent mode.\n· limit: int|None - An optional limit on the number of entries to execute; None indicates no limit.\n· conv_dir: Path|str - The directory where conversation histories will be saved.\n\n**Code Description**: The execute_from_mapping function is designed to facilitate the execution of writing tasks defined in a mapping file, specifically in JSON format. The function begins by ensuring that the directory specified for saving conversation histories exists, creating it if necessary. It then reads the mapping file, which contains pairs of file names and corresponding instructions, and loads this data into a list.\n\nIf a limit is specified, the function restricts the number of tasks to be executed to the first N entries. The core of the function is the run nested function, which processes each task by clearing the conversation history, executing the writing task through the process_single_task function, and saving the resulting conversation history to a JSON file in the specified directory. The output file is named based on the original file name and includes a timestamp to ensure uniqueness.\n\nThe function supports concurrent execution using a ThreadPoolExecutor when the concurrent parameter is set to True and the number of workers is greater than one. In this case, it submits tasks to the executor and waits for their completion, handling any exceptions that may arise during execution. If concurrent execution is not enabled, the function processes each task sequentially.\n\nFrom a functional perspective, execute_from_mapping is called within the start_task_execution function, which serves as the entry point for executing predefined tasks. The start_task_execution function parses command-line arguments to determine whether to execute tasks from a mapping file or as a single task. When the --from-mapping flag is used, it invokes execute_from_mapping with the appropriate parameters, thereby initiating the batch execution process.\n\n**Note**: Users should ensure that the mapping file is correctly formatted and accessible. Additionally, when using concurrent execution, it is important to manage the number of workers to avoid overwhelming system resources. The function does not return any value, as its primary purpose is to execute tasks and save conversation histories.",
        "**execute_from_mapping**: The function of execute_from_mapping is to process and execute writing tasks in bulk from a mapping file, with support for both sequential and concurrent execution modes.\n\n**parameters**: The parameters of this function are as follows:\n· mapping_file (Path | str): The path to the instruction_mapping.json file that contains the mapping of task file names to instructions.\n· concurrent (bool): A flag indicating whether the tasks should be executed concurrently.\n· workers (int): The number of threads to use when executing tasks concurrently.\n· limit (int | None): The maximum number of tasks to execute. If None, there is no limit.\n· conv_dir (Path | str): The directory where conversation history files will be saved.\n\n**Code Description**: The execute_from_mapping function is responsible for managing and executing tasks defined in a mapping file. The mapping file contains a dictionary where each entry is a pair of file names and corresponding instructions. The function allows for bulk execution of these tasks, either sequentially or concurrently, depending on the value of the `concurrent` parameter.\n\nThe function first checks if the concurrent execution mode is enabled. If it is, it disables the progress rendering feature in the settings to prevent interference when processing tasks concurrently. Next, it ensures the specified `conv_dir` exists, creating the directory if it does not.\n\nThe function attempts to read the mapping file in JSON format. If the file cannot be parsed, an error message is printed, and the function returns without further execution. Once the mapping file is successfully loaded, the function extracts the tasks to be executed, optionally limiting the number of tasks to be processed based on the `limit` parameter.\n\nA nested helper function, `run`, is defined to handle the execution of individual tasks. For each task, it clears the conversation history, processes the task via the `process_single_task` function, and saves the resulting conversation history to a file in the specified `conv_dir`. If a file referenced in the task does not exist, the function skips that task and prints an error message. Any other exceptions are also caught and reported, but the function continues with other tasks.\n\nIf the `concurrent` flag is set and more than one worker thread is specified, a ThreadPoolExecutor is used to execute the tasks concurrently. The function submits tasks to the executor, waits for them to complete, and handles any exceptions raised during execution. If concurrent execution is not enabled, tasks are processed sequentially.\n\nFrom a functional perspective, this function is called by the `start_task_execution` function, which serves as the command-line interface (CLI) entry point for initiating task execution. The `start_task_execution` function parses the user's command-line arguments and, if the `--from-mapping` flag is set, passes the relevant arguments to `execute_from_mapping`. This creates a direct relationship between the command-line input and the task execution process.\n\n**Note**: \n- Ensure that the mapping file is correctly formatted in JSON and accessible to avoid parsing errors.\n- The directory specified in `conv_dir` should be writable to ensure that the conversation history files can be saved without issues.\n- When using the concurrent mode, make sure the number of workers does not exceed the system's capability, as this could lead to performance degradation.\n\n**Output Example**:\nThe function does not return any value. However, it creates conversation history files in the specified directory. An example of the saved file name could be: `file_name_20230501_123456_conversation.json`. The content of these files will be the JSON representation of the conversation history generated by the `process_single_task` function for each task executed."
      ],
      "code_start_line": 52,
      "code_end_line": 133,
      "params": [
        "mapping_file",
        "concurrent",
        "workers",
        "limit",
        "conv_dir"
      ],
      "have_return": true,
      "code_content": "def execute_from_mapping(\n    mapping_file: Path | str,\n    concurrent: bool,\n    workers: int,\n    limit: int | None = None,\n    conv_dir: Path | str = \"conversation_histories\",\n):\n    \"\"\"\n    从 instruction_mapping.json 批量调度写作任务，支持顺序或并发执行。\n\n    根据 mapping_file 加载 {file_name: instruction} 映射表，可指定 limit\n    只执行前 N 条。对于每一对 (file_name, instruction)，先清空会话管理器，\n    再调用 process_single_task 完成写作；完成后将对话历史保存到\n    conv_dir/<file_stem>_<timestamp>_conversation.json。\n\n    Args:\n        mapping_file (Path|str): instruction_mapping.json 文件路径。\n        concurrent (bool): 是否并发执行所有任务。\n        workers (int): 并发模式下的最大线程数。\n        limit (int|None): 限制最多执行的条目数，None 表示不限制。\n        conv_dir (Path|str): 保存对话历史的目录。\n\n    Returns:\n        None\n    \"\"\"\n    # 并发时禁止在 process_single_task 中启动 Rich Live 渲染\n    if concurrent:\n        from criticsearch.config import settings\n        setattr(settings, \"disable_progress\", True)\n\n    conv_dir = Path(conv_dir)\n    conv_dir.mkdir(parents=True, exist_ok=True)\n\n    # 在这里单独处理 mapping.json 的 parse 错误\n    try:\n        mapping = json.loads(Path(mapping_file).read_text(encoding=\"utf-8\"))\n    except json.JSONDecodeError as e:\n        print(f\"[ERROR] 无法解析映射文件 {mapping_file}: {e}\")\n        return\n\n    items = list(mapping.items())\n    if limit:\n        items = items[:limit]\n\n    def run(pair):\n        file_name, task = pair\n        try:\n            BaseAgent.conversation_manager.clear_history()\n            # 真正跑写作主流程\n            records = process_single_task(task, file_name=file_name)\n        except FileNotFoundError as e:\n            # GT JSON 不存在，才跳过\n            print(f\"[ERROR] skip `{file_name}`: 文件不存在: {e}\")\n            traceback.print_exc()                            # 打印详细堆栈\n            return\n        except Exception as e:\n            # 其余所有异常都打印但不当作“无效 JSON”跳过\n            print(f\"[ERROR] task `{file_name}` 执行失败: {e}\")\n            traceback.print_exc()                            # 打印详细堆栈\n            return\n\n        # 到这里说明 process_single_task 正常返回了 records\n        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        out = Path(file_name).stem + f\"_{ts}_conversation.json\"\n        out_path = conv_dir / out\n        with out_path.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(records, f, ensure_ascii=False, indent=2)\n        print(f\"[INFO] saved {file_name} → {out_path}\")\n\n    if concurrent and workers > 1:\n        with ThreadPoolExecutor(max_workers=workers) as exe:\n            futures = {exe.submit(run, pair): pair for pair in items}\n            for fut in as_completed(futures):\n                fn, _ = futures[fut]\n                try:\n                    fut.result()\n                    print(f\"[INFO] done {fn}\")\n                except Exception as e:\n                    print(f\"[ERROR] {fn} failed: {e}\")\n    else:\n        for pair in items:\n            run(pair)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tasks_runner.py/start_task_execution"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "run",
      "md_content": [
        "**run**: The function of run is to execute a single task and save the conversation history related to that task into a JSON file.\n\n**parameters**: The parameters of this Function.\n· pair: A tuple containing two elements: the file name and the task to be processed.\n\n**Code Description**: The run function is designed to handle the execution of a specific task by first clearing any existing conversation history to ensure that the task is processed in isolation from previous interactions. It begins by unpacking the input parameter, pair, into file_name and task. The function then calls the clear_history method from the BaseAgent's conversation_manager to reset the conversation state.\n\nFollowing this, the function invokes process_single_task, passing the task and file_name as arguments. This function is responsible for the core processing of the task, which includes managing interactions between various agents, executing the task, and generating a report based on the results.\n\nAfter the task has been processed, the run function generates a timestamped output file name by combining the stem of the file_name with the current date and time. It then retrieves the conversation history by calling the model_dump method on the conversation_manager, which captures the current state of the conversation in a structured format.\n\nThe conversation history is then saved to a JSON file at the specified output path, ensuring that the data is encoded in UTF-8 and formatted with an indentation of 2 spaces for readability. Finally, the function prints an informational message to the console indicating that the conversation has been successfully saved.\n\nThe run function is integral to the workflow of task execution, ensuring that each task is processed independently and that the results are documented for future reference. It relies on the clear_history method to maintain a clean slate for each task and on process_single_task to handle the actual processing logic.\n\n**Note**: It is important to ensure that the input pair is correctly formatted, as the function expects a tuple with a valid file name and a task string. Additionally, invoking clear_history will permanently remove all previous conversation data, so any necessary data should be saved before this function is called.",
        "**run**: The function of run is to execute a specific task based on the provided input pair, handling any exceptions that may arise during the process.\n\n**parameters**: The parameters of this Function.\n· pair: A tuple containing two elements, where the first element is a string representing the file name and the second element is a string representing the task to be processed.\n\n**Code Description**: The run function is designed to manage the execution of a single task by utilizing the provided pair of inputs. It begins by unpacking the pair into file_name and task variables. The function then attempts to clear the conversation history by invoking the clear_history method from the BaseAgent's conversation_manager. This step ensures that any previous interactions do not affect the current task execution.\n\nFollowing the clearing of history, the function calls process_single_task, passing the task and file_name as arguments. This function is responsible for processing the task and generating a report based on the input data. If the file specified by file_name does not exist, a FileNotFoundError is raised, which is caught by the exception handling block. In this case, an error message is printed to indicate that the file is missing, and the function exits without further processing.\n\nIf any other exceptions occur during the execution of process_single_task, these are also caught, and a corresponding error message is printed, along with a traceback for debugging purposes. This ensures that the function handles unexpected errors gracefully without crashing the application.\n\nIf the task is processed successfully, the function generates a timestamp and constructs an output file name based on the original file name and the current timestamp. It then writes the generated records to a JSON file at the specified output path. A success message is printed to inform the user of the saved output file.\n\nThe run function is a crucial part of the task execution workflow, as it encapsulates the logic for handling individual tasks while ensuring that the environment is reset for each execution. Its relationship with the process_single_task function highlights its role in orchestrating task processing within the broader application.\n\n**Note**: It is important to ensure that the file specified by file_name exists before invoking the run function, as the absence of the file will lead to a FileNotFoundError. Additionally, users should be aware that invoking this function will overwrite any existing output files with the same name.\n\n**Output Example**: A possible appearance of the code's return value when executing the function could look like this:\n```json\n[\n    {\"from\": \"human\", \"value\": \"What is the status of the Syrian opposition?\"},\n    {\"from\": \"agent\", \"thought\": \"The current status is...\", \"action\": \"SEARCH\", \"action_content\": [\"query1\", \"query2\"], \"action_result\": [\"result1\", \"result2\"]},\n    {\"from\": \"verifier\", \"section\": \"Section 1\", \"accuracy\": 0.85},\n    {\"from\": \"agent\", \"final_report\": \"The comprehensive report is...\", \"citation\": [\"http://example.com/citation1\"]}\n]\n```"
      ],
      "code_start_line": 96,
      "code_end_line": 119,
      "params": [
        "pair"
      ],
      "have_return": true,
      "code_content": "    def run(pair):\n        file_name, task = pair\n        try:\n            BaseAgent.conversation_manager.clear_history()\n            # 真正跑写作主流程\n            records = process_single_task(task, file_name=file_name)\n        except FileNotFoundError as e:\n            # GT JSON 不存在，才跳过\n            print(f\"[ERROR] skip `{file_name}`: 文件不存在: {e}\")\n            traceback.print_exc()                            # 打印详细堆栈\n            return\n        except Exception as e:\n            # 其余所有异常都打印但不当作“无效 JSON”跳过\n            print(f\"[ERROR] task `{file_name}` 执行失败: {e}\")\n            traceback.print_exc()                            # 打印详细堆栈\n            return\n\n        # 到这里说明 process_single_task 正常返回了 records\n        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        out = Path(file_name).stem + f\"_{ts}_conversation.json\"\n        out_path = conv_dir / out\n        with out_path.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(records, f, ensure_ascii=False, indent=2)\n        print(f\"[INFO] saved {file_name} → {out_path}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/models.py/ConversationManager/clear_history"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "start_task_execution",
      "md_content": [
        "**start_task_execution**: The function of start_task_execution is to serve as the entry point for executing predefined tasks.\n\n**parameters**: The parameters of this Function are not explicitly defined as it does not take any input parameters.\n\n**Code Description**: The start_task_execution function is designed to initiate the execution of a predefined list of tasks. In its current implementation, it contains a single task: \"Write a report about 2024_Syrian_opposition_offensives event\". The function sets a constant MAX_ITERATION to 2, which indicates the maximum number of times each task will be executed. \n\nUpon invocation, the function attempts to execute the tasks by calling the execute_multiple_tasks function, passing the list of tasks and the MAX_ITERATION value as arguments. This function is responsible for handling the execution of multiple tasks iteratively and logging the conversation history associated with each task.\n\nThe start_task_execution function includes error handling to manage interruptions. If a KeyboardInterrupt exception is raised (for example, if the user interrupts the execution with a keyboard command), the function will catch this exception and print a message indicating that the execution was interrupted by the user.\n\nFrom a functional perspective, start_task_execution is called within the main execution context of the application, specifically in the src/criticsearch/__main__.py file. This indicates that it serves as a primary function that triggers the task execution process when the application is run. \n\nThe relationship between start_task_execution and execute_multiple_tasks is crucial, as the former initializes the task execution process, while the latter carries out the actual processing of the tasks. The effectiveness of start_task_execution relies on the proper implementation of execute_multiple_tasks, which manages the iterative execution and logging of tasks.\n\n**Note**: It is important to ensure that the tasks defined within the start_task_execution function are relevant and well-structured. Additionally, users should be aware that the function does not accept parameters, and the execution can be interrupted by user input. Proper error handling is in place to manage such interruptions gracefully.",
        "**start_task_execution**: The function of start_task_execution is to serve as the command-line interface (CLI) entry point for parsing command-line arguments and scheduling task execution.\n\n**parameters**: The parameters of this Function are as follows:\n· tasks (positional): A list of tasks to be executed when not using the --from-mapping option.\n· -f, --file-name: The name of the GT JSON file in single task mode.\n· -o, --output-file: The file for saving conversation history in single task mode, compatible with the previous version.\n· --from-mapping: A flag to indicate whether to execute tasks in batch mode from a mapping file.\n· --mapping-file: The path to the mapping file, defaulting to reportbench/instruction_mapping.json.\n· --concurrent: A flag to enable concurrent execution in batch mode.\n· -w, --workers: The number of threads to use during concurrent execution, defaulting to 5.\n· --limit: The maximum number of entries to execute in batch mode, defaulting to no limit.\n· --conv-dir: The directory for saving conversation histories, defaulting to conversation_histories.\n\n**Code Description**: The start_task_execution function is designed to facilitate the execution of tasks either as a single task or in batch mode based on user input from the command line. It utilizes the argparse library to define and parse various command-line arguments that control the behavior of the task execution.\n\nWhen the function is invoked, it first sets up an argument parser with a description of the task execution process. The user can specify a list of tasks directly or opt to execute tasks from a mapping file by using the --from-mapping flag. If the mapping file is specified, the function ensures that the path is absolute; if it is relative, it constructs the absolute path based on the current script's directory.\n\nThe function then attempts to execute the tasks based on the provided arguments. If the --from-mapping flag is set, it calls the execute_from_mapping function, passing the relevant parameters such as mapping_file, concurrent execution flag, number of workers, limit, and conversation directory. This function is responsible for reading the mapping file and executing the tasks defined within it, either sequentially or concurrently.\n\nIf the --from-mapping flag is not used, the function prepares a list of tasks to execute. If no tasks are provided, it defaults to a predefined task. It then calls the execute_multiple_tasks function, which handles the sequential execution of the specified tasks and manages the saving of conversation histories.\n\nThe start_task_execution function is called from the main entry point of the application, allowing users to initiate task execution directly from the command line. This establishes a clear relationship between the CLI interface and the underlying task execution logic.\n\n**Note**: Users should ensure that the command-line arguments are correctly specified to avoid execution errors. Additionally, when using the --from-mapping option, the mapping file must be properly formatted and accessible. The function does not return any value, as its primary purpose is to execute tasks and manage their execution flow.",
        "**start_task_execution**: The function of start_task_execution is to serve as the command-line interface (CLI) entry point for parsing command-line arguments and executing tasks.\n\n**parameters**: The parameters of this Function are as follows:\n· tasks (positional): A list of tasks to be executed; if not specified, a default task will be used.\n· -f, --file-name: The name of the GT JSON file in single-task mode.\n· -o, --output-file: The file where conversation history will be saved in single-task mode, compatible with older versions.\n· --from-mapping: A flag to indicate whether to execute tasks in bulk mode from a mapping file.\n· --mapping-file: The path to the mapping file, defaulting to reportbench/instruction_mapping.json.\n· --concurrent: A flag to enable concurrent execution of tasks in bulk mode.\n· -w, --workers: The number of threads to use for concurrent execution, defaulting to 5.\n· --limit: The maximum number of tasks to execute in bulk mode, defaulting to no limit.\n· --conv-dir: The directory where conversation histories will be saved, defaulting to conversation_histories.\n\n**Code Description**: The start_task_execution function is designed to facilitate the execution of tasks based on user input from the command line. It begins by initializing an argument parser to handle various command-line options that dictate how tasks should be executed. The function supports both single-task execution and bulk execution from a mapping file, allowing users to switch modes using the --from-mapping flag.\n\nUpon parsing the command-line arguments, the function checks if the user has opted for bulk execution. If so, it ensures that the mapping file path is absolute; if a relative path is provided, it resolves it based on the current script's directory. The function then calls execute_from_mapping, passing the relevant parameters to manage the bulk execution of tasks. This function handles reading the mapping file, executing tasks either sequentially or concurrently, and saving the conversation history.\n\nIf the user has not specified the --from-mapping flag, the function defaults to executing a list of tasks provided in the command line or a predefined default task. In this case, it calls execute_multiple_tasks, which is responsible for executing each task in sequence and saving the corresponding conversation history.\n\nThe start_task_execution function is called from the main entry point of the application, which is typically located in src/criticsearch/__main__.py. This establishes a clear relationship between user input and the underlying task execution logic, allowing for a seamless execution flow based on command-line arguments.\n\n**Note**: Users should ensure that the provided tasks are valid and well-defined. Additionally, when using the --from-mapping option, the mapping file must be correctly formatted in JSON and accessible to avoid parsing errors. The output_file parameter is currently not utilized in the function, and its presence is primarily for backward compatibility.",
        "## Function: `start_task_execution`\n\n### Description:\nThe `start_task_execution` function serves as the Command Line Interface (CLI) entry point for initiating the execution of tasks. It processes command-line arguments, allowing users to execute tasks either in a single-task mode or a batch mode using a mapping file. The function also supports controlling concurrency and limiting the number of tasks executed. Additionally, it manages the saving of conversation histories during task execution.\n\n### CLI Parameters:\n- `tasks` (positional, list of str): A list of tasks to be executed. If not provided, the function defaults to a predefined task list.\n- `-f`, `--file-name` (str, optional): In single-task mode, specifies the name of the GT JSON file.\n- `-o`, `--output-file` (Path, optional): Specifies the file where conversation history will be saved. This is used in single-task mode and maintains compatibility with previous versions.\n- `--from-mapping` (flag, optional): Activates batch mode, where tasks are executed from a mapping file.\n- `--mapping-file` (Path, optional): Specifies the path to the mapping file used in batch mode. The default value is `reportbench/instruction_mapping.json`.\n- `--concurrent` (flag, optional): Enables concurrent execution of tasks in batch mode.\n- `-w`, `--workers` (int, optional): Specifies the number of threads to use for concurrent execution. The default is 5.\n- `--limit` (int, optional): Limits the number of tasks to be executed in batch mode. By default, there is no limit.\n- `--conv-dir` (Path, optional): Defines the directory for saving conversation histories. The default directory is `conversation_histories`.\n\n### Returns:\n- `None`: This function does not return any value.\n\n### Functionality:\n1. **Argument Parsing**: The function starts by parsing the command-line arguments using `argparse`. Based on the arguments provided, it determines whether to run tasks in a single-task mode or batch mode.\n\n2. **Mapping File Handling**: If the `--from-mapping` flag is set, the function checks the validity of the provided mapping file path and adjusts it if necessary. The function then passes the mapping file and other relevant arguments to the task execution functions.\n\n3. **Task Execution**:\n    - **Batch Mode**: When the `--from-mapping` flag is specified, the function delegates task execution to the `execute_from_mapping` function. This function handles the processing of tasks listed in the provided mapping file. It also supports concurrency and allows task execution to be limited by the specified `--limit`.\n    - **Single-Task Mode**: If no mapping file is provided, the function uses the `execute_multiple_tasks` function to execute tasks individually. The tasks can be specified via the `tasks` argument or default to a predefined list.\n\n4. **Error Handling**: In case of a user interruption (e.g., via `KeyboardInterrupt`), the function gracefully handles the exception and prints a message indicating that execution was interrupted.\n\n### Usage Example:\n```bash\npython tasks_runner.py task1 task2 --from-mapping --mapping-file /path/to/mapping.json --concurrent --workers 10 --limit 100 --conv-dir /path/to/history\n```\n\nThis example will execute tasks `task1` and `task2` in batch mode, using a specified mapping file and enabling concurrent execution with 10 workers. The results will be saved in the specified conversation history directory, with a limit of 100 tasks.\n\n### Dependencies:\n- The function requires the `argparse` library for command-line argument parsing.\n- It calls the functions `execute_from_mapping` and `execute_multiple_tasks` for task execution.\n"
      ],
      "code_start_line": 136,
      "code_end_line": 206,
      "params": [],
      "have_return": false,
      "code_content": "def start_task_execution():\n    \"\"\"\n    CLI 入口：解析命令行参数并调度执行。\n\n    支持单任务模式和映射文件批量模式。可通过 --from-mapping 切换，\n    并通过 --concurrent、--workers 控制并发，--limit 控制条目数，\n    --conv-dir 指定对话历史保存目录。\n\n    CLI 参数:\n        tasks (positional): 不指定 --from-mapping 时，作为单任务列表。\n        -f, --file-name: 单任务模式下的 GT JSON 文件名。\n        -o, --output-file: 单任务模式下的历史保存文件，兼容旧版。\n        --from-mapping: 批量模式开关。\n        --mapping-file: 指定映射文件路径，默认 reportbench/instruction_mapping.json。\n        --concurrent: 批量模式下并发执行任务。\n        -w, --workers: 并发时线程数，默认 5。\n        --limit: 批量时最多执行条目数，默认无限制。\n        --conv-dir: 对话历史保存目录，默认 conversation_histories。\n\n    Returns:\n        None\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"批量执行 CriticSearch 任务\")\n    parser.add_argument(\"tasks\", nargs=\"*\", help=\"要执行的任务列表；不传时使用默认\")\n    parser.add_argument(\"-f\", \"--file-name\", type=str, default=None,\n                        help=\"单任务模式下的 GT JSON 文件名\")\n    parser.add_argument(\"-o\", \"--output-file\", type=Path,\n                        default=\"conversation_history_sharegpt.jsonl\",\n                        help=\"单任务模式下的历史保存文件\")\n    parser.add_argument(\"--from-mapping\", action=\"store_true\",\n                        help=\"从映射文件批量执行任务\")\n    parser.add_argument(\"--mapping-file\", type=Path,\n                        default=Path(__file__).parent / \"reportbench\" / \"instruction_mapping.json\",\n                        help=\"instruction_mapping.json 路径\")\n    parser.add_argument(\"--concurrent\", action=\"store_true\",\n                        help=\"批量模式下并发执行任务\")\n    parser.add_argument(\"-w\", \"--workers\", type=int, default=5,\n                        help=\"并发时的线程数\")\n    parser.add_argument(\"--limit\", type=int, default=None,\n                        help=\"批量执行时最多执行条目数\")\n    parser.add_argument(\"--conv-dir\", type=Path, default=Path(\"conversation_histories\"),\n                        help=\"对话历史保存目录\")\n    args = parser.parse_args()\n\n    if args.from_mapping:\n        mf = args.mapping_file\n        if not mf.is_absolute():\n            args.mapping_file = Path(__file__).parent / mf\n    # ——————————————————————————————————————————————————————————\n\n    try:\n        if args.from_mapping:\n            execute_from_mapping(\n                mapping_file=args.mapping_file,\n                concurrent=args.concurrent,\n                workers=args.workers,\n                limit=args.limit,\n                conv_dir=args.conv_dir,\n            )\n        else:\n            tasks = args.tasks or [\n                \"write an 2024 Syrian opposition_offensives comprehensive report\",\n            ]\n            execute_multiple_tasks(\n                tasks,\n                args.output_file,\n                file_name=args.file_name,\n                conv_dir=args.conv_dir,\n            )\n    except KeyboardInterrupt:\n        print(\"Execution interrupted by the user.\")\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/__main__.py"
      ],
      "reference_who": [
        "src/criticsearch/tasks_runner.py/execute_multiple_tasks",
        "src/criticsearch/tasks_runner.py/execute_from_mapping"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "src/criticsearch/session.py": [
    {
      "type": "ClassDef",
      "name": "Session",
      "md_content": [
        "**Session**: The function of Session is to wrap the CriticSearch process according to the step interface, enabling it to be invoked within the RAGEN environment.\n\n**Attributes**: \n- **user_prompt**: A string representing the user's input or prompt for the session. \n- **agent**: An instance of the `BaseAgent` class, which handles tasks such as receiving prompts and managing training data.\n- **search_results**: A string that stores the results of an asynchronous search operation.\n- **detailed_web_results**: A string that holds the detailed results obtained from web scraping.\n- **agent_report**: A string that accumulates the reports generated by the agent during the session.\n- **verifier**: An instance of the `ReportVerifier` class, which is used to evaluate the quality of the writing or generated content.\n- **last_score**: A floating-point value representing the score of the most recent verification process.\n\n**Code Description**: \nThe `Session` class encapsulates the process of conducting a CriticSearch within the context of a stepwise interface. It is designed to facilitate interaction within the RAGEN environment by managing the flow of tasks, including search, browsing, note-taking, and writing. The class is initialized with a user prompt and subsequently creates an agent to process the prompt and handle tasks such as searching and web scraping.\n\n- **`__init__(self, prompt: str)`**: This method initializes a new session. It accepts a user prompt, creates an agent instance, and sets up necessary data structures, such as the agent's training data and memo. The session also initializes variables for tracking the status of search results, web scraping results, agent reports, and evaluation scores.\n\n- **`search(self, queries: list[str]) -> str`**: This method performs an asynchronous search based on a list of queries. It uses the agent's search aggregator to fetch results and stores the results in the `search_results` attribute. The method returns the search results as a string.\n\n- **`browse(self, urls: list[str]) -> str`**: This method performs web scraping for the given list of URLs. It uses the agent's content scraper to fetch detailed web data, which is appended to the `detailed_web_results` attribute. It returns the scraped data as a string.\n\n- **`take_notes(self, text: str) -> str`**: This method allows the agent to take notes based on the provided text. It invokes the agent’s note-taking functionality and appends the generated notes to the agent's training data. The method returns the notes as a string.\n\n- **`start_writing(self, section: str) -> str`**: This method initiates the process of writing a section based on the previous search and browsing results. It utilizes the `_action_router` function to generate content for the section, and appends the generated content to the `agent_report`. The method also verifies the generated content using the `ReportVerifier` and stores the verification score in the `last_score` attribute. It returns the generated content as a string.\n\n**Note**:\n- The `Session` class is intended to manage the workflow of CriticSearch within the RAGEN environment and should be used in environments where such structured processes are required.\n- Asynchronous operations such as search and browsing are handled using `asyncio.run()`, ensuring that the system can perform tasks concurrently.\n- The agent is designed to accumulate training data, which helps refine the system's performance over time.\n\n**Output Example**:\n- After calling `search()` with a list of queries, the output might look like: `\"Search results for 'data science' include: [result1, result2, result3]\"`.\n- After calling `browse()` with a list of URLs, the output could be: `\"Detailed web scraping results: <scraped content>\"`.\n- Calling `take_notes()` might yield: `\"Notes: Data science involves the use of algorithms to extract insights.\"`.\n- Calling `start_writing()` could generate: `\"Generated content: In the field of data science, algorithms play a crucial role...\"`."
      ],
      "code_start_line": 6,
      "code_end_line": 62,
      "params": [],
      "have_return": true,
      "code_content": "class Session:\n    \"\"\"\n    按 step 接口包装 CriticSearch 流程，供 RAGEN 环境调用。\n    \"\"\"\n    def __init__(self, prompt: str):\n        # 初始化 agent\n        self.user_prompt = prompt\n        self.agent = BaseAgent()\n        self.agent.receive_task(prompt)\n        self.agent.training_data = [{\"from\": \"human\", \"value\": prompt}]\n        self.agent.memo = set()\n        # 流程状态\n        self.search_results = \"\"\n        self.detailed_web_results = \"\"\n        self.agent_report = \"\"\n        # 用于评估写作结果\n        self.verifier = ReportVerifier(self.agent)\n        self.last_score = 0.0\n\n    def search(self, queries: list[str]) -> str:\n        # 执行异步搜索\n        self.search_results = asyncio.run(\n            self.agent.search_aggregator.search(queries)\n        )\n        return self.search_results\n\n    def browse(self, urls: list[str]) -> str:\n        # 执行网页爬取\n        web = asyncio.run(self.agent.content_scraper.scrape(urls=urls))\n        self.detailed_web_results += \"\\n\\n\" + web\n        return web\n\n    def take_notes(self, text: str) -> str:\n        # 记录笔记\n        notes = self.agent.taking_notes(text)\n        self.agent.training_data.append({\n            \"from\": \"agent\", \"action\": \"TAKING_NOTES\", \"action_content\": notes\n        })\n        return notes\n\n    def start_writing(self, section: str) -> str:\n        # 调用 _action_router 生成本段落\n        content = _action_router(\n            self.agent,\n            self.search_results,\n            self.user_prompt,\n            section,\n            iteration=0,\n            agent_report=self.agent_report,\n            guide_line=section,\n            detailed_web_results=self.detailed_web_results,\n        )\n        # 累计报告\n        self.agent_report += \"\\n\" + content\n        # TODO: 验证本段落（此处第二参暂传空 list，如有 GT 可改为真实 facts）\n        self.last_score = self.verifier.verify_section(content, [])\n        return content",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize a Session object with a user prompt and set up the necessary components for the agent's operation.\n\n**parameters**: The parameters of this Function.\n· prompt: A string that represents the user's input or question that the agent will process.\n\n**Code Description**: The __init__ method is a constructor for the Session class, which is responsible for initializing the session with a user-provided prompt. Upon instantiation, the method takes a single parameter, prompt, which is expected to be a string. This prompt is stored in the instance variable user_prompt, allowing the agent to understand the context of the user's request.\n\nThe method then creates an instance of the BaseAgent class, which serves as the foundational component for handling interactions and processing tasks. The agent is initialized to receive the user's prompt through the receive_task method, which stores the prompt for future reference and processing. Additionally, the training_data attribute of the agent is populated with the user's prompt, formatted as a dictionary containing the source (\"human\") and the value (the prompt itself). This setup is crucial for the agent's learning and response generation capabilities.\n\nThe __init__ method also initializes several attributes related to the session's state. These include search_results, detailed_web_results, and agent_report, all of which are set to empty strings initially. These attributes are intended to hold the results of the agent's operations and any reports generated during the session.\n\nFurthermore, the method instantiates a ReportVerifier object, passing the agent as a parameter. This verifier is responsible for evaluating the accuracy of the agent's responses against factual data, ensuring that the information provided to the user is reliable and correct. The last_score attribute is initialized to 0.0, which will later be used to store the score from the verification process.\n\nThe relationship between the __init__ method and its callees is significant, as it establishes the foundational context for the agent's operations. By initializing the agent with the user's prompt and setting up the necessary components, the session is prepared to handle various tasks, including searching for information, generating responses, and verifying the accuracy of those responses.\n\n**Note**: It is essential to provide a well-defined prompt to the Session object upon initialization. The effectiveness of the agent's responses and the accuracy of the verification process depend heavily on the quality of the input prompt. Proper initialization ensures that the agent is ready to engage with the user's request effectively."
      ],
      "code_start_line": 10,
      "code_end_line": 23,
      "params": [
        "self",
        "prompt"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, prompt: str):\n        # 初始化 agent\n        self.user_prompt = prompt\n        self.agent = BaseAgent()\n        self.agent.receive_task(prompt)\n        self.agent.training_data = [{\"from\": \"human\", \"value\": prompt}]\n        self.agent.memo = set()\n        # 流程状态\n        self.search_results = \"\"\n        self.detailed_web_results = \"\"\n        self.agent_report = \"\"\n        # 用于评估写作结果\n        self.verifier = ReportVerifier(self.agent)\n        self.last_score = 0.0\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/receive_task",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [
        "**search**: The function of search is to perform an asynchronous search based on a list of queries.\n\n**parameters**: The parameters of this Function.\n· queries: A list of strings representing the search queries to be executed.\n\n**Code Description**: The search function is designed to execute an asynchronous search operation. It takes a single parameter, `queries`, which is expected to be a list of strings. Within the function, it utilizes the `asyncio.run()` method to run an asynchronous search operation provided by `self.agent.search_aggregator.search(queries)`. This means that the function will wait for the completion of the search operation before proceeding. The results of the search are stored in the instance variable `self.search_results`, which is then returned as the output of the function. This design allows for efficient handling of potentially time-consuming search operations without blocking the execution of other code.\n\n**Note**: It is important to ensure that the `queries` parameter is a list of strings, as passing an incorrect type may lead to runtime errors. Additionally, since the function performs an asynchronous operation, it should be called in an appropriate context that supports asynchronous execution.\n\n**Output Example**: An example of the return value could be a string containing the aggregated results of the search queries, such as \"Results for query1, query2, and query3: [result1, result2, result3]\"."
      ],
      "code_start_line": 25,
      "code_end_line": 30,
      "params": [
        "self",
        "queries"
      ],
      "have_return": true,
      "code_content": "    def search(self, queries: list[str]) -> str:\n        # 执行异步搜索\n        self.search_results = asyncio.run(\n            self.agent.search_aggregator.search(queries)\n        )\n        return self.search_results\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "browse",
      "md_content": [
        "**browse**: The function of browse is to execute a web scraping operation on the provided URLs and return the results.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs that need to be scraped for content.\n\n**Code Description**: The `browse` function is designed to initiate a web scraping process by utilizing an asynchronous call to the `scrape` method from the `ContentScraper` class. The function takes a list of URLs as input and uses the `asyncio.run()` method to execute the `scrape` function, which is expected to return the scraped content for the provided URLs.\n\nOnce the scraping process completes, the returned content is appended to the `detailed_web_results` attribute, which stores a cumulative record of the web scraping outcomes. This cumulative result is updated by appending the newly retrieved content, ensuring that previous results are preserved for later use or further processing.\n\nThe `scrape` method itself is responsible for extracting content from the given URLs. It handles both successful and failed attempts at scraping, with fallback mechanisms in place when necessary. The function's output is directly returned after the scraping process finishes, enabling the caller to work with the content.\n\nThis method is commonly used within the broader context of the system to collect web data from multiple sources, ensuring that the relevant information is retrieved and stored for later use.\n\n**Note**: The function expects a valid list of URLs to be provided for optimal performance. If the URLs are invalid or inaccessible, the behavior of the `scrape` method might involve retries or fallback mechanisms. The final content returned by the function can vary based on the success or failure of the scraping operation.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n{\n    \"data\": [\n        {\"url\": \"http://example.com\", \"content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"content\": \"No content available\"}\n    ]\n}\n```"
      ],
      "code_start_line": 32,
      "code_end_line": 36,
      "params": [
        "self",
        "urls"
      ],
      "have_return": true,
      "code_content": "    def browse(self, urls: list[str]) -> str:\n        # 执行网页爬取\n        web = asyncio.run(self.agent.content_scraper.scrape(urls=urls))\n        self.detailed_web_results += \"\\n\\n\" + web\n        return web\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "take_notes",
      "md_content": [
        "**take_notes**: The function of take_notes is to record notes extracted from a given text input.\n\n**parameters**: The parameters of this Function.\n· text: A string containing the text from which notes will be extracted.\n\n**Code Description**: The take_notes function is a method within the Session class that facilitates the process of recording notes based on the input text. When invoked, it calls the taking_notes method from the agent attribute, passing the provided text as an argument. This method is responsible for extracting relevant information from the text and returning it as notes.\n\nOnce the notes are obtained, the function appends a dictionary to the agent's training_data list. This dictionary contains three key-value pairs: \"from\" indicating the source as \"agent\", \"action\" specifying the action taken as \"TAKING_NOTES\", and \"action_content\" which holds the actual notes extracted. This structured logging of actions is crucial for tracking the agent's activities and improving its learning process.\n\nThe take_notes function ultimately returns the notes that were recorded, providing a direct output of the operation performed. This function is integral to maintaining an updated record of information that the agent can utilize for future tasks or interactions.\n\nThe take_notes function is typically called in scenarios where the agent needs to process new information and update its knowledge base accordingly, ensuring that it remains informed and capable of responding to user queries effectively.\n\n**Note**: It is important to ensure that the text parameter is well-structured and contains relevant information for effective note extraction. The function will return the notes as a string, which should be handled appropriately by the caller.\n\n**Output Example**: A possible return value from the take_notes function could be:\n```\n\"Extracted note 1: Important information regarding the topic. Extracted note 2: Additional insights and references.\"\n```"
      ],
      "code_start_line": 38,
      "code_end_line": 44,
      "params": [
        "self",
        "text"
      ],
      "have_return": true,
      "code_content": "    def take_notes(self, text: str) -> str:\n        # 记录笔记\n        notes = self.agent.taking_notes(text)\n        self.agent.training_data.append({\n            \"from\": \"agent\", \"action\": \"TAKING_NOTES\", \"action_content\": notes\n        })\n        return notes\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/taking_notes"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "start_writing",
      "md_content": [
        "### `start_writing` Method Documentation\n\n#### Overview\nThe `start_writing` method is a function within the `Session` class responsible for initiating the content generation process for a specified section of a task or document. It utilizes the `_action_router` function to guide the agent through the necessary steps to generate appropriate content for the given section, appends the generated content to the agent's report, and validates the accuracy of the generated content.\n\n#### Parameters\n- **section** (`str`): A string representing the specific section of the task or document for which the content is being generated.\n\n#### Return Value\n- **str**: The generated content for the specified section.\n\n#### Method Description\n1. **Action Routing**:\n   The method begins by calling the `_action_router` function. This function orchestrates the decision-making process, utilizing the agent's state, search results, and user-provided prompt. It generates the content for the specified section based on the current context and task. The function takes several parameters including:\n   - The `agent` (which is an instance of the agent handling the task),\n   - `search_results` (relevant results accumulated from previous searches),\n   - `user_prompt` (the prompt provided by the user for context),\n   - The specific `section` for which the content is generated,\n   - `iteration` (set to 0 for the first iteration),\n   - `agent_report` (the agent's accumulated report so far),\n   - `guide_line` (providing the necessary guidelines for the task),\n   - `detailed_web_results` (optional detailed results from any web scraping).\n\n2. **Agent Report Update**:\n   Once the content is generated, it is added to the agent's report. The `agent_report` string is updated by concatenating the newly generated content.\n\n3. **Content Verification**:\n   The method then proceeds to verify the accuracy of the generated content. This is done by calling the `verify_section` method of the `ReportVerifier` class, which assesses the factual accuracy of the content. In this method, the verification is performed with an empty list (i.e., no extracted facts are provided), though this list can be populated with factual data for more thorough validation in future iterations. The result of the verification process is stored in the `last_score` variable.\n\n4. **Returning the Generated Content**:\n   Finally, the method returns the generated content for the specified section.\n\n#### Usage Example\n```python\nsession = Session(agent, search_results, user_prompt, agent_report, detailed_web_results)\nsection_content = session.start_writing(\"Introduction\")\nprint(section_content)\n```\n\n#### Notes\n- The method does not perform detailed fact-checking on the generated content in its current form, as the list of extracted facts is empty. This could be modified for more robust verification.\n- The method relies on the `_action_router` function to perform the bulk of the decision-making and content generation process, ensuring that the agent’s actions align with the task's goals.\n"
      ],
      "code_start_line": 46,
      "code_end_line": 62,
      "params": [
        "self",
        "section"
      ],
      "have_return": true,
      "code_content": "    def start_writing(self, section: str) -> str:\n        # 调用 _action_router 生成本段落\n        content = _action_router(\n            self.agent,\n            self.search_results,\n            self.user_prompt,\n            section,\n            iteration=0,\n            agent_report=self.agent_report,\n            guide_line=section,\n            detailed_web_results=self.detailed_web_results,\n        )\n        # 累计报告\n        self.agent_report += \"\\n\" + content\n        # TODO: 验证本段落（此处第二参暂传空 list，如有 GT 可改为真实 facts）\n        self.last_score = self.verifier.verify_section(content, [])\n        return content",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/main.py/_action_router",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/verify_section"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "src/criticsearch/abstract_substitution/abs_exp_1.py": [
    {
      "type": "FunctionDef",
      "name": "_logged_printer_print",
      "md_content": [
        "**_logged_printer_print**: The function of _logged_printer_print is to print a message and log it using the logger.\n\n**parameters**: The parameters of this Function.\n· msg: The message to be printed and logged. This is a required parameter.\n· *args: Additional positional arguments that may be passed to the original print function.\n· **kwargs**: Additional keyword arguments that may be passed to the original print function.\n\n**Code Description**: The _logged_printer_print function serves two primary purposes: it invokes the original print functionality and logs the message using a logger. When the function is called, it first executes _orig_printer_print with the provided msg, *args, and **kwargs. This allows the function to maintain the standard behavior of printing messages to the console or output stream. Following this, it logs the same message at the info level using the logger's info method, converting the msg to a string if it is not already. This dual functionality ensures that messages are both displayed and recorded for future reference, which is particularly useful for debugging and monitoring application behavior.\n\n**Note**: It is important to ensure that the logger is properly configured before using this function to avoid any logging errors. Additionally, the _orig_printer_print function must be defined in the scope where _logged_printer_print is used, as it is called directly within this function."
      ],
      "code_start_line": 67,
      "code_end_line": 69,
      "params": [
        "msg"
      ],
      "have_return": false,
      "code_content": "def _logged_printer_print(msg, *args, **kwargs):\n    _orig_printer_print(msg, *args, **kwargs)\n    logger.info(str(msg))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_logged_printer_rule",
      "md_content": [
        "**_logged_printer_rule**: The function of _logged_printer_rule is to call an original printer rule function and log a message with the section label.\n\n**parameters**:\n· msg: A string message to be printed and logged, typically indicating a section label or message.\n· *args: A variable number of arguments passed to the original printer rule function.\n· **kwargs: A variable number of keyword arguments passed to the original printer rule function.\n\n**Code Description**:  \nThe function _logged_printer_rule is designed to first call another function, _orig_printer_rule, with the provided arguments, including the message (msg), any additional positional arguments (*args), and any keyword arguments (**kwargs). After this, the function logs the message using a logger at the \"info\" level, prefixed with the label \"SECTION:\". \n\nThe primary purpose of this function is to ensure that the original functionality of _orig_printer_rule is preserved while adding logging behavior, making it useful for tracking or debugging purposes. The logging action is done after the call to _orig_printer_rule, meaning the original functionality executes first before the log entry is made.\n\n**Note**:  \n- This function assumes that _orig_printer_rule and logger are defined and available in the scope where _logged_printer_rule is invoked.\n- The logging mechanism is set to the \"info\" level, meaning the log entry is intended for general informational purposes and will be visible in standard logging output.\n- If the function _orig_printer_rule performs important tasks (such as printing or processing data), they will be executed before the log is recorded."
      ],
      "code_start_line": 70,
      "code_end_line": 72,
      "params": [
        "msg"
      ],
      "have_return": false,
      "code_content": "def _logged_printer_rule(msg, *args, **kwargs):\n    _orig_printer_rule(msg, *args, **kwargs)\n    logger.info(f\"SECTION: {msg}\")\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "print",
      "md_content": [
        "**print**: The function of print is to output messages to the console and log them for tracking purposes.\n\n**parameters**: The parameters of this Function.\n· *args: A variable length argument list that allows passing multiple values to be printed and logged.\n· **kwargs: A variable length keyword argument list that allows passing additional options for the print function.\n\n**Code Description**: The print function is a wrapper around the original print function (_orig_print) that enhances its functionality by also logging the output to a logger. When called, it first invokes the original print function with the provided arguments (*args and **kwargs), which outputs the message to the console. Subsequently, it logs the same message at the INFO level using the logger. The message is constructed by joining the string representations of all arguments passed to the function, ensuring that they are formatted correctly for logging.\n\nThis function is called within the main function of the script located at src/criticsearch/abstract_substitution/abs_exp_1.py. In the main function, print is used to inform the user about the number of items saved to a specified output file after a single execution or during batch processing. The use of the print function in this context allows for both immediate feedback to the user via the console and persistent logging of the operation for later review or debugging.\n\n**Note**: When using this print function, ensure that the logger is properly configured to capture INFO level messages. Additionally, be aware that the output format may vary depending on the types of arguments passed, as they are converted to strings before logging."
      ],
      "code_start_line": 79,
      "code_end_line": 81,
      "params": [],
      "have_return": false,
      "code_content": "def print(*args, **kwargs):\n    _orig_print(*args, **kwargs)\n    logger.info(' '.join(str(a) for a in args))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "pretty_json",
      "md_content": [
        "**pretty_json**: The function of pretty_json is to convert a Python object into a JSON string with a specified format.\n\n**parameters**: The parameters of this Function.\n· data: The Python object (e.g., dictionary, list) that needs to be converted to a JSON string.\n\n**Code Description**: The pretty_json function utilizes the json module to serialize a given Python object into a JSON-formatted string. It specifically employs the `json.dumps()` method, which takes the input data and converts it into a JSON string. The parameters `ensure_ascii=False` and `indent=2` are used to ensure that non-ASCII characters are preserved in their original form and to format the output with an indentation of two spaces, respectively. This makes the resulting JSON string more readable.\n\nThis function is called in several places within the project, primarily to print structured data in a human-readable format. For instance, in the `tavily_extract` function, pretty_json is used to print the list of URLs being processed, enhancing the visibility of the data being handled. Similarly, in the `fallback_scrape` function, it formats the URLs and the results of the scraping operation, providing clarity on what data is being returned. The `query_update` and `generate_seed` functions also utilize pretty_json to display updated questions and evidence, ensuring that the output is easily interpretable by developers or users monitoring the process.\n\n**Note**: When using this function, it is important to ensure that the input data is serializable to JSON. If the data contains non-serializable types (like custom objects), a TypeError will be raised.\n\n**Output Example**: An example output of the pretty_json function when given a dictionary might look like this:\n```json\n{\n  \"name\": \"John Doe\",\n  \"age\": 30,\n  \"city\": \"New York\"\n}\n```"
      ],
      "code_start_line": 84,
      "code_end_line": 86,
      "params": [
        "data"
      ],
      "have_return": true,
      "code_content": "def pretty_json(data):\n    import json\n    return json.dumps(data, ensure_ascii=False, indent=2)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/tavily_extract",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/fallback_scrape",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_query_update",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/save"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "call_llm",
      "md_content": [
        "**call_llm**: The function of call_llm is to interact with the OpenAI chat completion API to generate responses based on a given prompt.\n\n**parameters**: The parameters of this Function.\n· prompt: A string that contains the input prompt for the language model.\n· model: An optional string that specifies the model to be used for generating the response, defaulting to GPT_MODEL.\n· temperature: An optional float that controls the randomness of the output, with a default value of 0.7.\n· system_prompt: An optional string that provides a system-level instruction to the model.\n\n**Code Description**: The call_llm function is designed to facilitate communication with the OpenAI chat completion service. It begins by printing a visual separator titled \"LLM Prompt\" to the console, using the rule method from the RichPrinter class to enhance output readability. The prompt provided by the user is displayed in bold yellow, and if a system prompt is supplied, it is printed in italic cyan.\n\nThe function then initializes an OpenAI client using the provided API key and base URL. It constructs a list of messages that will be sent to the API, starting with the system prompt (if provided) followed by the user prompt. This structured message format is crucial for the API to understand the context of the interaction.\n\nThe function subsequently calls the chat completion API with the specified model, messages, temperature, and a maximum token limit defined by MAX_TOKENS. The response from the API is captured, and the content of the first choice is extracted. Before returning the result, another visual separator titled \"LLM Raw Output\" is printed, followed by the raw output displayed in bold green.\n\nThe call_llm function relies on the print and rule methods from the RichPrinter class to manage console output effectively. This structured logging aids in debugging and provides clarity on the operation's flow, making it easier for developers to track the prompts sent and the responses received from the language model.\n\n**Note**: When using the call_llm function, ensure that the prompt and system prompt are clearly defined to maximize the quality of the generated response. Additionally, be mindful of the temperature setting, as it influences the creativity and variability of the output.\n\n**Output Example**: \n```\nLLM Prompt\nWhat is the capital of France?\n[system prompt]: Provide a concise answer.\nLLM Raw Output\nThe capital of France is Paris.\n```"
      ],
      "code_start_line": 89,
      "code_end_line": 116,
      "params": [
        "prompt"
      ],
      "have_return": true,
      "code_content": "def call_llm(\n    prompt: str,\n    *,\n    model: str = GPT_MODEL,\n    temperature: float = 0.7,\n    system_prompt: Optional[str] = None,\n) -> str:\n    \"\"\"Call OpenAI chat completion (stand‑alone).\"\"\"\n    printer.rule(\"LLM Prompt\")\n    printer.print(prompt, style=\"bold yellow\")\n    if system_prompt:\n        printer.print(f\"[system prompt]: {system_prompt}\", style=\"italic cyan\")\n    client = OpenAI(api_key=GPT_API_KEY, base_url=GPT_BASE_URL)\n    messages: List[Dict[str, str]] = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    resp = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=MAX_TOKENS,\n    )\n    result = resp.choices[0].message.content\n    printer.rule(\"LLM Raw Output\")\n    printer.print(result, style=\"bold green\")\n    return result\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "tavily_search",
      "md_content": [
        "**tavily_search**: The function of tavily_search is to perform an asynchronous search query using the Tavily API and return the results in the form of a list of dictionaries.\n\n**parameters**:\n· query: A string representing the search query that will be sent to the Tavily API.\n· include_raw_content: A boolean flag (default is True) that specifies whether raw content should be included in the search results. When set to True, the raw content is included; when False, it is excluded.\n\n**Code Description**:  \nThe `tavily_search` function is designed to interact with the Tavily API in an asynchronous manner. It accepts a query string as input and sends a POST request to the Tavily search endpoint with the query and an API key. It includes a boolean parameter, `include_raw_content`, which determines whether raw content should be included in the results. By default, this parameter is set to True. The function constructs a payload with the query and other parameters, and sends the request using the `httpx.AsyncClient`. \n\nOnce the request is sent, the function waits for the response and parses the returned JSON to extract the search results. These results are returned as a list of dictionaries, specifically the value found under the \"results\" key of the response.\n\nThe function is asynchronous and uses an `async with` block to manage the HTTP client, ensuring that the connection is properly managed and closed after use. Additionally, the function introduces a brief pause using `time.sleep(0.1)` after receiving the response, likely to manage the pacing of requests or avoid overwhelming the system.\n\nThe Tavily API is accessed via the `TAVILY_SEARCH_URL` constant, and the `TAVILY_API_KEY` is required for authentication. These values are assumed to be defined elsewhere in the code. The use of `http2=True` in the `AsyncClient` configuration allows for more efficient HTTP requests.\n\n**Note**: The function does not handle potential HTTP errors explicitly (e.g., network issues or API errors). In practice, you might want to add error handling to manage cases where the request fails or returns unexpected status codes.\n\n**Output Example**:  \nThe return value of the `tavily_search` function is a list of dictionaries. Each dictionary contains the search results from the Tavily API, which could look something like the following:\n\n```json\n[\n  {\n    \"title\": \"Sample Title 1\",\n    \"url\": \"https://example.com/sample1\",\n    \"snippet\": \"This is a sample snippet from the first search result.\",\n    \"raw_content\": \"Full raw content from the first result (if include_raw_content is True).\"\n  },\n  {\n    \"title\": \"Sample Title 2\",\n    \"url\": \"https://example.com/sample2\",\n    \"snippet\": \"This is a sample snippet from the second search result.\",\n    \"raw_content\": \"Full raw content from the second result (if include_raw_content is True).\"\n  }\n]\n``` \n\nIn this example, each dictionary contains a `title`, a `url`, a `snippet`, and potentially `raw_content`, depending on the value of the `include_raw_content` parameter."
      ],
      "code_start_line": 119,
      "code_end_line": 129,
      "params": [
        "query"
      ],
      "have_return": true,
      "code_content": "async def tavily_search(query: str, *, include_raw_content: bool = True) -> List[dict]:\n    # printer.rule(\"Tavily Search Query\")\n    # printer.print(query, style=\"bold yellow\")\n    payload = {\"query\": query, \"include_raw_content\": include_raw_content, \"api_key\": TAVILY_API_KEY}\n    async with httpx.AsyncClient(http2=True, timeout=30) as client:\n        r = await client.post(TAVILY_SEARCH_URL, json=payload)\n    time.sleep(0.1)\n    results = r.json().get(\"results\", [])\n    # printer.rule(\"Tavily Search Results\")\n    # printer.print(pretty_json(results), style=\"bold green\")\n    return results\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/generate_seed"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "tavily_extract",
      "md_content": [
        "**tavily_extract**: The function of tavily_extract is to send a request to the Tavily API for extracting information from a list of URLs and return the results in JSON format.\n\n**parameters**: The parameters of this Function.  \n· urls: A list of strings, where each string is a URL to be processed by the Tavily API.\n\n**Code Description**:  \nThe tavily_extract function is an asynchronous function designed to send a POST request to the Tavily API to extract data from a list of provided URLs. Upon calling this function, the following sequence of events occurs:\n\n1. **Logging the URLs**: The function begins by printing a visual separator in the console using the `printer.rule` function, with the title \"Tavily Extract URLs\". This is followed by the `printer.print` function, which prints the provided list of URLs in a formatted JSON style, making the data more readable in the console output.\n\n2. **Sending the Request**: The function uses the `httpx.AsyncClient` to initiate an asynchronous HTTP POST request to the Tavily API. The request sends the list of URLs (`urls`) in the body as a JSON object. Additionally, it includes an authorization header with a bearer token (`TAVILY_API_KEY`), which is necessary for authenticating the API request.\n\n3. **Processing the Response**: Once the request is sent, the function awaits the response from the API. The `r.json()` method is called to parse the JSON response returned by the API, which contains the results of the URL extraction.\n\n4. **Logging the Results**: After receiving the response, the function prints another visual separator using `printer.rule`, this time with the title \"Tavily Extract Results\". The response data is then printed in a formatted JSON style using `printer.print`, making it easier for developers to review the results in a human-readable format.\n\n5. **Returning the Result**: Finally, the parsed JSON result is returned by the function, making the extracted data available to the caller.\n\nThroughout the process, the `printer.rule` and `printer.print` methods are used to provide visual feedback to the user, ensuring that key stages (like the URLs being processed and the results from the API) are clearly separated and easy to identify in the console output. The function is dependent on the external `httpx` library for making asynchronous HTTP requests and assumes the presence of an API key for authorization.\n\n**Note**:  \n- Ensure that the `TAVILY_API_KEY` environment variable or configuration is correctly set, as it is required for authorization when making the API request.\n- The `urls` parameter should be a list of valid URLs that the Tavily API can process. Invalid or malformed URLs may lead to errors in the response or empty results.\n- The function relies on asynchronous programming, so it should be called within an asynchronous context (i.e., using `await`).\n\n**Output Example**:  \nA sample return value from the Tavily API might look like this:\n```json\n{\n  \"results\": [\n    {\n      \"url\": \"https://example.com\",\n      \"title\": \"Example Domain\",\n      \"description\": \"This is an example domain used for illustrative purposes.\"\n    },\n    {\n      \"url\": \"https://another-example.com\",\n      \"title\": \"Another Example Domain\",\n      \"description\": \"This is another example for demonstration.\"\n    }\n  ]\n}\n```"
      ],
      "code_start_line": 131,
      "code_end_line": 143,
      "params": [
        "urls"
      ],
      "have_return": true,
      "code_content": "async def tavily_extract(urls: List[str]) -> Dict[str, dict]:\n    printer.rule(\"Tavily Extract URLs\")\n    printer.print(pretty_json(urls), style=\"bold yellow\")\n    async with httpx.AsyncClient(http2=True, timeout=30) as client:\n        r = await client.post(\n            TAVILY_EXTRACT_URL,\n            json={\"urls\": urls},\n            headers={\"Authorization\": f\"Bearer {TAVILY_API_KEY}\"},\n        )\n    result = r.json()\n    printer.rule(\"Tavily Extract Results\")\n    printer.print(pretty_json(result), style=\"bold green\")\n    return result\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/pretty_json"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "fallback_scrape",
      "md_content": [
        "**fallback_scrape**: The function of fallback_scrape is to scrape content from a list of URLs using an asynchronous approach, and return the extracted text in a structured format.\n\n**parameters**: The parameters of this Function.\n· urls: A list of strings where each string is a URL to scrape content from.\n\n**Code Description**: The `fallback_scrape` function is an asynchronous function designed to fetch content from a list of URLs. It uses the `httpx.AsyncClient` for making HTTP requests asynchronously and `BeautifulSoup` for parsing the HTML content of each URL.\n\n1. **Input**: The function accepts a list of URLs (`urls`), where each URL represents a webpage to be scraped for its content.\n   \n2. **Process**:\n   - The function first prints a visual separator using the `rule` method of the `RichPrinter` class, with the title \"Fallback Scrape URLs\". This is followed by printing the list of URLs in a formatted manner using the `pretty_json` function.\n   - The `fetch` function is defined inside `fallback_scrape` to handle the scraping of individual URLs. This function attempts to fetch the HTML content of a URL using `httpx.AsyncClient` with a timeout of 10 seconds. If the response status is not 200 (indicating failure to fetch the page), it returns an empty string.\n   - Once the HTML content is retrieved, it is parsed using `BeautifulSoup`. Any unwanted tags (e.g., `script`, `style`, `noscript`, `meta`) are removed using the `decompose()` method. The content inside the `<p>` tags is then extracted, with all the text from each paragraph being concatenated into a single string.\n   - The function uses `asyncio.gather` to concurrently fetch content from all URLs in the provided list.\n\n3. **Output**: After processing all URLs, a dictionary (`result`) is constructed where each URL is associated with its corresponding scraped text. This dictionary is printed in a formatted manner using the `pretty_json` function, preceded by another visual separator, and returned as the output.\n\n4. **Callees**:\n   - The `rule` and `print` methods from the `RichPrinter` class are used to format and output messages and separators in the console, ensuring clear presentation of logs and results.\n   - The `pretty_json` function is used to convert the URLs and the results into a readable JSON format for console display.\n\n**Note**: \n- The function relies on asynchronous behavior to handle multiple URL requests concurrently. It is crucial that the URLs provided are accessible and the server allows for non-blocking, concurrent access.\n- The function returns an empty string for any URL that cannot be fetched or parsed correctly. Therefore, users should ensure that the URLs provided are valid and reachable.\n- This function is primarily useful in scenarios where content scraping from multiple pages needs to be done in parallel to optimize processing time.\n\n**Output Example**: The return value of `fallback_scrape` might look like the following JSON structure:\n\n```json\n{\n  \"https://example.com/page1\": \"This is the content extracted from page 1.\",\n  \"https://example.com/page2\": \"This is the content extracted from page 2.\",\n  \"https://example.com/page3\": \"This is the content extracted from page 3.\"\n}\n```"
      ],
      "code_start_line": 145,
      "code_end_line": 165,
      "params": [
        "urls"
      ],
      "have_return": true,
      "code_content": "async def fallback_scrape(urls: List[str]) -> Dict[str, str]:\n    printer.rule(\"Fallback Scrape URLs\")\n    printer.print(pretty_json(urls), style=\"bold yellow\")\n    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n    async def fetch(u: str) -> str:\n        try:\n            async with httpx.AsyncClient(timeout=10) as client:\n                r = await client.get(u, headers=headers)\n            if r.status_code != 200:\n                return \"\"\n            soup = BeautifulSoup(r.text, \"html.parser\")\n            for t in soup([\"script\", \"style\", \"noscript\", \"meta\"]):\n                t.decompose()\n            return \"\\n\".join(p.get_text(\" \", strip=True) for p in soup.find_all(\"p\"))\n        except Exception:\n            return \"\"\n    texts = await asyncio.gather(*(fetch(u) for u in urls))\n    result = {u: t for u, t in zip(urls, texts)}\n    printer.rule(\"Fallback Scrape Results\")\n    printer.print(pretty_json(result), style=\"bold green\")\n    return result\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/pretty_json"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "fetch",
      "md_content": [
        "**fetch**: The function of fetch is to asynchronously retrieve and parse the text content from a specified URL, returning the text of all paragraph elements while excluding certain HTML tags.\n\n**parameters**: The parameters of this Function.\n· u: A string representing the URL from which the content is to be fetched.\n\n**Code Description**: The fetch function is designed to perform an asynchronous HTTP GET request to the provided URL using the httpx library. It initializes an asynchronous HTTP client with a timeout of 10 seconds. Upon making the request, it checks the response status code. If the status code is not 200 (indicating a successful request), the function returns an empty string. If the request is successful, the function proceeds to parse the response text using BeautifulSoup, a library for parsing HTML and XML documents. \n\nThe function specifically removes unwanted elements from the parsed HTML, such as script, style, noscript, and meta tags, by calling the decompose method on each of these elements. This ensures that only the relevant text content remains. Finally, the function collects the text from all paragraph tags (<p>) found in the document, joining them into a single string with spaces between the texts of individual paragraphs and stripping any leading or trailing whitespace. If any exceptions occur during this process, the function catches them and returns an empty string to indicate failure.\n\n**Note**: It is important to ensure that the URL provided is valid and accessible. The function is designed to handle exceptions gracefully, returning an empty string in case of errors, which may be useful for error handling in the calling code.\n\n**Output Example**: An example of the return value from the fetch function could be:\n\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\""
      ],
      "code_start_line": 149,
      "code_end_line": 160,
      "params": [
        "u"
      ],
      "have_return": true,
      "code_content": "    async def fetch(u: str) -> str:\n        try:\n            async with httpx.AsyncClient(timeout=10) as client:\n                r = await client.get(u, headers=headers)\n            if r.status_code != 200:\n                return \"\"\n            soup = BeautifulSoup(r.text, \"html.parser\")\n            for t in soup([\"script\", \"style\", \"noscript\", \"meta\"]):\n                t.decompose()\n            return \"\\n\".join(p.get_text(\" \", strip=True) for p in soup.find_all(\"p\"))\n        except Exception:\n            return \"\"\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_boxed",
      "md_content": [
        "**extract_boxed**: The function of extract_boxed is to extract the content within a LaTeX-style boxed expression from a given text.\n\n**parameters**: The parameters of this function.\n· text: A string that potentially contains a LaTeX-style boxed expression (e.g., `\\boxed{...}`).\n\n**Code Description**:  \nThe `extract_boxed` function is responsible for extracting content enclosed within a LaTeX-style `\\boxed{}` from a given input text. The function uses a regular expression (`BOXED_RE`) to search for the pattern of a boxed expression within the provided string. If a match is found, it returns the content inside the box by accessing the matched group and stripping any leading or trailing whitespace. If no match is found, the function returns an empty string. \n\nThe regular expression is designed to locate text within the specific `\\boxed{}` syntax, which is commonly used in mathematical or structured expressions, particularly in contexts where the content needs to be emphasized or highlighted.\n\nThe function is used in multiple places within the project. It plays a critical role in verifying the answers generated by models in the context of QA (Question-Answer) validation. For instance, in the `multi_verify` method of the `ReverseUpgradeWorkflow` class, `extract_boxed` is called to extract the boxed answer from the model's response after it generates an answer to a question. The extracted boxed content is then compared to the expected answer to determine if the model's response is valid or if further verification steps (like search-based answers) are needed.\n\nAdditionally, in the `evaluate` function, the `extract_boxed` function is used to retrieve the boxed answer from the model's response to a question during evaluation, allowing for the comparison of the predicted answer with the ground truth answer.\n\nThis ensures that the model's output is consistent with the expected format, and it helps automate the validation process in scenarios involving multiple checks and retries.\n\n**Note**:  \n- The function assumes that the text will contain a properly formatted LaTeX `\\boxed{}` expression. If the input text does not follow this structure, an empty string is returned.\n- The function is case-sensitive and depends on the correctness of the regular expression (`BOXED_RE`) to identify the boxed content.\n\n**Output Example**:  \nIf the input text is `This is the model's response: \\boxed{42}`, the function will return the string `42`.  \nIf the input text is `The answer is \\boxed{Sorry, I don't know.}`, the function will return `Sorry, I don't know.`  \nIf no boxed expression is found, such as in the input `This is just a regular sentence.`, the function will return an empty string."
      ],
      "code_start_line": 171,
      "code_end_line": 173,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def extract_boxed(text: str) -> str:\n    m = BOXED_RE.search(text)\n    return m.group(1).strip() if m else \"\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/evaluate"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_answer_tag",
      "md_content": [
        "**extract_answer_tag**: The function of extract_answer_tag is to extract and return the answer tag from a given text string.\n\n**parameters**: The parameters of this Function.\n· text: A string input that contains the text from which the answer tag needs to be extracted.\n\n**Code Description**: The extract_answer_tag function utilizes a regular expression search to identify a specific pattern within the provided text. It employs the variable ANSWER_TAG_RE, which is expected to be a compiled regular expression pattern designed to match the answer tag format. The function searches the input text for this pattern. If a match is found, it retrieves the first capturing group from the match object using m.group(1), which is then stripped of any leading or trailing whitespace. If no match is found, the function returns an empty string. This function is particularly useful in scenarios where the text may contain structured data, and the answer tag needs to be isolated for further processing or analysis.\n\n**Note**: It is important to ensure that the ANSWER_TAG_RE variable is properly defined and compiled before calling this function. Additionally, the input text should be formatted correctly to increase the likelihood of a successful match.\n\n**Output Example**: If the input text is \"The answer is <answer>42</answer>\", and the ANSWER_TAG_RE is designed to capture the content within the <answer> tags, the function would return \"42\". If the input text does not contain a valid answer tag, the function would return an empty string."
      ],
      "code_start_line": 175,
      "code_end_line": 177,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def extract_answer_tag(text: str) -> str:\n    m = ANSWER_TAG_RE.search(text)\n    return m.group(1).strip() if m else \"\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "QAItem",
      "md_content": [
        "**QAItem**: The function of QAItem is to represent a question-answer item with associated metadata for processing in a QA system.\n\n**attributes**: The attributes of this Class.\n· level: int - Represents the depth level of the QAItem in a hierarchical structure.\n· question: str - The question text associated with this QAItem.\n· answer: str - The answer text corresponding to the question.\n· parent_question: Optional[str] - The question from which this QAItem is derived, if applicable.\n· evidence: List[str] - A list of evidence strings that support the answer provided.\n· strategy: str - The strategy used to derive or validate the answer.\n\n**Code Description**: The QAItem class is designed to encapsulate the essential components of a question-answer pair within a QA system. It includes attributes that define the question, its answer, the level of the item in a potential hierarchy, and any evidence that supports the answer. The class also includes a method, `to_dict`, which converts the QAItem instance into a dictionary format using the `asdict` function. This is useful for serialization or for passing the data in a structured format to other components of the system.\n\nThe QAItem class is utilized in various parts of the project, particularly within the ReverseUpgradeWorkflow module. It serves as a foundational data structure for multiple functions that handle the generation, verification, and updating of question-answer pairs. For instance, in the `generate_seed` method, a new QAItem is created to represent a seed fact generated from a query. Similarly, in the `query_update` method, an existing QAItem is updated based on new evidence and questions derived from a search process. The `multi_verify` method also takes a QAItem as input to validate whether the model can answer the question directly or through search results.\n\nThese interactions highlight the QAItem's role as a central component in managing and processing question-answer pairs throughout the workflow, ensuring that the system can effectively track and utilize these items in various operations.\n\n**Note**: When using the QAItem class, ensure that the attributes are populated correctly to maintain the integrity of the question-answer relationships. The `to_dict` method can be particularly useful for debugging or logging purposes, as it provides a clear representation of the QAItem's data.\n\n**Output Example**: A possible appearance of the code's return value when calling `to_dict` on a QAItem instance might look like this:\n```json\n{\n    \"level\": 1,\n    \"question\": \"What is the capital of France?\",\n    \"answer\": \"Paris\",\n    \"parent_question\": null,\n    \"evidence\": [\"France is a country in Europe.\", \"Paris is known as the capital city.\"],\n    \"strategy\": \"direct\"\n}\n```"
      ],
      "code_start_line": 181,
      "code_end_line": 190,
      "params": [],
      "have_return": true,
      "code_content": "class QAItem:\n    level: int\n    question: str\n    answer: str\n    parent_question: Optional[str]\n    evidence: List[str]\n    strategy: str\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/__init__",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_query_update"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "to_dict",
      "md_content": [
        "**to_dict**: The function of to_dict is to convert the object instance into a dictionary representation.\n\n**parameters**: \n- This function does not take any parameters.\n\n**Code Description**: \nThe `to_dict` method is a simple function that converts an instance of the class it belongs to into a dictionary using the `asdict` function. The `asdict` function is part of the `dataclasses` module and is commonly used to convert data class instances into dictionaries where each attribute of the instance becomes a key in the resulting dictionary, with the attribute values as the corresponding values.\n\nThis function does not perform any complex operations. It is typically used when there is a need to transform an object into a format (i.e., a dictionary) that can be easily serialized, logged, or processed in other ways, such as for saving to a file or sending over a network.\n\nThe `to_dict` method is invoked multiple times within the project in different contexts. For example, within the `save` method in the `ReverseUpgradeWorkflow` class, it is called on each item in the `self.items` list. This converts each item (which is presumably an object) into a dictionary representation before saving it into a JSON file. This operation is repeated for each item in the list, and the resulting dictionaries are aggregated and written to the file. Similarly, the `to_dict` method is used in the `run_one` function to convert workflow items into dictionaries that are then appended to an existing JSON file. This method is essential in ensuring that object data is easily transformed for persistent storage.\n\n**Note**: The `to_dict` function should be used when there is a need to convert an object into a dictionary format. It assumes that the class the method is part of is a dataclass or has been designed to support the `asdict` function.\n\n**Output Example**: \nIf an instance of a class contains attributes like `name`, `age`, and `location`, the return value of the `to_dict` method might look like the following:\n\n```python\n{\n    \"name\": \"John Doe\",\n    \"age\": 30,\n    \"location\": \"New York\"\n}\n```"
      ],
      "code_start_line": 189,
      "code_end_line": 190,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def to_dict(self) -> dict:\n        return asdict(self)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/save",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/main/_batch/run_one"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "ReverseUpgradeWorkflow",
      "md_content": [
        "**ReverseUpgradeWorkflow**: The function of ReverseUpgradeWorkflow is to manage a multi-level question-answering workflow that verifies and updates questions based on model responses.\n\n**attributes**: The attributes of this Class.\n· max_level: An integer that defines the maximum number of levels for question upgrades, defaulting to 5.\n· max_tries: An integer that specifies the maximum number of attempts for each upgrade, defaulting to 5.\n· items: A list that stores instances of QAItem, which represent the questions and answers processed during the workflow.\n\n**Code Description**: The ReverseUpgradeWorkflow class is designed to facilitate a structured approach to enhancing question-answering capabilities through iterative verification and updates. Upon initialization, it accepts parameters for max_level and max_tries, which dictate how many levels of question upgrades will be attempted and how many retries will be allowed for each upgrade, respectively.\n\nThe class contains several key methods:\n\n- `__init__`: Initializes the workflow with specified max_level and max_tries, and prepares an empty list for storing QAItem instances.\n- `random_domain`: A static method that randomly selects a domain from a predefined list, which is used to generate seed questions.\n- `method_choice`: This method determines which abstraction method to use for updating a question based on the model's response to a provided question and answer.\n- `multi_verify`: This asynchronous method performs multiple checks to verify if the model can answer a given question. It attempts to get a direct answer from the model and, if that fails, searches for relevant information to check if the model can answer based on search results.\n- `query_update`: This asynchronous method updates a question based on the chosen method and the queries generated, returning a new QAItem with the updated question and evidence.\n- `generate_seed`: This asynchronous method generates an initial QAItem (seed) by querying a random domain and extracting relevant questions and answers.\n- `run`: This asynchronous method orchestrates the entire workflow, generating a seed and iteratively upgrading questions up to the specified max_level, while ensuring that each upgrade passes the verification checks.\n- `save`: This method saves the results of the workflow to a specified file path, appending new items to any existing data.\n- `gpt_search_generate_seed`, `gpt_search_query_update`, and `gpt_search_run`: These methods are stubs for implementing a specialized workflow using GPT Search, indicating future functionality that will be added.\n\nThe ReverseUpgradeWorkflow class is invoked in the main function of the project, where it is instantiated and run based on command-line arguments. The main function handles both single and batch executions of the workflow, allowing for flexibility in how the workflow is executed and results are saved. In batch mode, it utilizes asynchronous tasks to run multiple instances of the workflow concurrently, managing the results efficiently.\n\n**Note**: Users should ensure that the necessary dependencies and configurations are in place for the agent and other components used within the workflow. Proper error handling is implemented to manage exceptions during execution.\n\n**Output Example**: A possible output of the workflow could be a JSON file containing multiple QAItem entries, each with fields such as question, answer, level, and evidence, structured as follows:\n\n```json\n[\n    {\n        \"level\": 0,\n        \"question\": \"What is the capital of France?\",\n        \"answer\": \"Paris\",\n        \"parent_question\": null,\n        \"evidence\": [],\n        \"strategy\": \"seed\"\n    },\n    {\n        \"level\": 1,\n        \"question\": \"Can you name a famous landmark in Paris?\",\n        \"answer\": \"Eiffel Tower\",\n        \"parent_question\": \"What is the capital of France?\",\n        \"evidence\": [],\n        \"strategy\": \"equivalent replacement\"\n    }\n]\n```"
      ],
      "code_start_line": 196,
      "code_end_line": 485,
      "params": [],
      "have_return": true,
      "code_content": "class ReverseUpgradeWorkflow:\n    def __init__(self, *, max_level: int = 5, max_tries: int = 5):\n        self.max_level = max_level\n        self.max_tries = max_tries\n        self.items: List[QAItem] = []\n\n    @staticmethod\n    def random_domain():\n        domains = [\n            \"TV shows & movies\",\n            \"Other\",\n            \"Science & technology\",\n            \"Art\",\n            \"History\",\n            \"Sports\",\n            \"Music\",\n            \"Video games\",\n            \"Geography\",\n            \"Politics\",\n        ]\n        return random.choice(domains)\n    \n    def method_choice(self, question: str, answer: str) -> str:\n        \n        methods = [\n            \"equivalent replacement\",\n            \"simple abstraction\",\n        ]\n        model_choice = agent.chat_with_template(\n            template_name=\"abs_method_choice.txt\",\n            template_data={\"question\": question, \"answer\": answer},\n            root_folder=PROMPT_ROOT_FOLDER,\n        )\n        method = extract_tag_content(model_choice, \"method\")\n        queries = extract_tag_content(model_choice, \"queries\")\n\n        norm = method.strip().lower().replace(\" \", \"\")\n        for m in methods:\n            if norm == m.lower().replace(\" \", \"\"):\n                return m, queries\n            \n        return methods[0], queries\n    \n    async def multi_verify(self, seed: QAItem) -> bool:\n        \"\"\"\n        对给定的 QAItem 进行多重校验。\n        Returns:\n            bool: True 表示验证通过（模型无法回答），False 表示验证失败（模型能回答）\n        \"\"\"\n        prompt = (\n            \"You need to answer the question\\n\"\n            \"if you cannot answer, please generate \\\\boxed{Sorry, I don't know.}\\n\"\n            f\"Question: {seed.question}\\n\"\n            f\"{FORMAT_INSTRUCTION}\"\n        )\n        \n        # 第一步：直接校验模型内部知识是否能回答\n        max_retries = 5\n        for attempt in range(1, max_retries + 1):\n            direct_answer = agent.chat(prompt, model = \"gpt-4o\")\n            boxed_answer = extract_boxed(direct_answer)\n            if not boxed_answer:\n                printer.rule(f\"格式错误重试 #{attempt}\")\n                printer.print(\"模型原始返回：\", style=\"bold red\")\n                printer.print(direct_answer, style=\"bold red\")\n                if attempt == max_retries:\n                    raise RuntimeError(f\"连续 {max_retries} 次格式错误（答案未包含在 \\\\boxed{{}} 中），终止执行\")\n                continue\n            \n            if boxed_answer == seed.answer:    \n                printer.print(\"验证失败：模型能直接回答\", style=\"bold red\")\n                return False\n            break\n\n        # 第二步：如果模型不能直接回答，进行搜索\n        search_results = await tavily_search(seed.question)\n        \n        for attempt in range(1, max_retries + 1):\n            search_based_answer = agent.chat(prompt + f\"here are some search results:\\n\\n{search_results}\", model = \"gpt-4o\") \n            boxed_answer = extract_boxed(search_based_answer)\n            if not boxed_answer:\n                printer.rule(f\"搜索验证格式错误重试 #{attempt}\")\n                printer.print(\"模型原始返回：\", style=\"bold red\")\n                printer.print(search_based_answer, style=\"bold red\")\n                if attempt == max_retries:\n                    raise RuntimeError(f\"连续 {max_retries} 次格式错误（基于搜索的答案未包含在 \\\\boxed{{}} 中），终止执行\")\n                continue\n                \n            if boxed_answer == seed.answer:\n                printer.print(\"验证失败：模型能通过搜索回答\", style=\"bold red\")\n                return False\n            break\n\n        printer.print(\"验证通过：模型无法直接回答也无法通过搜索回答\", style=\"bold green\")\n        return True\n    \n    async def query_update(self, method, queries, seed: Optional[QAItem]):\n        search_results = await asyncio.gather(*(tavily_search(q) for q in queries)) \n        question_update_resp = agent.chat_with_template(\n            template_name=\"abs_query_update.txt\",\n            template_data={\"method\": method, \"search_results\": search_results, \"question\": seed.question, \"answer\": seed.answer},\n            root_folder=PROMPT_ROOT_FOLDER,\n        )\n        # 提取出update之后的question\n        updated_item = extract_and_validate_json(extract_tag_content(question_update_resp, \"data\"))\n        updated_question = updated_item[\"updated_question\"].strip()\n        updated_evidence = updated_item.get(\"updated_evidence\", [])\n\n        printer.rule(\"Query Update Output\")\n        printer.print(pretty_json(updated_item), style=\"bold green\")\n        printer.rule(\"Query Update Evidence\")       \n        printer.print(pretty_json(updated_evidence), style=\"bold green\")\n\n        return QAItem(\n            level=seed.level + 1,\n            question=updated_question,\n            answer=seed.answer,\n            parent_question=seed.question,\n            evidence=seed.evidence + updated_evidence,\n            strategy=method,\n        )   \n\n    async def generate_seed(self) -> QAItem:\n        max_query_retries = 3\n        for attempt in range(1, max_query_retries + 1):\n            random_domain = self.random_domain()\n            query_resp = agent.chat_with_template(\n                template_name=\"search_query_for_seed_fact.txt\",\n                template_data={\"domain\": random_domain},\n                root_folder=PROMPT_ROOT_FOLDER,\n            )\n            queries = extract_queries_from_response(query_resp)[:3]\n            if queries:\n                printer.rule(f\"Extracted Queries for Seed Fact in {random_domain} Domain\")\n                printer.print(queries, style=\"bold cyan\")\n                break\n            printer.print(f\"第 {attempt} 次抽取到空 queries，重试中…\", style=\"bold red\")\n            printer.print(query_resp, style=\"bold red\")\n        else:\n            raise RuntimeError(\"连续 3 次 Extracted Queries 为空，终止执行\")\n\n        search_results = await asyncio.gather(*(tavily_search(q) for q in queries))\n\n        seed_fact_resp = extract_and_validate_json(agent.chat_with_template(            template_name=\"seed_idea_from_internet.txt\",            template_data={\"domain\": random_domain,\"search_results\": search_results,},             root_folder=PROMPT_ROOT_FOLDER        ))\n        printer.rule(f\"Generate Seed Fact Output after browsing in {random_domain}\")\n        printer.print(pretty_json(seed_fact_resp), style=\"bold green\")\n        return QAItem(\n            level=0,\n            question=seed_fact_resp[\"seed\"][\"question\"].strip(),\n            answer=seed_fact_resp[\"seed\"][\"answer\"].strip(),\n            parent_question=None,\n            evidence=seed_fact_resp[\"seed\"].get(\"evidence\", []),\n            strategy=\"seed\",\n        )\n\n    async def run(self):\n        printer.rule(f\"Workflow Start with {GPT_MODEL}\")\n        seed = await self.generate_seed()\n        self.items.append(seed)\n        current = seed\n\n        # 按照 max_level 和 max_tries 进行多级问题升级\n        for level in range(self.max_level):\n            retries = 0\n            while retries < self.max_tries:\n                retries += 1\n                printer.rule(f\"Level {level+1} Update Attempt {retries}\")\n                method, queries = self.method_choice(current.question, current.answer)\n                # 随机化一下方法，以后看表现调整\n                method = random.choice([method, \"simple abstraction\"])\n                updated = await self.query_update(method, queries, current)\n                passed = await self.multi_verify(updated)\n                if not passed:  # 验证失败（模型能回答）\n                    printer.print(\"多重校验未通过（模型能回答），重试 query_update …\", style=\"bold red\")\n                    continue\n                printer.print(\"多重校验通过（模型无法回答），记录更新后的 QAItem\", style=\"bold green\")\n                self.items.append(updated)  # 只有验证通过才记录\n                current = updated\n                break\n            else:\n                printer.print(f\"Stopped at level {current.level}; no valid update after {self.max_tries} tries.\", style=\"bold red\")\n                return\n\n        printer.rule(\"Workflow End\")\n\n    async def gpt_search_generate_seed(self, domain) -> QAItem:\n        \"\"\"Stub for GPT Search seed generation.\"\"\"\n        # TODO: 实现 GPT Search 专用的 seed 生成逻辑\n        resp = agent.chat_with_template(template_name=\"gpt_search_seed.txt\", root_folder=PROMPT_ROOT_FOLDER, template_data={\"domain\": domain})\n        json_resp = extract_and_validate_json(resp)\n        if not json_resp:\n            raise RuntimeError(\"GPT-Search seed generation failed\")\n        \n        return QAItem(\n                level=0,\n                question=json_resp[\"seed\"][\"question\"].strip(),\n                answer=json_resp[\"seed\"][\"answer\"].strip(),\n                parent_question=None,\n                evidence=json_resp[\"seed\"].get(\"evidence\", []),\n                strategy=\"seed\",\n            )\n\n    async def gpt_search_query_update(self, seed: QAItem) -> QAItem:\n        \"\"\"Stub for GPT Search query update.\"\"\"\n        # TODO: 实现 GPT Search 专用的 query update 逻辑\n\n        resp = agent.chat_with_template(\n            template_name=\"gpt_search_Q_update.txt\",\n            template_data={\"question\": seed.question, \"answer\": seed.answer},\n            root_folder=PROMPT_ROOT_FOLDER,\n        )\n\n        json_resp = extract_and_validate_json(extract_tag_content(resp, \"data\"))\n        if not json_resp:\n            raise RuntimeError(\"GPT-Search query update failed\")\n        \n        # 提取出update之后的question\n        updated_question = json_resp[\"updated_question\"].strip()\n        updated_evidence = json_resp.get(\"updated_evidence\", [])\n        method = json_resp.get(\"method\", \"None\")\n        printer.rule(\"GPT-Search Query Update Output\")\n        printer.print(pretty_json(json_resp), style=\"bold green\")\n        printer.rule(\"GPT-Search Query Update Evidence\")\n        printer.print(pretty_json(updated_evidence), style=\"bold green\")\n        # 记录更新后的 QAItem\n        updated = QAItem(\n            level=seed.level + 1,\n            question=updated_question,\n            answer=seed.answer,\n            parent_question=seed.question,\n            evidence=seed.evidence + updated_evidence,\n            strategy=method,\n        )\n\n        return updated\n\n    async def gpt_search_run(self):\n        GPT_MODEL = \"gpt-4o-search-preview\"\n        printer.rule(f\"GPT Search Workflow Start with {GPT_MODEL}\")\n        # 第一步：Seed 生成\n        seed = await self.gpt_search_generate_seed()\n        self.items.append(seed)\n        current = seed\n\n        # 按照 max_level 和 max_tries 进行多级问题升级\n        for level in range(self.max_level):\n            retries = 0\n            while retries < self.max_tries:\n                retries += 1\n                printer.rule(f\"GPT-Search Level {level+1} Update Attempt {retries}\")\n                # 使用 GPT Search 专用的 query update\n                updated = await self.gpt_search_query_update(current)\n                # 多重校验\n                passed = await self.multi_verify(updated)\n                if not passed:\n                    printer.print(\"GPT-Search: 多重校验未通过（模型能回答），重试 query_update …\", style=\"bold red\")\n                    continue\n                printer.print(\"GPT-Search: 多重校验通过（模型无法回答），记录更新后的 QAItem\", style=\"bold green\")\n                self.items.append(updated)\n                current = updated\n                break\n            else:\n                printer.print(\n                    f\"GPT-Search stopped at level {current.level}; no valid update after {self.max_tries} tries.\",\n                    style=\"bold red\",\n                )\n                return\n\n        printer.rule(\"GPT-Search Workflow End\")\n\n    def save(self, path: Path):\n        printer.rule(\"Saving Results\")\n        with file_lock:\n            existing = []\n            if path.exists():\n                with open(path, \"r\", encoding=\"utf-8\") as f:\n                    existing = json.load(f)\n            existing.extend([it.to_dict() for it in self.items])\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(existing, f, ensure_ascii=False, indent=2)\n        printer.print(f\"Saved {len(self.items)} items to {path}\", style=\"bold green\")\n        printer.rule(\"Saved JSON Preview\")\n        printer.print(pretty_json([it.to_dict() for it in self.items]), style=\"bold cyan\")\n\n    @staticmethod\n    def _safe_json(text: str) -> dict:\n        m = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)```\", text)\n        raw = m.group(1) if m else text\n        raw = raw.strip(\"`\\n \")\n        return json.loads(raw)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/main",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/main/_batch/run_one"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the attributes of an instance of the class with default or provided values.\n\n**parameters**: The parameters of this Function.\n· max_level: int - The maximum level for the workflow. It determines how deep the process can go in terms of hierarchical structure. Default is 5.\n· max_tries: int - The maximum number of attempts the workflow will make. It serves as a control for the retry mechanism in the workflow. Default is 5.\n\n**Code Description**: \nThe `__init__` method is a constructor that initializes an instance of a class. This method accepts two optional keyword arguments: `max_level` and `max_tries`. The parameter `max_level` is set to 5 by default, meaning the workflow can go up to a depth of 5 levels unless specified otherwise. The `max_tries` parameter is also set to 5 by default, representing the maximum number of retry attempts for a process in the workflow.\n\nThis method also initializes the `items` attribute as an empty list of type `QAItem`. `QAItem` is a class that represents a question-answer pair in a QA system, and its use is central to the operations in the workflow. By initializing this attribute as an empty list, the constructor prepares the object to store multiple instances of `QAItem`, which are later added during the workflow execution.\n\nThe purpose of this method is to ensure that each instance of the class starts with well-defined attributes and is ready to begin processing with sensible default values. The parameters can be adjusted based on specific needs for more fine-tuned control over the workflow's depth and retry behavior.\n\n**Note**: Ensure that the values for `max_level` and `max_tries` are set appropriately based on the workflow requirements. Additionally, the `items` attribute will store `QAItem` objects, which are essential for tracking the various question-answer pairs throughout the workflow process. If there is a need to modify the behavior based on different depth levels or retry attempts, those values should be passed explicitly when creating an instance of the class."
      ],
      "code_start_line": 197,
      "code_end_line": 200,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, *, max_level: int = 5, max_tries: int = 5):\n        self.max_level = max_level\n        self.max_tries = max_tries\n        self.items: List[QAItem] = []\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/QAItem"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "random_domain",
      "md_content": [
        "**random_domain**: The function of random_domain is to select and return a random domain from a predefined list of categories.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The random_domain function is designed to return a random choice from a list of specific domains related to various fields of interest. The domains included in the list are \"TV shows & movies,\" \"Other,\" \"Science & technology,\" \"Art,\" \"History,\" \"Sports,\" \"Music,\" \"Video games,\" \"Geography,\" and \"Politics.\" The function utilizes the random.choice method to select one of these domains at random.\n\nThis function is called within the generate_seed method of the ReverseUpgradeWorkflow class. In this context, random_domain is used to obtain a domain that will be incorporated into a search query template. The selected domain is essential for generating relevant queries for seed facts, which are then used to gather information from an external agent. The generate_seed method attempts to extract queries based on the randomly selected domain, and if successful, it proceeds to gather search results and generate a seed fact output.\n\n**Note**: It is important to ensure that the random_domain function is called within an appropriate context where the randomness of the domain selection is beneficial for generating diverse queries.\n\n**Output Example**: An example of the possible return value of the random_domain function could be \"Science & technology.\""
      ],
      "code_start_line": 203,
      "code_end_line": 216,
      "params": [],
      "have_return": true,
      "code_content": "    def random_domain():\n        domains = [\n            \"TV shows & movies\",\n            \"Other\",\n            \"Science & technology\",\n            \"Art\",\n            \"History\",\n            \"Sports\",\n            \"Music\",\n            \"Video games\",\n            \"Geography\",\n            \"Politics\",\n        ]\n        return random.choice(domains)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/generate_seed"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "method_choice",
      "md_content": [
        "**method_choice**: The function of method_choice is to determine the appropriate method for processing a given question and answer pair based on user input and predefined methods.\n\n**parameters**: The parameters of this Function.\n· question: A string representing the question that needs to be processed.  \n· answer: A string representing the answer associated with the question.  \n\n**Code Description**: The method_choice function is designed to select a processing method for a given question and answer pair. It begins by defining a list of available methods, specifically \"equivalent replacement\" and \"simple abstraction\". The function then utilizes the chat_with_template method from the BaseAgent class to interact with a conversational model. This method sends a template named \"abs_method_choice.txt\" along with the provided question and answer as template data, allowing the model to generate a response that includes a suggested method and related queries.\n\nThe response from the chat_with_template function is processed using the extract_tag_content function, which extracts the content associated with the \"method\" and \"queries\" tags from the model's output. The extracted method is normalized by stripping whitespace and converting it to lowercase for comparison purposes. The function then checks if the normalized method matches any of the predefined methods in the methods list. If a match is found, it returns the matched method along with the extracted queries.\n\nIf no match is found, the function defaults to returning the first method in the methods list (\"equivalent replacement\") along with the extracted queries. This ensures that the function always provides a valid method and associated queries, even if the model's suggestion is not recognized.\n\nThe method_choice function is called within the run method of the ReverseUpgradeWorkflow class. In this context, it plays a critical role in determining how to update a question-answer item during a multi-level upgrade process. The run method iterates through levels of updates, and for each attempt, it calls method_choice to decide on the processing method based on the current question and answer. This integration highlights the importance of method_choice in facilitating dynamic interactions with the conversational model and ensuring the workflow progresses effectively.\n\n**Note**: It is essential to ensure that the template \"abs_method_choice.txt\" exists in the specified directory and that the question and answer parameters are properly formatted to avoid errors during processing.\n\n**Output Example**: A possible return value from the method_choice function could be:\n```\n(\"simple abstraction\", [\"query1\", \"query2\"])\n```"
      ],
      "code_start_line": 218,
      "code_end_line": 237,
      "params": [
        "self",
        "question",
        "answer"
      ],
      "have_return": true,
      "code_content": "    def method_choice(self, question: str, answer: str) -> str:\n        \n        methods = [\n            \"equivalent replacement\",\n            \"simple abstraction\",\n        ]\n        model_choice = agent.chat_with_template(\n            template_name=\"abs_method_choice.txt\",\n            template_data={\"question\": question, \"answer\": answer},\n            root_folder=PROMPT_ROOT_FOLDER,\n        )\n        method = extract_tag_content(model_choice, \"method\")\n        queries = extract_tag_content(model_choice, \"queries\")\n\n        norm = method.strip().lower().replace(\" \", \"\")\n        for m in methods:\n            if norm == m.lower().replace(\" \", \"\"):\n                return m, queries\n            \n        return methods[0], queries\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/utils.py/extract_tag_content"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "multi_verify",
      "md_content": [
        "**multi_verify**: The function of multi_verify is to perform multiple verifications on a given QAItem to determine if a model can answer the associated question.\n\n**parameters**: The parameters of this Function.\n· seed: QAItem - An instance of the QAItem class that contains the question to be verified and the expected answer.\n\n**Code Description**: The multi_verify function is an asynchronous method designed to validate whether a conversational model can answer a specific question encapsulated within a QAItem instance. The function begins by constructing a prompt that instructs the model on how to respond. The prompt includes the question from the QAItem and a specific instruction for the model to indicate when it cannot answer by generating a boxed response in LaTeX format.\n\nThe verification process consists of two main steps:\n\n1. **Direct Verification**: The function first attempts to validate the model's internal knowledge by sending the constructed prompt to the model using the agent's chat method. It allows for a maximum of five retries in case the model's response format is incorrect (i.e., the answer is not enclosed in the expected LaTeX boxed format). If the boxed answer matches the expected answer from the QAItem, the function concludes that the verification has failed, indicating that the model can answer the question directly.\n\n2. **Search-Based Verification**: If the model cannot provide a direct answer, the function proceeds to perform a search using the tavily_search function, which queries an external API for relevant information. The search results are then appended to the original prompt, and the model is asked again if it can answer the question based on the provided search results. Similar to the first step, the function allows for retries in case of formatting issues. If the model can answer based on the search results, the verification fails again.\n\nIf neither verification step results in a successful answer from the model, the function concludes that the model cannot answer the question, returning True to indicate successful verification.\n\nThe multi_verify function is integral to the workflow of the ReverseUpgradeWorkflow class, where it is called during the run and gpt_search_run methods. These methods utilize multi_verify to ensure that the updates made to QAItems are valid and that the model's responses are appropriately validated against expected outcomes.\n\n**Note**: It is crucial to ensure that the QAItem passed to the multi_verify function is correctly populated with a question and an expected answer. The function relies on the proper formatting of the model's responses, and any deviations may lead to runtime errors or incorrect verification results.\n\n**Output Example**: A possible return value from the multi_verify function could be:\n```json\n{\n    \"verification_result\": true\n}\n```\nThis indicates that the model was unable to answer the question directly or through search results."
      ],
      "code_start_line": 239,
      "code_end_line": 290,
      "params": [
        "self",
        "seed"
      ],
      "have_return": true,
      "code_content": "    async def multi_verify(self, seed: QAItem) -> bool:\n        \"\"\"\n        对给定的 QAItem 进行多重校验。\n        Returns:\n            bool: True 表示验证通过（模型无法回答），False 表示验证失败（模型能回答）\n        \"\"\"\n        prompt = (\n            \"You need to answer the question\\n\"\n            \"if you cannot answer, please generate \\\\boxed{Sorry, I don't know.}\\n\"\n            f\"Question: {seed.question}\\n\"\n            f\"{FORMAT_INSTRUCTION}\"\n        )\n        \n        # 第一步：直接校验模型内部知识是否能回答\n        max_retries = 5\n        for attempt in range(1, max_retries + 1):\n            direct_answer = agent.chat(prompt, model = \"gpt-4o\")\n            boxed_answer = extract_boxed(direct_answer)\n            if not boxed_answer:\n                printer.rule(f\"格式错误重试 #{attempt}\")\n                printer.print(\"模型原始返回：\", style=\"bold red\")\n                printer.print(direct_answer, style=\"bold red\")\n                if attempt == max_retries:\n                    raise RuntimeError(f\"连续 {max_retries} 次格式错误（答案未包含在 \\\\boxed{{}} 中），终止执行\")\n                continue\n            \n            if boxed_answer == seed.answer:    \n                printer.print(\"验证失败：模型能直接回答\", style=\"bold red\")\n                return False\n            break\n\n        # 第二步：如果模型不能直接回答，进行搜索\n        search_results = await tavily_search(seed.question)\n        \n        for attempt in range(1, max_retries + 1):\n            search_based_answer = agent.chat(prompt + f\"here are some search results:\\n\\n{search_results}\", model = \"gpt-4o\") \n            boxed_answer = extract_boxed(search_based_answer)\n            if not boxed_answer:\n                printer.rule(f\"搜索验证格式错误重试 #{attempt}\")\n                printer.print(\"模型原始返回：\", style=\"bold red\")\n                printer.print(search_based_answer, style=\"bold red\")\n                if attempt == max_retries:\n                    raise RuntimeError(f\"连续 {max_retries} 次格式错误（基于搜索的答案未包含在 \\\\boxed{{}} 中），终止执行\")\n                continue\n                \n            if boxed_answer == seed.answer:\n                printer.print(\"验证失败：模型能通过搜索回答\", style=\"bold red\")\n                return False\n            break\n\n        printer.print(\"验证通过：模型无法直接回答也无法通过搜索回答\", style=\"bold green\")\n        return True\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/run",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/tavily_search",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/extract_boxed",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/QAItem"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "query_update",
      "md_content": [
        "**query_update**: The function of query_update is to asynchronously update a question-answer item based on search results and a specified method.\n\n**parameters**: The parameters of this Function.\n· method: A string representing the strategy or method used for updating the question-answer item.  \n· queries: A list of strings containing the search queries to be executed.  \n· seed: An optional QAItem instance that serves as the basis for the update, containing the original question and answer.\n\n**Code Description**: The query_update function is designed to enhance a QAItem by performing an asynchronous search based on provided queries and updating the question based on the results. It begins by executing multiple search queries concurrently using the asyncio.gather method, which calls the tavily_search function for each query in the queries list. The results of these searches are collected into the search_results variable.\n\nNext, the function utilizes the agent's chat_with_template method to generate a response based on a predefined template. This method takes in the template name, the search results, and the original question and answer from the seed QAItem. The response from this method is expected to contain updated information regarding the question.\n\nThe function then processes the response to extract the updated question and any new evidence using the extract_and_validate_json and extract_tag_content functions. The updated question is stripped of any leading or trailing whitespace, and the updated evidence is collected into a list.\n\nTo enhance the console output, the function employs the printer's rule and print methods to display the updated question and evidence in a structured format, making it easier for users to read the results.\n\nFinally, the function constructs and returns a new QAItem instance that reflects the updated question, while also preserving the original answer, parent question, and evidence, along with the specified strategy. This new QAItem represents the next level in the question-answer hierarchy.\n\nThe query_update function is called within the run method of the ReverseUpgradeWorkflow class, where it plays a crucial role in the iterative process of upgrading questions based on search results. The run method manages multiple levels of updates, and query_update is invoked to obtain updated QAItems that are then verified and potentially added to the workflow's collection of items.\n\n**Note**: When using the query_update function, ensure that the queries provided are relevant and that the seed QAItem is properly initialized to maintain the integrity of the update process. The function relies on the successful execution of the tavily_search and chat_with_template methods, so any issues with these components may affect the outcome.\n\n**Output Example**: A possible return value from the query_update function could be a QAItem instance that looks like this:\n```json\n{\n    \"level\": 2,\n    \"question\": \"What is the capital of France?\",\n    \"answer\": \"Paris\",\n    \"parent_question\": \"What is the capital?\",\n    \"evidence\": [\"Paris is the capital of France.\", \"It is located in northern central France.\"],\n    \"strategy\": \"advanced search\"\n}\n```"
      ],
      "code_start_line": 292,
      "code_end_line": 316,
      "params": [
        "self",
        "method",
        "queries",
        "seed"
      ],
      "have_return": true,
      "code_content": "    async def query_update(self, method, queries, seed: Optional[QAItem]):\n        search_results = await asyncio.gather(*(tavily_search(q) for q in queries)) \n        question_update_resp = agent.chat_with_template(\n            template_name=\"abs_query_update.txt\",\n            template_data={\"method\": method, \"search_results\": search_results, \"question\": seed.question, \"answer\": seed.answer},\n            root_folder=PROMPT_ROOT_FOLDER,\n        )\n        # 提取出update之后的question\n        updated_item = extract_and_validate_json(extract_tag_content(question_update_resp, \"data\"))\n        updated_question = updated_item[\"updated_question\"].strip()\n        updated_evidence = updated_item.get(\"updated_evidence\", [])\n\n        printer.rule(\"Query Update Output\")\n        printer.print(pretty_json(updated_item), style=\"bold green\")\n        printer.rule(\"Query Update Evidence\")       \n        printer.print(pretty_json(updated_evidence), style=\"bold green\")\n\n        return QAItem(\n            level=seed.level + 1,\n            question=updated_question,\n            answer=seed.answer,\n            parent_question=seed.question,\n            evidence=seed.evidence + updated_evidence,\n            strategy=method,\n        )   \n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_tag_content",
        "src/criticsearch/utils.py/extract_and_validate_json",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/pretty_json",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/tavily_search",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/QAItem"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "generate_seed",
      "md_content": [
        "**generate_seed**: The function of generate_seed is to asynchronously generate a seed question-answer item based on extracted queries from a randomly selected domain.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The generate_seed function is an asynchronous method that plays a crucial role in the ReverseUpgradeWorkflow class. Its primary purpose is to generate a seed question-answer item (QAItem) by interacting with an external conversational agent and processing the responses to extract relevant queries and seed facts.\n\nThe function begins by defining a maximum number of retries (max_query_retries) set to 3, which is used to handle cases where no queries are extracted from the agent's response. It then enters a loop that attempts to generate queries based on a randomly selected domain obtained from the random_domain function. This function returns a domain from a predefined list, ensuring diversity in the queries generated.\n\nWithin the loop, the function calls the agent's chat_with_template method, passing a template name and data that includes the randomly selected domain. This method is responsible for rendering a template and sending it to the conversational model, which returns a response. The response is then processed using the extract_queries_from_response function, which extracts up to three queries from the response text.\n\nIf queries are successfully extracted, the function prints the extracted queries for visibility and proceeds to gather search results by calling the tavily_search function asynchronously for each query. The tavily_search function interacts with the Tavily API to perform search queries and returns a list of results.\n\nOnce the search results are obtained, the function generates a seed fact by calling the agent's chat_with_template method again, this time using a different template that incorporates the domain and search results. The response from this call is processed using the extract_and_validate_json function to ensure it contains valid JSON data.\n\nFinally, the function constructs a QAItem instance using the extracted seed fact data, including the question, answer, evidence, and strategy. This QAItem is then returned as the output of the function.\n\nThe generate_seed function is called within the run method of the ReverseUpgradeWorkflow class, where it serves as the initial step in a workflow that involves multiple levels of question updates and verifications. The successful generation of a seed QAItem is essential for the subsequent processing and validation steps in the workflow.\n\n**Note**: It is important to ensure that the agent's responses are well-formed and contain valid query data to avoid runtime errors. The function's reliance on external services (the conversational agent and Tavily API) means that network issues or API errors could impact its execution.\n\n**Output Example**: A possible appearance of the code's return value when a QAItem is successfully generated might look like this:\n```json\n{\n    \"level\": 0,\n    \"question\": \"What are the latest advancements in AI technology?\",\n    \"answer\": \"Recent advancements include improvements in natural language processing and computer vision.\",\n    \"parent_question\": null,\n    \"evidence\": [\"AI technology has seen significant growth in recent years.\", \"New models are being developed to enhance AI capabilities.\"],\n    \"strategy\": \"seed\"\n}\n```"
      ],
      "code_start_line": 318,
      "code_end_line": 349,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    async def generate_seed(self) -> QAItem:\n        max_query_retries = 3\n        for attempt in range(1, max_query_retries + 1):\n            random_domain = self.random_domain()\n            query_resp = agent.chat_with_template(\n                template_name=\"search_query_for_seed_fact.txt\",\n                template_data={\"domain\": random_domain},\n                root_folder=PROMPT_ROOT_FOLDER,\n            )\n            queries = extract_queries_from_response(query_resp)[:3]\n            if queries:\n                printer.rule(f\"Extracted Queries for Seed Fact in {random_domain} Domain\")\n                printer.print(queries, style=\"bold cyan\")\n                break\n            printer.print(f\"第 {attempt} 次抽取到空 queries，重试中…\", style=\"bold red\")\n            printer.print(query_resp, style=\"bold red\")\n        else:\n            raise RuntimeError(\"连续 3 次 Extracted Queries 为空，终止执行\")\n\n        search_results = await asyncio.gather(*(tavily_search(q) for q in queries))\n\n        seed_fact_resp = extract_and_validate_json(agent.chat_with_template(            template_name=\"seed_idea_from_internet.txt\",            template_data={\"domain\": random_domain,\"search_results\": search_results,},             root_folder=PROMPT_ROOT_FOLDER        ))\n        printer.rule(f\"Generate Seed Fact Output after browsing in {random_domain}\")\n        printer.print(pretty_json(seed_fact_resp), style=\"bold green\")\n        return QAItem(\n            level=0,\n            question=seed_fact_resp[\"seed\"][\"question\"].strip(),\n            answer=seed_fact_resp[\"seed\"][\"answer\"].strip(),\n            parent_question=None,\n            evidence=seed_fact_resp[\"seed\"].get(\"evidence\", []),\n            strategy=\"seed\",\n        )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_queries_from_response",
        "src/criticsearch/utils.py/extract_and_validate_json",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/pretty_json",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/tavily_search",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/QAItem",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/random_domain"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        true,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "run",
      "md_content": [
        "**run**: The function of run is to execute a multi-level workflow for upgrading question-answer items based on generated seeds and verification processes.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The run function is an asynchronous method within the ReverseUpgradeWorkflow class that orchestrates a multi-level upgrade process for question-answer items (QAItems). It begins by logging the start of the workflow using the printer's rule method, which creates a visual separator in the console output. The function then generates an initial seed QAItem by calling the generate_seed method, which interacts with an external conversational agent to produce a question-answer pair based on a randomly selected domain.\n\nOnce the seed is generated, it is appended to the items list, and the current QAItem is set to this seed. The function then enters a loop that iterates through a range defined by max_level, indicating the number of upgrade levels to attempt. For each level, it initializes a retry counter and enters a while loop that continues until the maximum number of retries (max_tries) is reached.\n\nWithin the retry loop, the method_choice function is called to determine the appropriate method for processing the current question and answer. This function returns a method and associated queries, which are then randomized to include a \"simple abstraction\" method as a potential option. The query_update function is subsequently invoked to update the current QAItem based on the selected method and queries. This function performs asynchronous searches and generates an updated QAItem.\n\nAfter the update, the multi_verify function is called to validate whether the updated QAItem can be answered by the model. If the verification fails, indicating that the model can answer the question, the process continues with the next retry. If the verification passes, the updated QAItem is appended to the items list, and the current QAItem is updated to this new item. If no valid update is achieved after the maximum retries, the function logs a message indicating the stopping point and exits.\n\nThe run function is called in the main function of the project, where an instance of ReverseUpgradeWorkflow is created, and the run method is executed within an asyncio event loop. This integration highlights the function's role as a critical component of the overall workflow, facilitating the iterative enhancement of QAItems through a structured process of generation, updating, and verification.\n\n**Note**: It is essential to ensure that the max_level and max_tries parameters are set appropriately to avoid excessive iterations and potential performance issues. Additionally, the successful execution of the generate_seed, method_choice, query_update, and multi_verify functions is crucial for the proper functioning of the run method.\n\n**Output Example**: A possible appearance of the code's execution could result in the following console output:\n```\n======================== Workflow Start with gpt-4 ========================\nExtracted Queries for Seed Fact in Science Domain\n['What are the latest advancements in AI technology?', 'How does quantum computing work?']\nLevel 1 Update Attempt 1\nQuery Update Output\n{\n    \"updated_question\": \"What are the recent developments in AI?\",\n    \"updated_evidence\": [\"AI has made significant strides in natural language processing.\"]\n}\nLevel 2 Update Attempt 1\n...\n======================== Workflow End ========================\n```"
      ],
      "code_start_line": 351,
      "code_end_line": 379,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    async def run(self):\n        printer.rule(f\"Workflow Start with {GPT_MODEL}\")\n        seed = await self.generate_seed()\n        self.items.append(seed)\n        current = seed\n\n        # 按照 max_level 和 max_tries 进行多级问题升级\n        for level in range(self.max_level):\n            retries = 0\n            while retries < self.max_tries:\n                retries += 1\n                printer.rule(f\"Level {level+1} Update Attempt {retries}\")\n                method, queries = self.method_choice(current.question, current.answer)\n                # 随机化一下方法，以后看表现调整\n                method = random.choice([method, \"simple abstraction\"])\n                updated = await self.query_update(method, queries, current)\n                passed = await self.multi_verify(updated)\n                if not passed:  # 验证失败（模型能回答）\n                    printer.print(\"多重校验未通过（模型能回答），重试 query_update …\", style=\"bold red\")\n                    continue\n                printer.print(\"多重校验通过（模型无法回答），记录更新后的 QAItem\", style=\"bold green\")\n                self.items.append(updated)  # 只有验证通过才记录\n                current = updated\n                break\n            else:\n                printer.print(f\"Stopped at level {current.level}; no valid update after {self.max_tries} tries.\", style=\"bold red\")\n                return\n\n        printer.rule(\"Workflow End\")\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/main",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/main/_batch/run_one"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/method_choice",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/generate_seed"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "gpt_search_generate_seed",
      "md_content": [
        "## Documentation for `gpt_search_generate_seed` Function\n\n### Overview\n\nThe `gpt_search_generate_seed` function is an asynchronous method responsible for generating a seed for GPT search. It interacts with an agent to generate relevant content based on the specified domain and processes the response into a structured QAItem.\n\n### Parameters\n\n- **domain** (str): The domain parameter is used to customize the seed generation, guiding the process of generating relevant question-answer pairs.\n\n### Returns\n\n- **QAItem**: The function returns an instance of the `QAItem` class, which encapsulates a question-answer pair along with additional metadata, such as evidence and strategy. The returned `QAItem` will have the following attributes:\n  - `level`: Set to `0`, representing the base level of the QA item.\n  - `question`: A string containing the generated question.\n  - `answer`: A string containing the generated answer.\n  - `parent_question`: Always `None`, indicating there is no parent question in this context.\n  - `evidence`: A list containing any evidence provided with the question-answer pair, or an empty list if none is provided.\n  - `strategy`: Set to `\"seed\"`, identifying this as a seed-related item.\n\n### Function Flow\n\n1. **Template-Based Chat**: The function calls the `chat_with_template` method to generate a response based on a template (`gpt_search_seed.txt`) and the provided domain. This method is responsible for interacting with the agent and obtaining a response.\n   \n2. **JSON Extraction and Validation**: The response from the agent is passed to the `extract_and_validate_json` function, which extracts and validates the JSON content. If the JSON is invalid or cannot be parsed, a `RuntimeError` is raised, indicating that the seed generation failed.\n\n3. **QAItem Creation**: If the JSON response is valid, the function constructs a `QAItem` instance, populated with the extracted question, answer, and evidence (if available). The `level` is set to `0`, `parent_question` is set to `None`, and the strategy is set to `\"seed\"`.\n\n4. **Return**: Finally, the function returns the populated `QAItem`.\n\n### Error Handling\n\nIf the extracted JSON is invalid, a `RuntimeError` will be raised with the message: `\"GPT-Search seed generation failed\"`. This indicates that the seed generation process encountered an issue, and the returned response was not in the expected format.\n\n### Example Usage\n\n```python\ndomain = \"example_domain\"\nqa_item = await gpt_search_generate_seed(domain)\nprint(qa_item.question)\nprint(qa_item.answer)\n```\n\n### Dependencies\n\n- **`chat_with_template`**: This function is used to generate the initial response by rendering a template with the provided data.\n- **`extract_and_validate_json`**: This function is responsible for ensuring the response from the agent is valid JSON before proceeding to create a `QAItem`.\n\n### Notes\n\n- The function relies on the presence of a template (`gpt_search_seed.txt`) and the validity of the response from the agent. The specific logic for GPT search seed generation should be implemented in the future (as noted by the `TODO` comment).\n- Proper domain input is essential to ensure the seed generation logic functions as expected."
      ],
      "code_start_line": 381,
      "code_end_line": 396,
      "params": [
        "self",
        "domain"
      ],
      "have_return": true,
      "code_content": "    async def gpt_search_generate_seed(self, domain) -> QAItem:\n        \"\"\"Stub for GPT Search seed generation.\"\"\"\n        # TODO: 实现 GPT Search 专用的 seed 生成逻辑\n        resp = agent.chat_with_template(template_name=\"gpt_search_seed.txt\", root_folder=PROMPT_ROOT_FOLDER, template_data={\"domain\": domain})\n        json_resp = extract_and_validate_json(resp)\n        if not json_resp:\n            raise RuntimeError(\"GPT-Search seed generation failed\")\n        \n        return QAItem(\n                level=0,\n                question=json_resp[\"seed\"][\"question\"].strip(),\n                answer=json_resp[\"seed\"][\"answer\"].strip(),\n                parent_question=None,\n                evidence=json_resp[\"seed\"].get(\"evidence\", []),\n                strategy=\"seed\",\n            )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/utils.py/extract_and_validate_json",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/QAItem"
      ],
      "special_reference_type": [
        false,
        false,
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "gpt_search_query_update",
      "md_content": [
        "**gpt_search_query_update**: The function of gpt_search_query_update is to update a question-answer item using a GPT-based search query mechanism.\n\n**parameters**: The parameters of this Function.\n· seed: QAItem - An instance of the QAItem class representing the initial question-answer pair that serves as the basis for the update.\n\n**Code Description**: The gpt_search_query_update function is an asynchronous method designed to enhance a given QAItem by querying a GPT model for an updated question and associated evidence. It begins by invoking the agent's chat_with_template method, which renders a template for the GPT model using the question and answer from the provided QAItem (seed). The template used is specified as \"gpt_search_Q_update.txt\", and the data passed includes the original question and answer.\n\nOnce the response is received, the function extracts and validates the JSON content from the model's response using the extract_and_validate_json function. This function ensures that the response is properly formatted as JSON and handles any potential errors in parsing. If the JSON extraction fails, a RuntimeError is raised, indicating that the query update process was unsuccessful.\n\nIf the JSON extraction is successful, the function retrieves the updated question, any new evidence, and the method used for the update from the parsed JSON. It then prints the output and evidence using the printer's rule and print methods, which format the output for better readability in the console.\n\nFinally, the function constructs a new QAItem instance with the updated question, the original answer, and the combined evidence from the original and new sources. The level of the new QAItem is incremented by one to reflect its position in the hierarchy. This updated QAItem is then returned.\n\nThe gpt_search_query_update function is called within the gpt_search_run method of the ReverseUpgradeWorkflow class. In this context, it plays a crucial role in the iterative process of upgrading questions through multiple levels, where it is invoked repeatedly to refine the QAItem based on the responses from the GPT model.\n\n**Note**: It is essential to ensure that the seed parameter is a valid QAItem instance with properly populated attributes to maintain the integrity of the update process. The function relies on the correct formatting of the template and the JSON response to function effectively.\n\n**Output Example**: A possible return value from the gpt_search_query_update function could be an updated QAItem instance that looks like this:\n```json\n{\n    \"level\": 2,\n    \"question\": \"What is the capital of France?\",\n    \"answer\": \"Paris\",\n    \"parent_question\": \"What is the capital of France?\",\n    \"evidence\": [\"Paris is the capital of France.\", \"It is located in northern central France.\"],\n    \"strategy\": \"GPT-based update\"\n}\n```"
      ],
      "code_start_line": 398,
      "code_end_line": 430,
      "params": [
        "self",
        "seed"
      ],
      "have_return": true,
      "code_content": "    async def gpt_search_query_update(self, seed: QAItem) -> QAItem:\n        \"\"\"Stub for GPT Search query update.\"\"\"\n        # TODO: 实现 GPT Search 专用的 query update 逻辑\n\n        resp = agent.chat_with_template(\n            template_name=\"gpt_search_Q_update.txt\",\n            template_data={\"question\": seed.question, \"answer\": seed.answer},\n            root_folder=PROMPT_ROOT_FOLDER,\n        )\n\n        json_resp = extract_and_validate_json(extract_tag_content(resp, \"data\"))\n        if not json_resp:\n            raise RuntimeError(\"GPT-Search query update failed\")\n        \n        # 提取出update之后的question\n        updated_question = json_resp[\"updated_question\"].strip()\n        updated_evidence = json_resp.get(\"updated_evidence\", [])\n        method = json_resp.get(\"method\", \"None\")\n        printer.rule(\"GPT-Search Query Update Output\")\n        printer.print(pretty_json(json_resp), style=\"bold green\")\n        printer.rule(\"GPT-Search Query Update Evidence\")\n        printer.print(pretty_json(updated_evidence), style=\"bold green\")\n        # 记录更新后的 QAItem\n        updated = QAItem(\n            level=seed.level + 1,\n            question=updated_question,\n            answer=seed.answer,\n            parent_question=seed.question,\n            evidence=seed.evidence + updated_evidence,\n            strategy=method,\n        )\n\n        return updated\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_tag_content",
        "src/criticsearch/utils.py/extract_and_validate_json",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/pretty_json",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/QAItem"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "gpt_search_run",
      "md_content": [
        "**gpt_search_run**: The function of gpt_search_run is to execute a multi-level question-answer refinement process using a GPT-based search mechanism.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The gpt_search_run function is an asynchronous method that orchestrates a workflow for refining question-answer pairs through multiple levels of interaction with a GPT model. The process begins by initializing a specific GPT model, \"gpt-4o-search-preview\", and logging the start of the workflow using the printer's rule method to create a visual separator in the console output.\n\nThe first step in the workflow is to generate a seed QAItem by invoking the gpt_search_generate_seed method. This method interacts with an agent to produce an initial question-answer pair, which is then appended to the items list for further processing. The seed serves as the starting point for the iterative refinement process.\n\nThe function then enters a loop that iterates over a range defined by max_level, representing the number of refinement levels to be executed. Within this loop, another nested loop is initiated to handle retries, governed by the max_tries parameter. For each level, the function attempts to update the current QAItem using the gpt_search_query_update method. This method is responsible for querying the GPT model to obtain an updated question based on the current QAItem.\n\nAfter receiving the updated response, the function performs a multi-verification check by calling the multi_verify method. This method assesses whether the updated QAItem can be answered by the model, ensuring that the refinement process yields valid and meaningful results. If the verification fails, the function logs a message indicating that the model can answer the question and continues to retry the update process.\n\nIf the verification passes, indicating that the model cannot answer the updated question, the new QAItem is appended to the items list, and the current variable is updated to reflect this new item. The loop then breaks to proceed to the next level of refinement.\n\nIf the maximum number of retries is reached without a valid update, the function logs a message indicating that the process has stopped at the current level due to the inability to obtain a valid update after the specified number of attempts.\n\nFinally, the function concludes by logging the end of the GPT search workflow using the printer's rule method.\n\n**Note**: It is important to ensure that the max_level and max_tries parameters are set appropriately to control the depth and breadth of the refinement process. Additionally, the gpt_search_generate_seed and gpt_search_query_update methods must be implemented correctly to facilitate the generation and updating of QAItems.\n\n**Output Example**: The function does not return a value but logs the progress and results of the workflow. An example of console output might include:\n```\nGPT Search Workflow Start with gpt-4o-search-preview\nGPT-Search Level 1 Update Attempt 1\nGPT-Search: 多重校验通过（模型无法回答），记录更新后的 QAItem\nGPT-Search Level 2 Update Attempt 1\nGPT-Search stopped at level 2; no valid update after 3 tries.\nGPT-Search Workflow End\n```"
      ],
      "code_start_line": 432,
      "code_end_line": 464,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    async def gpt_search_run(self):\n        GPT_MODEL = \"gpt-4o-search-preview\"\n        printer.rule(f\"GPT Search Workflow Start with {GPT_MODEL}\")\n        # 第一步：Seed 生成\n        seed = await self.gpt_search_generate_seed()\n        self.items.append(seed)\n        current = seed\n\n        # 按照 max_level 和 max_tries 进行多级问题升级\n        for level in range(self.max_level):\n            retries = 0\n            while retries < self.max_tries:\n                retries += 1\n                printer.rule(f\"GPT-Search Level {level+1} Update Attempt {retries}\")\n                # 使用 GPT Search 专用的 query update\n                updated = await self.gpt_search_query_update(current)\n                # 多重校验\n                passed = await self.multi_verify(updated)\n                if not passed:\n                    printer.print(\"GPT-Search: 多重校验未通过（模型能回答），重试 query_update …\", style=\"bold red\")\n                    continue\n                printer.print(\"GPT-Search: 多重校验通过（模型无法回答），记录更新后的 QAItem\", style=\"bold green\")\n                self.items.append(updated)\n                current = updated\n                break\n            else:\n                printer.print(\n                    f\"GPT-Search stopped at level {current.level}; no valid update after {self.max_tries} tries.\",\n                    style=\"bold red\",\n                )\n                return\n\n        printer.rule(\"GPT-Search Workflow End\")\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/gpt_search_query_update"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "save",
      "md_content": [
        "### `save` Method Documentation\n\n**Function Name**: `save`\n\n**Location**: `src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow`\n\n---\n\n#### Purpose:\nThe `save` method is responsible for saving a list of items to a JSON file. It appends the current items to the existing content in the specified file, ensuring that the data is persisted in a structured format. If the file already contains data, the method loads the existing content, appends new items, and writes the updated content back to the file.\n\n---\n\n#### Parameters:\n- **path** (`Path`): The file path where the items will be saved. This is a required parameter. The method ensures the file at this path is either read from (if it exists) or created (if it does not exist).\n\n---\n\n#### Functionality:\n1. **File Locking**: The method begins by acquiring a file lock to ensure that no other process can access the file while it is being modified. This prevents race conditions during the save process.\n\n2. **Reading Existing Data**: If the specified file already exists, it opens the file in read mode and loads its contents into a list called `existing`. This data is expected to be in JSON format.\n\n3. **Appending New Data**: It then iterates over the `self.items` list, converting each item to a dictionary using the `to_dict()` method, and appends these dictionaries to the `existing` list.\n\n4. **Writing to File**: After the new items are appended, the method opens the file in write mode and saves the updated list back to the file in JSON format. The `json.dump()` method is used with the options `ensure_ascii=False` and `indent=2` to ensure the data is human-readable, with proper formatting.\n\n5. **Console Output**:\n    - **Status Message**: After the save operation, a message is printed to the console indicating how many items were saved to the file, styled in bold green.\n    - **Saved Data Preview**: A preview of the saved items is printed in a formatted JSON structure, styled in bold cyan for clarity.\n\n---\n\n#### Example Usage:\n```python\nworkflow.save(Path('path_to_file.json'))\n```\n\n---\n\n#### Notes:\n- The method makes use of file locking to prevent simultaneous writes to the same file, ensuring data integrity.\n- The `to_dict()` method of each item is used to convert the objects in `self.items` into a dictionary format suitable for JSON serialization.\n- The method outputs both a status message and a preview of the saved data to the console, providing feedback to the user about the operation.\n  \nThe `save` method is part of the `ReverseUpgradeWorkflow` class, and its primary role is to manage the persistence of the workflow's items in a JSON file format."
      ],
      "code_start_line": 466,
      "code_end_line": 478,
      "params": [
        "self",
        "path"
      ],
      "have_return": false,
      "code_content": "    def save(self, path: Path):\n        printer.rule(\"Saving Results\")\n        with file_lock:\n            existing = []\n            if path.exists():\n                with open(path, \"r\", encoding=\"utf-8\") as f:\n                    existing = json.load(f)\n            existing.extend([it.to_dict() for it in self.items])\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(existing, f, ensure_ascii=False, indent=2)\n        printer.print(f\"Saved {len(self.items)} items to {path}\", style=\"bold green\")\n        printer.rule(\"Saved JSON Preview\")\n        printer.print(pretty_json([it.to_dict() for it in self.items]), style=\"bold cyan\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/main"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/pretty_json",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/QAItem/to_dict"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_safe_json",
      "md_content": [
        "**_safe_json**: The function of _safe_json is to extract a JSON string from a block of text and return it as a Python dictionary.\n\n**parameters**: The parameters of this Function.\n· text: A string input that contains the potential JSON data enclosed in code block syntax (```json```) or just regular text.\n\n**Code Description**:  \nThe function `_safe_json` is designed to safely extract JSON data from a given string. It works by using a regular expression search (`re.search`) to look for a pattern that matches a JSON block within the text. The pattern looks for an optional \"json\" keyword following the code block delimiters (```), which is common in markdown-like formats. Once the relevant block of text is identified, it is extracted from the match group (`m.group(1)`). \n\nIf no match is found, the entire input text is used as-is. The extracted text is then stripped of unnecessary backticks (`\\``), newlines, and spaces that may be surrounding it. Finally, the stripped string is parsed into a Python dictionary using `json.loads()`, which is capable of converting a valid JSON string into a Python dictionary.\n\nThis function is useful in cases where JSON data is embedded within a larger text body, and the goal is to isolate and parse the JSON data.\n\n**Note**: \n- If the input text does not contain a JSON block, the entire input text is processed as the raw string, and the function will attempt to parse it as JSON, which may raise a `json.JSONDecodeError` if the string is not a valid JSON.\n- The regular expression used in this function expects a block of text that is formatted with markdown-style code block syntax (i.e., ```json ... ```). This may need adjustment if the input format differs significantly.\n\n**Output Example**:\nFor an input like:\n```text\nHere is some random text\n```json\n{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\n```\nThe function will return:\n```python\n{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\n```"
      ],
      "code_start_line": 481,
      "code_end_line": 485,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "    def _safe_json(text: str) -> dict:\n        m = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)```\", text)\n        raw = m.group(1) if m else text\n        raw = raw.strip(\"`\\n \")\n        return json.loads(raw)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "random_domain",
      "md_content": [
        "**random_domain**: The function of random_domain is to return a randomly selected domain from a predefined list of categories.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The random_domain function is designed to select and return a random domain from a predefined list of categories. The function begins by defining a list named 'domains', which contains various categories such as \"TV shows & movies\", \"Other\", \"Science & technology\", \"Art\", \"History\", \"Sports\", \"Music\", \"Video games\", \"Geography\", and \"Politics\". After defining this list, the function utilizes the random.choice method from the random module to select one item from the 'domains' list at random. Finally, the selected domain is returned as the output of the function. This function is useful in scenarios where a random category is needed, such as in games, quizzes, or any application that requires random selection from a set of options.\n\n**Note**: It is important to ensure that the random module is imported in the script where this function is used, as the function relies on it to perform the random selection. Additionally, since the function does not take any parameters, it will always return a random domain from the same list each time it is called.\n\n**Output Example**: A possible return value of the function could be \"Sports\", \"Art\", or \"Science & technology\", depending on the random selection made during the function call."
      ],
      "code_start_line": 487,
      "code_end_line": 489,
      "params": [],
      "have_return": true,
      "code_content": "def random_domain():\n    domains = [\"TV shows & movies\", \"Other\", \"Science & technology\", \"Art\", \"History\", \"Sports\", \"Music\", \"Video games\", \"Geography\", \"Politics\"]\n    return random.choice(domains)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "evaluate",
      "md_content": [
        "**evaluate**: The function of evaluate is to assess level-5 items in a JSON trace file, comparing model-generated answers to ground truth answers.\n\n**parameters**: The parameters of this Function.\n· json_file: A Path object representing the path to the JSON trace file that contains the records to be evaluated. The default value is \"trace_data.json\".  \n· use_cache: A boolean indicating whether to utilize a cache for storing evaluation results. The default value is True.  \n· cache_file: A Path object or None, specifying the path to the cache file. If None, the cache file is created in the same directory as json_file with the name \"<stem>_eval_cache.json\".\n\n**Code Description**: The evaluate function is designed to read a JSON file containing trace data, filter for records with a level of 5, and compare the answers provided by a language model against the ground truth answers. \n\nInitially, the function checks if a cache file is provided; if not, it constructs a default cache file path based on the json_file's location. If caching is enabled and the cache file exists, it loads the existing cache to avoid redundant evaluations. If caching is not used or the cache file does not exist, it initializes an empty cache.\n\nThe function then reads the JSON file and extracts all records. It filters these records to include only those with a level equal to 5. For each record, it retrieves the question and the corresponding ground truth answer. If caching is enabled and the question has already been evaluated, it skips to the next record.\n\nFor questions that require evaluation, the function constructs a prompt for the language model and calls the chat method from the BaseAgent class to obtain a predicted answer. The response is processed to extract the answer using the extract_boxed function, which retrieves content enclosed in LaTeX-style boxed expressions.\n\nThe predicted answer is then normalized by removing punctuation and comparing it to the normalized ground truth answer. The result of this comparison is stored in the cache.\n\nAfter processing all records, if caching is enabled, the updated cache is saved to the specified cache file. Finally, the function calculates the accuracy of the model's predictions by comparing the number of correct answers to the total number of evaluated items. It outputs the total number of level-5 items, the count of correct predictions, and the overall accuracy percentage using the printer's logging methods.\n\nThe evaluate function is called within the main function of the same module when the command-line argument `--evaluate` is specified. This allows users to run the evaluation process directly from the command line, providing a straightforward way to assess the model's performance on level-5 items.\n\n**Note**: It is important to ensure that the JSON trace file is correctly formatted and contains the necessary fields (level, question, answer) for the evaluation to function properly. Additionally, when using caching, ensure that the cache file path does not conflict with other files in the directory.\n\n**Output Example**: \n```\nEvaluation Results\nTotal level-5 items: 100\nCorrect predictions: 85\nAccuracy: 85.00%\n```"
      ],
      "code_start_line": 491,
      "code_end_line": 550,
      "params": [
        "json_file",
        "use_cache",
        "cache_file"
      ],
      "have_return": true,
      "code_content": "def evaluate(\n    json_file: Path = Path(\"trace_data.json\"),\n    use_cache: bool = True,\n    cache_file: Path | None = None,\n):\n    \"\"\"\n    Evaluate level-5 items in a JSON trace.\n    - json_file: 待评估的 trace JSON 路径\n    - use_cache: 是否启用缓存\n    - cache_file: 缓存文件路径，默认与 json_file 同目录，名为 \"<stem>_eval_cache.json\"\n    \"\"\"\n    if cache_file is None:\n        cache_file = json_file.parent / f\"{json_file.stem}_eval_cache.json\"\n    # 加载或初始化缓存：{question_norm: bool}\n    if use_cache and cache_file.exists():\n        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n            cache = json.load(f)\n    else:\n        cache = {}\n\n    # 读取所有记录\n    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n        records = json.load(f)\n\n    # 筛选 level==5\n    items = [r for r in records if r.get(\"level\") == 5]\n    total = len(items)\n    for rec in items:\n        q = rec[\"question\"]\n        ans_true = rec[\"answer\"]\n        key = q.strip().lower()\n        if use_cache and key in cache:\n            continue\n        # 构造Prompt并调用LLM\n        prompt = (\n            f\"{FORMAT_INSTRUCTION}\\nPlease answer the following question and return it strictly in the \\\\\\boxed{{...}} format.\\nQuestion: {q}\"\n        )\n        resp = agent.chat(prompt, model=\"gpt-4o-search-preview\")\n        printer.log(f\"Question:{key}\", style=\"bold yellow\")\n        printer.print(f\"GT Answer: {ans_true}\", style=\"bold yellow\")\n        ans_pred = extract_boxed(resp)\n        printer.print(f\"Model Answer: {ans_pred}\", style=\"bold yellow\")\n        # 标准化比较并去除标点符号\n        ans_pred_norm = \"\".join(c for c in ans_pred.strip().lower() if c.isalnum() or c.isspace())\n        ans_true_norm = \"\".join(c for c in ans_true.strip().lower() if c.isalnum() or c.isspace())\n        ok = ans_pred_norm == ans_true_norm\n        cache[key] = ok\n\n    # 保存缓存\n    if use_cache:\n        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(cache, f, ensure_ascii=False, indent=2)\n\n    # 统计准确率\n    correct = sum(cache.get(it[\"question\"].strip().lower(), False) for it in items)\n    acc = correct / total if total else 0.0\n    printer.rule(\"Evaluation Results\")\n    printer.print(f\"Total level-5 items: {total}\", style=\"bold\")\n    printer.print(f\"Correct predictions: {correct}\", style=\"bold\")\n    printer.print(f\"Accuracy: {acc:.4%}\", style=\"bold green\")\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_1.py/main"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/extract_boxed"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: The function of main is to serve as the entry point for executing the workflow, handling command-line arguments, and orchestrating the execution of the ReverseUpgradeWorkflow.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The main function is responsible for setting up the command-line interface for the application and managing the execution flow based on user inputs. It utilizes the argparse library to define and parse various command-line arguments that control the behavior of the workflow.\n\nThe function begins by creating an ArgumentParser object, which is used to define several command-line options:\n- `--out`: Specifies the output file path for saving results, defaulting to \"trace_data.json\".\n- `--max_level`: Sets the maximum number of levels for question upgrades, defaulting to 5.\n- `--max_tries`: Defines the maximum number of attempts for each upgrade, defaulting to 5.\n- `--batch`: Indicates the number of batch runs to execute, with a default of 1.\n- `--concurrency`: Specifies the maximum level of concurrent executions, defaulting to 1.\n- `--model`: Allows the user to specify a custom language model name.\n- `--evaluate`: A flag that, when set, triggers the evaluation of level-5 items and exits the program.\n\nAfter parsing the arguments, the function checks if a model name has been provided. If so, it updates the global variable `GPT_MODEL` to use the specified model. If the `--evaluate` flag is present, the function calls the `evaluate` function, passing the output file path and a cache option, and then exits.\n\nFor single executions (when `args.batch` is less than or equal to 1), the function instantiates the `ReverseUpgradeWorkflow` class with the specified maximum levels and tries, runs the workflow asynchronously using `asyncio.run()`, and saves the results to the specified output file. It also handles exceptions that may occur during execution, logging errors and providing feedback to the user.\n\nIn the case of batch executions (when `args.batch` is greater than 1), the function defines an inner asynchronous function `_batch` that manages concurrent executions of the workflow. It uses an asyncio semaphore to control the number of concurrent tasks and defines a nested function `run_one` to execute a single instance of the workflow. This function handles the saving of results in a thread-safe manner, ensuring that multiple batch runs can append their results to the same output file without conflicts.\n\nFinally, the `_batch` function is executed using `asyncio.run()`, allowing for efficient handling of multiple workflow instances.\n\nThe main function plays a crucial role in the overall application, serving as the interface for users to interact with the workflow and controlling the execution based on their input parameters.\n\n**Note**: It is important to ensure that the command-line arguments are correctly specified to avoid runtime errors. Users should also be aware of the implications of running batch processes, particularly regarding file access and concurrency.\n\n**Output Example**: A possible output of the command-line execution could be:\n```\nSaved 50 items to trace_data.json\nCompleted 5 runs; file at trace_data.json\n```"
      ],
      "code_start_line": 552,
      "code_end_line": 622,
      "params": [],
      "have_return": true,
      "code_content": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--out\", type=Path, default=Path(\"trace_data.json\"))\n    parser.add_argument(\"--max_level\", type=int, default=5)\n    parser.add_argument(\"--max_tries\", type=int, default=5)\n    parser.add_argument(\"--batch\", type=int, default=1, help=\"批量运行次数，>1 则追加写入同一文件\")\n    parser.add_argument(\"--concurrency\", type=int, default=1, help=\"最大并行并发数\")\n    parser.add_argument(\"--model\", type=str, default=None, help=\"LLM model name to use\")\n    parser.add_argument(\"--evaluate\", action=\"store_true\", help=\"只运行 evaluate()，输出 level-5 测试结果并退出\")\n\n    args = parser.parse_args()\n\n    # 支持通过命令行覆盖默认模型\n    if args.model:\n        global GPT_MODEL\n        GPT_MODEL = args.model\n\n    # 如果指定 --evaluate，则调用 evaluate 并退出\n    if args.evaluate:\n        evaluate(json_file=args.out, use_cache=True)\n        return\n\n    # 单次执行\n    if args.batch <= 1:\n        try:\n            wf = ReverseUpgradeWorkflow(max_level=args.max_level, max_tries=args.max_tries)\n            asyncio.run(wf.run())\n            wf.save(args.out)\n            print(f\"Saved {len(wf.items)} items to {args.out}\")\n        except Exception as e:\n            logger.exception(\"Error in single run\")\n            printer.print(f\"Error during single run: {e}\", style=\"bold red\")\n        return\n\n    # 批量并行执行并追加\n    async def _batch():\n\n        # 2. 并发控制信号量\n        sem = asyncio.Semaphore(args.concurrency)\n\n        # 3. 定义单次运行任务\n        async def run_one(idx: int):\n            async with sem:\n                try:\n                    printer.rule(f\"Batch Run {idx+1}/{args.batch}\")\n                    wf = ReverseUpgradeWorkflow(max_level=args.max_level, max_tries=args.max_tries)\n                    await wf.run()\n                    # 完成一次运行后立即读-插入-写回\n                    items = [it.to_dict() for it in wf.items]\n                    with file_lock:\n                        # 读取现有数据列表，文件不存在时视为空\n                        try:\n                            with open(args.out, \"r\", encoding=\"utf-8\") as f:\n                                existing = json.load(f)\n                        except FileNotFoundError:\n                            existing = []\n                        existing.extend(items)\n                        with open(args.out, \"w\", encoding=\"utf-8\") as f:\n                            json.dump(existing, f, ensure_ascii=False, indent=2)\n                    printer.print(f\"Batch saved {len(items)} items; total now {len(existing)}\", style=\"bold green\")\n                    return items\n                except Exception as e:\n                    logger.exception(f\"Error in batch run {idx+1}\")\n                    printer.print(f\"Error in batch run {idx+1}: {e}\", style=\"bold red\")\n                    return []\n\n        # 4. 提交所有任务并等待完成\n        await asyncio.gather(*(run_one(i) for i in range(args.batch)))\n        printer.print(f\"Completed {args.batch} runs; file at {args.out}\", style=\"bold green\")\n\n    asyncio.run(_batch())\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/print",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/run",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/save",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/evaluate"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_batch",
      "md_content": [
        "**_batch**: The function of _batch is to perform concurrent batch operations, running tasks asynchronously, and handling the results by saving them into a specified output file.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: \nThe `_batch` function is an asynchronous method designed to execute multiple batch tasks concurrently while managing concurrency with a semaphore and ensuring the results are processed and saved to an output file.\n\n1. **Concurrency Control**: The function begins by creating an `asyncio.Semaphore` object (`sem`), which limits the number of concurrent tasks to the value specified by `args.concurrency`. This semaphore ensures that no more than the allowed number of tasks run simultaneously, thus preventing resource exhaustion and maintaining controlled parallelism.\n\n2. **Task Definition**: Inside the `_batch` function, a nested asynchronous function `run_one` is defined. This function is responsible for executing a single batch task. Each task is initiated by calling `run_one(idx)`, where `idx` represents the index of the task. The `run_one` function utilizes the semaphore (`async with sem`) to ensure that only a limited number of tasks can run concurrently. For each task:\n   - A message is printed, indicating the start of the batch run using the `printer.rule` method.\n   - An instance of the `ReverseUpgradeWorkflow` is created with parameters `max_level` and `max_tries` from the `args` object. This instance manages the workflow and executes the required operations via `await wf.run()`.\n   - Once the workflow completes, the items generated by the workflow are converted to dictionaries and stored in a list.\n   - The function then attempts to read an existing output file specified by `args.out`. If the file exists, its contents are loaded into a list, and the newly generated items are appended to it. If the file is not found, it creates a new list to store the items.\n   - The updated list is written back to the output file using `json.dump`, ensuring the data is saved in a structured, human-readable format with proper indentation and UTF-8 encoding.\n   - A message is printed to indicate the number of items saved and the total number of items in the output file using the `printer.print` method.\n\n3. **Error Handling**: If any errors occur during the execution of the task (e.g., workflow failure, file I/O errors), they are caught by the `try-except` block, and the error message is logged using `logger.exception`. The function then prints an error message using `printer.print` in bold red style.\n\n4. **Task Submission and Completion**: After defining the task, the `_batch` function uses `asyncio.gather` to submit all tasks concurrently. This function waits for all tasks to complete before proceeding. Once all tasks are finished, a final message is printed, indicating the completion of the batch run and the location of the output file.\n\nThe `_batch` function interacts with several key components of the project, including the `ReverseUpgradeWorkflow` (for running tasks), `file_lock` (for managing file access), and the `printer` (for logging and output display). The asynchronous nature of the function allows it to handle multiple tasks concurrently, making it efficient for batch processing operations.\n\n**Note**: \n- Ensure that the `args` object contains the required parameters such as `concurrency`, `batch`, `max_level`, `max_tries`, and `out`. These parameters control the number of tasks, the task limits, and the output file path.\n- The output file (`args.out`) must be writable, and appropriate file permissions should be set.\n- Concurrency control via the semaphore is crucial for preventing excessive resource usage, especially when the batch size (`args.batch`) is large.\n- Proper error handling ensures that the system remains robust even in the face of unexpected issues, such as missing files or workflow failures.\n\n**Output Example**: \nAssuming the `args.batch` is set to 3, and each task generates a set of items, the final output might look as follows:\n\n```\nBatch Run 1/3\nBatch saved 10 items; total now 10\nBatch Run 2/3\nBatch saved 15 items; total now 25\nBatch Run 3/3\nBatch saved 8 items; total now 33\nCompleted 3 runs; file at output_data.json\n```\n\nThe output file `output_data.json` would contain the accumulated data from all three batch runs, with each task appending its results to the file."
      ],
      "code_start_line": 587,
      "code_end_line": 620,
      "params": [],
      "have_return": true,
      "code_content": "    async def _batch():\n\n        # 2. 并发控制信号量\n        sem = asyncio.Semaphore(args.concurrency)\n\n        # 3. 定义单次运行任务\n        async def run_one(idx: int):\n            async with sem:\n                try:\n                    printer.rule(f\"Batch Run {idx+1}/{args.batch}\")\n                    wf = ReverseUpgradeWorkflow(max_level=args.max_level, max_tries=args.max_tries)\n                    await wf.run()\n                    # 完成一次运行后立即读-插入-写回\n                    items = [it.to_dict() for it in wf.items]\n                    with file_lock:\n                        # 读取现有数据列表，文件不存在时视为空\n                        try:\n                            with open(args.out, \"r\", encoding=\"utf-8\") as f:\n                                existing = json.load(f)\n                        except FileNotFoundError:\n                            existing = []\n                        existing.extend(items)\n                        with open(args.out, \"w\", encoding=\"utf-8\") as f:\n                            json.dump(existing, f, ensure_ascii=False, indent=2)\n                    printer.print(f\"Batch saved {len(items)} items; total now {len(existing)}\", style=\"bold green\")\n                    return items\n                except Exception as e:\n                    logger.exception(f\"Error in batch run {idx+1}\")\n                    printer.print(f\"Error in batch run {idx+1}: {e}\", style=\"bold red\")\n                    return []\n\n        # 4. 提交所有任务并等待完成\n        await asyncio.gather(*(run_one(i) for i in range(args.batch)))\n        printer.print(f\"Completed {args.batch} runs; file at {args.out}\", style=\"bold green\")\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "run_one",
      "md_content": [
        "**run_one**: The function of run_one is to execute a single iteration of a batch process that runs a workflow and saves the results to a specified output file.\n\n**parameters**: The parameters of this Function.\n· idx: An integer representing the index of the current batch run.\n\n**Code Description**: The run_one function is an asynchronous method designed to manage a single execution of a batch process within a larger workflow. It begins by acquiring a semaphore lock to ensure that only a limited number of concurrent executions can occur, which is crucial for managing resources and preventing race conditions.\n\nThe function first logs the start of the batch run using the printer's rule method, which creates a visual separator in the console output to enhance readability. It then instantiates a ReverseUpgradeWorkflow object with specified parameters for maximum levels and maximum tries, which dictate how the workflow will operate.\n\nThe core of the function involves calling the run method of the ReverseUpgradeWorkflow instance, which orchestrates the entire workflow process. This includes generating seed questions, upgrading them through multiple levels, and verifying the results. After the workflow completes, the function collects the items processed during this run by converting each QAItem to a dictionary format using the to_dict method.\n\nSubsequently, the function attempts to read existing data from the specified output file. If the file does not exist, it initializes an empty list. The newly generated items are then appended to this list. The function writes the updated list back to the output file in JSON format, ensuring that the data is properly serialized and formatted for future use.\n\nIn case of any exceptions during the execution, the function logs the error details and prints an error message to the console, indicating the failure of the batch run. This robust error handling ensures that users are informed of any issues that arise during the process.\n\nThe run_one function is invoked multiple times within a broader batch processing context, allowing for the execution of several workflows in parallel. This design enhances the efficiency of the overall system by leveraging asynchronous programming.\n\n**Note**: It is essential to ensure that the output file path is correctly specified and that the necessary permissions are in place for reading and writing files. Additionally, users should be aware of the semaphore's limit to avoid exceeding the maximum number of concurrent executions.\n\n**Output Example**: A possible appearance of the code's return value could be a list of dictionaries representing the items processed during the batch run, structured as follows:\n```json\n[\n    {\n        \"level\": 0,\n        \"question\": \"What is the capital of France?\",\n        \"answer\": \"Paris\",\n        \"evidence\": [],\n        \"strategy\": \"seed\"\n    },\n    {\n        \"level\": 1,\n        \"question\": \"Can you name a famous landmark in Paris?\",\n        \"answer\": \"Eiffel Tower\",\n        \"evidence\": [],\n        \"strategy\": \"equivalent replacement\"\n    }\n]\n```"
      ],
      "code_start_line": 593,
      "code_end_line": 616,
      "params": [
        "idx"
      ],
      "have_return": true,
      "code_content": "        async def run_one(idx: int):\n            async with sem:\n                try:\n                    printer.rule(f\"Batch Run {idx+1}/{args.batch}\")\n                    wf = ReverseUpgradeWorkflow(max_level=args.max_level, max_tries=args.max_tries)\n                    await wf.run()\n                    # 完成一次运行后立即读-插入-写回\n                    items = [it.to_dict() for it in wf.items]\n                    with file_lock:\n                        # 读取现有数据列表，文件不存在时视为空\n                        try:\n                            with open(args.out, \"r\", encoding=\"utf-8\") as f:\n                                existing = json.load(f)\n                        except FileNotFoundError:\n                            existing = []\n                        existing.extend(items)\n                        with open(args.out, \"w\", encoding=\"utf-8\") as f:\n                            json.dump(existing, f, ensure_ascii=False, indent=2)\n                    printer.print(f\"Batch saved {len(items)} items; total now {len(existing)}\", style=\"bold green\")\n                    return items\n                except Exception as e:\n                    logger.exception(f\"Error in batch run {idx+1}\")\n                    printer.print(f\"Error in batch run {idx+1}: {e}\", style=\"bold red\")\n                    return []\n",
      "name_column": 18,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/QAItem/to_dict",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow",
        "src/criticsearch/abstract_substitution/abs_exp_1.py/ReverseUpgradeWorkflow/run"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/abstract_substitution/contemporary_test.py": [
    {
      "type": "FunctionDef",
      "name": "reorder_constrained_format",
      "md_content": [
        "**reorder_constrained_format**: The function of reorder_constrained_format is to rearrange the fields of each record in the input data such that the \"constrained_format\" field appears immediately after the \"GroundTruth\" field.\n\n**parameters**: The parameters of this Function.\n· data: A dictionary where each key is a unique identifier and each value is another dictionary representing a record containing various fields including \"GroundTruth\" and \"constrained_format\".\n\n**Code Description**: The reorder_constrained_format function takes a dictionary called data as input. This dictionary is expected to contain multiple entries, each identified by a unique key. Each entry is itself a dictionary that may include fields such as \"GroundTruth\" and \"constrained_format\", along with potentially other fields.\n\nThe function initializes an empty dictionary called new_data to store the reordered entries. It then iterates through each key-value pair in the input data. For each entry, it retrieves the value associated with the \"GroundTruth\" key and the value associated with the \"constrained_format\" key. \n\nNext, it constructs a new dictionary called others that contains all other fields in the entry, excluding \"GroundTruth\" and \"constrained_format\". This is achieved using a dictionary comprehension that filters out these two keys.\n\nThe function then creates a new dictionary called reordered, starting with the \"GroundTruth\" field. If the \"constrained_format\" field is not None, it adds this field to the reordered dictionary immediately after \"GroundTruth\". Finally, it updates the reordered dictionary with any remaining fields stored in others.\n\nThe newly constructed entry is then added to the new_data dictionary using the original key. Once all entries have been processed, the function returns the new_data dictionary, which contains all the original records but with the specified fields reordered.\n\n**Note**: It is important to ensure that the input data structure adheres to the expected format, with each entry containing the necessary fields. If an entry does not contain \"GroundTruth\" or \"constrained_format\", the function will still process it, but the output will reflect the absence of these fields accordingly.\n\n**Output Example**: \nGiven an input like:\n{\n    \"record1\": {\n        \"GroundTruth\": \"True\",\n        \"constrained_format\": \"Format1\",\n        \"other_field\": \"Value1\"\n    },\n    \"record2\": {\n        \"GroundTruth\": \"False\",\n        \"other_field\": \"Value2\"\n    }\n}\n\nThe output of the function would be:\n{\n    \"record1\": {\n        \"GroundTruth\": \"True\",\n        \"constrained_format\": \"Format1\",\n        \"other_field\": \"Value1\"\n    },\n    \"record2\": {\n        \"GroundTruth\": \"False\",\n        \"other_field\": \"Value2\"\n    }\n} \n\nIn this example, the \"constrained_format\" field appears after \"GroundTruth\" in the first record, while the second record does not include \"constrained_format\" but retains the order of the other fields."
      ],
      "code_start_line": 5,
      "code_end_line": 19,
      "params": [
        "data"
      ],
      "have_return": true,
      "code_content": "def reorder_constrained_format(data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"将每条记录中 constrained_format 放在 GroundTruth 之后\"\"\"\n    new_data = {}\n    for q, entry in data.items():\n        gt = entry.get(\"GroundTruth\")\n        cf = entry.get(\"constrained_format\")\n        # 其余字段\n        others = {k: v for k, v in entry.items() if k not in (\"GroundTruth\", \"constrained_format\")}\n        # 重建顺序\n        reordered = {\"GroundTruth\": gt}\n        if cf is not None:\n            reordered[\"constrained_format\"] = cf\n        reordered.update(others)\n        new_data[q] = reordered\n    return new_data\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "find_all_wrong_items",
      "md_content": [
        "**find_all_wrong_items**: The function of find_all_wrong_items is to return a mapping of entries (question -> entry) where all models have answered incorrectly.\n\n**parameters**:\n- data: A dictionary where the keys are questions (strings) and the values are entries (dictionaries). Each entry contains information about the question, including responses from multiple models, with fields like \"GroundTruth\" and \"constrained_format\" excluded for the check.\n\n**Code Description**:  \nThe `find_all_wrong_items` function processes a given dataset `data`, which consists of multiple question-answer entries. For each question, it examines the answers provided by various models. The function filters out the fields labeled \"GroundTruth\" and \"constrained_format\" from the analysis and checks if all models have answered incorrectly. A model's answer is considered incorrect if the value of its \"is_correct\" field is set to `False`.\n\nThe function performs the following steps:\n1. Initializes an empty dictionary `wrong_items` to store the entries where all models answered incorrectly.\n2. Iterates through each question (`q`) and its corresponding entry in the `data`.\n3. For each entry, it filters out the fields \"GroundTruth\" and \"constrained_format\" to gather the model responses.\n4. It checks if all the model responses contain the key \"is_correct\" set to `False`. If this condition is met, it considers the entire entry as wrong and adds it to the `wrong_items` dictionary.\n5. Finally, it returns the `wrong_items` dictionary, which maps questions to entries where all models answered incorrectly.\n\n**Note**:  \n- The function assumes that each model's response is a dictionary, and the \"is_correct\" field within each model's response is a boolean that indicates whether the answer is correct or not.\n- The \"GroundTruth\" and \"constrained_format\" fields are excluded from the analysis; only other model responses are considered.\n- The function will not include any questions in the output where at least one model answered correctly (i.e., if any model has \"is_correct\" set to `True`).\n\n**Output Example**:  \nAn example output could look like this:\n\n```python\n{\n    \"question_1\": {\n        \"model_1\": {\"is_correct\": False, \"answer\": \"incorrect_answer\"},\n        \"model_2\": {\"is_correct\": False, \"answer\": \"incorrect_answer\"},\n        \"model_3\": {\"is_correct\": False, \"answer\": \"incorrect_answer\"}\n    },\n    \"question_2\": {\n        \"model_1\": {\"is_correct\": False, \"answer\": \"incorrect_answer\"},\n        \"model_2\": {\"is_correct\": False, \"answer\": \"incorrect_answer\"},\n        \"model_3\": {\"is_correct\": False, \"answer\": \"incorrect_answer\"}\n    }\n}\n```  \nIn this example, both `question_1` and `question_2` are mapped in the output because all models for these questions have answered incorrectly."
      ],
      "code_start_line": 21,
      "code_end_line": 34,
      "params": [
        "data"
      ],
      "have_return": true,
      "code_content": "def find_all_wrong_items(data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    返回所有模型都答错的条目映射（question -> entry）。\n    判定：除 GroundTruth 和 constrained_format 外，\n    若所有其它字段中的 'is_correct' 都为 False，则认为此条目全部答错。\n    \"\"\"\n    wrong_items: Dict[str, Dict[str, Any]] = {}\n    for q, entry in data.items():\n        # 收集所有模型字段\n        model_fields = [v for k, v in entry.items() if k not in (\"GroundTruth\", \"constrained_format\")]\n        # 若至少有一个模型且全部 is_correct=False\n        if model_fields and all(not m.get(\"is_correct\", False) for m in model_fields if isinstance(m, dict)):\n            wrong_items[q] = entry\n    return wrong_items\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/abstract_substitution/abs_workflow.py": [
    {
      "type": "FunctionDef",
      "name": "_logged_printer_print",
      "md_content": [
        "**_logged_printer_print**: The function of _logged_printer_print is to print a message using the original printer print function and log the message for further processing or tracking.\n\n**parameters**: The parameters of this Function.\n· msg: The message to be printed and logged.\n· *args: Additional positional arguments to be passed to the original printer print function.\n· **kwargs: Additional keyword arguments to be passed to the original printer print function.\n\n**Code Description**: \nThe function _logged_printer_print is designed to wrap around an existing print functionality (_orig_printer_print) and add logging capabilities. \n\n1. **Print Message**: The first action within the function is to invoke the original print function (_orig_printer_print), passing along the message `msg` and any additional arguments (`*args`, `**kwargs`) that were provided to _logged_printer_print. This ensures that the message is printed to the console or the intended output, as would normally happen when using the original print function.\n\n2. **Logging the Message**: After printing the message, the function attempts to safely convert the message (`msg`) into a string for logging. This is done using the `str(msg)` method, which converts the provided message into its string representation.\n\n3. **Error Handling**: If the conversion to a string fails (for instance, if `msg` contains an unsupported data type), an exception is caught, and the function falls back on using `repr(msg)` to convert the message into its formal string representation, which is typically a more verbose or detailed version of the message.\n\n4. **Logging**: Once the message is safely converted into a string (or a fallback representation), the function logs the message using the `logger.info()` method. This ensures that the message is logged for informational purposes, enabling later retrieval or tracking in the system logs.\n\n**Note**: \n- The function relies on the presence of a logger, which must be initialized elsewhere in the code.\n- The original print functionality (_orig_printer_print) should be properly defined and accessible for this function to work correctly.\n- This function is useful when you need to both print a message to the console and keep a record of the message in a log for debugging or auditing purposes."
      ],
      "code_start_line": 71,
      "code_end_line": 78,
      "params": [
        "msg"
      ],
      "have_return": false,
      "code_content": "def _logged_printer_print(msg, *args, **kwargs):\n    _orig_printer_print(msg, *args, **kwargs)\n    # Attempt to convert msg to string safely for logging\n    try:\n        log_msg = str(msg)\n    except Exception:\n        log_msg = repr(msg) # Fallback to repr if str fails\n    logger.info(log_msg)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_logged_printer_rule",
      "md_content": [
        "**_logged_printer_rule**: The function of _logged_printer_rule is to call an original printer function and log the provided message.\n\n**parameters**:\n· msg: The message to be logged and passed to the original printer function.\n· *args: Additional positional arguments to be passed to the original printer function.\n· **kwargs: Additional keyword arguments to be passed to the original printer function.\n\n**Code Description**: \nThe _logged_printer_rule function performs two primary tasks. First, it calls the _orig_printer_rule function, passing along the message (msg), along with any additional positional (*args) and keyword (**kwargs) arguments. This ensures that the original functionality of _orig_printer_rule is executed as expected.\n\nSecondly, the function logs the message by utilizing a logger instance to record the message prefixed with the string \"SECTION: \". This is done by invoking the logger’s info method, which writes an informational log entry. This log entry helps to track the execution flow or specific sections of code by providing a record in the log files.\n\nThe usage of *args and **kwargs allows this function to handle any number of arguments, making it flexible for different contexts where the _orig_printer_rule might require varied inputs.\n\n**Note**: \n- Ensure that the _orig_printer_rule function is defined elsewhere in the code as it is integral to the operation of _logged_printer_rule.\n- The logger must be properly configured before using this function to avoid errors related to logging.\n- This function is primarily used for logging specific sections or messages while preserving the behavior of the original printer function."
      ],
      "code_start_line": 79,
      "code_end_line": 81,
      "params": [
        "msg"
      ],
      "have_return": false,
      "code_content": "def _logged_printer_rule(msg, *args, **kwargs):\n    _orig_printer_rule(msg, *args, **kwargs)\n    logger.info(f\"SECTION: {msg}\")\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "print",
      "md_content": [
        "**print**: The function of print is to log the output to both the standard output and a log file.\n\n**parameters**:\n· *args: A variable-length argument list that is passed to the print function. It contains all the arguments to be printed.\n· **kwargs: A variable-length keyword argument list that allows for the passing of additional options to the underlying print function.\n\n**Code Description**:  \nThe `print` function in this code acts as a custom wrapper around the built-in `print` function. It first calls the original `print` function (referred to as `_orig_print`) with the provided arguments (`*args` and `**kwargs`). This ensures that the standard output behavior of the print statement is retained. After this, the function logs the same arguments at the `info` level using the `logger`. The arguments passed to the print function are converted to strings, and then concatenated into a single string with spaces separating them. This string is then logged by the logger.\n\nThe relationship of this function with the rest of the project is clear in its role as a utility to provide consistent logging alongside standard output. The `print` function is used multiple times throughout the project's main execution flow. For example, in the `main` function, it is used to provide both the standard output and log entries for various informational messages, such as indicating which model is being used or showing the results of batch processes. This ensures that the user can see the output in the console while also storing it in a log file for future reference.\n\nThe print function is primarily invoked for logging various key steps or results in the process, like successful completions, errors, or specific statuses related to batch runs, combination tests, and evaluation. This helps in debugging and monitoring the execution of the program, as every message printed on the console is also stored in the log, providing traceability.\n\n**Note**: \n- Ensure that `_orig_print` is correctly defined elsewhere in the code as this function relies on it to maintain the default behavior of printing to the console.\n- The `logger` used here should be appropriately initialized to handle logging at the `info` level, and the log output should be configured as required in the project for effective logging."
      ],
      "code_start_line": 88,
      "code_end_line": 90,
      "params": [],
      "have_return": false,
      "code_content": "def print(*args, **kwargs):\n    _orig_print(*args, **kwargs)\n    logger.info(' '.join(str(a) for a in args))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "pretty_json",
      "md_content": [
        "**pretty_json**: The function of pretty_json is to format Python objects into a JSON string with indentation and ensure non-ASCII characters are preserved.\n\n**parameters**: The parameters of this Function.\n· data: The Python object (such as a dictionary or list) to be converted into a JSON-formatted string.\n\n**Code Description**: The pretty_json function uses Python’s built-in `json` module to convert the provided data into a JSON-formatted string. The function calls `json.dumps()` with two key arguments: `ensure_ascii=False` and `indent=2`. \n- `ensure_ascii=False` ensures that non-ASCII characters, such as characters from other languages, are included as-is in the output string (rather than being escaped into Unicode escape sequences).\n- `indent=2` adds indentation to the JSON string to make it more readable, with each level of nesting indented by 2 spaces.\n\nThe function is simple and is utilized in various parts of the codebase to print or log Python objects in a structured and readable JSON format. It is called by multiple functions within the project to print or log JSON data for debugging or user interaction. For example:\n1. In `tavily_extract`, `pretty_json(urls)` is used to print the URLs being extracted in a human-readable format.\n2. In `fallback_scrape`, it is used to print both the input URLs and the results of the scraping process.\n3. In `query_update`, it is used to display the updated query results and evidence.\n4. Similarly, in `generate_seed`, it formats and prints the output of the seed fact generation.\n5. In `save`, `pretty_json` is used to print a preview of the data being saved.\n\nThis consistency in using `pretty_json` for logging ensures that data is always presented in an easy-to-read JSON format across different parts of the code.\n\n**Note**: It is important to remember that the `pretty_json` function does not modify the original data; it merely converts it into a JSON string for display or logging purposes. Also, it is intended for use in scenarios where a readable representation of the data is necessary, particularly for debugging and monitoring.\n\n**Output Example**: Given a dictionary input like:\n```python\n{\n  \"name\": \"John Doe\",\n  \"age\": 30,\n  \"city\": \"New York\"\n}\n```\nThe function would return the following JSON string:\n```json\n{\n  \"name\": \"John Doe\",\n  \"age\": 30,\n  \"city\": \"New York\"\n}\n```"
      ],
      "code_start_line": 93,
      "code_end_line": 95,
      "params": [
        "data"
      ],
      "have_return": true,
      "code_content": "def pretty_json(data):\n    import json\n    return json.dumps(data, ensure_ascii=False, indent=2)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/tavily_extract",
        "src/criticsearch/abstract_substitution/abs_workflow.py/fallback_scrape",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/save",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_query_update"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "call_llm",
      "md_content": [
        "**call_llm**: The function of call_llm is to invoke the OpenAI chat completion API with a specified prompt and configuration options.\n\n**parameters**: The parameters of this Function.\n· prompt: A string that contains the text prompt to be sent to the language model.\n· model: An optional string that specifies the model to be used for the API call, defaulting to GPT_MODEL.\n· temperature: An optional float that controls the randomness of the output, with a default value of 0.7.\n· system_prompt: An optional string that provides a system-level instruction to the model.\n\n**Code Description**: The call_llm function is designed to facilitate communication with the OpenAI chat completion service. It begins by printing a visual separator titled \"LLM Prompt\" to enhance the readability of the console output. The prompt provided by the user is then printed in bold yellow for emphasis. If a system prompt is specified, it is also printed in italic cyan, providing context for the model's response.\n\nThe function initializes an OpenAI client using the provided API key and base URL. It constructs a list of messages that will be sent to the API, starting with the system prompt (if provided) followed by the user prompt. This structured message format is essential for the API to understand the context of the conversation.\n\nThe function then calls the chat completion API with the specified model, messages, temperature, and a maximum token limit defined by MAX_TOKENS. The response from the API is captured, and the content of the first choice is extracted. A visual separator titled \"LLM Raw Output\" is printed, followed by the model's response displayed in bold green.\n\nThis function is integral to the workflow of the application, as it allows for dynamic interaction with the language model, enabling various functionalities such as text generation, conversation simulation, and more. The use of the printer's rule and print methods enhances the output's clarity, making it easier for developers to debug and understand the flow of data.\n\n**Note**: When using the call_llm function, ensure that the prompt and system prompt are clear and concise to maximize the effectiveness of the model's response. Additionally, be mindful of the temperature setting, as it influences the creativity and variability of the output.\n\n**Output Example**: An example of the return value from the call_llm function could be a string such as: \"The quick brown fox jumps over the lazy dog.\" This output represents a typical response generated by the language model based on the provided prompt."
      ],
      "code_start_line": 98,
      "code_end_line": 125,
      "params": [
        "prompt"
      ],
      "have_return": true,
      "code_content": "def call_llm(\n    prompt: str,\n    *,\n    model: str = GPT_MODEL,\n    temperature: float = 0.7,\n    system_prompt: Optional[str] = None,\n) -> str:\n    \"\"\"Call OpenAI chat completion (stand‑alone).\"\"\"\n    printer.rule(\"LLM Prompt\")\n    printer.print(prompt, style=\"bold yellow\")\n    if system_prompt:\n        printer.print(f\"[system prompt]: {system_prompt}\", style=\"italic cyan\")\n    client = OpenAI(api_key=GPT_API_KEY, base_url=GPT_BASE_URL)\n    messages: List[Dict[str, str]] = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    resp = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=MAX_TOKENS,\n    )\n    result = resp.choices[0].message.content\n    printer.rule(\"LLM Raw Output\")\n    printer.print(result, style=\"bold green\")\n    return result\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "tavily_search",
      "md_content": [
        "**tavily_search**: The function of tavily_search is to send a search query to the Tavily API and retrieve search results based on that query.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the search query to be processed.  \n· include_raw_content: A boolean flag that indicates whether to include raw content in the search result. It defaults to True.\n\n**Code Description**: \nThe `tavily_search` function performs an asynchronous HTTP request to the Tavily API, which is an external service used to retrieve search results based on a provided query. The function accepts two parameters:\n1. `query`: A string containing the query you want to search for.\n2. `include_raw_content`: A boolean flag that, when set to `True`, includes raw content in the search results. If set to `False`, this raw content is excluded.\n\nHere is the flow of the function:\n- The function starts by defining the payload for the API request. This payload contains the search query, the flag to include raw content, and the API key (which is stored in the constant `TAVILY_API_KEY`).\n- It then creates an asynchronous HTTP client using `httpx.AsyncClient`. This client is used to send the POST request to the `TAVILY_SEARCH_URL` endpoint, which is the Tavily API's search endpoint.\n- Once the request is made, the function waits for the response. After receiving the response, it pauses for a brief moment using `time.sleep(0.1)` to ensure the response is fully processed.\n- The function extracts the results from the JSON response using `r.json().get(\"results\", [])`. If the API returns a successful response, this will contain a list of search results; if not, it returns an empty list.\n- Finally, the search results are returned to the caller.\n\nThis function is used in various parts of the project. Specifically, it is invoked within the methods `multi_verify`, `query_update`, and `generate_seed`. In these methods, `tavily_search` is called to retrieve search results that are then used as evidence or additional data for further operations. For example:\n- In `multi_verify`, the search results are utilized to verify whether a model can provide an answer based on external search data.\n- In `query_update`, the function is used to update queries by retrieving search results and incorporating them into a new query.\n- In `generate_seed`, `tavily_search` helps generate seed questions by retrieving relevant data to form a question based on external information.\n\nThis makes `tavily_search` an essential component in retrieving real-time data from the Tavily API to enhance decision-making in the workflow of these other methods.\n\n**Note**: It is important to ensure that the `TAVILY_API_KEY` is set correctly, as this is needed to authenticate the request to the Tavily API. Additionally, the use of `time.sleep(0.1)` is to introduce a small delay, which may be useful to avoid hitting rate limits or overloading the API in some cases.\n\n**Output Example**:  \nThe output of the `tavily_search` function is a list of dictionaries representing the search results. Each dictionary in the list typically contains fields such as the search result’s title, snippet, URL, and other relevant data.\n\nExample:\n```json\n[\n  {\n    \"title\": \"Example result title 1\",\n    \"snippet\": \"This is a short description of the result content.\",\n    \"url\": \"http://example.com/result1\"\n  },\n  {\n    \"title\": \"Example result title 2\",\n    \"snippet\": \"This is another short description of the result content.\",\n    \"url\": \"http://example.com/result2\"\n  }\n]\n```"
      ],
      "code_start_line": 128,
      "code_end_line": 138,
      "params": [
        "query"
      ],
      "have_return": true,
      "code_content": "async def tavily_search(query: str, *, include_raw_content: bool = True) -> List[dict]:\n    # printer.rule(\"Tavily Search Query\")\n    # printer.print(query, style=\"bold yellow\")\n    payload = {\"query\": query, \"include_raw_content\": include_raw_content, \"api_key\": TAVILY_API_KEY}\n    async with httpx.AsyncClient(http2=True, timeout=30) as client:\n        r = await client.post(TAVILY_SEARCH_URL, json=payload)\n    time.sleep(0.1)\n    results = r.json().get(\"results\", [])\n    # printer.rule(\"Tavily Search Results\")\n    # printer.print(pretty_json(results), style=\"bold green\")\n    return results\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/generate_seed"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "tavily_extract",
      "md_content": [
        "**tavily_extract**: The function of tavily_extract is to asynchronously extract data from a specified URL using a list of URLs and return the results in a structured format.\n\n**parameters**: The parameters of this Function.\n· urls: A list of strings representing the URLs to be extracted.\n\n**Code Description**: The tavily_extract function is designed to facilitate the extraction of data from a remote service by sending a POST request to a predefined endpoint (TAVILY_EXTRACT_URL). It accepts a single parameter, urls, which is a list of strings containing the URLs that need to be processed. \n\nUpon invocation, the function first utilizes the printer.rule method to create a visual separator in the console output, indicating the start of the URL extraction process. It then prints the list of URLs in a formatted JSON style using the pretty_json function, which enhances readability by indenting the output and preserving non-ASCII characters.\n\nThe function establishes an asynchronous HTTP client session using httpx.AsyncClient, which is configured to support HTTP/2 and has a timeout of 30 seconds. It sends a POST request to the TAVILY_EXTRACT_URL, including the list of URLs in the request body as JSON and attaching an authorization header that contains a bearer token (TAVILY_API_KEY) for authentication.\n\nOnce the request is completed, the function retrieves the response in JSON format and prints the results using the printer.rule and printer.print methods, similarly formatted for clarity. Finally, the function returns the extracted results as a dictionary.\n\nThe tavily_extract function is part of a broader workflow where it interacts with other components of the project, particularly in scenarios where data extraction from multiple URLs is required. Its use of asynchronous programming allows for efficient handling of I/O-bound operations, making it suitable for applications that require high performance and responsiveness.\n\n**Note**: When using the tavily_extract function, ensure that the URLs provided are valid and that the TAVILY_API_KEY is correctly set up to avoid authentication errors. Additionally, consider the rate limits and response times of the external service being accessed.\n\n**Output Example**: A possible return value from the tavily_extract function could look like the following JSON structure:\n```json\n{\n  \"url1\": {\n    \"data\": \"extracted data from url1\",\n    \"status\": \"success\"\n  },\n  \"url2\": {\n    \"data\": \"extracted data from url2\",\n    \"status\": \"success\"\n  },\n  \"url3\": {\n    \"error\": \"failed to extract data\",\n    \"status\": \"error\"\n  }\n}\n```"
      ],
      "code_start_line": 140,
      "code_end_line": 152,
      "params": [
        "urls"
      ],
      "have_return": true,
      "code_content": "async def tavily_extract(urls: List[str]) -> Dict[str, dict]:\n    printer.rule(\"Tavily Extract URLs\")\n    printer.print(pretty_json(urls), style=\"bold yellow\")\n    async with httpx.AsyncClient(http2=True, timeout=30) as client:\n        r = await client.post(\n            TAVILY_EXTRACT_URL,\n            json={\"urls\": urls},\n            headers={\"Authorization\": f\"Bearer {TAVILY_API_KEY}\"},\n        )\n    result = r.json()\n    printer.rule(\"Tavily Extract Results\")\n    printer.print(pretty_json(result), style=\"bold green\")\n    return result\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_workflow.py/pretty_json"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "fallback_scrape",
      "md_content": [
        "**fallback_scrape**: The function of fallback_scrape is to asynchronously scrape text content from a list of URLs and return the results in a structured format.\n\n**parameters**: The parameters of this Function.\n· urls: A List[str] that contains the URLs to be scraped.\n\n**Code Description**: The fallback_scrape function is designed to perform web scraping on a list of provided URLs. It begins by printing a visual separator titled \"Fallback Scrape URLs\" to enhance the readability of the console output. The URLs to be scraped are formatted into a pretty JSON string for clear display.\n\nThe function utilizes an asynchronous approach to fetch content from each URL. It defines an inner asynchronous function, fetch, which takes a single URL as an argument. Within fetch, an HTTP GET request is made using the httpx.AsyncClient, with a specified User-Agent header to mimic a standard web browser request. If the response status code is not 200, an empty string is returned, indicating that the content could not be retrieved.\n\nIf the response is successful, the HTML content is parsed using BeautifulSoup. The function removes unwanted tags such as script, style, noscript, and meta to focus on the main textual content. The text is then extracted from all paragraph tags (<p>) and concatenated into a single string, which is returned.\n\nThe main function gathers results from all URLs concurrently using asyncio.gather, which allows for efficient handling of multiple asynchronous calls. The results are compiled into a dictionary where each URL is mapped to its corresponding scraped text, including any empty results for URLs that failed to return valid content.\n\nAfter the scraping process, another visual separator titled \"Fallback Scrape Results\" is printed, followed by the formatted results in JSON. This structured output aids in understanding the results of the scraping operation.\n\nThe fallback_scrape function is particularly useful in scenarios where standard scraping methods may fail, providing a fallback mechanism to retrieve content from specified URLs. It is invoked in contexts where web content extraction is necessary, ensuring that even in cases of failure, the function attempts to gather as much information as possible.\n\n**Note**: When using the fallback_scrape function, ensure that the URLs provided are valid and accessible. The function is designed to handle exceptions gracefully, returning empty strings for any URLs that cannot be fetched. This behavior allows users to identify which URLs were problematic without disrupting the overall scraping process.\n\n**Output Example**: Given a list of URLs such as:\n```python\n[\"http://example.com/page1\", \"http://example.com/page2\"]\n```\nThe function might return a dictionary like:\n```json\n{\n  \"http://example.com/page1\": \"Text content from page 1.\",\n  \"http://example.com/page2\": \"Text content from page 2.\"\n}\n```"
      ],
      "code_start_line": 154,
      "code_end_line": 176,
      "params": [
        "urls"
      ],
      "have_return": true,
      "code_content": "async def fallback_scrape(urls: List[str]) -> Dict[str, str]:\n    printer.rule(\"Fallback Scrape URLs\")\n    printer.print(pretty_json(urls), style=\"bold yellow\")\n    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n    async def fetch(u: str) -> str:\n        try:\n            async with httpx.AsyncClient(timeout=10) as client:\n                r = await client.get(u, headers=headers)\n            if r.status_code != 200:\n                return \"\"\n            soup = BeautifulSoup(r.text, \"html.parser\")\n            for t in soup([\"script\", \"style\", \"noscript\", \"meta\"]): # Keeping original tags\n                t.decompose()\n            # Keeping original text extraction method\n            return \"\\n\".join(p.get_text(\" \", strip=True) for p in soup.find_all(\"p\"))\n        except Exception:\n             # Keeping original bare except\n            return \"\"\n    texts = await asyncio.gather(*(fetch(u) for u in urls))\n    result = {u: t for u, t in zip(urls, texts)} # Keeping original logic (includes empty results)\n    printer.rule(\"Fallback Scrape Results\")\n    printer.print(pretty_json(result), style=\"bold green\")\n    return result\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_workflow.py/pretty_json"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "fetch",
      "md_content": [
        "**fetch**: The function of fetch is to asynchronously retrieve the HTML content from a specified URL and extract the text from paragraph tags while ignoring certain other tags.\n\n**parameters**: The parameters of this Function.\n· u: A string representing the URL from which the HTML content is to be fetched.\n\n**Code Description**: The fetch function is designed to perform an asynchronous HTTP GET request to the provided URL using the httpx library. It initializes an AsyncClient with a timeout of 10 seconds to ensure that the request does not hang indefinitely. Upon making the request, it checks the response status code; if the status code is not 200 (indicating a successful request), the function returns an empty string. \n\nIf the request is successful, the function utilizes BeautifulSoup to parse the HTML content of the response. It specifically removes script, style, noscript, and meta tags from the parsed content to focus solely on the textual content. The function then extracts text from all paragraph tags, joining them into a single string with newline characters separating each paragraph. In the event of any exceptions during the process, the function catches the exception and returns an empty string, ensuring that the function fails gracefully without raising errors.\n\n**Note**: It is important to ensure that the URL provided is valid and accessible. The function is designed to handle exceptions, but users should be aware that network issues or invalid URLs may still result in an empty return value.\n\n**Output Example**: An example of the return value from the fetch function could be:\n```\n\"This is the first paragraph of the webpage content.\nThis is the second paragraph, providing more information.\"\n```"
      ],
      "code_start_line": 158,
      "code_end_line": 171,
      "params": [
        "u"
      ],
      "have_return": true,
      "code_content": "    async def fetch(u: str) -> str:\n        try:\n            async with httpx.AsyncClient(timeout=10) as client:\n                r = await client.get(u, headers=headers)\n            if r.status_code != 200:\n                return \"\"\n            soup = BeautifulSoup(r.text, \"html.parser\")\n            for t in soup([\"script\", \"style\", \"noscript\", \"meta\"]): # Keeping original tags\n                t.decompose()\n            # Keeping original text extraction method\n            return \"\\n\".join(p.get_text(\" \", strip=True) for p in soup.find_all(\"p\"))\n        except Exception:\n             # Keeping original bare except\n            return \"\"\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_boxed",
      "md_content": [
        "**extract_boxed**: The function of extract_boxed is to extract and return a specific substring from the input text that is enclosed within a predefined pattern.\n\n**parameters**: The parameters of this Function.\n· text: A string input from which the boxed content will be extracted.\n\n**Code Description**: The extract_boxed function utilizes a regular expression search to identify a specific pattern within the provided text. It employs the BOXED_RE regular expression object to search for a match. If a match is found, the function retrieves the first capturing group (the content within the matched pattern) and applies the strip method to remove any leading or trailing whitespace. If no match is found, the function returns an empty string. This function is particularly useful for extracting content that is formatted in a specific way, such as text enclosed in brackets or other delimiters defined by the BOXED_RE pattern.\n\n**Note**: It is essential to ensure that the BOXED_RE regular expression is correctly defined and imported in the context where this function is used. The function assumes that the input text is a valid string and does not handle exceptions related to invalid input types.\n\n**Output Example**: If the input text is \"Here is some text [extracted content] and more text\", and BOXED_RE is defined to match content within brackets, the function would return \"extracted content\". If the input text does not contain any matching pattern, the function would return an empty string."
      ],
      "code_start_line": 182,
      "code_end_line": 184,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def extract_boxed(text: str) -> str:\n    m = BOXED_RE.search(text)\n    return m.group(1).strip() if m else \"\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_answer_tag",
      "md_content": [
        "**extract_answer_tag**: The function of extract_answer_tag is to extract a specific portion of text identified by a regular expression pattern.\n\n**parameters**: \n· text: A string input that contains the text from which an answer tag is to be extracted.\n\n**Code Description**: \nThe `extract_answer_tag` function takes a string `text` as input and attempts to find a match for the regular expression pattern defined by `ANSWER_TAG_RE`. If a match is found, it retrieves the first capturing group from the match using the `group(1)` method. This value is then stripped of any leading or trailing whitespace and returned as the result. If no match is found, the function returns an empty string.\n\nThe function relies on the presence of a predefined regular expression pattern, `ANSWER_TAG_RE`, which is assumed to be designed to identify specific patterns within the input text. The pattern could be structured to match answer tags, such as those that denote answers in a question-and-answer format, though the exact pattern is not provided in the function itself.\n\n**Note**: \n- The function assumes that `ANSWER_TAG_RE` is already defined and accessible within the scope of this function.\n- The regular expression used by `ANSWER_TAG_RE` is critical for the correct extraction of the desired portion of the text. If the pattern is incorrectly defined, the function may fail to extract the intended data.\n- The `strip()` method is used to clean up any leading or trailing whitespace from the extracted value.\n\n**Output Example**: \nIf the input text is `\"<answer>42</answer>\"` and the `ANSWER_TAG_RE` pattern is designed to capture the text within the `<answer>` tags, the function will return `\"42\"`. If the input text does not contain a match, the function will return an empty string `\"\"`."
      ],
      "code_start_line": 186,
      "code_end_line": 188,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def extract_answer_tag(text: str) -> str:\n    m = ANSWER_TAG_RE.search(text)\n    return m.group(1).strip() if m else \"\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "QAItem",
      "md_content": [
        "**QAItem**: The function of QAItem is to represent a question-answer item with associated metadata for processing in a workflow.\n\n**attributes**: The attributes of this Class.\n· level: An integer representing the depth or level of the question in the hierarchy.\n· question: A string containing the text of the question.\n· answer: A string containing the text of the answer to the question.\n· constrained_format: A string that may specify any constraints on the format of the answer.\n· parent_question: An optional string that references the parent question, if applicable.\n· evidence: A list of strings that provide supporting evidence related to the question and answer.\n· strategy: A string that describes the strategy used to generate or verify the question-answer pair.\n\n**Code Description**: The QAItem class serves as a structured representation of a question and its corresponding answer, along with additional contextual information. This class includes several attributes that provide insight into the nature of the question-answer pair, such as its hierarchical level, the question text, the answer text, and any evidence supporting the answer. The `to_dict` method allows for easy conversion of the QAItem instance into a dictionary format, which can be useful for serialization or further processing.\n\nIn the context of the project, QAItem instances are utilized in various workflows, particularly within the ReverseUpgradeWorkflow. For instance, the `__init__` method of ReverseUpgradeWorkflow initializes a list of QAItem objects to store multiple question-answer pairs. The `multi_verify` method accepts a QAItem as input to perform multiple verification checks against the model's ability to answer the question. Similarly, the `query_update` method takes a QAItem as a seed to update the question based on new queries, while the `generate_seed` and `gpt_search_generate_seed` methods create new QAItem instances based on generated seeds from external sources. Each of these methods relies on the structured format provided by the QAItem class to ensure consistency and clarity in the workflow.\n\n**Note**: When using the QAItem class, it is important to ensure that all attributes are populated correctly, especially when creating new instances or updating existing ones. The `to_dict` method can be particularly useful for debugging or logging purposes, as it provides a clear representation of the QAItem's current state.\n\n**Output Example**: A possible appearance of the code's return value when converting a QAItem instance to a dictionary might look like this:\n{\n  \"level\": 1,\n  \"question\": \"What is the capital of France?\",\n  \"answer\": \"Paris\",\n  \"constrained_format\": \"\",\n  \"parent_question\": null,\n  \"evidence\": [\"Source A\", \"Source B\"],\n  \"strategy\": \"seed\"\n}"
      ],
      "code_start_line": 192,
      "code_end_line": 202,
      "params": [],
      "have_return": true,
      "code_content": "class QAItem:\n    level: int\n    question: str\n    answer: str\n    constrained_format: str\n    parent_question: Optional[str]\n    evidence: List[str]\n    strategy: str\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/__init__",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_query_update"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "to_dict",
      "md_content": [
        "**to_dict**: The function of to_dict is to convert the instance of the class into a dictionary representation.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The to_dict function utilizes the asdict function from the dataclasses module to convert the current instance of the class into a dictionary. This is particularly useful for serializing the object's data, allowing it to be easily transformed into formats such as JSON. The dictionary returned by this function will contain all the fields of the instance, with their corresponding values.\n\nThis function is called in the context of saving data in the ReverseUpgradeWorkflow class, specifically within the save method. In the save method, the to_dict function is invoked on each item in the workflow's items list. The resulting dictionaries are then aggregated and written to a specified file path in JSON format. This demonstrates the utility of the to_dict function in facilitating the serialization of complex objects into a simple, structured format that can be easily stored and retrieved.\n\nAdditionally, the to_dict function is also called in the run_one asynchronous function, where it is used to return a list of dictionaries representing the items processed during a batch run. This allows for a clear and structured output of the results, which can be useful for logging or further processing.\n\n**Note**: It is important to ensure that the class from which to_dict is called is defined as a dataclass, as the asdict function is specifically designed to work with dataclass instances.\n\n**Output Example**: A possible return value of the to_dict function could look like this:\n```json\n{\n    \"field1\": \"value1\",\n    \"field2\": \"value2\",\n    \"field3\": 123,\n    \"field4\": true\n}\n```"
      ],
      "code_start_line": 201,
      "code_end_line": 202,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def to_dict(self) -> dict:\n        return asdict(self)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/save",
        "src/criticsearch/abstract_substitution/abs_workflow.py/main/_batch/run_one"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "FuzzyQAItem",
      "md_content": [
        "**FuzzyQAItem**: The function of FuzzyQAItem is to represent a question-answer item that includes original and modified questions, answers, a format constraint, a strategy for modification, and supporting evidence.\n\n**attributes**: The attributes of this Class.\n· original_question: str - The original question before any modifications were made.\n· question: str - The modified question that may include fuzzy replacements.\n· answer: str - The answer corresponding to the original question.\n· constrained_format: str - The format constraints that the answer must adhere to.\n· strategy: str - The strategy used for generating the modified question.\n· evidence: List[str] - A list of evidence supporting the answer or the modifications made.\n\n**Code Description**: The FuzzyQAItem class serves as a structured representation of a question-answer pair that has undergone a fuzzy replacement process. It encapsulates the original question, the modified version, the corresponding answer, the constraints on the answer format, the strategy used for modification, and any evidence that supports the validity of the answer. \n\nThe class includes a method `to_dict()` which converts the instance attributes into a dictionary format. This is useful for serialization or when passing the data to other components of the application.\n\nThe FuzzyQAItem class is utilized within the broader context of the project, specifically in the `test_combination_workflow` function. This function orchestrates a series of operations that include generating a seed question-answer pair, extracting entities from the question, performing fuzzy replacements on those entities, and finally creating an instance of FuzzyQAItem with the results. The FuzzyQAItem instance is then validated using the `verify_fuzzy_item` function, which checks if the modified question can be answered correctly based on the provided evidence. If the validation fails, the FuzzyQAItem instance is discarded, ensuring that only valid question-answer pairs are retained for further processing.\n\n**Note**: It is important to ensure that the attributes of the FuzzyQAItem are populated correctly before passing the instance to the `verify_fuzzy_item` function, as the validation process relies heavily on the integrity of this data.\n\n**Output Example**: A possible appearance of the code's return value when converting a FuzzyQAItem instance to a dictionary might look like this:\n```json\n{\n  \"original_question\": \"What is the capital of France?\",\n  \"question\": \"What is the capital city of France?\",\n  \"answer\": \"Paris\",\n  \"constrained_format\": \"Provide the answer in one word.\",\n  \"strategy\": \"fuzzy_replacement\",\n  \"evidence\": [\"France is a country in Europe.\", \"Paris is known as the capital of France.\"]\n}\n```"
      ],
      "code_start_line": 205,
      "code_end_line": 214,
      "params": [],
      "have_return": true,
      "code_content": "class FuzzyQAItem:\n    original_question: str\n    question: str\n    answer: str\n    constrained_format: str\n    strategy: str\n    evidence: List[str]\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/verify_fuzzy_item",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_combination_workflow"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "to_dict",
      "md_content": [
        "**to_dict**: The function of to_dict is to convert the object to a dictionary representation.\n\n**parameters**: The function does not take any parameters.\n\n**Code Description**:  \nThe `to_dict` function is a method defined within a class, responsible for returning the instance of the class as a dictionary. It uses the `asdict()` function to achieve this transformation, which is a utility from Python’s `dataclasses` module that recursively converts all attributes of the class into key-value pairs. \n\nIn the context of the calling code, `to_dict` is invoked within a larger workflow. Specifically, when the result of a particular task (represented as an object) is successfully processed, the `to_dict` method is used to convert that object into a dictionary format. This dictionary is then appended to a list, which will later be saved to a JSON file.\n\nThe calling code uses `to_dict()` to ensure that the data structure is suitable for serializing into a JSON format. The main use case in this context is when handling the results of combination runs, which are stored after successful processing. In this workflow, multiple asynchronous tasks are executed in parallel, and once a result is obtained, the method is used to standardize the data format for easy storage and further processing.\n\n**Note**: The function is specifically useful in contexts where objects need to be saved or transmitted in a serialized format, such as when writing to a file or sending over a network. The `asdict()` method handles the conversion of the object, but it is important that the class is a dataclass, as `asdict()` is specifically designed for dataclass instances.\n\n**Output Example**:  \nAssuming the object contains the attributes `name`, `value`, and `status`, the output of `to_dict` would look like the following dictionary:\n\n```python\n{\n    'name': 'example_name',\n    'value': 42,\n    'status': 'success'\n}\n```"
      ],
      "code_start_line": 213,
      "code_end_line": 214,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def to_dict(self) -> dict:\n        return asdict(self)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/main/_combination_batch"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "verify_fuzzy_item",
      "md_content": [
        "**verify_fuzzy_item**: The function of verify_fuzzy_item is to validate whether a modified question can be answered correctly based on the provided evidence.\n\n**parameters**: The parameters of this Function.\n· item: FuzzyQAItem - An instance of the FuzzyQAItem class that contains the original question, modified question, expected answer, format constraints, and supporting evidence.\n\n**Code Description**: The verify_fuzzy_item function is an asynchronous function designed to assess the correctness of a modified question-answer pair represented by the FuzzyQAItem instance. The function performs several key operations:\n\n1. **Evidence Compilation**: It begins by compiling the evidence facts from the FuzzyQAItem instance. The evidence can be in the form of strings or dictionaries, and the function extracts the \"fact\" from dictionaries while directly appending strings. This results in a formatted string of facts that will be used to construct the prompt for the conversational model.\n\n2. **Prompt Construction**: A prompt is then constructed using the compiled facts, the modified question, and the format constraints specified in the FuzzyQAItem. This prompt is designed to instruct the conversational model to provide an answer in a specific format, namely within a LaTeX `\\boxed{}` expression.\n\n3. **Model Interaction**: The function calls the chat method from the BaseAgent class, passing the constructed prompt to interact with the conversational model. The model is expected to generate a response based on the provided input.\n\n4. **Response Processing**: Upon receiving the response, the function extracts the content enclosed within the `\\boxed{}` expression using the extract_boxed_content function. If no boxed content is returned, the function logs an error message and returns False, indicating validation failure.\n\n5. **Answer Validation**: The function normalizes both the predicted answer (from the model) and the expected answer (from the FuzzyQAItem) by removing spaces and converting them to lowercase. It then compares the normalized values. If they do not match, an error message is logged, and the function returns False. If they match, a success message is printed, and the function returns True.\n\n6. **Error Handling**: The function includes a try-except block to handle any exceptions that may occur during the process. If an exception is raised, it logs the error and returns False.\n\nThe verify_fuzzy_item function is called within the test_combination_workflow function, which orchestrates a series of operations involving entity extraction and fuzzy replacement. After generating a FuzzyQAItem instance, the test_combination_workflow function invokes verify_fuzzy_item to ensure that the modified question can be answered correctly based on the provided evidence. If the validation fails, the FuzzyQAItem instance is discarded, ensuring that only valid question-answer pairs are retained for further processing.\n\n**Note**: It is crucial that the FuzzyQAItem instance passed to verify_fuzzy_item is properly populated with accurate data, as the validation process relies heavily on the integrity of this information.\n\n**Output Example**: A possible return value from the verify_fuzzy_item function could be:\n```json\n{\n  \"result\": true,\n  \"message\": \"Validation passed: format and answer are correct.\"\n}\n```"
      ],
      "code_start_line": 216,
      "code_end_line": 259,
      "params": [
        "item"
      ],
      "have_return": true,
      "code_content": "async def verify_fuzzy_item(item: FuzzyQAItem) -> bool:\n    \"\"\"\n    验证模糊替换后的问题是否能从evidence中的facts中直接推理出答案:\n    - 拼接 evidence 中的 fact 条目\n    - 加入 question 和 constrained_format\n    - 要求模型以 \\\\boxed{} 格式作答\n    \"\"\"\n    # 拼接 evidence facts\n    lines: List[str] = []\n    for f in item.evidence:\n        if isinstance(f, dict):\n            fact = f.get(\"fact\")\n            if fact:\n                lines.append(f\"- {fact}\")\n        elif isinstance(f, str):\n            txt = f.strip()\n            if txt:\n                lines.append(f\"- {txt}\")\n    facts = \"\\n\".join(lines)\n\n    prompt = (\n        f\"Based on these facts:\\n{facts}\\n\\n\"\n        f\"Question: {item.question}\\n\"\n        f\"Format constraint: {item.constrained_format}\\n\"\n        f\"{FORMAT_INSTRUCTION}\"\n    )\n    try:\n        resp = agent.chat(prompt, model=\"o4-mini\")\n        pred = extract_boxed_content(resp)\n        if not pred:\n            printer.print(\"验证失败：未返回 boxed 内容\", style=\"bold red\")\n            return False\n        # 规范化比较\n        norm_pred = \"\".join(c for c in pred.lower() if c.isalnum() or c.isspace()).replace(\" \", \"\")\n        norm_true = \"\".join(c for c in item.answer.lower() if c.isalnum() or c.isspace()).replace(\" \", \"\")\n        if norm_pred != norm_true:\n            printer.print(f\"验证失败：答案不匹配 (pred={pred}  true={item.answer})\", style=\"bold red\")\n            return False\n        printer.print(\"验证通过：格式及答案均正确\", style=\"bold green\")\n        return True\n    except Exception as e:\n        printer.print(f\"验证过程出错：{e}\", style=\"bold red\")\n        logger.exception(\"verify_fuzzy_item error\")\n        return False\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_combination_workflow"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_boxed_content",
        "src/criticsearch/abstract_substitution/abs_workflow.py/FuzzyQAItem"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        true
      ]
    },
    {
      "type": "ClassDef",
      "name": "ReverseUpgradeWorkflow",
      "md_content": [
        "**ReverseUpgradeWorkflow**: The function of ReverseUpgradeWorkflow is to manage a multi-level workflow for generating and verifying question-answer pairs through a systematic process of query updates and validations.\n\n**attributes**: The attributes of this Class.\n· max_level: An integer representing the maximum number of levels for question upgrades, defaulting to 5.\n· max_tries: An integer indicating the maximum number of attempts for each level upgrade, defaulting to 5.\n· items: A list that stores the generated QAItem instances throughout the workflow.\n\n**Code Description**: The ReverseUpgradeWorkflow class is designed to facilitate a structured approach to generating and verifying question-answer pairs. It initializes with parameters for max_level and max_tries, which dictate how deep the workflow can go and how many attempts can be made at each level. The class contains several methods that work together to achieve its goals.\n\nThe primary methods include:\n- `__init__`: Initializes the workflow with specified maximum levels and tries, and prepares an empty list for storing QAItems.\n- `random_domain`: A static method that randomly selects a domain from a predefined list, which is used for generating seed questions.\n- `method_choice`: This method selects a method for question abstraction based on the provided question and answer, utilizing an external agent to assist in the decision-making process.\n- `multi_verify`: Conducts multiple verification checks on a given QAItem to determine if the model can answer the question directly or through search results.\n- `query_update`: Updates a QAItem based on the chosen method and queries, generating new questions and evidence through interactions with an external agent.\n- `generate_seed`: Generates a seed QAItem by querying for relevant information and processing the results.\n- `run`: The main execution method that orchestrates the workflow, generating seeds, upgrading questions through multiple levels, and verifying results at each stage.\n- `save`: Saves the generated QAItems to a specified file path in JSON format.\n- `_safe_json`: A static method for safely parsing JSON from text, ensuring that errors do not disrupt the workflow.\n\nThe class is called within the main execution flow of the application, particularly in the `main` function and the `test_combination_workflow` function. In `main`, it is instantiated to run either a standard or GPT-Search workflow based on user input. The `test_combination_workflow` function utilizes the ReverseUpgradeWorkflow to generate a seed QAItem and subsequently processes it through entity extraction and fuzzy replacement, demonstrating its capability to integrate with other components of the system.\n\n**Note**: It is important to ensure that the parameters for max_level and max_tries are set appropriately to balance the depth of the workflow with the computational resources available. Additionally, proper error handling is crucial during the execution of the workflow to manage any exceptions that may arise from external agent interactions.\n\n**Output Example**: A possible output from the workflow could be a JSON representation of a QAItem, such as:\n```json\n{\n    \"level\": 1,\n    \"question\": \"What are the main causes of climate change?\",\n    \"answer\": \"The main causes include greenhouse gas emissions, deforestation, and industrial processes.\",\n    \"parent_question\": \"What is climate change?\",\n    \"evidence\": [\"Scientific studies\", \"Government reports\"],\n    \"strategy\": \"equivalent replacement\"\n}\n```"
      ],
      "code_start_line": 265,
      "code_end_line": 664,
      "params": [],
      "have_return": true,
      "code_content": "class ReverseUpgradeWorkflow:\n    def __init__(self, *, max_level: int = 5, max_tries: int = 5):\n        self.max_level = max_level\n        self.max_tries = max_tries\n        self.items: List[QAItem] = []\n\n    @staticmethod\n    def random_domain():\n        domains = [\n            \"TV shows & movies\",\n            \"Other\",\n            \"Science & technology\",\n            \"Art\",\n            \"History\",\n            \"Sports\",\n            \"Music\",\n            \"Video games\",\n            \"Geography\",\n            \"Politics\",\n        ]\n        return random.choice(domains)\n\n    def method_choice(self, question: str, answer: str) -> tuple[str, str]: # Restored original return type hint assumption\n        methods = [\n            \"equivalent replacement\",\n            \"simple abstraction\",\n        ]\n        model_choice = agent.chat_with_template(\n            template_name=\"abs_method_choice.txt\",\n            template_data={\"question\": question, \"answer\": answer},\n            root_folder=PROMPT_ROOT_FOLDER,\n        )\n        method = extract_tag_content(model_choice, \"method\")\n        # Assuming queries is a string in the original logic based on return hint\n        queries = extract_tag_content(model_choice, \"queries\")\n\n        # Keeping original logic for method selection and default\n        norm = method.strip().lower().replace(\" \", \"\")\n        for m in methods:\n            if norm == m.lower().replace(\" \", \"\"):\n                return m, queries\n\n        # Original code defaulted to methods[0] without warning\n        return methods[0], queries\n\n    async def multi_verify(self, seed: QAItem) -> bool:\n        \"\"\"\n        对给定的 QAItem 进行多重校验。\n        Returns:\n            bool: True 表示验证通过（模型无法回答），False 表示验证失败（模型能回答）\n        \"\"\"\n        prompt = (\n            \"You need to answer the question\\n\"\n            \"if you cannot answer, please generate \\\\boxed{Sorry, I don't know.}\\n\"\n            f\"Question: {seed.question}\\n\"\n            f\"{FORMAT_INSTRUCTION}\"\n        )\n\n        # 第一步：直接校验模型内部知识是否能回答\n        max_retries = 5 # Keeping original retry count\n        for attempt in range(1, max_retries + 1):\n            # Keeping original direct call to agent.chat\n            direct_answer = agent.chat(prompt, model = \"gpt-4o\")\n            boxed_answer = extract_boxed_content(direct_answer)\n            if not boxed_answer:\n                printer.rule(f\"格式错误重试 #{attempt}\")\n                printer.print(\"模型原始返回：\", style=\"bold red\")\n                printer.print(direct_answer, style=\"bold red\")\n                if attempt == max_retries:\n                    # Keeping original RuntimeError\n                    raise RuntimeError(f\"连续 {max_retries} 次格式错误（答案未包含在 \\\\boxed{{}} 中），终止执行\")\n                continue # Original code just continued\n\n            # Keeping original direct string comparison\n            if boxed_answer == seed.answer:\n                printer.print(\"验证失败：模型能直接回答\", style=\"bold red\")\n                return False\n            break # Original code broke here if format was ok and answer didn't match\n\n        # 第二步：如果模型不能直接回答，进行搜索\n        # Keeping original search call logic\n        search_results = await tavily_search(seed.question) # Original didn't specify include_raw_content=False\n\n        # Keeping original second verification loop\n        for attempt in range(1, max_retries + 1):\n            # Keeping original construction of prompt with search results\n            search_based_answer = agent.chat(prompt + f\"here are some search results:\\n\\n{search_results}\", model = \"gpt-4o\")\n            boxed_answer = extract_boxed_content(search_based_answer)\n            if not boxed_answer:\n                printer.rule(f\"搜索验证格式错误重试 #{attempt}\")\n                printer.print(\"模型原始返回：\", style=\"bold red\")\n                printer.print(search_based_answer, style=\"bold red\")\n                if attempt == max_retries:\n                     # Keeping original RuntimeError\n                    raise RuntimeError(f\"连续 {max_retries} 次格式错误（基于搜索的答案未包含在 \\\\boxed{{}} 中），终止执行\")\n                continue\n\n            # Keeping original direct string comparison\n            if boxed_answer == seed.answer:\n                printer.print(\"验证失败：模型能通过搜索回答\", style=\"bold red\")\n                return False\n            break # Original code broke here\n\n        printer.print(\"验证通过：模型无法直接回答也无法通过搜索回答\", style=\"bold green\")\n        return True\n\n    async def query_update(self, method: str, queries: str, seed: Optional[QAItem]) -> QAItem: # Assuming queries is string based on method_choice\n        if not seed:\n             # Adding this check as it's logically necessary but was missing\n             raise ValueError(\"Cannot update query without a seed QAItem\")\n\n        # Assuming queries is a newline-separated string or similar based on original context\n        # Splitting queries string into a list for tavily_search\n        query_list = [q.strip() for q in queries.split('\\n') if q.strip()]\n        if not query_list: # Handle empty queries string\n            printer.print(\"Warning: No valid queries found in queries string. Using original question as query.\", style=\"bold yellow\")\n            query_list = [seed.question]\n\n\n        search_results = await asyncio.gather(*(tavily_search(q) for q in query_list))\n        question_update_resp = agent.chat_with_template(\n            template_name=\"abs_query_update.txt\",\n            template_data={\"method\": method, \"search_results\": search_results, \"question\": seed.question, \"answer\": seed.answer},\n            root_folder=PROMPT_ROOT_FOLDER,\n        )\n        # 提取出update之后的question\n        # Keeping original extraction logic\n        updated_item = extract_and_validate_json(extract_tag_content(question_update_resp, \"data\"))\n        updated_question = updated_item[\"updated_question\"].strip()\n        updated_evidence = updated_item.get(\"updated_evidence\", [])\n\n        printer.rule(\"Query Update Output\")\n        printer.print(pretty_json(updated_item), style=\"bold green\")\n        printer.rule(\"Query Update Evidence\")\n        printer.print(pretty_json(updated_evidence), style=\"bold green\")\n\n        # Keeping original evidence combination logic\n        return QAItem(\n            level=seed.level + 1,\n            question=updated_question,\n            answer=seed.answer,\n            parent_question=seed.question,\n            evidence=seed.evidence + updated_evidence,\n            strategy=method,\n        )\n\n    async def generate_seed(self) -> QAItem:\n        max_query_retries = 3\n        queries = [] # Initialize queries here\n        random_domain = self.random_domain() # Moved up to be available for logging if needed\n        for attempt in range(1, max_query_retries + 1):\n            # Keeping original logic for generating queries\n            query_resp = agent.chat_with_template(\n                template_name=\"search_query_for_seed_fact.txt\",\n                template_data={\"domain\": random_domain},\n                root_folder=PROMPT_ROOT_FOLDER,\n            )\n            queries = extract_queries_from_response(query_resp)[:3]\n            if queries:\n                printer.rule(f\"Extracted Queries for Seed Fact in {random_domain} Domain\")\n                printer.print(queries, style=\"bold cyan\")\n                break\n            printer.print(f\"第 {attempt} 次抽取到空 queries，重试中…\", style=\"bold red\")\n            printer.print(query_resp, style=\"bold red\")\n            # Add a small delay if retrying (optional but good practice)\n            # await asyncio.sleep(0.5)\n        else:\n            # Keeping original RuntimeError\n            raise RuntimeError(\"连续 3 次 Extracted Queries 为空，终止执行\")\n\n        # Keeping original search and seed fact generation logic\n        search_results = await asyncio.gather(*(tavily_search(q) for q in queries))\n\n        seed_fact_resp = extract_and_validate_json(agent.chat_with_template(\n            template_name=\"seed_idea_from_internet.txt\",\n            template_data={\"domain\": random_domain,\"search_results\": search_results,},\n            root_folder=PROMPT_ROOT_FOLDER\n        ))\n        printer.rule(f\"Generate Seed Fact Output after Browse in {random_domain}\")\n        printer.print(pretty_json(seed_fact_resp), style=\"bold green\")\n        # Keeping original QAItem creation\n        return QAItem(\n            level=0,\n            question=seed_fact_resp[\"seed\"][\"question\"].strip(),\n            answer=seed_fact_resp[\"seed\"][\"answer\"].strip(),\n            parent_question=None,\n            evidence=seed_fact_resp[\"seed\"].get(\"evidence\", []),\n            strategy=\"seed\",\n        )\n\n    async def run(self):\n        printer.rule(f\"Workflow Start with {GPT_MODEL}\")\n        # Seed 生成\n        try:\n            seed = await self.generate_seed()\n            self.items.append(seed)\n            current = seed\n        except Exception as e:\n            # Keeping original exception logging and return\n            logger.exception(\"Error in generate_seed\")\n            printer.print(f\"Error in generate_seed: {e}\", style=\"bold red\")\n            return\n\n        # 按照 max_level 和 max_tries 进行多级问题升级\n        for level in range(self.max_level):\n            retries = 0\n            # Using a flag to break outer loop, similar logic to original 'else' on inner loop\n            level_update_successful = False\n            while retries < self.max_tries:\n                retries += 1\n                printer.rule(f\"Level {level+1} Update Attempt {retries}\")\n                try:\n                    # Keeping original method choice and update logic\n                    method, queries = self.method_choice(current.question, current.answer)\n                    # Keeping original random method override logic\n                    method = random.choice([method, \"simple abstraction\"])\n                    updated = await self.query_update(method, queries, current)\n                    passed = await self.multi_verify(updated)\n                except Exception as e:\n                    # Keeping original exception handling for the update process\n                    logger.exception(f\"Error in level {level+1} attempt {retries}\")\n                    printer.print(f\"Error in update process: {e}\", style=\"bold red\")\n                    continue # Continue to next retry\n\n                # Keeping original verification check logic\n                if not passed:\n                    printer.print(\"多重校验未通过（模型能回答），重试 query_update …\", style=\"bold red\")\n                    continue # Continue to next retry\n\n                # Keeping original success logic\n                printer.print(\"多重校验通过（模型无法回答），记录更新后的 QAItem\", style=\"bold green\")\n                self.items.append(updated)\n                current = updated\n                level_update_successful = True # Mark level as successful\n                break # Break retry loop\n\n            # If after all retries the level wasn't updated successfully\n            if not level_update_successful:\n                printer.print(f\"Stopped at level {current.level}; no valid update after {self.max_tries} tries.\", style=\"bold red\")\n                return # Stop the entire workflow run\n\n        printer.rule(\"Workflow End\")\n\n    async def entity_decompose_run(self):\n        \n        pass\n\n    def save(self, path: Path):\n        # Keeping original save logic exactly\n        printer.rule(\"Saving Results\")\n        with file_lock:\n            existing = []\n            if path.exists():\n                 # Check for empty file before loading JSON\n                if path.stat().st_size > 0:\n                    try:\n                        with open(path, \"r\", encoding=\"utf-8\") as f:\n                            existing = json.load(f)\n                            # Add check if loaded data is actually a list\n                            if not isinstance(existing, list):\n                                logger.warning(f\"Existing file {path} did not contain a list. Overwriting.\")\n                                existing = []\n                    except json.JSONDecodeError:\n                        logger.error(f\"Could not decode JSON from existing file {path}. Overwriting.\")\n                        existing = []\n                    except Exception as e:\n                        logger.exception(f\"Error reading existing file {path}. Overwriting.\")\n                        existing = []\n                else:\n                    logger.info(f\"Existing file {path} is empty. Starting fresh list.\")\n\n            existing.extend([it.to_dict() for it in self.items])\n            try:\n                with open(path, \"w\", encoding=\"utf-8\") as f:\n                    json.dump(existing, f, ensure_ascii=False, indent=2)\n                # Original print statement:\n                printer.print(f\"Saved {len(self.items)} items to {path}\", style=\"bold green\")\n                printer.rule(\"Saved JSON Preview\")\n                printer.print(pretty_json([it.to_dict() for it in self.items]), style=\"bold cyan\")\n            except Exception as e:\n                 logger.exception(f\"Error writing JSON to {path}\")\n                 printer.print(f\"Error saving results to {path}: {e}\", style=\"bold red\")\n\n\n    # Keeping original _safe_json method (even if unused)\n    @staticmethod\n    def _safe_json(text: str) -> dict:\n        m = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)```\", text)\n        raw = m.group(1) if m else text\n        raw = raw.strip(\"`\\n \")\n        try:\n            return json.loads(raw)\n        except json.JSONDecodeError:\n             # Original didn't log error here, just returned {}\n             return {} # Return empty dict on failure\n\n\n    # Keeping original GPT-Search methods exactly\n    # 恢复 GPT-Search 专用的 Seed 生成\n    async def gpt_search_generate_seed(self, domain) -> QAItem:\n        \"\"\"Stub for GPT Search seed generation.\"\"\"\n        # TODO: 实现 GPT Search 专用的 seed 生成逻辑\n        resp = agent.chat_with_template(\n            template_name=\"gpt_search_seed.txt\",\n            root_folder=PROMPT_ROOT_FOLDER,\n            template_data={\"domain\": domain},\n        )\n        printer.rule(\"GPT-Search Seed Generation Output\")\n        printer.print(resp, style=\"bold green\")\n\n        json_resp = extract_and_validate_json(resp)\n        if not json_resp:\n            raise RuntimeError(\"GPT-Search seed generation failed\")\n        return QAItem(\n            level=0,\n            question=json_resp[\"seed\"][\"question\"].strip(),\n            answer=json_resp[\"seed\"][\"answer\"].strip(),\n            constrained_format=json_resp[\"seed\"].get(\"constrained_format\", \"\"),\n            parent_question=None,\n            evidence=json_resp[\"seed\"].get(\"evidence\", []),\n            strategy=\"seed\",\n        )\n\n    # 恢复 GPT-Search 专用的 Query 更新\n    async def gpt_search_query_update(self, seed: QAItem) -> QAItem:\n        \"\"\"Stub for GPT Search query update.\"\"\"\n        # TODO: 实现 GPT Search 专用的 query update 逻辑\n        resp = agent.chat_with_template(\n            template_name=\"gpt_search_Q_update.txt\",\n            template_data={\"question\": seed.question, \"answer\": seed.answer},\n            root_folder=PROMPT_ROOT_FOLDER,\n        )\n        json_resp = extract_and_validate_json(extract_tag_content(resp, \"data\"))\n        if not json_resp:\n            raise RuntimeError(\"GPT-Search query update failed\")\n        updated_question = json_resp[\"updated_question\"].strip()\n        updated_evidence = json_resp.get(\"updated_evidence\", [])\n        method = json_resp.get(\"method\", \"None\")\n        printer.rule(\"GPT-Search Query Update Output\")\n        printer.print(pretty_json(json_resp), style=\"bold green\")\n        printer.rule(\"GPT-Search Query Update Evidence\")\n        printer.print(pretty_json(updated_evidence), style=\"bold green\")\n        return QAItem(\n            level=seed.level + 1,\n            question=updated_question,\n            answer=seed.answer,\n            constrained_format=seed.constrained_format,\n            parent_question=seed.question,\n            evidence=seed.evidence + updated_evidence,\n            strategy=method,\n        )\n\n    # GPT-Search 主流程入口\n    async def gpt_search_run(self):\n        GPT_MODEL = \"gpt-4o-search-preview\" # Original code set this here\n        printer.rule(f\"GPT-Search Workflow Start with {GPT_MODEL}\")\n        # Seed 生成\n        try:\n            seed = await self.gpt_search_generate_seed(self.random_domain())\n            self.items.append(seed)\n            current = seed\n        except Exception as e:\n            logger.exception(\"Error in gpt_search_generate_seed\")\n            printer.print(f\"Error in gpt_search_generate_seed: {e}\", style=\"bold red\")\n            return\n\n        # 多级升级\n        for level in range(self.max_level):\n            retries = 0\n            # Using a flag similar to run() method for consistency with original logic flow\n            level_update_successful_gs = False\n            while retries < self.max_tries:\n                retries += 1\n                printer.rule(f\"GPT-Search Level {level+1} Update Attempt {retries}\")\n                try:\n                    updated = await self.gpt_search_query_update(current)\n                    passed = await self.multi_verify(updated)\n                except Exception as e:\n                    logger.exception(f\"Error in GPT-Search level {level+1} attempt {retries}\")\n                    printer.print(f\"Error in GPT-Search update: {e}\", style=\"bold red\")\n                    continue\n\n                if not passed:\n                    printer.print(\"GPT-Search: 多重校验未通过（模型能回答），重试 query_update …\", style=\"bold red\")\n                    continue\n\n                printer.print(\"GPT-Search: 多重校验通过（模型无法回答），记录更新后的 QAItem\", style=\"bold green\")\n                self.items.append(updated)\n                current = updated\n                level_update_successful_gs = True\n                break # Break retry loop\n\n            if not level_update_successful_gs:\n                printer.print(\n                    f\"GPT-Search stopped at level {current.level}; no valid update after {self.max_tries} tries.\",\n                    style=\"bold red\",\n                )\n                return\n\n        printer.rule(\"GPT-Search Workflow End\")\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_combination_workflow",
        "src/criticsearch/abstract_substitution/abs_workflow.py/main",
        "src/criticsearch/abstract_substitution/abs_workflow.py/main/_batch/run_one"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the ReverseUpgradeWorkflow class with specific configuration settings.\n\n**parameters**: The parameters of this Function.\n· max_level: An integer that determines the maximum level of depth for the workflow. Default value is 5.\n· max_tries: An integer that sets the maximum number of attempts the workflow will make. Default value is 5.\n\n**Code Description**:  \nThe __init__ method is a constructor for the ReverseUpgradeWorkflow class. It is responsible for initializing the object with specific attributes that define the workflow's behavior. The method accepts two optional parameters: `max_level` and `max_tries`. These parameters are used to control the depth and the number of attempts allowed within the workflow, respectively. If these parameters are not provided during object instantiation, the default values of 5 for both parameters are used.\n\nIn addition to these two parameters, the constructor also initializes an empty list `items` that is typed to hold instances of the `QAItem` class. This list is intended to store multiple question-answer pairs that will be processed within the ReverseUpgradeWorkflow. The `QAItem` class, which is referenced in the project structure, represents a single question-answer pair and includes attributes such as level, question, answer, and supporting evidence. The `items` list will be populated with `QAItem` instances as part of the workflow's processing logic.\n\nFrom a functional perspective, the __init__ method lays the foundation for the workflow's execution by ensuring that key parameters (max_level, max_tries) are set, and an empty list for storing `QAItem` objects is ready for use. Other methods in the ReverseUpgradeWorkflow class will later manipulate this list and interact with the `QAItem` instances it contains.\n\n**Note**:  \nIt is important to be mindful of the default values for the parameters `max_level` and `max_tries`. These defaults are set to 5, but they can be adjusted when creating an instance of the ReverseUpgradeWorkflow to suit specific use cases that require more or fewer levels or attempts. The `items` list is an essential part of the workflow, and its manipulation occurs throughout the class's lifecycle. Understanding how to interact with the `QAItem` instances within `items` is crucial for effectively utilizing this workflow."
      ],
      "code_start_line": 266,
      "code_end_line": 269,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, *, max_level: int = 5, max_tries: int = 5):\n        self.max_level = max_level\n        self.max_tries = max_tries\n        self.items: List[QAItem] = []\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/QAItem"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "random_domain",
      "md_content": [
        "**random_domain**: The function of random_domain is to randomly select and return a domain from a predefined list of categories.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The random_domain function is designed to return a random domain from a specified list of categories, which includes various fields such as \"TV shows & movies,\" \"Science & technology,\" \"Art,\" and others. The function utilizes the random.choice method to select one domain from the list, ensuring that each call to the function can yield a different result, thereby introducing variability in the domain selection process.\n\nThis function is called by several other methods within the ReverseUpgradeWorkflow class, specifically in the generate_seed, gpt_search_run, and test_combination_workflow methods. In generate_seed, the random_domain function is invoked to provide a domain that is used in generating search queries for seed facts. This integration allows the workflow to adaptively create queries based on different domains, enhancing the diversity of the generated content.\n\nIn gpt_search_run, random_domain is used to generate a seed for the GPT search process, ensuring that the search is contextually relevant to a randomly selected domain. This contributes to the overall robustness of the search workflow by allowing it to explore various topics.\n\nSimilarly, in test_combination_workflow, random_domain is utilized to generate a seed question, which is then processed through a series of steps involving entity extraction and fuzzy replacement. This demonstrates the function's role in facilitating the testing of workflows by providing varied input scenarios.\n\n**Note**: It is important to ensure that the random module is imported in the context where this function is used, as the function relies on it to perform the random selection.\n\n**Output Example**: A possible return value of the random_domain function could be \"Music,\" indicating that the function has randomly selected this domain from the predefined list."
      ],
      "code_start_line": 272,
      "code_end_line": 285,
      "params": [],
      "have_return": true,
      "code_content": "    def random_domain():\n        domains = [\n            \"TV shows & movies\",\n            \"Other\",\n            \"Science & technology\",\n            \"Art\",\n            \"History\",\n            \"Sports\",\n            \"Music\",\n            \"Video games\",\n            \"Geography\",\n            \"Politics\",\n        ]\n        return random.choice(domains)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_run",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_combination_workflow"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "method_choice",
      "md_content": [
        "**method_choice**: The function of method_choice is to select the appropriate method for processing a given question and answer pair based on a conversation model's response.\n\n**parameters**: The parameters of this Function.\n· question: A string representing the question in a question-answer pair.  \n· answer: A string representing the answer in a question-answer pair.\n\n**Code Description**: The method_choice function is responsible for determining the most suitable method for addressing a given question-answer pair, by interacting with a conversational model. \n\nThe function begins by defining two potential methods for handling the question-answer pair: \"equivalent replacement\" and \"simple abstraction\". These methods are stored in the `methods` list. The core of the function's decision-making process relies on an interaction with the conversational model, which is performed through the `chat_with_template` method. This method sends the question and answer to the model by rendering them within a predefined template (abs_method_choice.txt), and receives a response that suggests the method and queries to be used. The response is extracted using the `extract_tag_content` function to pull out the `method` and `queries` values from the model's output.\n\nOnce the model’s output is received, the `method_choice` function compares the returned method (stripped of whitespace and converted to lowercase) with the possible methods in the predefined `methods` list. If a match is found, the function returns the matched method along with the associated queries. If no match is found, it defaults to the first method in the list, which is \"equivalent replacement\", and returns the corresponding queries.\n\nThis function plays a key role in workflows like the one in `run` (from the ReverseUpgradeWorkflow class), where it helps to dynamically determine the best method for updating a given question-answer pair during a multi-step upgrade process. By interacting with the conversational model to receive real-time advice on method selection, it ensures that the workflow adapts to the specific context of each question-answer pair.\n\n**Note**: The `chat_with_template` method used here requires the model to be properly configured and the template file (abs_method_choice.txt) to be present in the correct directory. If the template does not match expected inputs or the model fails to return a suitable method, the function defaults to the first method in the `methods` list. Additionally, the `extract_tag_content` function ensures the content is retrieved even from multiline or case-insensitive responses, but it will return an empty string if the expected tags are missing from the model's output.\n\n**Output Example**:  \nA possible output from the `method_choice` function could be:  \n```\n(\"simple abstraction\", \"query1, query2\")\n```"
      ],
      "code_start_line": 287,
      "code_end_line": 308,
      "params": [
        "self",
        "question",
        "answer"
      ],
      "have_return": true,
      "code_content": "    def method_choice(self, question: str, answer: str) -> tuple[str, str]: # Restored original return type hint assumption\n        methods = [\n            \"equivalent replacement\",\n            \"simple abstraction\",\n        ]\n        model_choice = agent.chat_with_template(\n            template_name=\"abs_method_choice.txt\",\n            template_data={\"question\": question, \"answer\": answer},\n            root_folder=PROMPT_ROOT_FOLDER,\n        )\n        method = extract_tag_content(model_choice, \"method\")\n        # Assuming queries is a string in the original logic based on return hint\n        queries = extract_tag_content(model_choice, \"queries\")\n\n        # Keeping original logic for method selection and default\n        norm = method.strip().lower().replace(\" \", \"\")\n        for m in methods:\n            if norm == m.lower().replace(\" \", \"\"):\n                return m, queries\n\n        # Original code defaulted to methods[0] without warning\n        return methods[0], queries\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/utils.py/extract_tag_content"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "multi_verify",
      "md_content": [
        "**multi_verify**: The function of multi_verify is to perform multiple verification checks on a given QAItem to determine if a model can answer the associated question.\n\n**parameters**: The parameters of this Function.\n· seed: QAItem - An instance of the QAItem class that contains the question and expected answer to be verified.\n\n**Code Description**: The multi_verify function is an asynchronous method designed to validate whether a conversational model can answer a specific question encapsulated within a QAItem instance. The function begins by constructing a prompt that instructs the model to respond to the question. The prompt also includes specific formatting instructions, indicating that if the model cannot answer, it should generate a response enclosed in LaTeX-style boxed content.\n\nThe verification process consists of two main steps:\n\n1. **Direct Model Verification**: The function first attempts to verify the model's ability to answer the question directly. It does this by calling the agent's chat method with the constructed prompt. The function allows for a maximum of five retries to handle potential formatting errors in the model's response. If the response does not contain the expected boxed content, the function logs the error and retries. If the boxed answer matches the expected answer from the QAItem, the function concludes that the verification has failed, indicating that the model can indeed answer the question.\n\n2. **Search-Based Verification**: If the model cannot answer directly, the function proceeds to perform a search using the tavily_search function, which queries an external API for relevant information. The search results are then incorporated into a new prompt, and the model is asked again if it can provide an answer based on this additional context. Similar to the first step, this process also allows for retries and checks for the boxed content in the model's response. If the model can answer based on the search results, the verification fails again.\n\nIf both verification steps conclude that the model cannot provide an answer, the function returns True, indicating that the verification was successful.\n\nThe multi_verify function is invoked within the run and gpt_search_run methods of the ReverseUpgradeWorkflow class. In these contexts, it is used to validate the updated QAItem instances generated during the workflow, ensuring that the model's responses are appropriately assessed against the expected outcomes.\n\n**Note**: It is essential to ensure that the QAItem passed to the multi_verify function is correctly populated with the question and expected answer. The function relies on the proper formatting of the model's responses, and any deviations may lead to repeated retries or runtime errors.\n\n**Output Example**: A possible return value from the multi_verify function could be:\n```python\nTrue  # Indicates that the model could not answer the question\n```"
      ],
      "code_start_line": 310,
      "code_end_line": 369,
      "params": [
        "self",
        "seed"
      ],
      "have_return": true,
      "code_content": "    async def multi_verify(self, seed: QAItem) -> bool:\n        \"\"\"\n        对给定的 QAItem 进行多重校验。\n        Returns:\n            bool: True 表示验证通过（模型无法回答），False 表示验证失败（模型能回答）\n        \"\"\"\n        prompt = (\n            \"You need to answer the question\\n\"\n            \"if you cannot answer, please generate \\\\boxed{Sorry, I don't know.}\\n\"\n            f\"Question: {seed.question}\\n\"\n            f\"{FORMAT_INSTRUCTION}\"\n        )\n\n        # 第一步：直接校验模型内部知识是否能回答\n        max_retries = 5 # Keeping original retry count\n        for attempt in range(1, max_retries + 1):\n            # Keeping original direct call to agent.chat\n            direct_answer = agent.chat(prompt, model = \"gpt-4o\")\n            boxed_answer = extract_boxed_content(direct_answer)\n            if not boxed_answer:\n                printer.rule(f\"格式错误重试 #{attempt}\")\n                printer.print(\"模型原始返回：\", style=\"bold red\")\n                printer.print(direct_answer, style=\"bold red\")\n                if attempt == max_retries:\n                    # Keeping original RuntimeError\n                    raise RuntimeError(f\"连续 {max_retries} 次格式错误（答案未包含在 \\\\boxed{{}} 中），终止执行\")\n                continue # Original code just continued\n\n            # Keeping original direct string comparison\n            if boxed_answer == seed.answer:\n                printer.print(\"验证失败：模型能直接回答\", style=\"bold red\")\n                return False\n            break # Original code broke here if format was ok and answer didn't match\n\n        # 第二步：如果模型不能直接回答，进行搜索\n        # Keeping original search call logic\n        search_results = await tavily_search(seed.question) # Original didn't specify include_raw_content=False\n\n        # Keeping original second verification loop\n        for attempt in range(1, max_retries + 1):\n            # Keeping original construction of prompt with search results\n            search_based_answer = agent.chat(prompt + f\"here are some search results:\\n\\n{search_results}\", model = \"gpt-4o\")\n            boxed_answer = extract_boxed_content(search_based_answer)\n            if not boxed_answer:\n                printer.rule(f\"搜索验证格式错误重试 #{attempt}\")\n                printer.print(\"模型原始返回：\", style=\"bold red\")\n                printer.print(search_based_answer, style=\"bold red\")\n                if attempt == max_retries:\n                     # Keeping original RuntimeError\n                    raise RuntimeError(f\"连续 {max_retries} 次格式错误（基于搜索的答案未包含在 \\\\boxed{{}} 中），终止执行\")\n                continue\n\n            # Keeping original direct string comparison\n            if boxed_answer == seed.answer:\n                printer.print(\"验证失败：模型能通过搜索回答\", style=\"bold red\")\n                return False\n            break # Original code broke here\n\n        printer.print(\"验证通过：模型无法直接回答也无法通过搜索回答\", style=\"bold green\")\n        return True\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/run",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_boxed_content",
        "src/criticsearch/abstract_substitution/abs_workflow.py/tavily_search",
        "src/criticsearch/abstract_substitution/abs_workflow.py/QAItem"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "query_update",
      "md_content": [
        "**query_update**: The function of query_update is to update a QAItem based on new queries and a specified method, utilizing search results to refine the question.\n\n**parameters**: The parameters of this Function.\n· method: A string that specifies the method used for updating the question.  \n· queries: A string containing newline-separated queries to be processed.  \n· seed: An optional QAItem instance that serves as the basis for the update.\n\n**Code Description**: The query_update function is an asynchronous method designed to enhance a given QAItem by updating its question based on new queries and a specified method. The function begins by validating the presence of the seed parameter, raising a ValueError if it is not provided, as it is essential for the update process.\n\nThe queries parameter is expected to be a string, which is split into a list of individual queries based on newline characters. If the resulting list is empty, a warning is printed, and the original question from the seed QAItem is used as the query.\n\nThe function then performs asynchronous searches using the tavily_search function for each query in the query_list. The results of these searches are gathered concurrently using asyncio.gather, which improves efficiency by allowing multiple search requests to be processed simultaneously.\n\nFollowing the search, the function calls the chat_with_template method from the BaseAgent class to generate a response based on the search results and the original question and answer from the seed. This method utilizes a template to format the data appropriately for the conversational model.\n\nThe response from the chat_with_template method is processed to extract the updated question and any new evidence. The extract_and_validate_json function is employed to ensure that the response is valid JSON and to retrieve the necessary fields. The updated question and evidence are then formatted for output.\n\nFinally, the function constructs and returns a new QAItem instance that reflects the updated question, maintaining the original answer and evidence while incorporating any new evidence obtained from the search results. This new QAItem is assigned a level incremented from the seed's level, indicating its position in the hierarchy.\n\nThe query_update function is called within the run method of the ReverseUpgradeWorkflow class. It plays a crucial role in the multi-level question updating process, where it is invoked to refine questions based on search results and user feedback. The successful execution of query_update is essential for the workflow to progress through its defined levels, as it directly influences the quality and relevance of the questions being processed.\n\n**Note**: It is important to ensure that the queries string is well-formed and contains valid queries to avoid generating empty search results. Additionally, the seed QAItem must be provided to facilitate the update process.\n\n**Output Example**: A possible return value from the query_update function could be a QAItem instance structured as follows:\n{\n  \"level\": 2,\n  \"question\": \"What is the capital of France?\",\n  \"answer\": \"Paris\",\n  \"parent_question\": \"What is the capital?\",\n  \"evidence\": [\"Paris is the capital of France.\", \"Source A\"],\n  \"strategy\": \"search-based update\"\n}"
      ],
      "code_start_line": 371,
      "code_end_line": 409,
      "params": [
        "self",
        "method",
        "queries",
        "seed"
      ],
      "have_return": true,
      "code_content": "    async def query_update(self, method: str, queries: str, seed: Optional[QAItem]) -> QAItem: # Assuming queries is string based on method_choice\n        if not seed:\n             # Adding this check as it's logically necessary but was missing\n             raise ValueError(\"Cannot update query without a seed QAItem\")\n\n        # Assuming queries is a newline-separated string or similar based on original context\n        # Splitting queries string into a list for tavily_search\n        query_list = [q.strip() for q in queries.split('\\n') if q.strip()]\n        if not query_list: # Handle empty queries string\n            printer.print(\"Warning: No valid queries found in queries string. Using original question as query.\", style=\"bold yellow\")\n            query_list = [seed.question]\n\n\n        search_results = await asyncio.gather(*(tavily_search(q) for q in query_list))\n        question_update_resp = agent.chat_with_template(\n            template_name=\"abs_query_update.txt\",\n            template_data={\"method\": method, \"search_results\": search_results, \"question\": seed.question, \"answer\": seed.answer},\n            root_folder=PROMPT_ROOT_FOLDER,\n        )\n        # 提取出update之后的question\n        # Keeping original extraction logic\n        updated_item = extract_and_validate_json(extract_tag_content(question_update_resp, \"data\"))\n        updated_question = updated_item[\"updated_question\"].strip()\n        updated_evidence = updated_item.get(\"updated_evidence\", [])\n\n        printer.rule(\"Query Update Output\")\n        printer.print(pretty_json(updated_item), style=\"bold green\")\n        printer.rule(\"Query Update Evidence\")\n        printer.print(pretty_json(updated_evidence), style=\"bold green\")\n\n        # Keeping original evidence combination logic\n        return QAItem(\n            level=seed.level + 1,\n            question=updated_question,\n            answer=seed.answer,\n            parent_question=seed.question,\n            evidence=seed.evidence + updated_evidence,\n            strategy=method,\n        )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_tag_content",
        "src/criticsearch/utils.py/extract_and_validate_json",
        "src/criticsearch/abstract_substitution/abs_workflow.py/pretty_json",
        "src/criticsearch/abstract_substitution/abs_workflow.py/tavily_search",
        "src/criticsearch/abstract_substitution/abs_workflow.py/QAItem"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "generate_seed",
      "md_content": [
        "**generate_seed**: The function of generate_seed is to asynchronously generate a seed question-answer item based on queries extracted from a conversational model's response.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The generate_seed function is an asynchronous method within the ReverseUpgradeWorkflow class that is responsible for generating a seed question-answer item (QAItem) based on queries derived from a conversational model's response. The function begins by initializing a maximum number of query retries and an empty list for queries. It then calls the random_domain method to select a domain randomly, which is used to generate contextually relevant queries.\n\nThe function enters a loop that attempts to generate queries up to a specified number of retries. During each attempt, it invokes the chat_with_template method from the BaseAgent class, providing a template for generating search queries based on the selected domain. The response from this method is processed using the extract_queries_from_response function to extract up to three queries. If queries are successfully extracted, they are printed to the console for logging purposes, and the loop breaks. If no queries are found after the maximum number of attempts, a RuntimeError is raised, indicating that the extraction failed.\n\nOnce valid queries are obtained, the function proceeds to perform asynchronous searches using the tavily_search function for each query. The results of these searches are gathered and used to generate a seed fact by calling chat_with_template again, this time with a different template designed to create a seed idea from the internet. The response is validated and extracted using the extract_and_validate_json function.\n\nFinally, the function constructs a QAItem instance using the extracted seed question and answer, along with any associated evidence, and returns this instance.\n\nThe generate_seed function is called within the run method of the ReverseUpgradeWorkflow class. It serves as the initial step in the workflow, where a seed QAItem is generated before proceeding to further levels of question upgrades. The successful execution of generate_seed is crucial for the workflow, as it sets the foundation for subsequent operations that rely on the generated seed.\n\n**Note**: It is important to ensure that the templates used in chat_with_template exist and are correctly formatted to avoid rendering errors. Additionally, the random_domain function should be functioning correctly to provide varied domains for query generation.\n\n**Output Example**: A possible return value of the generate_seed function could be a QAItem instance structured as follows:\n{\n  \"level\": 0,\n  \"question\": \"What is the capital of France?\",\n  \"answer\": \"Paris\",\n  \"parent_question\": null,\n  \"evidence\": [\"Source A\", \"Source B\"],\n  \"strategy\": \"seed\"\n}"
      ],
      "code_start_line": 411,
      "code_end_line": 453,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    async def generate_seed(self) -> QAItem:\n        max_query_retries = 3\n        queries = [] # Initialize queries here\n        random_domain = self.random_domain() # Moved up to be available for logging if needed\n        for attempt in range(1, max_query_retries + 1):\n            # Keeping original logic for generating queries\n            query_resp = agent.chat_with_template(\n                template_name=\"search_query_for_seed_fact.txt\",\n                template_data={\"domain\": random_domain},\n                root_folder=PROMPT_ROOT_FOLDER,\n            )\n            queries = extract_queries_from_response(query_resp)[:3]\n            if queries:\n                printer.rule(f\"Extracted Queries for Seed Fact in {random_domain} Domain\")\n                printer.print(queries, style=\"bold cyan\")\n                break\n            printer.print(f\"第 {attempt} 次抽取到空 queries，重试中…\", style=\"bold red\")\n            printer.print(query_resp, style=\"bold red\")\n            # Add a small delay if retrying (optional but good practice)\n            # await asyncio.sleep(0.5)\n        else:\n            # Keeping original RuntimeError\n            raise RuntimeError(\"连续 3 次 Extracted Queries 为空，终止执行\")\n\n        # Keeping original search and seed fact generation logic\n        search_results = await asyncio.gather(*(tavily_search(q) for q in queries))\n\n        seed_fact_resp = extract_and_validate_json(agent.chat_with_template(\n            template_name=\"seed_idea_from_internet.txt\",\n            template_data={\"domain\": random_domain,\"search_results\": search_results,},\n            root_folder=PROMPT_ROOT_FOLDER\n        ))\n        printer.rule(f\"Generate Seed Fact Output after Browse in {random_domain}\")\n        printer.print(pretty_json(seed_fact_resp), style=\"bold green\")\n        # Keeping original QAItem creation\n        return QAItem(\n            level=0,\n            question=seed_fact_resp[\"seed\"][\"question\"].strip(),\n            answer=seed_fact_resp[\"seed\"][\"answer\"].strip(),\n            parent_question=None,\n            evidence=seed_fact_resp[\"seed\"].get(\"evidence\", []),\n            strategy=\"seed\",\n        )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_queries_from_response",
        "src/criticsearch/utils.py/extract_and_validate_json",
        "src/criticsearch/abstract_substitution/abs_workflow.py/pretty_json",
        "src/criticsearch/abstract_substitution/abs_workflow.py/tavily_search",
        "src/criticsearch/abstract_substitution/abs_workflow.py/QAItem",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/random_domain"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        true,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "run",
      "md_content": [
        "**run**: The function of run is to execute a multi-level workflow for upgrading question-answer items based on a generated seed.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The run function is an asynchronous method within the ReverseUpgradeWorkflow class that orchestrates the entire workflow for upgrading question-answer items (QAItems). It begins by printing a visual separator to indicate the start of the workflow, using the GPT_MODEL variable to specify the model being utilized.\n\nThe function first attempts to generate a seed QAItem by calling the generate_seed method. This seed serves as the initial question-answer pair for the workflow. If the seed generation fails, it logs the error and prints a message to the console, terminating the workflow early.\n\nOnce the seed is successfully generated, the function enters a loop that iterates through a predefined number of levels (max_level). For each level, it attempts to update the current QAItem through a series of retries (max_tries). Within this nested loop, it selects an appropriate method for updating the QAItem by calling the method_choice function, which interacts with a conversational model to determine the best approach based on the current question and answer.\n\nThe chosen method is then used to update the QAItem by invoking the query_update function, which processes the queries and refines the question based on search results. After updating, the function verifies the updated QAItem using the multi_verify function, which checks if the model can answer the new question. If the verification fails, it indicates that the model can answer the question, prompting the workflow to retry the update process.\n\nIf the update is successful and passes verification, the updated QAItem is appended to the items list, and the current QAItem is updated to the newly created one. If the maximum number of retries is reached without a successful update, the function logs a message indicating that the workflow has stopped at the current level.\n\nThe run function is called from the main function, which serves as the entry point for executing the workflow. It can also be invoked in batch runs, allowing for multiple instances of the workflow to be executed concurrently.\n\n**Note**: It is essential to ensure that the generate_seed, method_choice, query_update, and multi_verify functions are functioning correctly, as they are critical components of the workflow. Any issues in these functions may lead to failures in the overall execution of the run method.\n\n**Output Example**: The run function does not return a value but modifies the internal state of the ReverseUpgradeWorkflow instance by appending updated QAItems to the items list. A possible appearance of the items list after execution could be:\n```python\n[\n  {\n    \"level\": 0,\n    \"question\": \"What is the capital of France?\",\n    \"answer\": \"Paris\",\n    \"evidence\": [\"Source A\", \"Source B\"],\n    \"strategy\": \"seed\"\n  },\n  {\n    \"level\": 1,\n    \"question\": \"What is the capital of Germany?\",\n    \"answer\": \"Berlin\",\n    \"evidence\": [\"Source C\"],\n    \"strategy\": \"search-based update\"\n  }\n]\n```"
      ],
      "code_start_line": 455,
      "code_end_line": 506,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    async def run(self):\n        printer.rule(f\"Workflow Start with {GPT_MODEL}\")\n        # Seed 生成\n        try:\n            seed = await self.generate_seed()\n            self.items.append(seed)\n            current = seed\n        except Exception as e:\n            # Keeping original exception logging and return\n            logger.exception(\"Error in generate_seed\")\n            printer.print(f\"Error in generate_seed: {e}\", style=\"bold red\")\n            return\n\n        # 按照 max_level 和 max_tries 进行多级问题升级\n        for level in range(self.max_level):\n            retries = 0\n            # Using a flag to break outer loop, similar logic to original 'else' on inner loop\n            level_update_successful = False\n            while retries < self.max_tries:\n                retries += 1\n                printer.rule(f\"Level {level+1} Update Attempt {retries}\")\n                try:\n                    # Keeping original method choice and update logic\n                    method, queries = self.method_choice(current.question, current.answer)\n                    # Keeping original random method override logic\n                    method = random.choice([method, \"simple abstraction\"])\n                    updated = await self.query_update(method, queries, current)\n                    passed = await self.multi_verify(updated)\n                except Exception as e:\n                    # Keeping original exception handling for the update process\n                    logger.exception(f\"Error in level {level+1} attempt {retries}\")\n                    printer.print(f\"Error in update process: {e}\", style=\"bold red\")\n                    continue # Continue to next retry\n\n                # Keeping original verification check logic\n                if not passed:\n                    printer.print(\"多重校验未通过（模型能回答），重试 query_update …\", style=\"bold red\")\n                    continue # Continue to next retry\n\n                # Keeping original success logic\n                printer.print(\"多重校验通过（模型无法回答），记录更新后的 QAItem\", style=\"bold green\")\n                self.items.append(updated)\n                current = updated\n                level_update_successful = True # Mark level as successful\n                break # Break retry loop\n\n            # If after all retries the level wasn't updated successfully\n            if not level_update_successful:\n                printer.print(f\"Stopped at level {current.level}; no valid update after {self.max_tries} tries.\", style=\"bold red\")\n                return # Stop the entire workflow run\n\n        printer.rule(\"Workflow End\")\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/main",
        "src/criticsearch/abstract_substitution/abs_workflow.py/main/_batch/run_one"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/method_choice",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/query_update",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/generate_seed"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "entity_decompose_run",
      "md_content": [
        "**entity_decompose_run**: The function of entity_decompose_run is to serve as an asynchronous method intended for decomposing entities within a workflow.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The entity_decompose_run function is defined as an asynchronous method, indicated by the `async` keyword. This suggests that the function is designed to perform operations that may involve waiting for external processes or I/O operations without blocking the execution of other code. Currently, the function body contains a `pass` statement, which means that it does not execute any operations or return any values. This indicates that the function is either a placeholder for future implementation or is intended to be overridden in a subclass. As it stands, the function does not interact with any data or perform any computations, making it a non-functional stub in its current state.\n\n**Note**: It is important to recognize that since the function is asynchronous, it should be awaited when called in an asynchronous context to ensure proper execution flow. Additionally, developers should implement the necessary logic within this function to fulfill its intended purpose of entity decomposition when the functionality is defined."
      ],
      "code_start_line": 508,
      "code_end_line": 510,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    async def entity_decompose_run(self):\n        \n        pass\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "save",
      "md_content": [
        "**save**: The function of save is to persist the current state of items in the ReverseUpgradeWorkflow to a specified JSON file.\n\n**parameters**: The parameters of this Function.\n· path: A Path object representing the file path where the results will be saved.\n\n**Code Description**: The save function is responsible for saving the current items of the ReverseUpgradeWorkflow instance to a JSON file at the specified path. The function begins by printing a visual separator titled \"Saving Results\" using the printer's rule method, which enhances the readability of console output.\n\nThe function then acquires a file lock to prevent concurrent write operations. It checks if the specified path already exists. If it does, the function further checks if the file is not empty before attempting to read its contents. If the file is empty, it logs an informational message and initializes an empty list for existing data.\n\nWhen reading the existing file, the function attempts to load its contents as JSON. If the file contains valid JSON data, it checks whether the loaded data is a list. If the data is not a list, a warning is logged, and the existing data is reset to an empty list. In case of a JSON decoding error or any other exception during file reading, an error is logged, and the existing data is also reset.\n\nAfter handling the existing data, the function extends this list with the current items from the workflow, converting each item to a dictionary using the to_dict method. This method is essential for serializing the items into a format suitable for JSON storage.\n\nThe function then attempts to write the combined list of existing and new items back to the specified path in JSON format. It uses the json.dump method with parameters to ensure non-ASCII characters are preserved and the output is indented for readability. Upon successful saving, it prints the number of items saved and provides a preview of the saved data in a formatted JSON style.\n\nIf any exceptions occur during the writing process, an error is logged, and an error message is printed to the console, indicating the failure to save results.\n\nThe save function is called in various contexts, particularly within the main function of the script, where it is invoked after running the workflow. This ensures that the results of the workflow execution are stored persistently. It is also called within the _batch function, which handles multiple concurrent runs of the workflow, ensuring that results from each run are saved appropriately.\n\n**Note**: When using the save function, ensure that the specified path is valid and accessible. Additionally, be aware that the function will overwrite existing files if they contain invalid data or if they are not in the expected format. Proper error handling is implemented to manage potential issues during file reading and writing."
      ],
      "code_start_line": 512,
      "code_end_line": 546,
      "params": [
        "self",
        "path"
      ],
      "have_return": false,
      "code_content": "    def save(self, path: Path):\n        # Keeping original save logic exactly\n        printer.rule(\"Saving Results\")\n        with file_lock:\n            existing = []\n            if path.exists():\n                 # Check for empty file before loading JSON\n                if path.stat().st_size > 0:\n                    try:\n                        with open(path, \"r\", encoding=\"utf-8\") as f:\n                            existing = json.load(f)\n                            # Add check if loaded data is actually a list\n                            if not isinstance(existing, list):\n                                logger.warning(f\"Existing file {path} did not contain a list. Overwriting.\")\n                                existing = []\n                    except json.JSONDecodeError:\n                        logger.error(f\"Could not decode JSON from existing file {path}. Overwriting.\")\n                        existing = []\n                    except Exception as e:\n                        logger.exception(f\"Error reading existing file {path}. Overwriting.\")\n                        existing = []\n                else:\n                    logger.info(f\"Existing file {path} is empty. Starting fresh list.\")\n\n            existing.extend([it.to_dict() for it in self.items])\n            try:\n                with open(path, \"w\", encoding=\"utf-8\") as f:\n                    json.dump(existing, f, ensure_ascii=False, indent=2)\n                # Original print statement:\n                printer.print(f\"Saved {len(self.items)} items to {path}\", style=\"bold green\")\n                printer.rule(\"Saved JSON Preview\")\n                printer.print(pretty_json([it.to_dict() for it in self.items]), style=\"bold cyan\")\n            except Exception as e:\n                 logger.exception(f\"Error writing JSON to {path}\")\n                 printer.print(f\"Error saving results to {path}: {e}\", style=\"bold red\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/main",
        "src/criticsearch/abstract_substitution/abs_workflow.py/main/_batch/run_one"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_workflow.py/pretty_json",
        "src/criticsearch/abstract_substitution/abs_workflow.py/QAItem/to_dict"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_safe_json",
      "md_content": [
        "**_safe_json**: The function of _safe_json is to extract and safely parse a JSON object from a given text input, returning a dictionary.\n\n**parameters**:\n· text: A string that potentially contains a JSON object enclosed in triple backticks, possibly preceded by the label \"json\".\n\n**Code Description**:  \nThe _safe_json function is designed to safely extract and parse a JSON object from a given string input. The function uses a regular expression to detect and isolate content enclosed within triple backticks (```) that may contain a JSON object. Specifically, the regular expression looks for a block that might optionally start with \"json\" and then extracts the content within the backticks. \n\nThe flow of the function is as follows:\n1. The function first attempts to locate a substring that begins with triple backticks and optionally the string \"json\". It does this using the regular expression `r\"```(?:json)?\\s*([\\s\\S]*?)```\"`. This pattern matches the block of text enclosed by triple backticks, whether or not the \"json\" keyword precedes the backticks.\n2. If a match is found, the content within the backticks is extracted. If no match is found, the entire input text is used instead.\n3. The function then strips any leading or trailing backticks, newlines, or spaces from the extracted content using `raw.strip(\"`\\n \")`.\n4. Next, the function attempts to parse the resulting string as JSON using `json.loads()`. If the parsing succeeds, the resulting Python dictionary is returned.\n5. If parsing fails (due to malformed JSON), a `json.JSONDecodeError` is caught, and instead of raising an error, the function returns an empty dictionary `{}`.\n\nThis approach ensures that even if the input contains malformed JSON or does not match the expected structure, the function will not fail but instead return a safe, empty dictionary.\n\n**Note**:  \n- If no valid JSON is found or parsing fails, the function defaults to returning an empty dictionary, which ensures robustness and avoids exceptions that could disrupt program execution.\n- The function assumes that the input string may or may not contain a JSON-formatted block inside triple backticks. This is particularly useful for processing text from sources that may include markdown or code blocks.\n\n**Output Example**:  \nGiven an input text such as:\n```text\nHere is a JSON block:\n```json\n{\n    \"key\": \"value\",\n    \"another_key\": \"another_value\"\n}\n```\nThe output would be:\n```python\n{\n    \"key\": \"value\",\n    \"another_key\": \"another_value\"\n}\n```\n\nFor an invalid JSON or no JSON block, the output would be:\n```python\n{}\n```"
      ],
      "code_start_line": 551,
      "code_end_line": 559,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "    def _safe_json(text: str) -> dict:\n        m = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)```\", text)\n        raw = m.group(1) if m else text\n        raw = raw.strip(\"`\\n \")\n        try:\n            return json.loads(raw)\n        except json.JSONDecodeError:\n             # Original didn't log error here, just returned {}\n             return {} # Return empty dict on failure\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "gpt_search_generate_seed",
      "md_content": [
        "**gpt_search_generate_seed**: The function of gpt_search_generate_seed is to generate a seed question-answer item based on a specified domain using a GPT model.\n\n**parameters**: The parameters of this Function.\n· domain: A string representing the domain for which the seed question and answer are to be generated.\n\n**Code Description**: The gpt_search_generate_seed function is an asynchronous method designed to create a seed question-answer item (QAItem) by interacting with a conversational model through a predefined template. The function begins by calling the agent's chat_with_template method, passing the template name \"gpt_search_seed.txt\", the root folder for the template, and a dictionary containing the domain as template data. This interaction is intended to generate a response that includes a question and answer relevant to the specified domain.\n\nUpon receiving the response, the function utilizes the printer's rule method to create a visual separator in the console output, indicating the start of the seed generation output. The response from the chat_with_template method is then printed in a bold green style for visibility.\n\nNext, the function calls extract_and_validate_json to parse the response and validate its structure. If the response does not contain valid JSON, a RuntimeError is raised, indicating that the seed generation has failed. If the JSON is valid, the function extracts the question, answer, optional constrained format, and evidence from the parsed JSON. It then constructs a QAItem instance with this information, setting the level to 0, indicating that this is a base-level question-answer pair.\n\nThe gpt_search_generate_seed function is called within the gpt_search_run method of the ReverseUpgradeWorkflow class, where it is used to generate the initial seed QAItem before proceeding with further updates and verifications in the workflow. Additionally, it is referenced in the test_combination_workflow function, which tests the integration of entity extraction and fuzzy replacement using the generated seed.\n\n**Note**: When using the gpt_search_generate_seed function, ensure that the domain parameter is well-defined and relevant to the context of the seed generation. Proper handling of the response is crucial to avoid runtime errors related to JSON validation.\n\n**Output Example**: A possible return value from the gpt_search_generate_seed function could be a QAItem instance structured as follows:\n{\n  \"level\": 0,\n  \"question\": \"What are the benefits of using AI in healthcare?\",\n  \"answer\": \"AI can improve diagnostics, personalize treatment, and enhance patient care.\",\n  \"constrained_format\": \"\",\n  \"parent_question\": null,\n  \"evidence\": [\"Source A\", \"Source B\"],\n  \"strategy\": \"seed\"\n}"
      ],
      "code_start_line": 564,
      "code_end_line": 586,
      "params": [
        "self",
        "domain"
      ],
      "have_return": true,
      "code_content": "    async def gpt_search_generate_seed(self, domain) -> QAItem:\n        \"\"\"Stub for GPT Search seed generation.\"\"\"\n        # TODO: 实现 GPT Search 专用的 seed 生成逻辑\n        resp = agent.chat_with_template(\n            template_name=\"gpt_search_seed.txt\",\n            root_folder=PROMPT_ROOT_FOLDER,\n            template_data={\"domain\": domain},\n        )\n        printer.rule(\"GPT-Search Seed Generation Output\")\n        printer.print(resp, style=\"bold green\")\n\n        json_resp = extract_and_validate_json(resp)\n        if not json_resp:\n            raise RuntimeError(\"GPT-Search seed generation failed\")\n        return QAItem(\n            level=0,\n            question=json_resp[\"seed\"][\"question\"].strip(),\n            answer=json_resp[\"seed\"][\"answer\"].strip(),\n            constrained_format=json_resp[\"seed\"].get(\"constrained_format\", \"\"),\n            parent_question=None,\n            evidence=json_resp[\"seed\"].get(\"evidence\", []),\n            strategy=\"seed\",\n        )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_run",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_combination_workflow"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_and_validate_json",
        "src/criticsearch/abstract_substitution/abs_workflow.py/QAItem"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "gpt_search_query_update",
      "md_content": [
        "**gpt_search_query_update**: The function of gpt_search_query_update is to update a search query using GPT by processing a given QAItem and returning an updated QAItem.\n\n**parameters**: The parameters of this Function.\n· seed: QAItem - An instance of the QAItem class that contains the original question, answer, and associated metadata to be processed for the query update.\n\n**Code Description**: The gpt_search_query_update function is an asynchronous method designed to enhance a search query by leveraging a conversational model (GPT). It accepts a single parameter, seed, which is an instance of the QAItem class. This instance contains the original question and answer, along with additional metadata such as the level of the question, evidence, and any constraints on the answer format.\n\nThe function begins by invoking the agent's chat_with_template method, passing a template name and the data extracted from the seed QAItem. The template used is \"gpt_search_Q_update.txt\", and the data includes the question and answer from the seed. This interaction with the conversational model aims to generate a response that includes an updated question and potentially new evidence.\n\nOnce the response is received, the function utilizes the extract_tag_content function to extract the relevant JSON data from the response. It then validates this JSON data using the extract_and_validate_json function. If the JSON extraction fails, a RuntimeError is raised, indicating that the GPT-Search query update was unsuccessful.\n\nIf the JSON extraction is successful, the function retrieves the updated question, evidence, and method from the parsed JSON. It then prints the output using the printer's rule and print methods to provide a structured and visually distinct output in the console.\n\nFinally, the function constructs and returns a new QAItem instance, which represents the updated question-answer pair. This new instance has an incremented level, the updated question, the original answer, and the combined evidence from the original and updated sources.\n\nThe gpt_search_query_update function is called within the gpt_search_run method of the ReverseUpgradeWorkflow class. In this context, it plays a crucial role in the multi-level upgrade process, where it is invoked repeatedly to refine the search query based on the model's responses. The successful execution of gpt_search_query_update is essential for the overall workflow, as it directly impacts the quality and relevance of the generated question-answer pairs.\n\n**Note**: It is important to ensure that the seed QAItem passed to this function is properly populated with valid data to avoid errors during the query update process. The function relies on the successful extraction and validation of JSON data to function correctly.\n\n**Output Example**: A possible return value from the gpt_search_query_update function could be an updated QAItem instance that looks like this:\n{\n  \"level\": 2,\n  \"question\": \"What is the capital of France after considering recent geopolitical changes?\",\n  \"answer\": \"Paris\",\n  \"constrained_format\": \"\",\n  \"parent_question\": \"What is the capital of France?\",\n  \"evidence\": [\"Paris is the capital of France.\", \"Recent changes in governance.\"],\n  \"strategy\": \"refined query\"\n}"
      ],
      "code_start_line": 589,
      "code_end_line": 615,
      "params": [
        "self",
        "seed"
      ],
      "have_return": true,
      "code_content": "    async def gpt_search_query_update(self, seed: QAItem) -> QAItem:\n        \"\"\"Stub for GPT Search query update.\"\"\"\n        # TODO: 实现 GPT Search 专用的 query update 逻辑\n        resp = agent.chat_with_template(\n            template_name=\"gpt_search_Q_update.txt\",\n            template_data={\"question\": seed.question, \"answer\": seed.answer},\n            root_folder=PROMPT_ROOT_FOLDER,\n        )\n        json_resp = extract_and_validate_json(extract_tag_content(resp, \"data\"))\n        if not json_resp:\n            raise RuntimeError(\"GPT-Search query update failed\")\n        updated_question = json_resp[\"updated_question\"].strip()\n        updated_evidence = json_resp.get(\"updated_evidence\", [])\n        method = json_resp.get(\"method\", \"None\")\n        printer.rule(\"GPT-Search Query Update Output\")\n        printer.print(pretty_json(json_resp), style=\"bold green\")\n        printer.rule(\"GPT-Search Query Update Evidence\")\n        printer.print(pretty_json(updated_evidence), style=\"bold green\")\n        return QAItem(\n            level=seed.level + 1,\n            question=updated_question,\n            answer=seed.answer,\n            constrained_format=seed.constrained_format,\n            parent_question=seed.question,\n            evidence=seed.evidence + updated_evidence,\n            strategy=method,\n        )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_tag_content",
        "src/criticsearch/utils.py/extract_and_validate_json",
        "src/criticsearch/abstract_substitution/abs_workflow.py/pretty_json",
        "src/criticsearch/abstract_substitution/abs_workflow.py/QAItem"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "gpt_search_run",
      "md_content": [
        "**gpt_search_run**: The function of gpt_search_run is to execute a multi-level search workflow using a GPT model to generate and verify question-answer items.\n\n**parameters**: The parameters of this Function.\n· self: An instance of the ReverseUpgradeWorkflow class, which contains the context and state for the workflow execution.\n\n**Code Description**: The gpt_search_run function is an asynchronous method that orchestrates a multi-level search process utilizing a GPT model, specifically designed to generate and validate question-answer items (QAItems). The function begins by defining the GPT model to be used, which is set to \"gpt-4o-search-preview\". It then prints a message indicating the start of the GPT-Search workflow.\n\nThe first step in the function is to generate a seed QAItem by calling the gpt_search_generate_seed method, which takes a randomly selected domain from the random_domain method. This seed generation is critical as it provides the initial question-answer pair that will be processed in subsequent steps. If an error occurs during seed generation, the function logs the error and prints an error message, terminating the workflow early.\n\nFollowing the successful generation of the seed, the function enters a loop that iterates through a predefined number of levels, specified by the max_level attribute of the class. For each level, it attempts to update the current QAItem through a series of retries, as defined by the max_tries attribute. Within this loop, the function calls gpt_search_query_update to refine the current QAItem based on the model's response. After updating, it verifies the updated QAItem using the multi_verify method to ensure that the model cannot answer the question directly.\n\nIf the verification fails, indicating that the model can answer the question, the function continues to retry the update process. If the verification passes, the updated QAItem is appended to the items list, and the current QAItem is updated to the new version. If the maximum number of retries is reached without a successful update, the function logs a message indicating that the workflow has stopped at the current level and exits.\n\nThe gpt_search_run function is called within the main function of the project, specifically when the user opts for a single run of the GPT search workflow. It is also invoked in batch runs, where multiple instances of the workflow can be executed concurrently. This integration highlights the function's role in the overall architecture of the application, facilitating the generation and evaluation of multi-level question-answer pairs.\n\n**Note**: It is essential to ensure that the workflow's parameters, such as max_level and max_tries, are appropriately set before invoking this function to avoid unexpected behavior during execution.\n\n**Output Example**: A possible return value from the gpt_search_run function could be a list of QAItem instances structured as follows:\n```json\n[\n  {\n    \"level\": 0,\n    \"question\": \"What are the benefits of using AI in healthcare?\",\n    \"answer\": \"AI can improve diagnostics, personalize treatment, and enhance patient care.\",\n    \"constrained_format\": \"\",\n    \"parent_question\": null,\n    \"evidence\": [\"Source A\", \"Source B\"],\n    \"strategy\": \"seed\"\n  },\n  {\n    \"level\": 1,\n    \"question\": \"How does AI improve diagnostics?\",\n    \"answer\": \"AI algorithms can analyze medical images faster and more accurately than human radiologists.\",\n    \"constrained_format\": \"\",\n    \"parent_question\": \"What are the benefits of using AI in healthcare?\",\n    \"evidence\": [\"Study X\", \"Research Y\"],\n    \"strategy\": \"refined query\"\n  }\n]\n```"
      ],
      "code_start_line": 618,
      "code_end_line": 664,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    async def gpt_search_run(self):\n        GPT_MODEL = \"gpt-4o-search-preview\" # Original code set this here\n        printer.rule(f\"GPT-Search Workflow Start with {GPT_MODEL}\")\n        # Seed 生成\n        try:\n            seed = await self.gpt_search_generate_seed(self.random_domain())\n            self.items.append(seed)\n            current = seed\n        except Exception as e:\n            logger.exception(\"Error in gpt_search_generate_seed\")\n            printer.print(f\"Error in gpt_search_generate_seed: {e}\", style=\"bold red\")\n            return\n\n        # 多级升级\n        for level in range(self.max_level):\n            retries = 0\n            # Using a flag similar to run() method for consistency with original logic flow\n            level_update_successful_gs = False\n            while retries < self.max_tries:\n                retries += 1\n                printer.rule(f\"GPT-Search Level {level+1} Update Attempt {retries}\")\n                try:\n                    updated = await self.gpt_search_query_update(current)\n                    passed = await self.multi_verify(updated)\n                except Exception as e:\n                    logger.exception(f\"Error in GPT-Search level {level+1} attempt {retries}\")\n                    printer.print(f\"Error in GPT-Search update: {e}\", style=\"bold red\")\n                    continue\n\n                if not passed:\n                    printer.print(\"GPT-Search: 多重校验未通过（模型能回答），重试 query_update …\", style=\"bold red\")\n                    continue\n\n                printer.print(\"GPT-Search: 多重校验通过（模型无法回答），记录更新后的 QAItem\", style=\"bold green\")\n                self.items.append(updated)\n                current = updated\n                level_update_successful_gs = True\n                break # Break retry loop\n\n            if not level_update_successful_gs:\n                printer.print(\n                    f\"GPT-Search stopped at level {current.level}; no valid update after {self.max_tries} tries.\",\n                    style=\"bold red\",\n                )\n                return\n\n        printer.rule(\"GPT-Search Workflow End\")\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/main",
        "src/criticsearch/abstract_substitution/abs_workflow.py/main/_batch/run_one"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/random_domain",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/multi_verify",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_generate_seed",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_query_update"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "random_domain",
      "md_content": [
        "**random_domain**: The function of random_domain is to return a randomly selected domain from a predefined list of domains.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The random_domain function is designed to select and return a random domain from a specified list of domains. The function begins by defining a list named `domains`, which contains various categories such as \"TV shows & movies\", \"Other\", \"Science & technology\", \"Art\", \"History\", \"Sports\", \"Music\", \"Video games\", \"Geography\", and \"Politics\". After the list is established, the function utilizes the `random.choice()` method from the random module to select one item randomly from the `domains` list. Finally, the selected domain is returned as the output of the function. This function can be useful in applications where a random selection from a set of options is required, such as in games, quizzes, or content generation.\n\n**Note**: It is important to ensure that the random module is imported in the script where this function is used. Without the import statement, the function will raise a NameError when attempting to call `random.choice()`.\n\n**Output Example**: A possible return value of the function could be \"Music\", indicating that the function has randomly selected this domain from the list."
      ],
      "code_start_line": 667,
      "code_end_line": 669,
      "params": [],
      "have_return": true,
      "code_content": "def random_domain():\n    domains = [\"TV shows & movies\", \"Other\", \"Science & technology\", \"Art\", \"History\", \"Sports\", \"Music\", \"Video games\", \"Geography\", \"Politics\"]\n    return random.choice(domains)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "find_all_wrong_items",
      "md_content": [
        "**find_all_wrong_items**: The function of find_all_wrong_items is to return a mapping of all entries where every model has answered incorrectly for a given question.\n\n**parameters**: The parameters of this Function.\n· data: A dictionary where keys are questions (strings) and values are entries (dictionaries) containing model responses.\n\n**Code Description**: The find_all_wrong_items function processes a dictionary of data to identify and return entries where all models have answered incorrectly. The function begins by initializing an empty dictionary called `wrong_items` to store the results. It then iterates over each question and its corresponding entry in the provided data.\n\nFor each entry, the function collects all model fields, excluding the \"GroundTruth\" and \"constrained_format\" fields. It checks if there are any model fields present. If there are, it evaluates whether all models have their 'is_correct' field set to False. This is done using a generator expression that checks each model's response. If the condition is met, the question and its entry are added to the `wrong_items` dictionary.\n\nFinally, the function returns the `wrong_items` dictionary, which contains only those entries where all models have answered incorrectly.\n\n**Note**: It is important to ensure that the input data is structured correctly, with each entry containing the necessary fields for evaluation. The function assumes that the 'is_correct' field is present in the model responses and that the entries are dictionaries.\n\n**Output Example**: \n{\n    \"What is the capital of France?\": {\n        \"model1\": {\"is_correct\": False},\n        \"model2\": {\"is_correct\": False},\n        \"GroundTruth\": \"Paris\",\n        \"constrained_format\": \"N/A\"\n    },\n    \"What is 2 + 2?\": {\n        \"model1\": {\"is_correct\": False},\n        \"model2\": {\"is_correct\": False},\n        \"GroundTruth\": \"4\",\n        \"constrained_format\": \"N/A\"\n    }\n}"
      ],
      "code_start_line": 671,
      "code_end_line": 684,
      "params": [
        "data"
      ],
      "have_return": true,
      "code_content": "def find_all_wrong_items(data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    返回所有模型都答错的条目映射（question -> entry）。\n    判定：除 GroundTruth 和 constrained_format 外，\n    若所有其它字段中的 'is_correct' 都为 False，则认为此条目全部答错。\n    \"\"\"\n    wrong_items: Dict[str, Dict[str, Any]] = {}\n    for q, entry in data.items():\n        # 收集所有模型字段\n        model_fields = [v for k, v in entry.items() if k not in (\"GroundTruth\", \"constrained_format\")]\n        # 若至少有一个模型且全部 is_correct=False\n        if model_fields and all(not m.get(\"is_correct\", False) for m in model_fields if isinstance(m, dict)):\n            wrong_items[q] = entry\n    return wrong_items\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "evaluate",
      "md_content": [
        "**evaluate**: The function of evaluate is to assess level-5 items in a JSON trace file using concurrent calls to a language model, while managing caching of results.\n\n**parameters**: The parameters of this Function.\n· json_file: The path to the JSON trace file that contains the items to be evaluated. Default is \"trace_data.json\".\n· use_cache: A boolean indicating whether to utilize cached results. Default is True.\n· cache_file: The path to the cache file where results are stored. If None, defaults to \"<stem>_eval_cache.json\" in the same directory as json_file.\n· eval_concurrency: An integer specifying the maximum number of concurrent evaluations to perform. Default is 10.\n· search_model: A string representing the model to be used for evaluation LLM calls. Default is \"gpt-4o-search-preview\".\n· level: An optional integer that specifies the level of items to evaluate. If None, all records are evaluated.\n\n**Code Description**: The evaluate function is designed to process items from a JSON file, specifically targeting those that are at level 5. It begins by printing the model being used for evaluation. If caching is enabled and a cache file exists, it attempts to load the cache, which contains previously evaluated items and their results. The function then reads the JSON file to load the records and filters them based on the specified level, preparing them for evaluation.\n\nThe function constructs a mapping of questions to their corresponding items and checks if any of these items have already been evaluated and stored in the cache. If an item is found in the cache, it retrieves the prediction and correctness status. If not, it adds the item to a list that requires evaluation.\n\nA worker function, evaluate_item_worker, is defined within evaluate to handle the evaluation of individual items using the specified language model. This function constructs a prompt for the model and processes the response to determine if the prediction matches the ground truth answer.\n\nThe evaluations are executed concurrently using a ThreadPoolExecutor, allowing multiple items to be processed simultaneously, which enhances efficiency. After evaluations are completed, the results are combined with the cache, and if caching is enabled, the updated cache is saved to the specified cache file.\n\nFinally, the function prints a report summarizing the evaluation results, including the total number of items considered, the number evaluated, the number of correct predictions, and the overall accuracy for the specified model.\n\nThe evaluate function is called from the main function in the same module, which serves as the entry point for the application. It is invoked when the command-line argument `--evaluate` is provided, indicating that only the evaluation should be performed. This design allows for flexibility in running the evaluation independently from other processes in the application.\n\n**Note**: When using the evaluate function, ensure that the JSON file is correctly formatted and contains the expected structure. Additionally, be mindful of the cache file's location and ensure that it is accessible for reading and writing.\n\n**Output Example**: \n```\nEvaluation Results for Model: gpt-4o-search-preview\nTotal items considered: 100\nTotal items evaluated for 'gpt-4o-search-preview' (cache + new): 80\nCorrect predictions for 'gpt-4o-search-preview': 65\nAccuracy for 'gpt-4o-search-preview': 81.25%\n```",
        "**evaluate**: The function of evaluate is to assess level-5 items in a JSON trace file using concurrent calls to a language model, while managing caching of results.\n\n**parameters**: The parameters of this Function.\n· json_file: The path to the JSON trace file to be evaluated, defaulting to \"trace_data.json\".\n· use_cache: A boolean indicating whether to enable caching of results, defaulting to True.\n· cache_file: The path to the cache file; if None, it defaults to \"<stem>_eval_cache.json\" in the same directory as json_file.\n· eval_concurrency: An integer specifying the maximum number of concurrent evaluations, defaulting to 10.\n· search_model: A string indicating the model to be used for evaluation LLM calls, defaulting to \"gpt-4o-search-preview\".\n· level: An optional integer specifying the level to evaluate; if None, all records are evaluated.\n\n**Code Description**: The evaluate function is designed to perform evaluations on items from a specified JSON trace file, particularly focusing on level-5 items. It begins by printing the model being used for evaluation. If caching is enabled and a cache file exists, it attempts to load the cache, processing it into a dictionary format for later use. The function then reads the JSON trace file, ensuring it contains a valid list of records. If the specified level is None, it prepares to evaluate all items; otherwise, it filters the records to include only those matching the specified level.\n\nThe function constructs an item map to facilitate quick lookups and checks for existing cache entries. It identifies items that need evaluation and counts how many items are found in the cache versus those that require new evaluations. A worker function, evaluate_item_worker, is defined to handle the evaluation of individual items using the specified language model. This worker function constructs a prompt based on the question and expected answer format, sends it to the language model, and processes the response to determine correctness.\n\nThe evaluations are executed concurrently using a ThreadPoolExecutor, allowing multiple items to be processed simultaneously, which enhances efficiency. After evaluations are completed, the results are combined with the cache, and if caching is enabled, the updated cache is saved to the specified cache file. Finally, the function prints a summary of the evaluation results, including total items considered, evaluated, correct predictions, and accuracy.\n\nThe evaluate function is called from the main function of the application, specifically when the `--evaluate` flag is set. This integration allows users to trigger evaluations directly from the command line, providing flexibility in how the application is used. The main function also handles various command-line arguments, including options for output file paths, concurrency limits, and model selection, thereby facilitating a comprehensive workflow for generating and evaluating benchmark questions.\n\n**Note**: When using the evaluate function, ensure that the JSON trace file is correctly formatted and accessible. The caching mechanism can significantly speed up evaluations by reusing previous results, but it requires proper management of cache files. Additionally, the choice of language model can impact the evaluation outcomes, so select an appropriate model based on the evaluation context.\n\n**Output Example**: \n```\nEvaluation Results for Model: gpt-4o-search-preview\nTotal items considered: 100\nTotal items evaluated for 'gpt-4o-search-preview' (cache + new): 80\nCorrect predictions for 'gpt-4o-search-preview': 65\nAccuracy for 'gpt-4o-search-preview': 81.25%\n```"
      ],
      "code_start_line": 687,
      "code_end_line": 892,
      "params": [
        "json_file",
        "use_cache",
        "cache_file",
        "eval_concurrency",
        "search_model",
        "level"
      ],
      "have_return": true,
      "code_content": "def evaluate(\n    json_file: Path = Path(\"trace_data.json\"),\n    use_cache: bool = True,\n    cache_file: Path | None = None,\n    eval_concurrency: int = 10,\n    search_model: str = \"gpt-4o-search-preview\",  # Added search_model parameter\n    level: Optional[int] = None                 \n):\n    \"\"\"\n    Evaluate level-5 items in a JSON trace, using concurrency for LLM calls.\n    Stores results per model in the cache.\n\n    - json_file: 待评估的 trace JSON 路径\n    - use_cache: 是否启用缓存\n    - cache_file: 缓存文件路径，默认与 json_file 同目录，名为 \"<stem>_eval_cache.json\"\n    - eval_concurrency: Max concurrent evaluations.\n    - search_model: Model to use for evaluation LLM calls. This model name is used as the key in the cache.\n    - level: 指定评估的 level，若为 None 则评估所有记录。\n    \"\"\"\n    printer.print(f\"Evaluation using model: {search_model}\", style=\"bold cyan\")\n\n    if cache_file is None:\n        cache_file = json_file.parent / f\"{json_file.stem}_eval_cache.json\"\n\n    # --- Load Cache ---\n    cache: Dict[str, Dict[str, Any]] = {}\n    if use_cache and cache_file.exists():\n        try:\n            with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n                raw = json.load(f)\n            if isinstance(raw, dict):\n                for q, models in raw.items():\n                    if isinstance(models, dict):\n                        cache[q] = {}\n                        for m, val in models.items():\n                            if m == \"GroundTruth\":\n                                cache[q][\"GroundTruth\"] = val\n                            elif isinstance(val, dict):\n                                cache[q][m] = val\n                            elif isinstance(val, bool):\n                                cache[q][m] = {\"is_correct\": val, \"prediction\": \"\"}\n                            else:\n                                logger.warning(f\"Ignoring invalid cache value for {q}/{m}\")\n            else:\n                logger.warning(f\"Cache file {cache_file} invalid, resetting\")\n                cache = {}\n        except Exception:\n            logger.exception(\"Error loading cache, resetting\")\n            cache = {}\n\n    # --- Load Records ---\n    try:\n        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n            records = json.load(f)\n            if not isinstance(records, list):\n                 printer.print(f\"Error: Evaluation file {json_file} does not contain a JSON list.\", style=\"bold red\")\n                 return\n    except FileNotFoundError:\n        printer.print(f\"Error: Evaluation file not found: {json_file}\", style=\"bold red\")\n        return\n    except json.JSONDecodeError:\n        printer.print(f\"Error: Could not decode JSON from evaluation file: {json_file}\", style=\"bold red\")\n        return\n    except Exception as e:\n        logger.exception(f\"Error reading evaluation file {json_file}: {e}\")\n        printer.print(f\"Error reading evaluation file {json_file}: {e}\", style=\"bold red\")\n        return\n\n    # --- Filter and Prepare Items ---\n    if level is None:\n        items_to_evaluate = [r for r in records if isinstance(r, dict)]\n    else:\n        items_to_evaluate = [r for r in records if isinstance(r, dict) and r.get(\"level\") == level]\n    total_items = len(items_to_evaluate)\n    if total_items == 0:\n        printer.print(f\"No items{' of level '+str(level) if level is not None else ''} found in {json_file}.\", style=\"yellow\")\n        return\n\n    item_map = {item['question'].strip().lower(): item for item in items_to_evaluate}\n\n    # 在构建 item_map 之后，确保 cache 里有 constrained_format\n    for key, item in item_map.items():\n        if key in cache:\n            # 如果 cache[key] 中没有 constrained_format，就补上\n            if 'constrained_format' not in cache[key]:\n                cache[key]['constrained_format'] = item.get('constrained_format', '')\n                logger.info(f\"补充 cache 中缺失的 constrained_format: {key} -> {cache[key]['constrained_format']}\")\n\n    items_needing_eval: List[Dict] = []\n    items_found_in_cache_for_this_model = correct_in_cache_for_this_model = 0\n\n    for item in items_to_evaluate:\n        if \"question\" not in item or \"answer\" not in item:\n            printer.print(f\"Skipping record due to missing keys: {item}\", style=\"yellow\")\n            continue # Skip malformed records\n\n        key = item['question'].strip().lower()\n\n        if use_cache and key in cache and search_model in cache[key]:\n            entry = cache[key][search_model]\n            items_found_in_cache_for_this_model += 1\n            if entry.get(\"is_correct\"):\n                correct_in_cache_for_this_model += 1\n            # 打印缓存预测\n            pred = entry.get(\"prediction\", \"\")\n            printer.print(f\"[Cache] Q: {key}  Pred: {pred}\", style=\"dim\")\n        else:\n            if key not in {i['question'].strip().lower() for i in items_needing_eval}:\n                items_needing_eval.append(item)\n\n    printer.print(f\"Total items found: {total_items}\")\n    printer.print(f\"Items found in cache for model '{search_model}': {items_found_in_cache_for_this_model} ({correct_in_cache_for_this_model} correct)\")\n    printer.print(f\"Items needing evaluation by model '{search_model}' (LLM call): {len(items_needing_eval)}\")\n\n    # --- Worker Function ---\n    def evaluate_item_worker(item: Dict, worker_search_model: str) -> Tuple[str, bool, str, str]:\n        \"\"\"Worker function to evaluate a single item using LLM.\"\"\"\n        q = item[\"question\"]\n        ans_true = item[\"answer\"]\n        constrained_format = item.get(\"constrained_format\", \"\")\n        key = q.strip().lower()\n        is_correct = False\n\n        prompt = (\n            f\"{FORMAT_INSTRUCTION}\\nPlease answer the following question and return it strictly in the \\\\boxed{{...}} format.\\n\"\n            f\"For the answer you provided in \\\\boxed{{}}, this answer must follow this format: {constrained_format}\\n\"\n            \"If you cannot answer, please generate \\\\boxed{Sorry, I don't know.}\\n\"\n            f\"Question: {q}\\n\"\n        )\n        try:\n            resp = agent.chat(prompt, model=worker_search_model)\n            ans_pred = extract_boxed_content(resp)\n\n            # 打印出GT answer 和preidct answer\n            printer.print(f\"Evaluating Question: {q}\")\n            printer.print(f\"GT Answer: {ans_true}\", style=\"bold red\")\n            printer.print(f\"Predicted Answer: {ans_pred}\", style=\"bold green\")  \n\n            if ans_pred:\n                 ans_pred_norm = \"\".join(c for c in ans_pred.strip().lower() if c.isalnum() or c.isspace())\n                 ans_true_norm = \"\".join(c for c in ans_true.strip().lower() if c.isalnum() or c.isspace())\n                 is_correct = (ans_pred_norm == ans_true_norm)\n                \n\n        except Exception as e:\n            logger.exception(f\"Error during evaluation LLM call for Q: {q} using model {worker_search_model}\")\n            printer.print(f\"Error evaluating question {q} with model {worker_search_model}: {e}\", style=\"bold red\")\n            is_correct = False\n\n        return key, is_correct, ans_pred, ans_true\n\n    # --- Concurrent Execution ---\n    newly_evaluated: Dict[str, Dict[str, Any]] = {}\n    correct_newly_evaluated: int = 0\n\n    if items_needing_eval:\n        printer.rule(f\"Running {len(items_needing_eval)} evaluations concurrently (max {eval_concurrency}) for model '{search_model}'...\")\n        with ThreadPoolExecutor(max_workers=eval_concurrency) as executor:\n            # Pass the specified search_model to the worker\n            future_to_item = {executor.submit(evaluate_item_worker, item, search_model): item for item in items_needing_eval}\n\n            processed_count = 0\n            for future in as_completed(future_to_item):\n                key, ok, ans_pred, ans_true = future.result()\n                # 同时保存预测和正确答案\n                newly_evaluated[key] = {\n                    \"is_correct\": ok,\n                    \"prediction\": ans_pred,\n                    \"answer\": ans_true\n                }\n                if ok:\n                    correct_newly_evaluated += 1\n                if processed_count % 10 == 0 or processed_count == len(items_needing_eval):\n                     printer.print(f\"  Evaluated {processed_count}/{len(items_needing_eval)}...\", style=\"dim\")\n\n    # --- Combine Results & Save Cache ---\n    if use_cache:\n        for key, res in newly_evaluated.items():\n            if key not in cache:\n                cache[key] = {}\n            # 保存 GroundTruth\n            cache[key][\"GroundTruth\"] = res[\"answer\"]\n            # 保存 constrained_format\n            cache[key][\"constrained_format\"] = item_map.get(key, {}).get(\"constrained_format\", \"\")\n            # 各模型结果放在模型名下\n            cache[key][search_model] = {\n                \"is_correct\": res[\"is_correct\"],\n                \"prediction\": res[\"prediction\"]\n            }\n        try:\n            with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n                json.dump(cache, f, ensure_ascii=False, indent=2)\n            printer.print(f\"Cache updated and saved to {cache_file}\", style=\"dim\")\n        except Exception as e:\n             logger.exception(f\"Error saving cache file {cache_file}: {e}\")\n             printer.print(f\"Error saving cache file {cache_file}: {e}\", style=\"red\")\n\n    # --- Final Report ---\n    total_correct_for_this_model = correct_in_cache_for_this_model + correct_newly_evaluated\n    total_evaluated_for_this_model = items_found_in_cache_for_this_model + len(newly_evaluated)\n    acc = total_correct_for_this_model / total_evaluated_for_this_model if total_evaluated_for_this_model else 0.0\n    printer.rule(f\"Evaluation Results for Model: {search_model}\")\n    printer.print(f\"Total items considered: {total_items}\", style=\"bold\")\n    printer.print(f\"Total items evaluated for '{search_model}' (cache + new): {total_evaluated_for_this_model}\", style=\"bold\")\n    printer.print(f\"Correct predictions for '{search_model}': {total_correct_for_this_model}\", style=\"bold\")\n    printer.print(f\"Accuracy for '{search_model}': {acc:.4%}\", style=\"bold green\")\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/main"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "evaluate_item_worker",
      "md_content": [
        "**evaluate_item_worker**: The function of evaluate_item_worker is to evaluate a single item using a language model (LLM) and determine the correctness of the provided answer.\n\n**parameters**: The parameters of this Function.\n· item: A dictionary containing the question, the true answer, and optionally a constrained format for the answer.  \n· worker_search_model: A string representing the model to be used for generating the response.\n\n**Code Description**: The evaluate_item_worker function is designed to assess the correctness of an answer provided for a specific question by utilizing a language model. It begins by extracting the question and the true answer from the input dictionary, along with any specified constrained format. The function normalizes the question to create a unique key for tracking and initializes a flag to indicate whether the predicted answer is correct.\n\nA prompt is constructed to instruct the language model on how to respond. This prompt includes the question and specifies the required format for the answer, ensuring that the model's output adheres to the expected structure. If the model is unable to provide an answer, it is instructed to respond with a default message indicating that it does not know the answer.\n\nThe function then invokes the chat method from the BaseAgent class, passing the constructed prompt and the specified model. This method facilitates the interaction with the language model and retrieves the model's response. The response is processed to extract the content enclosed within a LaTeX \\boxed{} expression using the extract_boxed_content function. This extraction is crucial for evaluating the model's output against the true answer.\n\nThe function prints the question, the ground truth answer, and the predicted answer to the console for debugging and evaluation purposes. It then normalizes both the predicted and true answers by stripping whitespace and converting them to lowercase, allowing for a fair comparison. If the normalized answers match, the function sets the correctness flag to True.\n\nIn the event of an exception during the evaluation process, an error message is logged, and the correctness flag is set to False. Finally, the function returns a tuple containing the normalized question key, the correctness flag, the predicted answer, and the true answer.\n\nThis function is integral to the evaluation workflow, as it directly interacts with the language model to validate answers based on user-defined criteria. It relies on the chat method to generate responses and the extract_boxed_content function to parse the model's output, ensuring that the evaluation process is both accurate and efficient.\n\n**Note**: It is important to ensure that the item parameter contains valid data, including a well-formed question and answer. The worker_search_model should be a valid model name recognized by the system to avoid errors during the evaluation process.\n\n**Output Example**: A possible return value from the evaluate_item_worker function could be:\n(\"what is the capital of France?\", True, \"Paris\", \"Paris\")"
      ],
      "code_start_line": 802,
      "code_end_line": 836,
      "params": [
        "item",
        "worker_search_model"
      ],
      "have_return": true,
      "code_content": "    def evaluate_item_worker(item: Dict, worker_search_model: str) -> Tuple[str, bool, str, str]:\n        \"\"\"Worker function to evaluate a single item using LLM.\"\"\"\n        q = item[\"question\"]\n        ans_true = item[\"answer\"]\n        constrained_format = item.get(\"constrained_format\", \"\")\n        key = q.strip().lower()\n        is_correct = False\n\n        prompt = (\n            f\"{FORMAT_INSTRUCTION}\\nPlease answer the following question and return it strictly in the \\\\boxed{{...}} format.\\n\"\n            f\"For the answer you provided in \\\\boxed{{}}, this answer must follow this format: {constrained_format}\\n\"\n            \"If you cannot answer, please generate \\\\boxed{Sorry, I don't know.}\\n\"\n            f\"Question: {q}\\n\"\n        )\n        try:\n            resp = agent.chat(prompt, model=worker_search_model)\n            ans_pred = extract_boxed_content(resp)\n\n            # 打印出GT answer 和preidct answer\n            printer.print(f\"Evaluating Question: {q}\")\n            printer.print(f\"GT Answer: {ans_true}\", style=\"bold red\")\n            printer.print(f\"Predicted Answer: {ans_pred}\", style=\"bold green\")  \n\n            if ans_pred:\n                 ans_pred_norm = \"\".join(c for c in ans_pred.strip().lower() if c.isalnum() or c.isspace())\n                 ans_true_norm = \"\".join(c for c in ans_true.strip().lower() if c.isalnum() or c.isspace())\n                 is_correct = (ans_pred_norm == ans_true_norm)\n                \n\n        except Exception as e:\n            logger.exception(f\"Error during evaluation LLM call for Q: {q} using model {worker_search_model}\")\n            printer.print(f\"Error evaluating question {q} with model {worker_search_model}: {e}\", style=\"bold red\")\n            is_correct = False\n\n        return key, is_correct, ans_pred, ans_true\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_boxed_content"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_fuzzy_replacement",
      "md_content": [
        "**test_fuzzy_replacement**: The function of test_fuzzy_replacement is to test the fuzzy_replacement prompt with example inputs.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The test_fuzzy_replacement function is an asynchronous function designed to test the fuzzy replacement functionality of a conversational agent. It begins by defining a list of test cases, which currently contains a single input string \"全球\" (meaning \"global\" in Chinese). \n\nThe function utilizes the printer.rule method to create a visual separator in the console output, indicating the start of the fuzzy replacement test. For each test input in the test_cases list, the function attempts to interact with the agent using the chat_with_template method from the BaseAgent class. This method is called with the template name \"fuzzy_replacement.txt\", passing the test input as template data, along with the root folder and model parameters.\n\nUpon receiving the result from the agent, the function calls extract_and_validate_json to parse and validate the JSON response. If the parsing is successful, the input and output are printed to the console using the printer.print method, with distinct styles for clarity. In case of an error during processing, the function catches the exception, logs the error using the logger, and prints an error message to the console.\n\nThe test_fuzzy_replacement function is invoked from the main function, which serves as the entry point of the application. It is triggered when the command-line argument --test_fuzzy is provided, allowing users to run this specific test case as part of the overall functionality of the application.\n\n**Note**: It is essential to ensure that the template file \"fuzzy_replacement.txt\" exists in the specified directory and that the input provided in the test cases is valid for the fuzzy replacement functionality to work correctly. Additionally, proper error handling is implemented to manage any issues that arise during the testing process."
      ],
      "code_start_line": 894,
      "code_end_line": 912,
      "params": [],
      "have_return": false,
      "code_content": "async def test_fuzzy_replacement():\n    \"\"\"Test the fuzzy_replacement prompt with some example inputs.\"\"\"\n    test_cases = [\"全球\"]\n    \n    printer.rule(\"Testing Fuzzy Replacement\")\n    for test_input in test_cases:\n        try:\n            result = agent.chat_with_template(\n                template_name=\"fuzzy_replacement.txt\",\n                template_data={\"input\": test_input},\n                root_folder=PROMPT_ROOT_FOLDER,\n                model=\"gpt-4o\"\n            )\n            parsed_result = extract_and_validate_json(result)\n            printer.print(f\"\\nInput: {test_input}\", style=\"bold yellow\")\n            printer.print(f\"Output: {parsed_result}\", style=\"bold green\")\n        except Exception as e:\n            printer.print(f\"Error processing {test_input}: {e}\", style=\"bold red\")\n            logger.exception(f\"Error in fuzzy replacement test for input: {test_input}\")\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/main"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_and_validate_json"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_entity_extraction",
      "md_content": [
        "**test_entity_extraction**: The function of test_entity_extraction is to test the entity extraction functionality using a series of predefined input cases.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The test_entity_extraction function is an asynchronous function designed to validate the entity extraction capabilities of a conversational agent. It begins by defining a list of test cases, which are strings containing various sentences that include named entities, such as people, organizations, locations, and dates. \n\nThe function first invokes the printer's rule method to create a visual separator in the console output, indicating the start of the entity extraction tests. It then iterates over each test case in the predefined list. For each test input, the function attempts to call the agent's chat_with_template method, passing the template name \"entity_extraction.txt\" along with the current test input as template data. This method is responsible for rendering the template and interacting with the conversational model to generate a response.\n\nUpon receiving the response, the function prints the input and output to the console, using the printer's print method to format the output with specific styles for better visibility. It then attempts to parse the output as JSON using the extract_and_validate_json function. This function is designed to handle the extraction of JSON content from the model's response, ensuring that it is valid and properly formatted. If the parsing is successful, the function prints the extracted entities categorized by their types. If the output cannot be parsed as JSON, a warning message is displayed.\n\nIn the event of any exceptions during the processing of a test case, the function logs the error and prints an error message to the console. This ensures that any issues encountered during the testing process are documented for further investigation.\n\nThe test_entity_extraction function is called within the main function of the project, specifically when the command-line argument `--test_entity` is provided. This allows users to execute the entity extraction tests as part of the overall workflow, enabling them to validate the functionality of the entity extraction feature in a controlled manner.\n\n**Note**: It is essential to ensure that the template file \"entity_extraction.txt\" exists in the specified directory and that the responses from the agent's chat_with_template method are formatted correctly to avoid parsing errors. Proper handling of exceptions is crucial for maintaining the robustness of the testing process."
      ],
      "code_start_line": 914,
      "code_end_line": 963,
      "params": [],
      "have_return": false,
      "code_content": "async def test_entity_extraction():\n    \"\"\"Test the entity_extraction prompt with example inputs.\"\"\"\n    test_cases = [\n        \"The CEO of Apple Inc. in 2024 is Tim Cook.\",\n        \"2020年东京奥运会最终在2021年举行。\",\n        \"习近平是中国国家主席。\",\n        \"The Eiffel Tower is located in Paris, France.\",\n        \"2022年北京冬奥会由中国举办。\",\n        \"Elon Musk founded SpaceX in 2002.\",\n        \"2023年诺贝尔文学奖得主是一位非洲作家。\",\n        \"The capital of Japan is Tokyo.\",\n        \"2021年7月，河南遭遇严重洪灾。\",\n        \"Barack Obama served as the U.S. president from 2009 to 2017.\",\n        \"2024年11月，美国将举行总统大选。\",\n        \"Harry Potter was written by J.K. Rowling.\",\n        \"The COVID-19 pandemic began in late 2019.\",\n        \"乔丹是NBA历史上最伟大的球员之一。\",\n        \"The United Nations was founded in 1945.\",\n        \"2023年杭州亚运会吸引了数千名运动员参与。\",\n        \"The Great Wall of China can be seen from space, though it's a myth.\",\n        \"2022年，俄罗斯与乌克兰爆发冲突。\",\n        \"Albert Einstein was awarded the Nobel Prize in Physics in 1921.\",\n        \"2023年5月，ChatGPT在全球范围内被广泛使用。\",\n    ]\n    \n    printer.rule(\"Testing Entity Extraction\")\n    for test_input in test_cases:\n        try:\n            result = agent.chat_with_template(\n                template_name=\"entity_extraction.txt\",\n                template_data={\"input\": test_input},\n                root_folder=PROMPT_ROOT_FOLDER,\n                model=\"gpt-4o\"\n            )\n            printer.print(f\"\\nInput: {test_input}\", style=\"bold yellow\")\n            printer.print(f\"Output: {result}\", style=\"bold green\")\n\n            # 尝试解析JSON结果并格式化显示\n            try:\n                parsed_result = extract_and_validate_json(result)\n                printer.print(\"\\nParsed entities:\", style=\"bold blue\")\n                for entity_type, entities in parsed_result.get(\"entities\", {}).items():\n                    if entities:  # 只显示非空的实体类型\n                        printer.print(f\"{entity_type}: {', '.join(entities)}\")\n            except json.JSONDecodeError:\n                printer.print(\"Warning: Failed to parse JSON output\", style=\"bold red\")\n\n        except Exception as e:\n            printer.print(f\"Error processing {test_input}: {e}\", style=\"bold red\")\n            logger.exception(f\"Error in entity extraction test for input: {test_input}\")\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/main"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_and_validate_json"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_combination_workflow",
      "md_content": [
        "**test_combination_workflow**: The function of test_combination_workflow is to test the combination of entity extraction and fuzzy replacement using seed generation.\n\n**parameters**: The parameters of this Function.\n· out_file: Path - A Path object representing the output file where results may be saved or logged.\n\n**Code Description**: The test_combination_workflow function is an asynchronous function designed to execute a series of operations that integrate entity extraction and fuzzy replacement processes. The workflow begins by printing a rule header to the console to indicate the start of the testing process. It utilizes the ReverseUpgradeWorkflow class to generate a seed question-answer pair based on a randomly selected domain.\n\nThe function proceeds with the following steps:\n\n1. **Seed Generation**: It calls the gpt_search_generate_seed method of the ReverseUpgradeWorkflow instance to generate a seed question and answer. This method interacts with a conversational model to produce relevant content based on a randomly chosen domain.\n\n2. **Entity Extraction**: The seed question is then processed through the agent's chat_with_template method, which utilizes a predefined template for entity extraction. The extracted entities are validated to ensure that they conform to expected formats.\n\n3. **Fuzzy Replacement**: For each entity extracted (excluding those categorized as \"OTHER\"), the function performs fuzzy replacements concurrently using a ThreadPoolExecutor. Each entity is processed to generate a modified version, and the results are collected.\n\n4. **Fuzzy Question Generation**: The original seed question is modified by replacing the original entities with their fuzzy counterparts. The evidence supporting these modifications is also aggregated.\n\n5. **Final Question Polishing**: The function prepares input data for final question refinement, which includes the original question, answer, extracted entities, and the results of the fuzzy replacements. It then calls the agent's chat_with_template method again to polish the modified question.\n\n6. **Validation**: A FuzzyQAItem instance is created to encapsulate the results of the fuzzy replacement process. The function then calls verify_fuzzy_item to validate the correctness of the modified question-answer pair based on the provided evidence.\n\n7. **Error Handling**: Throughout the workflow, the function includes error handling mechanisms to catch exceptions and log errors appropriately. If any step fails, the function returns None, indicating that the workflow did not complete successfully.\n\nThe test_combination_workflow function is invoked by the sync_run_one_wrapper function, which is responsible for executing the workflow in an asynchronous context. This wrapper function allows for the integration of the test_combination_workflow into broader testing or execution frameworks, facilitating the evaluation of the combined entity extraction and fuzzy replacement processes.\n\n**Note**: It is essential to ensure that the output file path provided as the out_file parameter is valid and accessible for logging or saving results. Additionally, the function relies on the proper configuration of templates and models used in the chat_with_template method to ensure accurate processing of inputs and outputs.\n\n**Output Example**: A possible return value from the test_combination_workflow function could be a FuzzyQAItem instance structured as follows:\n```json\n{\n  \"original_question\": \"What is the capital of France?\",\n  \"question\": \"What is the capital city of France?\",\n  \"answer\": \"Paris\",\n  \"constrained_format\": \"Provide the answer in one word.\",\n  \"strategy\": \"fuzzy_replacement\",\n  \"evidence\": [\"France is a country in Europe.\", \"Paris is known as the capital of France.\"]\n}\n```"
      ],
      "code_start_line": 965,
      "code_end_line": 1079,
      "params": [
        "out_file"
      ],
      "have_return": true,
      "code_content": "async def test_combination_workflow(out_file: Path):\n    \"\"\"Test the combination of entity extraction and fuzzy replacement using seed generation.\"\"\"\n    printer.rule(\"Testing Combination Workflow\")\n    \n    # 1. 使用 gpt_search_generate_seed 生成种子QA\n    wf = ReverseUpgradeWorkflow()\n    try:\n        seed = await wf.gpt_search_generate_seed(wf.random_domain())\n        printer.print(f\"\\nSeed Question: {seed.question}\", style=\"dim underline\")\n        printer.print(f\"Seed Answer: {seed.answer}\", style=\"bold green\")\n        \n        # 2. 对问题进行实体抽取\n        result = agent.chat_with_template(\n            template_name=\"entity_extraction.txt\",\n            template_data={\"input\": seed.question},\n            root_folder=PROMPT_ROOT_FOLDER,\n            model=\"gpt-4o\"\n        )\n        \n        parsed_entities = extract_and_validate_json(result)\n        if not parsed_entities or \"entities\" not in parsed_entities:\n            raise ValueError(\"Failed to extract entities from the seed question\")\n\n        # 3. 对每个实体进行模糊替换（并发处理）\n        replacements = []\n        entities_to_process = []\n        for entity_type, entities in parsed_entities[\"entities\"].items():\n            if entity_type == \"OTHER\":\n                continue\n            entities_to_process.extend([(entity_type, entity) for entity in entities])\n        \n        def process_entity(args):\n            entity_type, entity = args\n            try:\n                fuzzy_result = agent.chat_with_template(\n                    template_name=\"fuzzy_replacement.txt\",\n                    template_data={\"input\": entity},\n                    root_folder=PROMPT_ROOT_FOLDER,\n                    model=\"gpt-4o-search-preview\"\n                )\n                parsed_fuzzy = extract_and_validate_json(fuzzy_result)\n                if parsed_fuzzy and \"output\" in parsed_fuzzy:\n                    return {\n                        \"type\": entity_type,\n                        \"original\": entity,\n                        \"fuzzy\": parsed_fuzzy[\"output\"],\n                        \"evidence\": parsed_fuzzy.get(\"evidence\", [])\n                    }\n            except Exception as e:\n                printer.print(f\"Error processing fuzzy replacement for {entity}: {e}\", style=\"bold red\")\n                return None\n        \n        with ThreadPoolExecutor(max_workers=20) as executor:\n            results = list(executor.map(process_entity, entities_to_process))\n            replacements = [r for r in results if r is not None]\n\n        # 4. 生成草稿模糊问题\n        fuzzy_question = seed.question\n        for rep in replacements:\n            fuzzy_question = fuzzy_question.replace(rep[\"original\"], rep[\"fuzzy\"])\n            seed.evidence.extend(rep[\"evidence\"])\n        \n        # 5. 准备最终问题润色的输入数据\n        input_data = {\n            \"original\": {\n                \"question\": seed.question,\n                \"answer\": seed.answer\n            },\n            \"entities\": parsed_entities[\"entities\"],\n            \"replacements\": replacements,\n            \"fuzzy_result\": {\n                \"question\": fuzzy_question,\n                \"answer\": seed.answer,\n                \"constrained_format\": seed.constrained_format,\n                \"evidence\": seed.evidence,\n                \"strategy\": \"fuzzy_replacement\"\n            }\n        }\n        \n        # 6. 进行最终问题润色\n        final_result = agent.chat_with_template(\n            template_name=\"entity_final_question.txt\",\n            template_data={\"input_data\": json.dumps(input_data, ensure_ascii=False)},\n            root_folder=PROMPT_ROOT_FOLDER,\n            model=\"o4-mini\"\n        )\n        \n        try:\n            parsed_final = extract_and_validate_json(final_result)\n            if parsed_final and \"polished_question\" in parsed_final:\n                input_data[\"fuzzy_result\"][\"question\"] = parsed_final[\"polished_question\"]\n        except Exception as e:\n            printer.print(f\"Error parsing final polish result: {e}\", style=\"bold red\")\n        \n        # 7. 将结果转换为 FuzzyQAItem\n        fuzzy_item = FuzzyQAItem(\n            original_question=seed.question,\n            question=input_data[\"fuzzy_result\"][\"question\"],\n            answer=seed.answer,\n            constrained_format=seed.constrained_format,\n            strategy=\"fuzzy_replacement\",\n            evidence=input_data[\"fuzzy_result\"][\"evidence\"]\n        )\n\n        # 8. 验证, 未通过则丢弃\n        if not await verify_fuzzy_item(fuzzy_item):\n            printer.print(\"核验未通过，丢弃此条结果\", style=\"bold yellow\")\n            return None\n\n        return fuzzy_item\n\n    except Exception as e:\n        printer.print(f\"Error in combination workflow: {e}\", style=\"bold red\")\n        logger.exception(\"Error in combination workflow\")\n        return None  # 返回 None 表示执行失败\n",
      "name_column": 10,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/main/_combination_batch/sync_run_one_wrapper"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_and_validate_json",
        "src/criticsearch/abstract_substitution/abs_workflow.py/FuzzyQAItem",
        "src/criticsearch/abstract_substitution/abs_workflow.py/verify_fuzzy_item",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/random_domain",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_generate_seed"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "process_entity",
      "md_content": [
        "**process_entity**: The function of process_entity is to process an entity by performing a fuzzy replacement using a conversational model and returning the results in a structured format.\n\n**parameters**: The parameters of this Function.\n· args: A tuple containing two elements - the first element is the entity type (string) and the second element is the entity itself (string).\n\n**Code Description**: The process_entity function is designed to handle the processing of an entity by utilizing a conversational model to perform fuzzy replacements. It begins by unpacking the input argument `args`, which is expected to be a tuple containing the entity type and the entity string. \n\nThe function then attempts to call the `chat_with_template` method from the `BaseAgent` class, passing a specific template name (\"fuzzy_replacement.txt\") along with the entity data as template data. This method is responsible for rendering the template and interacting with the conversational model to obtain a response. The `root_folder` and `model` parameters are also specified to guide the template loading and model selection.\n\nUpon receiving the response from the `chat_with_template` method, the function invokes `extract_and_validate_json` to parse and validate the JSON content returned by the model. This step is crucial as it ensures that the response is in the correct format and can be processed further. If the parsing is successful and the parsed JSON contains an \"output\" key, the function constructs and returns a dictionary containing the entity type, the original entity, the fuzzy replacement output, and any associated evidence.\n\nIf any exceptions occur during this process, such as issues with the model interaction or JSON parsing, the function catches the exception and utilizes the `print` method from the `RichPrinter` class to log an error message to the console, indicating the failure and the specific entity that caused the error. In such cases, the function returns None, signaling that the processing was unsuccessful.\n\nThe process_entity function is integral to the workflow of the application, as it serves as a bridge between user input (entities) and the conversational model's output, facilitating the extraction of meaningful information through fuzzy replacements.\n\n**Note**: It is important to ensure that the input provided to the function is well-formed and that the template file specified exists in the designated directory. Proper error handling is implemented to manage any issues that may arise during the processing of entities.\n\n**Output Example**: A possible return value from the process_entity function could be a dictionary structured as follows:\n{\n    \"type\": \"location\",\n    \"original\": \"Paris\",\n    \"fuzzy\": \"The capital of France is Paris.\",\n    \"evidence\": [\"Paris is known for its art, fashion, and culture.\"]\n}"
      ],
      "code_start_line": 996,
      "code_end_line": 1015,
      "params": [
        "args"
      ],
      "have_return": true,
      "code_content": "        def process_entity(args):\n            entity_type, entity = args\n            try:\n                fuzzy_result = agent.chat_with_template(\n                    template_name=\"fuzzy_replacement.txt\",\n                    template_data={\"input\": entity},\n                    root_folder=PROMPT_ROOT_FOLDER,\n                    model=\"gpt-4o-search-preview\"\n                )\n                parsed_fuzzy = extract_and_validate_json(fuzzy_result)\n                if parsed_fuzzy and \"output\" in parsed_fuzzy:\n                    return {\n                        \"type\": entity_type,\n                        \"original\": entity,\n                        \"fuzzy\": parsed_fuzzy[\"output\"],\n                        \"evidence\": parsed_fuzzy.get(\"evidence\", [])\n                    }\n            except Exception as e:\n                printer.print(f\"Error processing fuzzy replacement for {entity}: {e}\", style=\"bold red\")\n                return None\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_and_validate_json"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: The function of main is to serve as the entry point for executing the multi-level reverse-upgrade benchmark question generation and evaluation workflow.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The main function initializes the command-line interface for the application, allowing users to specify various options for generating or evaluating multi-level reverse-upgrade benchmark questions. It utilizes the argparse library to parse command-line arguments, which include options for output file paths, maximum levels and tries for question generation, batch execution settings, concurrency limits, model selection, and testing modes.\n\nUpon parsing the arguments, the function checks for specific flags to determine the workflow to execute. If the `--test_fuzzy` flag is set, it runs the `test_fuzzy_replacement` function, which tests the fuzzy replacement functionality of the conversational agent. If the `--test_entity` flag is set, it invokes the `test_entity_extraction` function to validate the entity extraction capabilities. If the `--test_combination` flag is specified, it initiates a combination testing workflow using the specified batch and concurrency settings.\n\nFor evaluation purposes, if the `--evaluate` flag is present, the function calls the `evaluate` function, which assesses level-5 items from a specified JSON trace file using concurrent calls to a language model. This function manages caching of results and provides a summary of evaluation outcomes.\n\nIn the case of a standard run, the function creates an instance of the `ReverseUpgradeWorkflow` class, which orchestrates the multi-level question generation process. Depending on the user's input, it either runs a standard workflow or a GPT-Search workflow, handling both single and batch executions. The results of the workflow are saved to the specified output file.\n\nThe main function is critical as it serves as the central hub for user interaction with the application, directing the flow of execution based on user-defined parameters and ensuring that the appropriate workflows are executed.\n\n**Note**: Ensure that all required command-line arguments are provided when executing the script, as missing parameters may lead to errors or unintended behavior. Additionally, the specified output paths should be valid and accessible for writing results.\n\n**Output Example**: The main function does not return a value but may produce console output indicating the progress and results of the executed workflows, such as:\n```\nUsing overridden model for generation runs: gpt-4o\nStarting Batch Run: 5 instances with concurrency 2\nSuccessfully saved 10 items to trace_data.json\n```"
      ],
      "code_start_line": 1081,
      "code_end_line": 1271,
      "params": [],
      "have_return": true,
      "code_content": "def main():\n    # Added --search_model argument\n    parser = argparse.ArgumentParser(description=\"Generates or evaluates multi-level reverse-upgrade benchmark questions.\")\n    parser.add_argument(\"--out\", type=Path, default=Path(\"trace_data.json\"))\n    parser.add_argument(\"--max_level\", type=int, default=5)\n    parser.add_argument(\"--max_tries\", type=int, default=5)\n    parser.add_argument(\"--batch\", type=int, default=1, help=\"批量运行次数，>1 则追加写入同一文件\")\n    parser.add_argument(\"--concurrency\", type=int, default=1, help=\"最大并行并发数 (for batch runs)\")\n    parser.add_argument(\"--model\", type=str, default=None, help=\"LLM model name to use (for non-GPT-Search runs)\")\n    parser.add_argument(\"--evaluate\", action=\"store_true\", help=\"只运行 evaluate()，输出测试结果并退出\")\n    parser.add_argument(\"--eval_concurrency\", type=int, default=10, help=\"最大并行并发数 (for --evaluate mode)\")\n    parser.add_argument(\"--search_model\", type=str, default=\"gpt-4o-search-preview\", help=\"Model to use for evaluation LLM calls\")\n    parser.add_argument(\"--eval_level\", type=int, default=None, help=\"Specify level to evaluate; omit for all records\")\n    parser.add_argument(\"--test_fuzzy\", action=\"store_true\", help=\"运行 fuzzy replacement 测试\")\n    parser.add_argument(\"--test_entity\", action=\"store_true\", help=\"运行实体抽取测试\")\n    parser.add_argument(\"--test_combination\", action=\"store_true\", help=\"运行组合测试流程\")\n    parser.add_argument(\"--combination_batch\", type=int, default=1, help=\"组合测试运行批次\")\n    parser.add_argument(\"--combination_concurrency\", type=int, default=1, help=\"组合测试并发数\")\n    parser.add_argument(\"--combination_out\", type=Path, default=Path(\"fuzzy_replacement_bench.json\"), help=\"组合测试结果输出文件\")\n    \n    args = parser.parse_args()\n\n    # Keeping original model override logic\n    if args.model:\n        global GPT_MODEL\n        GPT_MODEL = args.model\n        printer.print(f\"Using overridden model for generation runs: {GPT_MODEL}\", style=\"bold yellow\")\n\n    # Add test branches\n    if args.test_fuzzy:\n        asyncio.run(test_fuzzy_replacement())\n        return\n\n    if args.test_entity:\n        asyncio.run(test_entity_extraction())\n        return\n\n    if args.test_combination:\n        printer.rule(f\"Starting Combination Workflow: {args.combination_batch} instances with concurrency {args.combination_concurrency}\")\n        \n        def _combination_batch():\n            def sync_run_one_wrapper(idx: int):\n                return asyncio.run(test_combination_workflow(args.combination_out))\n\n            successful_results = []  # 存储成功执行的结果\n\n            with ThreadPoolExecutor(max_workers=args.combination_concurrency) as executor:\n                futures = {executor.submit(sync_run_one_wrapper, i): i \n                          for i in range(args.combination_batch)}\n                for future in as_completed(futures):\n                    try:\n                        result = future.result()  # 获取执行结果\n                        if result is not None:  # 只保存成功的结果\n                            successful_results.append(result.to_dict())\n                    except Exception as exc:\n                        run_idx = futures[future] + 1\n                        logger.exception(f\"Combination run {run_idx} failed: {exc}\")\n                        printer.print(f\"Combination run {run_idx} failed: {exc}\", style=\"bold red\")\n                        continue  # 跳过失败的运行\n\n            # 只有在有成功结果时才更新文件\n            if successful_results:\n                with file_lock:\n                    existing = []\n                    if args.combination_out.exists() and args.combination_out.stat().st_size > 0:\n                        try:\n                            with open(args.combination_out, \"r\", encoding=\"utf-8\") as f:\n                                existing = json.load(f)\n                        except json.JSONDecodeError:\n                            logger.error(f\"Could not decode JSON from {args.combination_out}. Starting fresh.\")\n                        except Exception as e:\n                            logger.exception(f\"Error reading {args.combination_out}\")\n\n                    existing.extend(successful_results)\n                    \n                    try:\n                        with open(args.combination_out, \"w\", encoding=\"utf-8\") as f:\n                            json.dump(existing, f, ensure_ascii=False, indent=2)\n                        printer.print(f\"Successfully saved {len(successful_results)} items to {args.combination_out}\", style=\"bold green\")\n                    except Exception as e:\n                        logger.exception(f\"Error saving to {args.combination_out}\")\n                        printer.print(f\"Error saving results: {e}\", style=\"bold red\")\n\n            printer.print(f\"Completed {args.combination_batch} combination runs\", style=\"bold green\")\n            printer.print(f\"Successfully processed: {len(successful_results)} items\", style=\"bold green\")\n            printer.print(f\"Failed: {args.combination_batch - len(successful_results)} items\", style=\"bold yellow\")\n            printer.print(f\"Results saved to: {args.combination_out}\", style=\"bold green\")\n        \n        _combination_batch()\n        return\n\n    # --- Evaluation Branch ---\n    if args.evaluate:\n        evaluate(\n            json_file=args.out,\n            use_cache=True,\n            eval_concurrency=args.eval_concurrency,\n            search_model=args.search_model,\n            level=args.eval_level\n        )\n        return # Exit after evaluation\n\n    # --- Single Run Branches (Unchanged) ---\n    if args.gptsearch and args.batch <= 1:\n        wf = ReverseUpgradeWorkflow(max_level=args.max_level, max_tries=args.max_tries)\n        try:\n            asyncio.run(wf.gpt_search_run())\n        except Exception as e:\n            logger.exception(\"Error in single gptsearch run\")\n            printer.print(f\"Error during single gptsearch run: {e}\", style=\"bold red\")\n        finally:\n            try:\n                wf.save(args.out)\n                print(f\"Saved {len(wf.items)} items to {args.out}\")\n            except Exception as e:\n                logger.exception(\"Error saving results for single gptsearch run\")\n                printer.print(f\"Error saving results: {e}\", style=\"bold red\")\n        return\n\n    if args.batch <= 1: # Handles non-gptsearch single run\n        wf = ReverseUpgradeWorkflow(max_level=args.max_level, max_tries=args.max_tries)\n        try:\n            asyncio.run(wf.run())\n        except Exception as e:\n            logger.exception(\"Error in single run\")\n            printer.print(f\"Error during single run: {e}\", style=\"bold red\")\n        finally:\n            try:\n                wf.save(args.out)\n                print(f\"Saved {len(wf.items)} items to {args.out}\")\n            except Exception as e:\n                logger.exception(\"Error saving results for single run\")\n                printer.print(f\"Error saving results: {e}\", style=\"bold red\")\n        return\n\n    # --- Batch Run Branch (Using ThreadPoolExecutor - Confirmed uses args.concurrency) ---\n    printer.rule(f\"Starting Batch Run: {args.batch} instances with concurrency {args.concurrency}\")\n    # This function is now synchronous and uses ThreadPoolExecutor\n    def _batch():\n        # Define the async function exactly as it was in the original _batch\n        async def run_one(idx: int):\n            printer.rule(f\"Batch Run {idx+1}/{args.batch}\")\n            wf = ReverseUpgradeWorkflow(max_level=args.max_level, max_tries=args.max_tries)\n            try:\n                # 根据标志选择流程\n                if args.gptsearch:\n                    await wf.gpt_search_run()\n                else:\n                    await wf.run()\n                # 统一保存\n                wf.save(args.out) # Save handles locking\n                count = len(wf.items)\n                printer.print(f\"Batch[{idx+1}]: saved {count} items.\", style=\"bold green\")\n                return [it.to_dict() for it in wf.items] # Original returned items\n            except Exception as e:\n                logger.exception(f\"Error in batch run {idx+1}\")\n                printer.print(f\"Error in batch run {idx+1}: {e}\", style=\"bold red\")\n                 # Try to save partial results even on error\n                try:\n                     wf.save(args.out)\n                     count = len(wf.items)\n                     printer.print(f\"Batch[{idx+1}]: saved {count} partial items after error.\", style=\"yellow\")\n                except Exception as save_e:\n                     logger.exception(f\"Error saving partial results for batch {idx+1}: {save_e}\")\n                     printer.print(f\"Error saving partial results for batch {idx+1}: {save_e}\", style=\"red\")\n                return [] # Original returned empty list on error\n\n        # Synchronous wrapper to call the async run_one using asyncio.run\n        def sync_run_one_wrapper(idx: int):\n            # Each thread needs its own event loop\n            return asyncio.run(run_one(idx))\n\n        # Use ThreadPoolExecutor to run the sync wrappers concurrently\n        all_results = []\n        # CONFIRMED: max_workers uses args.concurrency here\n        with ThreadPoolExecutor(max_workers=args.concurrency) as executor:\n            futures = {executor.submit(sync_run_one_wrapper, i): i for i in range(args.batch)}\n            for future in as_completed(futures):\n                try:\n                    result = future.result() # Get result (list of dicts or empty list)\n                    all_results.extend(result) # Collect results if needed (original gathered them)\n                except Exception as exc:\n                    run_idx = futures[future] + 1\n                    logger.exception(f\"Batch run {run_idx} failed in executor: {exc}\")\n                    printer.print(f'Batch run {run_idx} generated an exception: {exc}', style=\"bold red\")\n\n        # Original final print statement\n        printer.print(f\"Completed {args.batch} runs; file at {args.out}\", style=\"bold green\")\n\n    # Call the synchronous _batch function directly\n    _batch()\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_workflow.py/print",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/run",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/save",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_run",
        "src/criticsearch/abstract_substitution/abs_workflow.py/evaluate",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_fuzzy_replacement",
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_entity_extraction"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_batch",
      "md_content": [
        "**_batch**: The function of _batch is to perform batch processing using asynchronous workflows while managing concurrency, error handling, and saving results.\n\n**parameters**: The parameters of this Function.\n· No parameters.\n\n**Code Description**: \nThe `_batch` function handles batch processing using a combination of asynchronous and synchronous operations. It defines an asynchronous function `run_one(idx: int)`, which is responsible for executing a single batch operation. The function accepts an index `idx`, which is used to identify the specific batch being processed.\n\nThe `run_one` function creates an instance of the `ReverseUpgradeWorkflow` class, passing configuration values such as `max_level` and `max_tries` to its constructor. Depending on the value of the `args.gptsearch` flag, the workflow either runs a GPT search or a default operation. After the operation, the workflow results are saved to the specified output location `args.out`. The `save` method handles the necessary locking to ensure that the results are safely written. If the batch process completes without errors, it returns a list of items in dictionary format.\n\nIn case of an error, the exception is logged and an attempt is made to save any partial results. If saving partial results also fails, an error message is displayed. The function ensures that, even in error conditions, the program continues running by returning an empty list in case of failure.\n\nTo run `run_one` concurrently for multiple batches, the `_batch` function defines a synchronous wrapper `sync_run_one_wrapper(idx: int)`, which calls the asynchronous `run_one` function using `asyncio.run()`. The function then utilizes a `ThreadPoolExecutor` with a number of worker threads determined by the `args.concurrency` value. Each thread executes the synchronous wrapper function, allowing for concurrent execution of batch runs.\n\nThe results from each batch run are collected in the `all_results` list. After all batches are completed, a final message is printed to indicate the number of batches processed and the location of the saved output file.\n\n**Note**: \n- The function utilizes `ThreadPoolExecutor` to handle concurrency, which ensures that multiple batch operations can be executed concurrently without blocking each other.\n- The `asyncio.run()` method is used to execute asynchronous functions in a synchronous context, ensuring compatibility with thread-based execution.\n- Error handling is crucial in this function. It attempts to save partial results even if an error occurs, which ensures that no data is lost during batch processing.\n- The function’s behavior is controlled by external arguments such as `args.batch`, `args.concurrency`, `args.gptsearch`, and `args.out`, which are expected to be provided elsewhere in the codebase.\n\n**Output Example**:\nIf the batch process completes successfully, the following output may be seen:\n```\nBatch Run 1/10\nBatch[1]: saved 50 items.\nBatch Run 2/10\nBatch[2]: saved 45 items.\n...\nCompleted 10 runs; file at /path/to/output/file\n```\nIn the event of an error, the output may look like:\n```\nBatch Run 1/10\nError in batch run 1: Some error message\nBatch[1]: saved 30 partial items after error.\n...\nCompleted 10 runs; file at /path/to/output/file\n```"
      ],
      "code_start_line": 1219,
      "code_end_line": 1268,
      "params": [],
      "have_return": true,
      "code_content": "    def _batch():\n        # Define the async function exactly as it was in the original _batch\n        async def run_one(idx: int):\n            printer.rule(f\"Batch Run {idx+1}/{args.batch}\")\n            wf = ReverseUpgradeWorkflow(max_level=args.max_level, max_tries=args.max_tries)\n            try:\n                # 根据标志选择流程\n                if args.gptsearch:\n                    await wf.gpt_search_run()\n                else:\n                    await wf.run()\n                # 统一保存\n                wf.save(args.out) # Save handles locking\n                count = len(wf.items)\n                printer.print(f\"Batch[{idx+1}]: saved {count} items.\", style=\"bold green\")\n                return [it.to_dict() for it in wf.items] # Original returned items\n            except Exception as e:\n                logger.exception(f\"Error in batch run {idx+1}\")\n                printer.print(f\"Error in batch run {idx+1}: {e}\", style=\"bold red\")\n                 # Try to save partial results even on error\n                try:\n                     wf.save(args.out)\n                     count = len(wf.items)\n                     printer.print(f\"Batch[{idx+1}]: saved {count} partial items after error.\", style=\"yellow\")\n                except Exception as save_e:\n                     logger.exception(f\"Error saving partial results for batch {idx+1}: {save_e}\")\n                     printer.print(f\"Error saving partial results for batch {idx+1}: {save_e}\", style=\"red\")\n                return [] # Original returned empty list on error\n\n        # Synchronous wrapper to call the async run_one using asyncio.run\n        def sync_run_one_wrapper(idx: int):\n            # Each thread needs its own event loop\n            return asyncio.run(run_one(idx))\n\n        # Use ThreadPoolExecutor to run the sync wrappers concurrently\n        all_results = []\n        # CONFIRMED: max_workers uses args.concurrency here\n        with ThreadPoolExecutor(max_workers=args.concurrency) as executor:\n            futures = {executor.submit(sync_run_one_wrapper, i): i for i in range(args.batch)}\n            for future in as_completed(futures):\n                try:\n                    result = future.result() # Get result (list of dicts or empty list)\n                    all_results.extend(result) # Collect results if needed (original gathered them)\n                except Exception as exc:\n                    run_idx = futures[future] + 1\n                    logger.exception(f\"Batch run {run_idx} failed in executor: {exc}\")\n                    printer.print(f'Batch run {run_idx} generated an exception: {exc}', style=\"bold red\")\n\n        # Original final print statement\n        printer.print(f\"Completed {args.batch} runs; file at {args.out}\", style=\"bold green\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "run_one",
      "md_content": [
        "**run_one**: The function of run_one is to execute a single batch run of the ReverseUpgradeWorkflow, processing a question-answering task based on the provided index.\n\n**parameters**: The parameters of this Function.\n· idx: An integer representing the index of the current batch run.\n\n**Code Description**: The run_one function is an asynchronous method designed to handle a single execution of a batch run within the context of the ReverseUpgradeWorkflow. It begins by printing a visual separator that indicates the start of the batch run, formatted with the current index and the total number of batches specified in the arguments.\n\nThe function initializes an instance of the ReverseUpgradeWorkflow class, which is responsible for managing the multi-level workflow for generating and verifying question-answer pairs. The initialization includes parameters for maximum levels and maximum tries, which dictate the depth and attempts for question upgrades.\n\nThe core logic of the function involves a try-except block that attempts to execute the workflow. If the gptsearch flag is set in the arguments, it calls the gpt_search_run method of the workflow instance, which utilizes a GPT model for generating and verifying question-answer pairs. If the gptsearch flag is not set, it defaults to calling the run method, which follows a standard workflow process.\n\nAfter executing the workflow, the function saves the results to the specified output path using the save method of the workflow instance. This method handles the serialization of the generated QAItems into a JSON format, ensuring that the results are stored correctly and can be accessed later.\n\nIn the event of an exception during the workflow execution, the function logs the error and attempts to save any partial results that may have been generated up to that point. This ensures that even in the case of failure, some progress is preserved. The function concludes by returning a list of dictionaries representing the processed QAItems, or an empty list if an error occurred.\n\nThe run_one function is called within the sync_run_one_wrapper function, which is responsible for executing the run_one function in a separate event loop for each thread. This design allows for concurrent execution of multiple batch runs, enhancing the overall efficiency of the workflow processing.\n\n**Note**: When using the run_one function, it is crucial to ensure that the arguments provided, such as max_level and max_tries, are set appropriately to avoid unexpected behavior during execution. Additionally, proper error handling is implemented to manage any exceptions that may arise during the workflow execution.\n\n**Output Example**: A possible return value from the run_one function could be a list of QAItem instances structured as follows:\n```json\n[\n  {\n    \"level\": 0,\n    \"question\": \"What is the capital of France?\",\n    \"answer\": \"Paris\",\n    \"evidence\": [\"Source A\", \"Source B\"],\n    \"strategy\": \"seed\"\n  },\n  {\n    \"level\": 1,\n    \"question\": \"What is the capital of Germany?\",\n    \"answer\": \"Berlin\",\n    \"evidence\": [\"Source C\"],\n    \"strategy\": \"search-based update\"\n  }\n]\n```"
      ],
      "code_start_line": 1221,
      "code_end_line": 1246,
      "params": [
        "idx"
      ],
      "have_return": true,
      "code_content": "        async def run_one(idx: int):\n            printer.rule(f\"Batch Run {idx+1}/{args.batch}\")\n            wf = ReverseUpgradeWorkflow(max_level=args.max_level, max_tries=args.max_tries)\n            try:\n                # 根据标志选择流程\n                if args.gptsearch:\n                    await wf.gpt_search_run()\n                else:\n                    await wf.run()\n                # 统一保存\n                wf.save(args.out) # Save handles locking\n                count = len(wf.items)\n                printer.print(f\"Batch[{idx+1}]: saved {count} items.\", style=\"bold green\")\n                return [it.to_dict() for it in wf.items] # Original returned items\n            except Exception as e:\n                logger.exception(f\"Error in batch run {idx+1}\")\n                printer.print(f\"Error in batch run {idx+1}: {e}\", style=\"bold red\")\n                 # Try to save partial results even on error\n                try:\n                     wf.save(args.out)\n                     count = len(wf.items)\n                     printer.print(f\"Batch[{idx+1}]: saved {count} partial items after error.\", style=\"yellow\")\n                except Exception as save_e:\n                     logger.exception(f\"Error saving partial results for batch {idx+1}: {save_e}\")\n                     printer.print(f\"Error saving partial results for batch {idx+1}: {save_e}\", style=\"red\")\n                return [] # Original returned empty list on error\n",
      "name_column": 18,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/main/_batch/sync_run_one_wrapper"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_workflow.py/QAItem/to_dict",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/run",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/save",
        "src/criticsearch/abstract_substitution/abs_workflow.py/ReverseUpgradeWorkflow/gpt_search_run"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "sync_run_one_wrapper",
      "md_content": [
        "**sync_run_one_wrapper**: The function of sync_run_one_wrapper is to execute the run_one function within an asyncio event loop for a given index.\n\n**parameters**: The parameters of this Function.\n· idx: An integer representing the index of the current batch run.\n\n**Code Description**: The sync_run_one_wrapper function is designed to facilitate the execution of the run_one function in a manner that is compatible with asynchronous programming. It takes a single parameter, idx, which indicates the index of the batch run being processed. This index is crucial for tracking the progress of batch executions.\n\nInside the function, the asyncio.run method is called with the run_one function as its argument, passing the idx parameter. This ensures that each thread that invokes sync_run_one_wrapper operates within its own event loop, allowing for concurrent execution of multiple batch runs. The use of asyncio.run is essential here, as it manages the lifecycle of the event loop, including starting and closing it appropriately.\n\nThe run_one function, which is called by sync_run_one_wrapper, is responsible for executing a single batch run of the ReverseUpgradeWorkflow. It processes a question-answering task based on the provided index. The relationship between sync_run_one_wrapper and run_one is that the former serves as a wrapper to ensure that the latter can be executed asynchronously, thereby enhancing the efficiency of batch processing.\n\nThis design pattern is particularly useful in scenarios where multiple batch runs need to be executed simultaneously, as it allows for better resource utilization and faster overall processing times.\n\n**Note**: When using sync_run_one_wrapper, it is important to ensure that the idx parameter is correctly set to reflect the current batch run index. This will help maintain the integrity of the batch processing workflow.\n\n**Output Example**: The sync_run_one_wrapper function does not return a value directly, as its purpose is to initiate the execution of run_one. However, the run_one function, when executed, may return a list of dictionaries representing processed QAItems, structured as follows:\n```json\n[\n  {\n    \"level\": 0,\n    \"question\": \"What is the capital of France?\",\n    \"answer\": \"Paris\",\n    \"evidence\": [\"Source A\", \"Source B\"],\n    \"strategy\": \"seed\"\n  },\n  {\n    \"level\": 1,\n    \"question\": \"What is the capital of Germany?\",\n    \"answer\": \"Berlin\",\n    \"evidence\": [\"Source C\"],\n    \"strategy\": \"search-based update\"\n  }\n]\n```"
      ],
      "code_start_line": 1249,
      "code_end_line": 1251,
      "params": [
        "idx"
      ],
      "have_return": true,
      "code_content": "        def sync_run_one_wrapper(idx: int):\n            # Each thread needs its own event loop\n            return asyncio.run(run_one(idx))\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/main/_batch/run_one"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_combination_batch",
      "md_content": [
        "**_combination_batch**: The function of _combination_batch is to execute a batch of asynchronous combination tasks concurrently and save the successful results to a specified output file.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The _combination_batch function is designed to manage the execution of multiple asynchronous tasks that are part of a combination workflow. It utilizes a ThreadPoolExecutor to run these tasks concurrently, allowing for efficient processing of a specified number of combination runs, as defined by the `args.combination_batch` parameter.\n\nWithin the function, a nested function named sync_run_one_wrapper is defined, which serves as a wrapper to execute a single combination workflow using the asyncio.run method. This approach ensures that the asynchronous test_combination_workflow function is executed in a synchronous manner, allowing it to be submitted to the ThreadPoolExecutor.\n\nThe function initializes an empty list called successful_results to store the results of successfully executed tasks. It then creates a ThreadPoolExecutor with a maximum number of workers specified by `args.combination_concurrency`. A dictionary comprehension is used to submit tasks to the executor, where each task corresponds to an index in the range of `args.combination_batch`.\n\nAs the futures complete, the function iterates over them using as_completed. For each future, it attempts to retrieve the result. If the result is not None, it is converted to a dictionary using the to_dict method and appended to the successful_results list. If an exception occurs during the execution of a task, it logs the error and prints a message indicating the failure of that specific combination run.\n\nAfter all tasks have been processed, the function checks if there are any successful results. If so, it acquires a file lock to ensure thread-safe access to the output file. It reads any existing results from the file, appends the new successful results, and writes the updated list back to the file in JSON format. The function also handles potential errors during file operations, logging them appropriately.\n\nFinally, the function prints summary messages indicating the total number of combination runs completed, the number of successfully processed items, and the number of failures. It also confirms the location where the results have been saved.\n\nThis function is integral to the overall workflow, as it coordinates the execution of multiple combination tasks and manages the storage of their results, ensuring that the application can efficiently handle batch processing of tasks.\n\n**Note**: It is important to ensure that the output file specified by `args.combination_out` is accessible and writable. Additionally, the successful results must be in a format compatible with JSON serialization, as they will be saved in that format.\n\n**Output Example**: A possible appearance of the output file after successful execution might look like the following JSON structure:\n\n```json\n[\n    {\n        \"name\": \"example_combination_1\",\n        \"value\": 42,\n        \"status\": \"success\"\n    },\n    {\n        \"name\": \"example_combination_2\",\n        \"value\": 36,\n        \"status\": \"success\"\n    }\n]\n```"
      ],
      "code_start_line": 1121,
      "code_end_line": 1167,
      "params": [],
      "have_return": true,
      "code_content": "        def _combination_batch():\n            def sync_run_one_wrapper(idx: int):\n                return asyncio.run(test_combination_workflow(args.combination_out))\n\n            successful_results = []  # 存储成功执行的结果\n\n            with ThreadPoolExecutor(max_workers=args.combination_concurrency) as executor:\n                futures = {executor.submit(sync_run_one_wrapper, i): i \n                          for i in range(args.combination_batch)}\n                for future in as_completed(futures):\n                    try:\n                        result = future.result()  # 获取执行结果\n                        if result is not None:  # 只保存成功的结果\n                            successful_results.append(result.to_dict())\n                    except Exception as exc:\n                        run_idx = futures[future] + 1\n                        logger.exception(f\"Combination run {run_idx} failed: {exc}\")\n                        printer.print(f\"Combination run {run_idx} failed: {exc}\", style=\"bold red\")\n                        continue  # 跳过失败的运行\n\n            # 只有在有成功结果时才更新文件\n            if successful_results:\n                with file_lock:\n                    existing = []\n                    if args.combination_out.exists() and args.combination_out.stat().st_size > 0:\n                        try:\n                            with open(args.combination_out, \"r\", encoding=\"utf-8\") as f:\n                                existing = json.load(f)\n                        except json.JSONDecodeError:\n                            logger.error(f\"Could not decode JSON from {args.combination_out}. Starting fresh.\")\n                        except Exception as e:\n                            logger.exception(f\"Error reading {args.combination_out}\")\n\n                    existing.extend(successful_results)\n                    \n                    try:\n                        with open(args.combination_out, \"w\", encoding=\"utf-8\") as f:\n                            json.dump(existing, f, ensure_ascii=False, indent=2)\n                        printer.print(f\"Successfully saved {len(successful_results)} items to {args.combination_out}\", style=\"bold green\")\n                    except Exception as e:\n                        logger.exception(f\"Error saving to {args.combination_out}\")\n                        printer.print(f\"Error saving results: {e}\", style=\"bold red\")\n\n            printer.print(f\"Completed {args.combination_batch} combination runs\", style=\"bold green\")\n            printer.print(f\"Successfully processed: {len(successful_results)} items\", style=\"bold green\")\n            printer.print(f\"Failed: {args.combination_batch - len(successful_results)} items\", style=\"bold yellow\")\n            printer.print(f\"Results saved to: {args.combination_out}\", style=\"bold green\")\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/abstract_substitution/abs_workflow.py/FuzzyQAItem/to_dict"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "sync_run_one_wrapper",
      "md_content": [
        "**sync_run_one_wrapper**: The function of sync_run_one_wrapper is to execute the test_combination_workflow function within an asynchronous context.\n\n**parameters**: \n· idx: int - An integer parameter used as an index, though it is not directly utilized within the function.\n\n**Code Description**: The `sync_run_one_wrapper` function is a wrapper designed to run the `test_combination_workflow` function in a synchronous manner. It achieves this by utilizing the `asyncio.run()` function to execute the asynchronous `test_combination_workflow` within a synchronous context.\n\nThe function accepts one parameter, `idx`, which is an integer. However, it does not utilize this parameter in its current implementation. The primary purpose of `sync_run_one_wrapper` is to call the `test_combination_workflow` function with the necessary argument (`args.combination_out`), which is presumably a file path where the output from the workflow is saved or logged.\n\nThe `test_combination_workflow` function, which is called within `sync_run_one_wrapper`, is responsible for a series of operations including seed generation, entity extraction, fuzzy replacements, and validation of question-answer pairs, as described in its respective documentation. By using `asyncio.run()`, the wrapper ensures that the asynchronous `test_combination_workflow` is executed in a blocking (synchronous) manner, making it easier to integrate into systems or processes that expect synchronous behavior.\n\nThis wrapper function does not return any value itself. Instead, the result is returned by the `test_combination_workflow` function, which may return a `FuzzyQAItem` or `None` depending on the success or failure of the workflow.\n\n**Note**: The `sync_run_one_wrapper` function expects `args.combination_out` to be a valid file path, as it is passed to `test_combination_workflow` for output purposes. The `idx` parameter, although part of the function signature, is not utilized in the current implementation and may serve as a placeholder for potential future use.\n\n**Output Example**: The return value of the function is dependent on the result of `test_combination_workflow`. If successful, it could return a `FuzzyQAItem` instance such as:\n```json\n{\n  \"original_question\": \"What is the capital of France?\",\n  \"question\": \"What is the capital city of France?\",\n  \"answer\": \"Paris\",\n  \"constrained_format\": \"Provide the answer in one word.\",\n  \"strategy\": \"fuzzy_replacement\",\n  \"evidence\": [\"France is a country in Europe.\", \"Paris is known as the capital of France.\"]\n}\n```\nIf the workflow fails at any step, the function returns `None`."
      ],
      "code_start_line": 1122,
      "code_end_line": 1123,
      "params": [
        "idx"
      ],
      "have_return": true,
      "code_content": "            def sync_run_one_wrapper(idx: int):\n                return asyncio.run(test_combination_workflow(args.combination_out))\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/abstract_substitution/abs_workflow.py/test_combination_workflow"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "src/criticsearch/abstract_substitution/abs_exp_origin.py": [
    {
      "type": "FunctionDef",
      "name": "extract_boxed",
      "md_content": [
        "**extract_boxed**: The function of extract_boxed is to extract and return the content inside the first pair of \\boxed{} from the provided text.\n\n**parameters**: \n- text: str - The input string that potentially contains one or more \\boxed{} pairs. This string is searched for the first occurrence of a \\boxed{} pair.\n\n**Code Description**: The function `extract_boxed` is designed to search for and return the content enclosed within the first occurrence of the LaTeX-style `\\boxed{}` pair in a given string. It utilizes a regular expression, defined by `BOXED_RE`, to locate the first instance of `\\boxed{}` and capture the content inside the braces.\n\nHere is a detailed breakdown of the function:\n1. The input `text` is a string that is passed to the function.\n2. The function then uses the `BOXED_RE.search(text)` method to search for the first match of a `\\boxed{}` pair within the provided string. If a match is found, the content inside the braces is captured and returned after being stripped of leading and trailing whitespace.\n3. If no match is found, the function returns an empty string, indicating that no boxed content was present.\n\nThis function is useful in scenarios where a specific portion of text, typically formatted within a `\\boxed{}` LaTeX construct, needs to be extracted from a larger body of text.\n\nThe function is utilized in the project by two key components: `knowledge_validator` and `search_validator`. In both of these functions, `extract_boxed` is used to extract the relevant boxed content from the answers provided by a model or scraped from search results.\n\n- In `knowledge_validator`, the function is called after receiving the response from an agent in the form of a chat message. The content inside the `\\boxed{}` is extracted to compare it with a known answer to determine whether the model has answered the question correctly.\n- In `search_validator`, the function is used similarly to extract the boxed value from a model's response after providing context from a web search.\n\n**Note**: The regular expression `BOXED_RE` used in the function is a critical component that dictates how the boxed content is identified. Ensure that this regular expression is correctly defined to handle various cases of LaTeX-style boxed expressions in the input text.\n\n**Output Example**:\n- If the input text is `The result is \\boxed{42}`, the function will return `42`.\n- If the input text is `No boxed value here`, the function will return an empty string."
      ],
      "code_start_line": 49,
      "code_end_line": 52,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def extract_boxed(text: str) -> str:\n    \"\"\"Return content inside first \\boxed{} pair or empty string.\"\"\"\n    m = BOXED_RE.search(text)\n    return m.group(1).strip() if m else \"\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/knowledge_validator",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/search_validator"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_answer_tag",
      "md_content": [
        "**extract_answer_tag**: The function of extract_answer_tag is to extract and return the raw content from the first `<answer>...</answer>` block found in the provided text.\n\n**parameters**:\n- text: A string containing the content from which the `<answer>...</answer>` block will be extracted.\n\n**Code Description**: \nThe function `extract_answer_tag` is responsible for searching and extracting the raw content from the first occurrence of an `<answer>...</answer>` block in a given string. It uses the `ANSWER_TAG_RE` regular expression to search for this block within the provided `text` parameter. If a match is found, the function returns the content inside the `<answer>` tags, stripped of any leading or trailing whitespace. If no match is found, it returns an empty string.\n\nInternally, the function works as follows:\n1. It calls `ANSWER_TAG_RE.search(text)` to search the input text for a match to the regular expression pattern designed to capture the content between the `<answer>` tags.\n2. If a match (`m`) is found, the method `m.group(1)` is used to retrieve the content inside the `<answer>` tags.\n3. The content is then stripped of any surrounding whitespace using the `strip()` method before being returned.\n4. If no match is found (i.e., the search does not find any `<answer>` block), an empty string is returned.\n\nThis function is primarily used in contexts where an answer is provided within a specific HTML-like structure, and the goal is to extract only the content of the first `<answer>` tag.\n\nIn the context of its callers:\n1. **knowledge_validator**: This function is called within `knowledge_validator` to extract the raw content from the model's response. The `text` provided to `extract_answer_tag` in this case is the reply from the model, which is expected to contain an `<answer>...</answer>` block. The extracted answer is then further processed (with `extract_boxed`) to obtain a clean, boxed answer. This is used to compare against the expected answer and validate the correctness of the model's response.\n  \n2. **search_validator**: Similarly, in `search_validator`, the function is used to extract the answer from a model's response generated after the search results are provided as context. The reply from the model contains an answer inside an `<answer>...</answer>` block, which is extracted by `extract_answer_tag`. The extracted answer is then passed through `extract_boxed` before being validated against the expected answer.\n\n**Note**: \n- The regular expression `ANSWER_TAG_RE` must be properly defined to accurately capture the `<answer>` tags and their content. Any changes in the pattern could lead to incorrect extractions.\n- The function assumes that there is at least one `<answer>` tag in the provided text, and if not, it will return an empty string, which could affect downstream processing if not handled properly.\n\n**Output Example**: \nIf the input `text` is:\n```html\n<response><answer>42</answer></response>\n```\nThe function will return:\n```\n42\n```"
      ],
      "code_start_line": 55,
      "code_end_line": 58,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def extract_answer_tag(text: str) -> str:\n    \"\"\"Return raw content of first <answer>…</answer> block.\"\"\"\n    m = ANSWER_TAG_RE.search(text)\n    return m.group(1).strip() if m else \"\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/knowledge_validator",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/search_validator"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_thought",
      "md_content": [
        "**extract_thought**: The function of extract_thought is to extract a specific thought from a given text based on a predefined pattern.\n\n**parameters**: The parameters of this Function.\n· text: A string input that contains the text from which the thought needs to be extracted.\n\n**Code Description**: The extract_thought function utilizes a regular expression search to identify and extract a specific segment of text that matches the pattern defined by the constant THOUGHT_TAG_RE. The function takes a single parameter, text, which is expected to be a string. It applies the search method of the regular expression object to the input text. If a match is found, the function retrieves the first capturing group from the match object using m.group(1), which represents the desired thought. The extracted thought is then stripped of any leading or trailing whitespace using the strip() method. If no match is found, the function returns an empty string. This ensures that the function always returns a string, either containing the extracted thought or an empty string if no thought is present in the input text.\n\n**Note**: It is important to ensure that the input text is formatted correctly to match the expected pattern defined by THOUGHT_TAG_RE. If the pattern does not match, the function will return an empty string, which may not indicate an error but rather the absence of a matching thought.\n\n**Output Example**: If the input text is \"Here is a thought: [This is the thought I want to extract].\", and assuming THOUGHT_TAG_RE is designed to capture the content within the brackets, the function would return \"This is the thought I want to extract\". If the input text does not contain any matching pattern, such as \"No thoughts here.\", the function would return an empty string."
      ],
      "code_start_line": 61,
      "code_end_line": 63,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def extract_thought(text: str) -> str:\n    m = THOUGHT_TAG_RE.search(text)\n    return m.group(1).strip() if m else \"\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "QAItem",
      "md_content": [
        "**QAItem**: The function of QAItem is to represent a question-answer item with associated metadata for use in a question upgrading workflow.\n\n**attributes**: The attributes of this Class.\n· level: int - Represents the difficulty level of the question, where 0 indicates a seed question and levels 1 to 5 indicate progressively upgraded questions.  \n· question: str - The text of the question.  \n· answer: str - The correct answer to the question, referred to as the gold answer.  \n· parent_question: Optional[str] - The question from which this item was derived, if applicable.  \n· support_urls: List[str] - A list of URLs that provide additional support or verification for the answer.  \n· generated_by_model: str - The identifier of the model that generated this QAItem.\n\n**Code Description**: The QAItem class serves as a structured representation of a question and its corresponding answer, along with metadata that provides context about the question's difficulty and its origins. The class includes a method, `to_dict`, which converts the instance into a dictionary format. This method ensures that the `support_urls` attribute is returned as a list, facilitating easier serialization and data handling.\n\nThe QAItem class is utilized within the ReverseUpgradeWorkflow module, specifically in the methods `generate_seed` and `upgrade_question`. In `generate_seed`, a new QAItem is created as a seed question by invoking an external agent to generate a unique question and answer pair. This method initializes a QAItem with a level of 0, indicating that it is a foundational question. \n\nIn the `upgrade_question` method, an existing QAItem is passed as an argument, and a new QAItem is generated based on the original question. This method increases the level of the question by one, indicating that it is a more complex version of the original. The answer remains unchanged, ensuring that the new question retains the same definitive answer as its predecessor. The `parent_question` attribute is populated with the original question, establishing a clear lineage between the two items.\n\nOverall, the QAItem class is integral to the workflow of generating and upgrading questions, providing a consistent structure for managing question-answer pairs and their associated metadata.\n\n**Note**: When using the QAItem class, it is important to ensure that the `support_urls` attribute is properly populated to maintain the integrity of the information provided.\n\n**Output Example**: A possible appearance of the code's return value when calling `to_dict` on a QAItem instance might look like this:\n```json\n{\n    \"level\": 0,\n    \"question\": \"What is the capital of France?\",\n    \"answer\": \"Paris\",\n    \"parent_question\": null,\n    \"support_urls\": [\"https://en.wikipedia.org/wiki/Paris\"],\n    \"generated_by_model\": \"gpt-4o\"\n}\n```"
      ],
      "code_start_line": 70,
      "code_end_line": 81,
      "params": [],
      "have_return": true,
      "code_content": "class QAItem:\n    level: int  # 0 = seed, 1‑5 upgraded\n    question: str\n    answer: str  # gold answer\n    parent_question: Optional[str]\n    support_urls: List[str]\n    generated_by_model: str\n\n    def to_dict(self):\n        d = asdict(self)\n        d[\"support_urls\"] = list(self.support_urls)\n        return d\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/__init__",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/upgrade_question"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "to_dict",
      "md_content": [
        "**to_dict**: The function of to_dict is to convert the instance of the class into a dictionary representation.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The to_dict function is designed to create a dictionary representation of the instance it is called on. It utilizes the `asdict` function, which is part of the `dataclasses` module, to convert the instance's attributes into a dictionary format. After this conversion, the function adds an additional key-value pair to the dictionary: it includes a list of support URLs by accessing the `support_urls` attribute of the instance. This ensures that the output dictionary contains all relevant information about the instance, including the support URLs, which may be critical for further processing or serialization.\n\nThe to_dict function is called within the export method of the ReverseUpgradeWorkflow class. In this context, the export method iterates over a collection of items (presumably instances of the same class or a related class) and calls the to_dict function on each item. The resulting dictionaries are then serialized into a JSON format and written to a specified file path. This demonstrates the utility of the to_dict function in transforming instance data into a format suitable for storage or transmission, thereby facilitating the export of structured data.\n\n**Note**: It is important to ensure that the support_urls attribute is properly initialized and contains valid data before calling this function, as it directly influences the output of the dictionary.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```json\n{\n    \"attribute1\": \"value1\",\n    \"attribute2\": \"value2\",\n    \"support_urls\": [\"http://example.com/support1\", \"http://example.com/support2\"]\n}\n```"
      ],
      "code_start_line": 78,
      "code_end_line": 81,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def to_dict(self):\n        d = asdict(self)\n        d[\"support_urls\"] = list(self.support_urls)\n        return d\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/export"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "ReverseUpgradeWorkflow",
      "md_content": [
        "**ReverseUpgradeWorkflow**: The function of ReverseUpgradeWorkflow is to generate and validate progressively more difficult questions based on an initial seed fact, utilizing a language model and web search capabilities.\n\n**attributes**: The attributes of this Class.\n· agent: An instance of BaseAgent used to interact with the language model for generating questions and validating answers.\n· model: A string representing the model version to be used, defaulting to \"gpt-4o\".\n· max_level: An integer indicating the maximum number of levels of question difficulty to generate, defaulting to 5.\n· max_tries: An integer specifying the maximum number of attempts to generate a valid question at each level, defaulting to 5.\n· search_aggregator: An instance of SearchAggregator used for conducting web searches to validate questions.\n· scraper: An instance of TavilyExtract used to extract content from web pages.\n· items: A list of QAItem instances that stores the generated questions and answers.\n\n**Code Description**: The ReverseUpgradeWorkflow class is designed to facilitate the generation of a series of progressively more challenging questions based on an initial seed fact. It begins by generating a seed question using the generate_seed method, which prompts the language model to provide a unique and verifiable fact. This seed question is then stored as the first item in the items list.\n\nThe class employs an iterative process to create harder versions of the initial question through the upgrade_question method. This method takes a previous question (QAItem) and generates a new question that maintains the same definitive answer but increases in difficulty. The process of upgrading continues until the maximum specified level is reached or until the maximum number of attempts to generate a valid question at each level is exhausted.\n\nValidation of the generated questions is performed using two methods: knowledge_validator and search_validator. The knowledge_validator checks if the model can answer the generated question correctly, while the search_validator ensures that the question is not easily answerable through a simple web search. If both validations fail, the new question is accepted and added to the items list.\n\nThe run method orchestrates the entire workflow, managing the generation and validation of questions in a loop until the maximum level is reached or no valid questions can be produced. Finally, the export method allows the user to save the generated questions and answers to a specified JSON file.\n\nThis class is called within the cli function, which serves as the command-line interface for executing the workflow. It initializes the ReverseUpgradeWorkflow with parameters for output file path, maximum levels, and maximum tries, and then runs the workflow asynchronously. The results are exported to a JSON file, providing a structured output of the generated QA levels.\n\n**Note**: It is important to ensure that the BaseAgent and other dependencies are correctly configured before running the workflow, as they are integral to the generation and validation processes.\n\n**Output Example**: A possible appearance of the code's return value could be as follows:\n```json\n[\n  {\n    \"level\": 0,\n    \"question\": \"What is the significance of the Turing Test in artificial intelligence?\",\n    \"answer\": \"The Turing Test is a measure of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human.\",\n    \"parent_question\": null,\n    \"support_urls\": [\"https://en.wikipedia.org/wiki/Turing_test\"],\n    \"generated_by_model\": \"gpt-4o\"\n  },\n  {\n    \"level\": 1,\n    \"question\": \"How does the Turing Test challenge the definition of intelligence?\",\n    \"answer\": \"The Turing Test is a measure of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human.\",\n    \"parent_question\": \"What is the significance of the Turing Test in artificial intelligence?\",\n    \"support_urls\": [\"https://en.wikipedia.org/wiki/Turing_test\"],\n    \"generated_by_model\": \"gpt-4o\"\n  }\n]\n```"
      ],
      "code_start_line": 87,
      "code_end_line": 227,
      "params": [],
      "have_return": true,
      "code_content": "class ReverseUpgradeWorkflow:\n    def __init__(\n        self,\n        agent: BaseAgent,\n        *,\n        model: str = \"gpt-4o\",\n        max_level: int = 5,\n        max_tries: int = 5,\n    ) -> None:\n        self.agent = agent\n        self.model = model\n        self.max_level = max_level\n        self.max_tries = max_tries\n        self.search_aggregator = SearchAggregator()\n        self.scraper = TavilyExtract(TAVILY_API_KEY)\n        self.items: List[QAItem] = []\n\n    # ------------------------------------------------------------------\n    # Step 0 – get a seed fact\n    # ------------------------------------------------------------------\n    async def generate_seed(self) -> QAItem:\n        seed_prompt = (\n            \"You are an expert benchmark designer. Pick **one** field you find fascinating and \"\n            \"provide *one* DIFFICULT seed fact that cannot be answered by simply googling the fact \"\n            \"verbatim, yet the answer is unique and verifiable from the web.  \"\n            \"Return **ONLY** a JSON object with keys: question (string), answer (string), \"\n            \"support_urls (array of URLs).\"\n        )\n        response = self.agent.chat(usr_prompt=seed_prompt, model=self.model)\n        payload = self._safe_json(response)\n        return QAItem(\n            level=0,\n            question=payload[\"question\"].strip(),\n            answer=payload[\"answer\"].strip(),\n            parent_question=None,\n            support_urls=payload.get(\"support_urls\", []),\n            generated_by_model=self.model,\n        )\n\n    # ------------------------------------------------------------------\n    # Produce one harder version of *prev_item*\n    # ------------------------------------------------------------------\n    async def upgrade_question(self, prev_item: QAItem) -> QAItem:\n        ug_prompt = (\n            \"You are improving benchmark difficulty by *reverse‑upgrading* the question below.\\n\\n\"\n            f\"ORIGINAL_QUESTION:\\n{prev_item.question}\\n\\n\"\n            \"Perform **one** of: equivalent replacement, simple abstraction, OR complex abstraction + clarification, \"\n            \"as defined in the spec.  The new question MUST still have the SAME definitive answer\\n\"\n            f\"(which is: {prev_item.answer!r}).\\n\"\n            \"Return **only** valid JSON with keys: new_question (string), support_urls (array, can reuse), \"\n            \"strategy (string).\"\n        )\n        response = self.agent.chat(usr_prompt=ug_prompt, model=self.model)\n        payload = self._safe_json(response)\n        return QAItem(\n            level=prev_item.level + 1,\n            question=payload[\"new_question\"].strip(),\n            answer=prev_item.answer,  # answer unchanged\n            parent_question=prev_item.question,\n            support_urls=payload.get(\"support_urls\", prev_item.support_urls),\n            generated_by_model=self.model,\n        )\n\n    # ------------------------------------------------------------------\n    # Validation helpers\n    # ------------------------------------------------------------------\n    async def knowledge_validator(self, question: str) -> bool:\n        \"\"\"True if model answers *correctly*.\"\"\"\n        k_prompt = (\n            f\"<question>{question}</question>\\n\"\n            \"Answer ONLY with: <answer>…</answer>, with the value wrapped in \\\\boxed{}.\")\n        reply = self.agent.chat(usr_prompt=k_prompt, model=self.model)\n        ans_raw = extract_answer_tag(reply)\n        predicted = extract_boxed(ans_raw)\n        return predicted.strip() == self.items[0].answer  # gold in first item\n\n    async def search_validator(self, question: str) -> bool:\n        # use *one* search query (same as question) then LLM answer\n        # 1. search\n        search_results = await self.search_aggregator.search(query=[question])\n        urls = [r.url for r in search_results[:3]]  # take a few\n        # 2. grab content\n        scraped = await self.scraper.extract_content(urls)\n        corpus = \"\\n\\n\".join(s.get(\"raw_content\", \"\") for s in scraped.values())\n        # 3. ask LLM *with* context, forcing single turn\n        s_prompt = (\n            f\"Refer ONLY to the context below (from at most one web search).\\n\"\n            \"<context>\\n\" + corpus[:4000] + \"\\n</context>\\n\"  # truncate if needed\n            f\"<question>{question}</question>\\nAnswer with boxed value.\")\n        reply = self.agent.chat(usr_prompt=s_prompt, model=self.model)\n        predicted = extract_boxed(extract_answer_tag(reply))\n        return predicted.strip() == self.items[0].answer\n\n    # ------------------------------------------------------------------\n    # Orchestration\n    # ------------------------------------------------------------------\n    async def run(self):\n        # Seed phase\n        seed_item = await self.generate_seed()\n        self.items.append(seed_item)\n        current = seed_item\n\n        # iterative reverse‑upgrade\n        for _ in range(self.max_level):\n            tries = 0\n            success = False\n            while tries < self.max_tries and not success:\n                tries += 1\n                candidate = await self.upgrade_question(current)\n                # 1st validator\n                if await self.knowledge_validator(candidate.question):\n                    continue  # knowledge too easy\n                # 2nd validator\n                if await self.search_validator(candidate.question):\n                    continue  # one‑search too easy\n                # both failed ⇒ accept\n                self.items.append(candidate)\n                current = candidate\n                success = True\n            if not success:\n                print(f\"Level {current.level+1} failed after {self.max_tries} retries; terminating trace.\")\n                break\n\n    # ------------------------------------------------------------------\n    def export(self, path: Path):\n        with path.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump([it.to_dict() for it in self.items], f, ensure_ascii=False, indent=2)\n\n    # ------------------------------------------------------------------\n    @staticmethod\n    def _safe_json(text: str) -> Dict[str, Any]:\n        \"\"\"Extract JSON from raw LLM text (handles fenced blocks).\"\"\"\n        fence = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)```\", text)\n        raw = fence.group(1) if fence else text\n        raw = raw.strip()\n        try:\n            return json.loads(raw)\n        except json.JSONDecodeError:\n            # fallback: strip backticks and retry\n            raw2 = raw.replace(\"```\", \"\").strip()\n            return json.loads(raw2)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/cli"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the ReverseUpgradeWorkflow class, setting up the necessary parameters and components for the workflow.\n\n**parameters**: The parameters of this Function.\n· agent: BaseAgent - An instance of the BaseAgent class that will be used to manage interactions and operations within the workflow.  \n· model: str - An optional string parameter that specifies the model to be used, defaulting to \"gpt-4o\".  \n· max_level: int - An optional integer parameter that defines the maximum level of question upgrades, defaulting to 5.  \n· max_tries: int - An optional integer parameter that sets the maximum number of attempts for certain operations, defaulting to 5.\n\n**Code Description**: The __init__ method serves as the constructor for the ReverseUpgradeWorkflow class. It initializes the instance by accepting an agent of type BaseAgent, which is crucial for managing the workflow's operations. The method also allows for optional parameters such as model, max_level, and max_tries, which provide flexibility in configuring the workflow's behavior.\n\nUpon initialization, the method assigns the provided agent to the instance variable self.agent. It also sets the model, max_level, and max_tries attributes based on the provided arguments or their default values. This setup ensures that the workflow can operate with the specified model and constraints.\n\nAdditionally, the constructor initializes two important components: self.search_aggregator and self.scraper. The search_aggregator is an instance of the SearchAggregator class, which is responsible for managing and executing search queries across multiple search engines. The scraper is an instance of the TavilyExtract class, which interacts with the Tavily API to extract content from URLs. The scraper is initialized with a predefined API key, TAVILY_API_KEY, ensuring that it can authenticate requests to the Tavily API.\n\nThe items attribute is also initialized as an empty list, which is intended to hold instances of QAItem. This list will be populated as the workflow progresses, allowing for the management of question-answer pairs throughout the upgrading process.\n\nThe relationship with its callees in the project is significant, as the ReverseUpgradeWorkflow class relies on the functionalities provided by the BaseAgent, SearchAggregator, and TavilyExtract classes. The BaseAgent facilitates interactions and manages the conversation history, while the SearchAggregator enables the execution of search queries. The TavilyExtract class provides the capability to scrape content from the web, which is essential for enhancing the quality of the questions and answers being processed.\n\n**Note**: It is important to ensure that the TAVILY_API_KEY is correctly set and accessible for the TavilyExtract instance to function properly. Additionally, the max_level and max_tries parameters should be configured according to the specific requirements of the workflow to optimize its performance."
      ],
      "code_start_line": 88,
      "code_end_line": 102,
      "params": [
        "self",
        "agent"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        agent: BaseAgent,\n        *,\n        model: str = \"gpt-4o\",\n        max_level: int = 5,\n        max_tries: int = 5,\n    ) -> None:\n        self.agent = agent\n        self.model = model\n        self.max_level = max_level\n        self.max_tries = max_tries\n        self.search_aggregator = SearchAggregator()\n        self.scraper = TavilyExtract(TAVILY_API_KEY)\n        self.items: List[QAItem] = []\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/QAItem",
        "src/criticsearch/tools/content_scraper/tavily_extract.py/TavilyExtract",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "generate_seed",
      "md_content": [
        "**generate_seed**: The function of generate_seed is to asynchronously generate a seed question and answer pair, encapsulated within a QAItem object.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the class in which the function is defined.\n\n**Code Description**: The generate_seed function is an asynchronous method designed to create a seed question and its corresponding answer by interacting with a conversational model. The function begins by defining a prompt, which instructs the model to select a fascinating field and provide a unique, difficult fact that cannot be easily found through a simple search. This prompt is structured to elicit a response that is both verifiable and informative.\n\nThe function then calls the chat method of the agent, passing the seed prompt and the model to generate a response. The chat method is responsible for facilitating the interaction with the conversational model, processing the user prompt, and returning a structured response. The response is expected to be in JSON format, containing keys for the question, answer, and support URLs.\n\nOnce the response is received, the function utilizes the _safe_json method to parse the response safely, ensuring that any potential formatting issues are handled appropriately. The parsed data is then used to create a new instance of the QAItem class, which encapsulates the generated question, answer, and associated metadata such as the difficulty level and the model that produced it.\n\nThe generate_seed function is called within the run method of the ReverseUpgradeWorkflow class. In this context, it serves as the initial step in a workflow that aims to generate a series of progressively upgraded questions. The seed item generated by this function is appended to the items list, establishing the foundation for subsequent iterations of question upgrading.\n\nOverall, the generate_seed function plays a crucial role in initiating the question generation process, ensuring that the first question is both challenging and unique, setting the stage for further enhancements in the workflow.\n\n**Note**: It is important to ensure that the conversational model is properly configured and capable of generating relevant responses based on the provided prompt. Additionally, the structure of the response must adhere to the expected JSON format to facilitate successful parsing.\n\n**Output Example**: A possible appearance of the code's return value when calling generate_seed might look like this:\n```json\n{\n    \"level\": 0,\n    \"question\": \"What is the significance of the Fermi Paradox in modern astrophysics?\",\n    \"answer\": \"The Fermi Paradox highlights the contradiction between the high probability of extraterrestrial life and the lack of evidence for, or contact with, such civilizations.\",\n    \"parent_question\": null,\n    \"support_urls\": [\"https://en.wikipedia.org/wiki/Fermi_paradox\"],\n    \"generated_by_model\": \"gpt-4o\"\n}\n```"
      ],
      "code_start_line": 107,
      "code_end_line": 124,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    async def generate_seed(self) -> QAItem:\n        seed_prompt = (\n            \"You are an expert benchmark designer. Pick **one** field you find fascinating and \"\n            \"provide *one* DIFFICULT seed fact that cannot be answered by simply googling the fact \"\n            \"verbatim, yet the answer is unique and verifiable from the web.  \"\n            \"Return **ONLY** a JSON object with keys: question (string), answer (string), \"\n            \"support_urls (array of URLs).\"\n        )\n        response = self.agent.chat(usr_prompt=seed_prompt, model=self.model)\n        payload = self._safe_json(response)\n        return QAItem(\n            level=0,\n            question=payload[\"question\"].strip(),\n            answer=payload[\"answer\"].strip(),\n            parent_question=None,\n            support_urls=payload.get(\"support_urls\", []),\n            generated_by_model=self.model,\n        )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/QAItem",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/_safe_json"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        true,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "upgrade_question",
      "md_content": [
        "## Function: `upgrade_question`\n\n### Overview\nThe `upgrade_question` function is an asynchronous method that improves the difficulty of a benchmark question by applying a reverse-upgrade technique. It modifies a given question while maintaining the same definitive answer, using strategies defined in the specification.\n\n### Parameters\n- **prev_item** (`QAItem`): This parameter represents the previous question item, which includes the original question, the answer, and related metadata such as support URLs and question level.\n\n### Returns\n- **QAItem**: The function returns a new `QAItem` object with the following fields:\n  - `level`: The level of the new question is incremented by 1 from the previous item's level.\n  - `question`: A new question generated by the function, modified based on one of the defined strategies (equivalent replacement, simple abstraction, or complex abstraction + clarification).\n  - `answer`: The answer remains unchanged from the previous item.\n  - `parent_question`: The original question from `prev_item`.\n  - `support_urls`: The support URLs from the generated response or the original item's support URLs if not provided.\n  - `generated_by_model`: The model that generated the new question.\n\n### Functionality\n1. **Input Preparation**: The function constructs a prompt (`ug_prompt`) that instructs the conversational model to reverse-upgrade the question. The prompt includes the original question and its answer from the `prev_item` and specifies the task of applying one of the defined strategies to the question.\n   \n2. **Model Interaction**: The function sends the prompt to the conversational model through the `chat` method. This interaction facilitates the generation of a new question that adheres to the specified upgrading strategies.\n\n3. **Response Processing**: Upon receiving the model's response, the function processes the output, ensuring that the response is in valid JSON format. It extracts the new question and support URLs from the response, while maintaining the original answer and other metadata.\n\n4. **Return**: The function returns a new `QAItem` that encapsulates the upgraded question, its associated answer, and other relevant metadata, thus facilitating the next step in the workflow.\n\n### Usage\nThe `upgrade_question` function is typically used within workflows where questions need to be refined or upgraded without altering the underlying answer. It is particularly useful in scenarios where the complexity of questions needs to be increased while ensuring consistency in the answer provided.\n\n### Example\n```python\n# Assuming prev_item is a QAItem with a defined question and answer\nnew_qa_item = upgrade_question(prev_item)\n```\n\nIn this example, the `upgrade_question` function takes the `prev_item` (which contains the original question and answer) and returns a new `QAItem` with an upgraded question, maintaining the same answer."
      ],
      "code_start_line": 129,
      "code_end_line": 148,
      "params": [
        "self",
        "prev_item"
      ],
      "have_return": true,
      "code_content": "    async def upgrade_question(self, prev_item: QAItem) -> QAItem:\n        ug_prompt = (\n            \"You are improving benchmark difficulty by *reverse‑upgrading* the question below.\\n\\n\"\n            f\"ORIGINAL_QUESTION:\\n{prev_item.question}\\n\\n\"\n            \"Perform **one** of: equivalent replacement, simple abstraction, OR complex abstraction + clarification, \"\n            \"as defined in the spec.  The new question MUST still have the SAME definitive answer\\n\"\n            f\"(which is: {prev_item.answer!r}).\\n\"\n            \"Return **only** valid JSON with keys: new_question (string), support_urls (array, can reuse), \"\n            \"strategy (string).\"\n        )\n        response = self.agent.chat(usr_prompt=ug_prompt, model=self.model)\n        payload = self._safe_json(response)\n        return QAItem(\n            level=prev_item.level + 1,\n            question=payload[\"new_question\"].strip(),\n            answer=prev_item.answer,  # answer unchanged\n            parent_question=prev_item.question,\n            support_urls=payload.get(\"support_urls\", prev_item.support_urls),\n            generated_by_model=self.model,\n        )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/QAItem",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/_safe_json"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        true,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "knowledge_validator",
      "md_content": [
        "### Function Documentation: `knowledge_validator`\n\n#### Overview:\nThe `knowledge_validator` function is an asynchronous method designed to validate whether a model provides the correct answer to a given question. The function evaluates the accuracy of the model's response by comparing the predicted answer with a predefined correct answer.\n\n#### Parameters:\n- **question** (`str`): The question to be asked to the model. It is expected to be in the form of a string.\n\n#### Return Value:\n- **bool**: The function returns `True` if the model's response matches the correct answer (gold answer) stored in the first item of the `items` list. Otherwise, it returns `False`.\n\n#### Functionality:\n1. **Prompt Construction**: The function begins by constructing a prompt string that incorporates the provided `question`. This prompt is formatted to instruct the model to provide its answer within a specific XML-like structure. \n   \n   The prompt format is as follows:\n   ```\n   <question>{question}</question>\\n\n   Answer ONLY with: <answer>…</answer>, with the value wrapped in \\\\boxed{}\n   ```\n\n2. **Model Interaction**: The function then invokes the `chat` method of the `agent` object, passing the constructed prompt (`usr_prompt=k_prompt`) along with the model (`model=self.model`) for generating a response.\n\n3. **Response Processing**: The response from the model is processed to extract the answer. Specifically, the function uses two utility functions:\n   - `extract_answer_tag`: Extracts the raw content within the `<answer>` tags.\n   - `extract_boxed`: Extracts the boxed answer (if any) from the raw response.\n\n4. **Comparison**: The stripped, boxed answer is compared with the correct answer stored in the first item of the `items` list (`self.items[0].answer`). If the predicted answer matches the correct answer exactly, the function returns `True`; otherwise, it returns `False`.\n\n#### Example Usage:\n```python\nvalidator = knowledge_validator(question=\"What is the capital of France?\")\nis_correct = await validator.validate()\nif is_correct:\n    print(\"The model answered correctly.\")\nelse:\n    print(\"The model answered incorrectly.\")\n```\n\n#### Notes:\n- This function requires the presence of an active `agent` object with a configured `model` to interact with the conversational model.\n- The `items` list should contain the correct answer for comparison, which is stored in the first item (`self.items[0].answer`).\n- The function expects the model’s response to be wrapped in specific tags (`<answer>` and `\\boxed{}`). If the format deviates, the validation may fail.\n\n"
      ],
      "code_start_line": 153,
      "code_end_line": 161,
      "params": [
        "self",
        "question"
      ],
      "have_return": true,
      "code_content": "    async def knowledge_validator(self, question: str) -> bool:\n        \"\"\"True if model answers *correctly*.\"\"\"\n        k_prompt = (\n            f\"<question>{question}</question>\\n\"\n            \"Answer ONLY with: <answer>…</answer>, with the value wrapped in \\\\boxed{}.\")\n        reply = self.agent.chat(usr_prompt=k_prompt, model=self.model)\n        ans_raw = extract_answer_tag(reply)\n        predicted = extract_boxed(ans_raw)\n        return predicted.strip() == self.items[0].answer  # gold in first item\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/extract_boxed",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/extract_answer_tag"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "search_validator",
      "md_content": [
        "**search_validator**: The function of search_validator is to validate the correctness of a model's answer to a given question based on search results.\n\n**parameters**: The parameters of this Function.\n· question: str - The input question that needs to be validated against the model's response.\n\n**Code Description**: The search_validator function is an asynchronous method that performs a series of operations to validate whether a model can accurately answer a given question based on context derived from web search results. \n\nThe function begins by executing a search query using the provided question. It utilizes the search_aggregator's search method, which is designed to handle multiple search queries concurrently. The search results are limited to the top three URLs for efficiency. \n\nNext, the function proceeds to scrape the content from these URLs using the scraper's extract_content method. This method sends requests to the specified URLs and retrieves the raw content, which is then combined into a single corpus. The corpus is truncated to 4000 characters to ensure it fits within the constraints of the language model's input.\n\nFollowing the content extraction, the function constructs a prompt for the language model (LLM) that includes the context from the scraped content and the original question. The prompt is formatted to instruct the model to refer only to the provided context and to respond with a boxed value.\n\nThe function then calls the agent's chat method, passing the constructed prompt along with the specified model. The chat method facilitates the interaction with the language model and returns a response. The response is processed to extract the boxed answer using the extract_answer_tag and extract_boxed functions. \n\nFinally, the function compares the extracted answer with the expected answer stored in self.items[0].answer. If they match, the function returns True, indicating that the model's response is valid; otherwise, it returns False.\n\nThe search_validator function is called within the run method of the ReverseUpgradeWorkflow class. In this context, it serves as a second validation step after the knowledge_validator. If both validators fail, the candidate question is accepted for further processing. This highlights the function's role in ensuring the robustness of the model's responses by leveraging real-time search capabilities.\n\n**Note**: It is crucial to ensure that the question parameter is well-formed and relevant to the context of the search. Additionally, the function relies on the availability of search engines and the proper functioning of the scraper to retrieve content effectively.\n\n**Output Example**: A possible return value from the search_validator function could be:\nTrue or False, depending on whether the model's response matches the expected answer."
      ],
      "code_start_line": 163,
      "code_end_line": 178,
      "params": [
        "self",
        "question"
      ],
      "have_return": true,
      "code_content": "    async def search_validator(self, question: str) -> bool:\n        # use *one* search query (same as question) then LLM answer\n        # 1. search\n        search_results = await self.search_aggregator.search(query=[question])\n        urls = [r.url for r in search_results[:3]]  # take a few\n        # 2. grab content\n        scraped = await self.scraper.extract_content(urls)\n        corpus = \"\\n\\n\".join(s.get(\"raw_content\", \"\") for s in scraped.values())\n        # 3. ask LLM *with* context, forcing single turn\n        s_prompt = (\n            f\"Refer ONLY to the context below (from at most one web search).\\n\"\n            \"<context>\\n\" + corpus[:4000] + \"\\n</context>\\n\"  # truncate if needed\n            f\"<question>{question}</question>\\nAnswer with boxed value.\")\n        reply = self.agent.chat(usr_prompt=s_prompt, model=self.model)\n        predicted = extract_boxed(extract_answer_tag(reply))\n        return predicted.strip() == self.items[0].answer\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/run"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/extract_boxed",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/extract_answer_tag",
        "src/criticsearch/tools/content_scraper/tavily_extract.py/TavilyExtract/extract_content",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "run",
      "md_content": [
        "**run**: The function of run is to execute the reverse-upgrade workflow, generating a series of progressively difficult questions based on an initial seed question.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the class in which the function is defined.\n\n**Code Description**: The run function is an asynchronous method that orchestrates the reverse-upgrade process for generating benchmark questions. It begins by generating a seed question using the generate_seed method, which creates an initial question and answer pair encapsulated within a QAItem object. This seed item is then appended to the items list, establishing the foundation for the iterative upgrading process.\n\nThe function enters a loop that runs for a maximum number of levels defined by the max_level attribute. Within this loop, it attempts to upgrade the current question iteratively. For each level, it initializes a retry mechanism, allowing a specified number of attempts (max_tries) to generate a valid upgraded question. \n\nDuring each attempt, the function calls the upgrade_question method, passing the current question as a parameter. The upgraded question is validated through two separate validators: the knowledge_validator and the search_validator. The knowledge_validator checks if the model's answer to the upgraded question is correct based on predefined criteria, while the search_validator assesses the answer's validity against real-time search results.\n\nIf either validator determines that the upgraded question is too easy, the function continues to the next attempt without accepting the candidate. If both validators fail, the candidate question is accepted, appended to the items list, and becomes the current question for the next iteration. If the maximum number of attempts is reached without a successful upgrade, the function prints a message indicating the failure and terminates the trace.\n\nThe run function is called within the cli function, which serves as the command-line interface for executing the reverse-upgrade benchmark generator. The cli function initializes the ReverseUpgradeWorkflow with parameters for maximum levels and tries, then invokes the run method to start the question generation process.\n\n**Note**: It is essential to ensure that the underlying model and agent are correctly configured to facilitate the generation and validation of questions. Additionally, the performance of the run function is contingent on the effectiveness of the validators and the quality of the generated questions."
      ],
      "code_start_line": 183,
      "code_end_line": 208,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    async def run(self):\n        # Seed phase\n        seed_item = await self.generate_seed()\n        self.items.append(seed_item)\n        current = seed_item\n\n        # iterative reverse‑upgrade\n        for _ in range(self.max_level):\n            tries = 0\n            success = False\n            while tries < self.max_tries and not success:\n                tries += 1\n                candidate = await self.upgrade_question(current)\n                # 1st validator\n                if await self.knowledge_validator(candidate.question):\n                    continue  # knowledge too easy\n                # 2nd validator\n                if await self.search_validator(candidate.question):\n                    continue  # one‑search too easy\n                # both failed ⇒ accept\n                self.items.append(candidate)\n                current = candidate\n                success = True\n            if not success:\n                print(f\"Level {current.level+1} failed after {self.max_tries} retries; terminating trace.\")\n                break\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/cli"
      ],
      "reference_who": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/upgrade_question",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/knowledge_validator",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/search_validator"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "export",
      "md_content": [
        "**export**: The function of export is to serialize a collection of items to a JSON file.\n\n**parameters**: The parameters of this Function.\n· path: Path – The path to the output JSON file where the serialized data will be stored.\n\n**Code Description**: The export function is responsible for writing a collection of items, specifically the `items` attribute of the class instance, to a JSON file. The function takes one parameter, `path`, which represents the file path to save the data. The method opens the specified file in write mode with UTF-8 encoding. It then uses the `json.dump` function to serialize a list of dictionaries (generated by calling `to_dict()` on each item in `self.items`) and writes this list to the file.\n\nThe `to_dict()` method, which is called on each item in `self.items`, converts the individual items into a dictionary format, making them suitable for JSON serialization. This conversion is essential because the JSON format requires data to be in a serializable structure such as a dictionary, list, string, etc. The `ensure_ascii=False` argument in `json.dump` ensures that non-ASCII characters are correctly handled in the output file, while the `indent=2` argument formats the JSON with a 2-space indentation for better readability.\n\nThe export function is invoked in the `cli()` function, where the output file path is passed as an argument. The `cli()` function is part of a command-line interface for the program, which handles user input, including specifying the output file path and other parameters. The `export` method is called after the workflow has completed its execution, ensuring that the resulting data is serialized and stored as a JSON file.\n\nIn summary, the `export` function serves the purpose of persisting the `items` data of the current instance in a structured and readable JSON format, making it easy to store or transfer the data.\n\n**Note**: \n- Ensure that the `items` attribute is properly populated before calling the export method, as the method relies on this data to generate the output file.\n- The `path` parameter should point to a valid location where the program has write permissions, otherwise, an error will occur when attempting to save the file."
      ],
      "code_start_line": 211,
      "code_end_line": 213,
      "params": [
        "self",
        "path"
      ],
      "have_return": false,
      "code_content": "    def export(self, path: Path):\n        with path.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump([it.to_dict() for it in self.items], f, ensure_ascii=False, indent=2)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/cli"
      ],
      "reference_who": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/QAItem/to_dict"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_safe_json",
      "md_content": [
        "**_safe_json**: The function of _safe_json is to extract and safely parse JSON data from a raw text input, specifically handling fenced code blocks.\n\n**parameters**: The parameters of this Function.\n· text: A string that contains the raw text input potentially formatted with JSON data.\n\n**Code Description**: The _safe_json function is designed to extract JSON data from a given string input, which may include fenced code blocks (indicated by triple backticks). The function first attempts to locate a JSON block within the input text using a regular expression. If a fenced block is found, it extracts the content within the backticks. If no such block is found, it defaults to using the entire input text. The extracted text is then stripped of leading and trailing whitespace.\n\nThe function attempts to parse the cleaned text as JSON using the json.loads method. If this parsing fails due to a JSONDecodeError, the function implements a fallback mechanism: it removes any backticks from the text and attempts to parse the modified string again. This ensures that even if the input is not perfectly formatted, the function can still attempt to retrieve valid JSON data.\n\nThe _safe_json function is called within two other asynchronous methods: generate_seed and upgrade_question. In generate_seed, it processes the response from an agent chat that is expected to return a JSON object containing a question, answer, and support URLs. Similarly, in upgrade_question, it processes the response from another agent chat, which is expected to return a new question and support URLs. In both cases, the function ensures that the data extracted from the agent's response is correctly formatted as a JSON object, allowing the subsequent code to access specific fields without encountering errors.\n\n**Note**: It is important to ensure that the input text is formatted correctly to maximize the chances of successful JSON extraction. The function is robust against minor formatting issues due to its fallback mechanism, but significant deviations from expected JSON structure may still lead to errors.\n\n**Output Example**: A possible appearance of the code's return value could be:\n{\n    \"question\": \"What is the capital of France?\",\n    \"answer\": \"Paris\",\n    \"support_urls\": [\"https://example.com/france-capital\"]\n}"
      ],
      "code_start_line": 217,
      "code_end_line": 227,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "    def _safe_json(text: str) -> Dict[str, Any]:\n        \"\"\"Extract JSON from raw LLM text (handles fenced blocks).\"\"\"\n        fence = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)```\", text)\n        raw = fence.group(1) if fence else text\n        raw = raw.strip()\n        try:\n            return json.loads(raw)\n        except json.JSONDecodeError:\n            # fallback: strip backticks and retry\n            raw2 = raw.replace(\"```\", \"\").strip()\n            return json.loads(raw2)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/generate_seed",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/upgrade_question"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "cli",
      "md_content": [
        "**cli**: The function of cli is to serve as the command-line interface for the reverse-upgrade benchmark generator.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The cli function initializes and executes the reverse-upgrade benchmark generation workflow. It begins by creating an instance of the argparse.ArgumentParser, which is used to handle command-line arguments. The description provided to the parser indicates the purpose of the tool, which is to generate a reverse-upgrade benchmark.\n\nThe function defines several command-line arguments:\n- `--out`: This argument specifies the output file path for the generated JSON data. It defaults to \"trace.json\" if not provided by the user.\n- `--max_level`: This argument sets the maximum number of levels of question difficulty to generate, with a default value of 5.\n- `--max_tries`: This argument determines the maximum number of attempts to generate a valid question at each level, also defaulting to 5.\n\nAfter parsing the command-line arguments, the cli function creates an instance of the BaseAgent class, which serves as the foundational agent for managing interactions and generating questions. Subsequently, it initializes the ReverseUpgradeWorkflow class, passing the agent instance along with the specified maximum levels and maximum tries.\n\nThe workflow is executed asynchronously using asyncio.run, which calls the run method of the ReverseUpgradeWorkflow instance. This method orchestrates the generation of progressively difficult questions based on an initial seed question. Once the workflow completes, the export method is invoked to save the generated questions and answers to the specified output file.\n\nFinally, the function prints a message indicating the number of QA levels saved to the output file, providing feedback to the user regarding the operation's success.\n\nThe cli function is crucial as it acts as the entry point for users to interact with the reverse-upgrade benchmark generator, allowing them to customize the output and control the generation parameters.\n\n**Note**: It is important to ensure that the command-line arguments are provided correctly when executing the cli function, as improper usage may lead to unexpected behavior or errors. Additionally, the output file path should be writable to avoid file system errors during the export process."
      ],
      "code_start_line": 234,
      "code_end_line": 249,
      "params": [],
      "have_return": false,
      "code_content": "def cli():\n    parser = argparse.ArgumentParser(description=\"Reverse‑upgrade benchmark generator\")\n    parser.add_argument(\"--out\", type=Path, default=Path(\"trace.json\"), help=\"output JSON file\")\n    parser.add_argument(\"--max_level\", type=int, default=5)\n    parser.add_argument(\"--max_tries\", type=int, default=5)\n    args = parser.parse_args()\n\n    agent = BaseAgent()\n    workflow = ReverseUpgradeWorkflow(\n        agent,\n        max_level=args.max_level,\n        max_tries=args.max_tries,\n    )\n    asyncio.run(workflow.run())\n    workflow.export(args.out)\n    print(f\"Saved {len(workflow.items)} QA levels to {args.out}\")\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/run",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/export"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/tools/note_manager.py": [
    {
      "type": "FunctionDef",
      "name": "set_session",
      "md_content": [
        "**set_session**: The function of set_session is to set the current session's session_id.\n\n**parameters**: The parameters of this Function.\n· session_id: A string representing the unique identifier for the current session.\n\n**Code Description**: The set_session function is designed to set the session identifier for the current context of the application. It takes a single parameter, session_id, which is expected to be a string. The function utilizes a thread-local storage mechanism, indicated by the use of `_current_session_id.set(session_id)`, to ensure that the session_id is stored in a way that is unique to the current thread of execution. This is particularly useful in multi-threaded environments where different threads may handle different user sessions concurrently.\n\nThe set_session function is called within the constructor of the WorkflowExecutor class in the workflow.py file. During the initialization of a WorkflowExecutor instance, a unique session_id is generated using `uuid.uuid4()`, which provides a universally unique identifier. This session_id is then passed to the set_session function to establish the current session context. This integration ensures that all subsequent operations within the WorkflowExecutor instance can reference the correct session, facilitating the organization and retrieval of notes and other session-specific data.\n\n**Note**: It is important to ensure that the session_id provided to the set_session function is valid and unique for each session to prevent any potential conflicts or data integrity issues within the application."
      ],
      "code_start_line": 33,
      "code_end_line": 35,
      "params": [
        "session_id"
      ],
      "have_return": false,
      "code_content": "def set_session(session_id: str):\n    \"\"\"设置当前会话的 session_id\"\"\"\n    _current_session_id.set(session_id)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/workflow.py",
        "src/criticsearch/workflow.py/WorkflowExecutor/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_taking_notes",
      "md_content": [
        "**_taking_notes**: The function of _taking_notes is to store notes in a specified session.\n\n**parameters**: The parameters of this Function.\n· session_id: A string representing the unique identifier for the current session in which the note is being stored.\n· note: A string containing the content of the note to be stored.\n\n**Code Description**: The _taking_notes function is a private utility designed to insert a note into a SQLite database associated with a specific session. It generates a unique identifier for the note using the uuid library, ensuring that each note can be distinctly referenced. The function begins by acquiring a lock to prevent concurrent access issues, which is crucial in a multi-threaded environment. It then establishes a connection to the SQLite database specified by the _DB_PATH variable, ensuring that the connection is thread-safe by setting check_same_thread to False.\n\nOnce the connection is established, the function executes an SQL INSERT statement to add the note into the 'notes' table, which includes the generated note ID, the session ID, and the note content. After executing the insert operation, the function commits the transaction to save the changes to the database and subsequently closes the connection to free up resources.\n\nThe function returns a dictionary indicating the status of the operation, with a status key set to \"ok\" and the note_id of the newly created note. This function is called by the taking_notes function, which is responsible for processing a list of notes provided in a JSON format. The taking_notes function validates the input, ensuring that it is correctly formatted and that each note adheres to the expected structure. For each valid note, it invokes _taking_notes to store the note in the database, collecting the generated note IDs for successful entries and returning them in the response.\n\n**Note**: It is important to ensure that the session_id is valid and that the note content is properly formatted before invoking this function. Failure to do so may lead to errors during the note storage process.\n\n**Output Example**: A possible return value from the function could be:\n```json\n{\n    \"status\": \"ok\",\n    \"note_id\": \"123e4567-e89b-12d3-a456-426614174000\"\n}\n```"
      ],
      "code_start_line": 38,
      "code_end_line": 49,
      "params": [
        "session_id",
        "note"
      ],
      "have_return": true,
      "code_content": "def _taking_notes(session_id: str, note: str) -> dict:\n    \"\"\"私有：在指定会话存储笔记\"\"\"\n    note_id = str(uuid.uuid4())\n    with _lock:\n        conn = sqlite3.connect(str(_DB_PATH), check_same_thread=False)\n        conn.execute(\n            'INSERT INTO notes (id, session_id, note) VALUES (?, ?, ?)',\n            (note_id, session_id, note)\n        )\n        conn.commit()\n        conn.close()\n    return {\"status\": \"ok\", \"note_id\": note_id}\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/note_manager.py/taking_notes"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_retrieve_notes",
      "md_content": [
        "**_retrieve_notes**: The function of _retrieve_notes is to retrieve all notes associated with a specified session ID from a SQLite database.\n\n**parameters**: The parameters of this Function.\n· session_id: A string representing the unique identifier for the session whose notes are to be retrieved.\n\n**Code Description**: The _retrieve_notes function is designed to access a SQLite database and fetch all notes linked to a given session ID. It begins by acquiring a lock to ensure thread safety during the database operation. The function establishes a connection to the database using the path specified by the _DB_PATH variable, with the option to allow access from multiple threads. \n\nOnce the connection is established, it executes a SQL query to select all notes from the 'notes' table where the session_id matches the provided session_id. The results are ordered by the timestamp to maintain the chronological order of the notes. After fetching the results, the connection to the database is closed to free up resources.\n\nThe function processes the fetched rows to extract the note content, which is stored in a list. Finally, it concatenates all notes into a single string, separating each note with a newline character, and returns this string.\n\nThis function is called by the retrieve_notes function, which is responsible for obtaining the current session ID and ensuring that it is set before invoking _retrieve_notes. If the session ID is not available, retrieve_notes raises a RuntimeError. Thus, _retrieve_notes operates as a private utility function that supports the public interface of retrieve_notes by encapsulating the database access logic.\n\n**Note**: It is important to ensure that the session_id passed to _retrieve_notes is valid and corresponds to an existing session in the database. Failure to do so may result in an empty string being returned if no notes are found for the given session ID.\n\n**Output Example**: An example of the return value from _retrieve_notes could be:\n```\n\"Note 1 content\\nNote 2 content\\nNote 3 content\"\n```"
      ],
      "code_start_line": 52,
      "code_end_line": 63,
      "params": [
        "session_id"
      ],
      "have_return": true,
      "code_content": "def _retrieve_notes(session_id: str) -> str:\n    \"\"\"私有：获取指定会话所有笔记\"\"\"\n    with _lock:\n        conn = sqlite3.connect(str(_DB_PATH), check_same_thread=False)\n        cursor = conn.execute(\n            'SELECT note FROM notes WHERE session_id = ? ORDER BY timestamp',\n            (session_id,)\n        )\n        rows = cursor.fetchall()\n        conn.close()\n    notes = [row[0] for row in rows]\n    return \"\\n\".join(notes)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/note_manager.py/retrieve_notes"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "taking_notes",
      "md_content": [
        "**taking_notes**: The function of taking_notes is to store notes for the current session in a structured format.\n\n**parameters**: The parameters of this function.\n· note: A JSON-formatted string representing a list of notes, each enclosed within `<note>...</note>` tags. The notes are expected to contain content that may reference external citations using `<citation>...</citation>` tags.\n\n**Code Description**: The `taking_notes` function is designed to accept a JSON string representing a list of notes, each formatted with `<note>...</note>` tags. These notes are parsed and validated before being stored in the system. \n\n1. **Session Validation**: The function first retrieves the current session ID through the `_current_session_id.get()` method. If the session ID is not set, a `RuntimeError` is raised, indicating that the notes cannot be stored without a valid session ID.\n\n2. **JSON Parsing and Validation**: The function attempts to parse the input `note` string using Python's `json.loads()` method. If the input is not a valid JSON, an error message with the details of the parsing failure is returned. If the parsed object is not a list, another error message is returned indicating that the input must be a JSON list of note strings.\n\n3. **Note Validation**: The function then iterates over the list of notes. For each note:\n   - It checks whether the note is a string.\n   - It ensures that each note starts with `<note>` and ends with `</note>`. If any note fails this validation, an error message is returned, indicating the specific index of the invalid note and the nature of the error.\n\n4. **Storing Notes**: For each valid note, the function calls the `_taking_notes` function, passing the session ID and the raw note content. The `_taking_notes` function is responsible for saving the note to the database. If the note is successfully stored, the generated note ID is appended to a list of saved note IDs.\n\n5. **Final Return**: Once all notes have been processed, the function returns a dictionary. If all notes were successfully saved, the dictionary contains a status of \"ok\" and the list of generated note IDs. If any note failed to be saved, the function returns an error message detailing the failure.\n\nThis function is invoked by components such as the `WorkflowExecutor` class in the `src/criticsearch/workflow.py` module, where it is used as part of a tool registry that processes user queries and stores related notes.\n\n**Note**: \n- The session ID must be set before calling this function. If it is not set, the function will raise a `RuntimeError`.\n- The input JSON string must strictly follow the expected structure, with each note being a string enclosed in `<note>...</note>` tags.\n- The `_taking_notes` function, which is called by `taking_notes`, is responsible for the actual persistence of the notes to the database. It ensures that each note is saved under the correct session, generating a unique note ID in the process.\n\n**Output Example**: A successful execution of the function might return the following JSON structure:\n\n```json\n{\n    \"status\": \"ok\",\n    \"note_ids\": [\n        \"123e4567-e89b-12d3-a456-426614174000\",\n        \"987e6543-e21b-32d4-a567-789654321000\"\n    ]\n}\n``` \n\nIf an error occurs, the returned JSON would look like:\n\n```json\n{\n    \"status\": \"error\",\n    \"message\": \"Invalid note format at index 1: must start with <note> and end with </note>.\"\n}\n```"
      ],
      "code_start_line": 67,
      "code_end_line": 117,
      "params": [
        "note"
      ],
      "have_return": true,
      "code_content": "def taking_notes(note: str) -> dict:\n    \"\"\"\n    Store notes for the current session.\n\n    This function expects a JSON-formatted string representing a list of note entries,\n    each in the following format:\n\n        [\n            \"<note>First note content with <citation>original URL</citation> where data was used.</note>\",\n            \"<note>Second note content with <citation>another URL</citation> example.</note>\"\n        ]\n\n    The notes will be parsed, validated, and each entry saved under the current session.\n\n    Args:\n        note: A JSON string representing a list of <note>...</note> entries.\n\n    Returns:\n        dict: A result object with keys:\n            status: 'ok' if saved successfully, otherwise 'error'.\n            note_ids: List of generated note IDs when status is 'ok'.\n            message: Detailed error message when status is 'error'.\n\n    Raises:\n        RuntimeError: If session_id is not set.\n    \"\"\"\n    session_id = _current_session_id.get()\n    if not session_id:\n        raise RuntimeError(\"Session ID 未设置，无法存储笔记。\")\n    # Parse input string to list\n    import json\n    try:\n        notes_list = json.loads(note)\n    except json.JSONDecodeError as e:\n        return {\"status\": \"error\", \"message\": f\"Invalid JSON format: {e}\"}\n    if not isinstance(notes_list, list):\n        return {\"status\": \"error\", \"message\": \"Input must be a JSON list of note strings.\"}\n\n    saved_ids = []\n    for idx, raw in enumerate(notes_list):\n        if not isinstance(raw, str):\n            return {\"status\": \"error\", \"message\": f\"Element at index {idx} is not a string.\"}\n        if not (raw.strip().startswith('<note>') and raw.strip().endswith('</note>')):\n            return {\"status\": \"error\", \"message\": f\"Invalid note format at index {idx}: must start with <note> and end with </note>.\"}\n        # Save each note entry\n        res = _taking_notes(session_id, raw)\n        if res.get(\"status\") == \"ok\":\n            saved_ids.append(res.get(\"note_id\"))\n        else:\n            return {\"status\": \"error\", \"message\": f\"Failed to save note at index {idx}: {res}\"}\n    return {\"status\": \"ok\", \"note_ids\": saved_ids}\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py",
        "src/criticsearch/workflow.py",
        "src/criticsearch/workflow.py/WorkflowExecutor/__init__"
      ],
      "reference_who": [
        "src/criticsearch/tools/note_manager.py/_taking_notes"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "retrieve_notes",
      "md_content": [
        "**retrieve_notes**: The function of retrieve_notes is to retrieve all notes associated with the current session.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe retrieve_notes function is responsible for gathering all notes associated with the current session. It first attempts to retrieve the session ID using the _current_session_id.get() method. If the session ID is not set (i.e., if it is None or empty), the function raises a RuntimeError, indicating that the session ID is not available and notes cannot be retrieved.\n\nIf a valid session ID is found, the function proceeds by calling the _retrieve_notes function, passing the session ID as an argument. The _retrieve_notes function is responsible for accessing the underlying database to fetch the notes associated with the session ID. It returns a concatenated string of all the notes, separated by newline characters.\n\nThis function is integral to managing session-based data in the application, specifically for retrieving notes linked to the ongoing session. It acts as a public-facing method that ensures a valid session ID exists before delegating the actual work to the internal _retrieve_notes function.\n\nThe retrieve_notes function is typically called in scenarios where the notes for a particular session need to be accessed or presented, such as when displaying the history of actions or information related to a user query.\n\n**Note**: \n- If the session ID is not set, the function raises a RuntimeError, which should be handled appropriately by the calling code to ensure smooth operation.\n- The function relies on the session ID being correctly initialized earlier in the workflow, so developers should ensure the session ID is set before calling retrieve_notes.\n\n**Output Example**: \nA possible output of the retrieve_notes function would be:\n```\n\"Note 1 content\\nNote 2 content\\nNote 3 content\"\n```\nThis output consists of a series of notes, each separated by a newline character, ready for consumption by the system or user interface."
      ],
      "code_start_line": 119,
      "code_end_line": 139,
      "params": [],
      "have_return": true,
      "code_content": "def retrieve_notes() -> str:\n    \"\"\"\n    Retrieve all notes for the current session.\n\n    Returns a single string concatenating all stored notes for this session,\n    each separated by a newline. The string is intended for model consumption\n    and retains original <note>...</note> formatting.\n\n    Args:\n        None\n\n    Returns:\n        str: All notes concatenated with '\\n', or an empty string if none exist.\n\n    Raises:\n        RuntimeError: If session_id is not set.\n    \"\"\"\n    session_id = _current_session_id.get()\n    if not session_id:\n        raise RuntimeError(\"Session ID 未设置，无法检索笔记。\")\n    return _retrieve_notes(session_id)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py",
        "src/criticsearch/workflow.py",
        "src/criticsearch/workflow.py/WorkflowExecutor/__init__"
      ],
      "reference_who": [
        "src/criticsearch/tools/note_manager.py/_retrieve_notes"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "src/criticsearch/tools/tool_registry.py": [
    {
      "type": "ClassDef",
      "name": "ToolRegistry",
      "md_content": [
        "**ToolRegistry**: The function of ToolRegistry is to manage tools by using their function names as keys, allowing easy retrieval and creation of schemas for those tools.\n\n**Attributes**:\n- **_tools**: A dictionary that maps function names (as strings) to their respective tool schemas. It is initialized as an empty dictionary and is used to store the schemas of tools for quick access and reuse.\n\n**Code Description**:  \nThe `ToolRegistry` class is designed to manage the registration and retrieval of tool schemas based on function names. It allows the creation of new schemas for tools if they are not already registered. The primary use case of this class is within systems that work with a variety of tools, where each tool is associated with a function, and its schema needs to be retrieved or generated.\n\n- **`__init__`**: This method initializes an empty registry (`_tools`) for storing tool schemas. The registry is a dictionary where keys are the names of functions (as strings) and values are the schemas related to those functions. The class starts with no schemas stored.\n\n- **`get_or_create_tool_schema`**: This method accepts one or more function references (`target_functions`) as arguments. For each function provided, it checks whether the schema for that function already exists in the `_tools` dictionary. If a schema does not exist for the function, the method creates it using a class method `Tool.create_schema_from_function` and then stores it in the `_tools` dictionary. After the schema is retrieved or created, it is added to a list and returned. This method ensures that schemas are not repeatedly created for the same functions, promoting efficiency and reuse.\n\n    - **Arguments**:\n      - `*target_functions`: One or more functions whose schemas need to be retrieved or created.\n    - **Returns**: A list of dictionaries representing the schemas of the provided functions.\n\nIn the context of the larger project, the `ToolRegistry` class is used in the `BaseAgent` class (found in `src/criticsearch/base_agent.py`). Specifically, instances of `ToolRegistry` are used to manage the schemas for different tools within the agent, such as `search_aggregator` and `content_scraper`. When initializing these tools, `BaseAgent` calls `get_or_create_tool_schema` to retrieve or create the schemas associated with their functions. These schemas are essential for the tools to be correctly used in subsequent operations, such as when the agent interacts with external services or processes data.\n\nThe `ToolRegistry` class plays a crucial role in ensuring that the agent can consistently access the correct schema for its tools, avoiding the need to repeatedly recreate schemas and thus improving efficiency.\n\n**Note**:  \n- The `Tool.create_schema_from_function` method (referenced in `get_or_create_tool_schema`) is expected to be responsible for creating a schema from a function. This method should handle the specifics of schema creation and may include validation or other processing steps.\n- The `_tools` dictionary is managed entirely within the `ToolRegistry` class. Users of this class do not need to directly modify this attribute.\n- This class assumes that function names are unique and can be used as reliable keys for storing schemas.\n\n**Output Example**:  \nWhen calling `get_or_create_tool_schema` with the `search` function of `search_aggregator` and the `scrape` function of `content_scraper`, a possible return value could look like this:\n\n```python\n[\n    {\n        \"function_name\": \"search\",\n        \"schema_details\": { ... }  # Detailed schema information for the 'search' function\n    },\n    {\n        \"function_name\": \"scrape\",\n        \"schema_details\": { ... }  # Detailed schema information for the 'scrape' function\n    }\n]\n```",
        "**ToolRegistry**: The function of ToolRegistry is to manage tools using their function names as keys, allowing for the retrieval or creation of schemas for these tools.\n\n**attributes**: The attributes of this Class.\n· _tools: A dictionary mapping function names (as strings) to their respective schemas (as dictionaries).\n\n**Code Description**: The ToolRegistry class serves as a centralized registry for managing tool schemas, which are essential for the operation of various tools within the application. Upon initialization, it creates an empty dictionary, _tools, to store the schemas associated with different functions. \n\nThe primary method of this class, get_or_create_tool_schema, accepts one or more functions as arguments. For each function, it checks if a schema already exists in the _tools dictionary. If a schema does not exist, it invokes the Tool.create_schema_from_function method to generate a new schema based on the provided function and stores it in the _tools dictionary. This method also logs the creation of new schemas for tracking purposes.\n\nThe ToolRegistry class is utilized by the BaseAgent class, which acts as a foundational component for intelligent agents in the project. The BaseAgent creates an instance of ToolRegistry to manage the schemas for various tools it employs, such as the search aggregator and content scraper. By leveraging the ToolRegistry, the BaseAgent can efficiently retrieve and utilize the necessary schemas for its operations, ensuring that the tools are correctly configured and ready for use.\n\n**Note**: It is crucial to ensure that the ToolRegistry is populated with the required schemas for the tools being used, as the functionality of the BaseAgent and its ability to perform tasks depend on these schemas being available.\n\n**Output Example**: A possible appearance of the code's return value when retrieving schemas for a function might look like this:\n```json\n{\n  \"schemas\": [\n    {\n      \"function_name\": \"search\",\n      \"schema\": {\n        \"parameters\": {\n          \"query\": \"string\",\n          \"results\": \"list\"\n        },\n        \"description\": \"Schema for the search tool.\"\n      }\n    },\n    {\n      \"function_name\": \"scrape\",\n      \"schema\": {\n        \"parameters\": {\n          \"urls\": \"list\",\n          \"content\": \"string\"\n        },\n        \"description\": \"Schema for the content scraper tool.\"\n      }\n    }\n  ]\n}\n```",
        "# ToolRegistry Class Documentation\n\n## Overview\n\nThe `ToolRegistry` class is designed to manage a collection of tools using their function names as keys. It allows for efficient retrieval and creation of tool schemas, supporting seamless tool management by providing methods to register new tools and retrieve or create schemas for existing ones. This registry facilitates easy access and reuse of tools in different contexts.\n\n## Attributes\n\n- **_tools** (`Dict[str, dict]`): A dictionary that stores tool schemas, where the keys are function names (as strings) and the values are the corresponding schemas. This registry helps track and manage tool schemas for various functions.\n\n## Methods\n\n### `__init__(self) -> None`\nInitializes an empty registry for storing tool schemas.\n\n- **Description**: This method sets up the initial state of the `ToolRegistry` class by creating an empty dictionary to hold the tool schemas. Each tool is stored with its function name as the key.\n\n- **Attributes**: \n  - `_tools`: An empty dictionary that will later store tool schemas.\n\n---\n\n### `get_or_create_tool_schema(self, *target_functions: Callable) -> List[Dict]`\nRetrieves or creates tool schemas for the given functions.\n\n- **Arguments**:\n  - `*target_functions` (`Callable`): One or more functions for which schemas are to be retrieved or created.\n\n- **Returns**:\n  - `List[Dict]`: A list of schemas corresponding to the provided functions. If a schema does not exist for a function, it will be created and added to the registry.\n\n- **Description**: This method checks if the schema for a given function is already registered. If the schema is not found, it will use the `Tool.create_schema_from_function` method to generate and register a new schema. The resulting schemas are then returned in a list.\n\n---\n\n### `register_tool(self, name: str, func: Callable, description: str, parameters: Dict[str, Any] = None) -> Dict[str, Any]`\nManually registers a new tool with its associated function and schema.\n\n- **Arguments**:\n  - `name` (`str`): The name of the tool (either the function name or an identifier for a function call).\n  - `func` (`Callable`): The actual Python function that performs the tool's task.\n  - `description` (`str`): A description of the tool's functionality.\n  - `parameters` (`Dict[str, Any]`, optional): A dictionary representing the function's parameters in JSON schema format. If not provided, it will be automatically generated from the function's signature.\n\n- **Returns**:\n  - `Dict[str, Any]`: The schema of the registered tool, in a format compatible with OpenAI function calls or tool specifications.\n\n- **Description**: This method allows for the manual registration of a tool in the registry. It creates a schema for the provided function (either from the function's signature or the provided parameters) and adds it to the `_tools` registry. The schema is then returned for use in the system.\n\n---\n\n## Usage Example\n\n```python\ntool_registry = ToolRegistry()\n\n# Register a tool manually\ndef sample_function(param1: str, param2: int):\n    return f\"Received {param1} and {param2}\"\n\ntool_schema = tool_registry.register_tool(\n    name=\"sample_tool\",\n    func=sample_function,\n    description=\"A sample tool for demonstration purposes\"\n)\n\n# Retrieve or create schemas for a function\nschemas = tool_registry.get_or_create_tool_schema(sample_function)\n```\n\n## Conclusion\n\nThe `ToolRegistry` class provides an essential service for managing and organizing tool schemas within an application. By supporting the automatic creation and retrieval of tool schemas, as well as manual registration, it simplifies the integration and management of various tools in a consistent manner.",
        "**ToolRegistry**: The function of ToolRegistry is to manage tools by storing and retrieving their schemas based on function names.\n\n**attributes**: The attributes of this Class.\n· _tools: A dictionary mapping function names to their respective schemas.  \n· _funcs: A dictionary mapping function names to their actual callable functions.  \n\n**Code Description**: The ToolRegistry class serves as a centralized registry for managing tools, allowing for the retrieval and creation of schemas associated with specific functions. Upon initialization, it sets up two private dictionaries: `_tools`, which holds the schemas of the tools indexed by their function names, and `_funcs`, which stores the actual callable functions corresponding to those names.\n\nThe primary method, `get_or_create_tool_schema`, accepts one or more functions as arguments. It checks if a schema for each function is already registered. If not, it creates a new schema using the `Tool.create_schema_from_function` method and logs the creation process. This method returns a list of schemas corresponding to the provided functions, ensuring that tools can be reused efficiently without the need for redundant schema creation.\n\nAnother important method, `register_tool`, allows for the manual registration of tools. It requires the tool's name, the function that implements the tool, a description, and optional parameters. If parameters are not provided, it automatically generates them from the function's signature. This method updates the `_tools` and `_funcs` dictionaries with the new tool's schema and logs the registration.\n\nThe `invoke_tool` method enables the invocation of a registered tool by its name, passing the specified arguments to the corresponding function. If the function is not found in the registry, it raises a KeyError. This method also handles asynchronous function calls, ensuring that if the invoked function returns an awaitable, it is executed properly.\n\nThe ToolRegistry class is utilized within the BaseAgent class, which serves as a foundational component for intelligent agents. The BaseAgent creates an instance of ToolRegistry to manage tool schemas, allowing it to retrieve and register tools as needed during its operations. This relationship is crucial as it enables the BaseAgent to leverage various tools for executing tasks, managing conversation history, and performing searches.\n\n**Note**: It is essential to ensure that tools are properly registered before invoking them to avoid KeyErrors. Additionally, when registering tools, providing accurate descriptions and parameters will enhance the usability and clarity of the tool schemas.\n\n**Output Example**: A possible appearance of the code's return value when invoking a registered tool might look like this:\n```json\n{\n  \"result\": \"Tool executed successfully.\",\n  \"tool_name\": \"example_tool\",\n  \"arguments\": {\n    \"arg1\": \"value1\",\n    \"arg2\": \"value2\"\n  }\n}\n```"
      ],
      "code_start_line": 9,
      "code_end_line": 107,
      "params": [],
      "have_return": true,
      "code_content": "class ToolRegistry:\n    \"\"\"\n    A registry for managing tools using their function names as keys.\n\n    This class provides functionality to retrieve or create schemas for tools\n    based on provided functions, storing them for reuse and easy access.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize an empty registry for storing tool schemas.\n\n        Attributes:\n            _tools (Dict[str, dict]): A dictionary mapping function names\n                                      to their respective schemas.\n            _funcs (Dict[str, Callable]): A dictionary mapping function names\n                                          to their actual callable functions.\n        \"\"\"\n        self._tools: Dict[str, dict] = {}\n        self._funcs: Dict[str, Callable] = {}\n\n    def get_or_create_tool_schema(self, *target_functions: Callable) -> List[Dict]:\n        \"\"\"\n        Retrieve or create tool schemas for the given functions.\n\n        If a function's schema is not already registered, it will be created\n        using `Tool.create_schema_from_function` and added to the registry.\n\n        Args:\n            *target_functions (Callable): One or more functions for which\n                                          schemas are to be retrieved or created.\n\n        Returns:\n            List[Dict]: A list of schemas corresponding to the provided functions.\n        \"\"\"\n        schemas = []\n        for target_function in target_functions:\n            func_name = target_function.__name__\n\n            # Create schema if not already registered\n            if func_name not in self._tools:\n                self._tools[func_name] = Tool.create_schema_from_function(\n                    target_function\n                )\n                self._funcs[func_name] = target_function\n                printer.log(\n                    f\"Created tool schema for: {func_name}, schema: {self._tools[func_name]}\"\n                )\n\n            schemas.append(self._tools[func_name])\n\n        return schemas\n\n\n    def register_tool(\n        self,\n        name: str,\n        func: Callable,\n        description: str,\n        parameters: Dict[str, Any] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        手动注册一个工具（Function Call 或 MCP）。\n        Args:\n            name: MCP 工具名或函数名\n            func: 实际执行该工具的 Python 函数\n            description: 工具描述\n            parameters: 可选，函数参数 JSON Schema，若不传则自动从 func 签名生成\n        Returns:\n            schema: 一个符合 OpenAI function call/Tool 格式的 dict\n        \"\"\"\n        # 创建 schema\n        if parameters is None:\n            schema = Tool.create_schema_from_function(func)\n        else:\n            schema = {\n                \"name\": name,\n                \"description\": description,\n                \"parameters\": parameters,\n                \"type\": \"function\"\n            }\n        # 存入注册表\n        self._tools[name] = schema\n        self._funcs[name] = func\n        printer.log(f\"Registered tool: {name}, schema: {schema}\")\n        return schema\n    \n    def invoke_tool(self, name: str, arguments: Dict[str, Any]) -> Any:\n        \"\"\"\n        Invoke a registered tool by name with provided arguments and return its result.\n        \"\"\"\n        func = self._funcs.get(name)\n        if func is None:\n            raise KeyError(f\"No such tool registered: {name}\")\n        # Call the tool; if it returns an awaitable, run it synchronously\n        result = func(**arguments)\n        if inspect.isawaitable(result):\n            return asyncio.run(result)\n        return result",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py",
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/tools/__init__.py",
        "src/criticsearch/workflow.py"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an empty registry for storing tool schemas.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The __init__ function is a constructor method that is called when an instance of the ToolRegistry class is created. This function initializes an instance variable named _tools, which is a dictionary. The purpose of this dictionary is to map function names (as strings) to their respective schemas (as dictionaries). By initializing _tools as an empty dictionary, the function sets up a clean state for the ToolRegistry instance, allowing it to store tool schemas that can be added later. This design ensures that the registry starts without any pre-existing data, providing a fresh environment for subsequent operations.\n\n**Note**: It is important to understand that this function does not take any parameters and does not return any value. It solely serves the purpose of setting up the initial state of the ToolRegistry instance. Users should ensure that the _tools dictionary is populated with valid tool schemas through other methods provided in the ToolRegistry class after the instance is initialized.",
        "**__init__**: The function of __init__ is to initialize an empty registry for storing tool schemas and associated callable functions.\n\n**parameters**: This function does not accept any parameters.\n\n**Code Description**: \nThe `__init__` method is the constructor for the class it is a part of. It is responsible for initializing two attributes when an instance of the class is created:\n1. **_tools**: A dictionary that is intended to store tool schemas. Each entry in this dictionary maps a tool's name (as a string) to its corresponding schema, which is stored as a dictionary.\n2. **_funcs**: A dictionary that holds callable functions. Each entry in this dictionary maps a tool's name (as a string) to its associated callable function.\n\nThese attributes, `_tools` and `_funcs`, are both initialized as empty dictionaries. The `_tools` dictionary will later store mappings between tool names and their schemas, and the `_funcs` dictionary will store mappings between tool names and their callable functions.\n\n**Note**: \n- The method does not take any parameters as it only sets up the initial state of the class instance.\n- The two dictionaries, `_tools` and `_funcs`, are essential for the subsequent operation of the class, which likely involves storing and managing tool-related information and callable functions.\n"
      ],
      "code_start_line": 17,
      "code_end_line": 28,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        \"\"\"\n        Initialize an empty registry for storing tool schemas.\n\n        Attributes:\n            _tools (Dict[str, dict]): A dictionary mapping function names\n                                      to their respective schemas.\n            _funcs (Dict[str, Callable]): A dictionary mapping function names\n                                          to their actual callable functions.\n        \"\"\"\n        self._tools: Dict[str, dict] = {}\n        self._funcs: Dict[str, Callable] = {}\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_or_create_tool_schema",
      "md_content": [
        "**get_or_create_tool_schema**: The function of get_or_create_tool_schema is to retrieve or create tool schemas for specified functions.\n\n**parameters**: The parameters of this Function.\n· target_functions: One or more functions for which schemas are to be retrieved or created.\n\n**Code Description**: The get_or_create_tool_schema method is a member of the ToolRegistry class, responsible for managing the schemas associated with various tools (functions) within the application. This method accepts one or more callable functions as arguments and checks if a schema for each function is already registered in the internal registry (self._tools). \n\nIf a function's schema is not found, the method invokes the Tool class's create_schema_from_function method to generate a new schema based on the function's metadata, including its name, description, and parameters. This newly created schema is then stored in the registry for future reference. The method logs the creation of the schema for debugging purposes.\n\nThe method returns a list of schemas corresponding to the provided functions, ensuring that all requested schemas are either retrieved from the registry or newly created. This functionality is crucial for maintaining a structured representation of functions within the application, allowing for consistent access to their metadata.\n\nThe get_or_create_tool_schema method is called within the __init__ method of the BaseAgent class. During the initialization of a BaseAgent instance, it retrieves or creates schemas for the search aggregator and content scraper tools. These schemas are then stored in the conversation manager's available tools, facilitating their use in subsequent interactions.\n\n**Note**: When using this method, ensure that the functions passed as arguments are callable and properly defined, as the method relies on their metadata to generate the schemas.\n\n**Output Example**: A possible appearance of the code's return value when invoking get_or_create_tool_schema might look like this:\n```json\n[\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"search\",\n            \"description\": \"Performs a search operation.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"str\",\n                        \"description\": \"The search query.\"\n                    },\n                    \"limit\": {\n                        \"type\": \"int\",\n                        \"description\": \"The maximum number of results to return.\"\n                    }\n                },\n                \"required\": [\"query\"],\n                \"additionalProperties\": false\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"scrape\",\n            \"description\": \"Scrapes content from a given URL.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"url\": {\n                        \"type\": \"str\",\n                        \"description\": \"The URL to scrape.\"\n                    }\n                },\n                \"required\": [\"url\"],\n                \"additionalProperties\": false\n            }\n        }\n    }\n]\n```",
        "## Method: `get_or_create_tool_schema`\n\n### Description:\nThe `get_or_create_tool_schema` method retrieves or creates tool schemas for the given functions. If the schema for a function is not already registered, it will be created using the `Tool.create_schema_from_function` method and added to the registry. This ensures that the necessary tool schemas are available for the provided functions.\n\n### Parameters:\n- `*target_functions` (Callable): One or more functions for which schemas are to be retrieved or created. The parameter accepts a variable number of function arguments.\n\n### Returns:\n- `List[Dict]`: A list of schemas corresponding to the provided functions. Each schema is returned as a dictionary.\n\n### Behavior:\n1. The method iterates through each provided function in `target_functions`.\n2. For each function, it checks if its schema already exists in the registry (using the function's name).\n3. If the schema is not registered:\n   - The schema is created by calling `Tool.create_schema_from_function` with the target function.\n   - The schema is added to the registry.\n   - A log message is printed indicating the creation of the schema.\n4. Finally, the method returns a list of schemas corresponding to the provided functions, whether they were newly created or retrieved from the registry.\n\n### Example Usage:\n```python\ntool_registry = ToolRegistry()\nschemas = tool_registry.get_or_create_tool_schema(function1, function2)\n```\n\n### Notes:\n- The method leverages the `Tool.create_schema_from_function` to generate the schema when necessary.\n- Logging of schema creation is handled by the `printer.log` function, providing feedback on the schema creation process.",
        "## `get_or_create_tool_schema` Function Documentation\n\n### Description\n\nThe `get_or_create_tool_schema` function is responsible for retrieving or creating tool schemas for one or more given functions. If a schema for a function is not already registered, the function will create a new schema using the `Tool.create_schema_from_function` method and add it to the registry. This ensures that the tool schema for each function is available for use within the system.\n\n### Parameters\n\n- `*target_functions` (`Callable`): One or more functions for which schemas are to be retrieved or created. Each function is expected to be callable.\n\n### Returns\n\n- `List[Dict]`: A list of dictionaries, each representing the schema corresponding to one of the provided functions. These schemas are either retrieved from the registry or newly created.\n\n### Behavior\n\n- For each function in the `target_functions` argument:\n  1. The function's name is extracted.\n  2. If the function’s schema is not already registered (i.e., not present in the internal `_tools` dictionary), a new schema is created using the `Tool.create_schema_from_function` method.\n  3. The newly created schema is added to the `_tools` registry and the function is registered in the `_funcs` dictionary.\n  4. A log message is generated indicating the creation of the new tool schema, using the `printer.log` method.\n  5. The schema for the function is appended to the result list.\n\n### Example Usage\n\n```python\ntool_registry.get_or_create_tool_schema(func1, func2)\n```\n\nIn this example, the function `get_or_create_tool_schema` will retrieve or create schemas for the functions `func1` and `func2`. The corresponding schemas will be returned in a list.\n\n### Notes\n\n- The function relies on the internal `_tools` and `_funcs` dictionaries to manage the function schemas and their corresponding callable functions.\n- If a schema already exists for a function, it will simply be retrieved from the `_tools` registry without modification."
      ],
      "code_start_line": 30,
      "code_end_line": 60,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_or_create_tool_schema(self, *target_functions: Callable) -> List[Dict]:\n        \"\"\"\n        Retrieve or create tool schemas for the given functions.\n\n        If a function's schema is not already registered, it will be created\n        using `Tool.create_schema_from_function` and added to the registry.\n\n        Args:\n            *target_functions (Callable): One or more functions for which\n                                          schemas are to be retrieved or created.\n\n        Returns:\n            List[Dict]: A list of schemas corresponding to the provided functions.\n        \"\"\"\n        schemas = []\n        for target_function in target_functions:\n            func_name = target_function.__name__\n\n            # Create schema if not already registered\n            if func_name not in self._tools:\n                self._tools[func_name] = Tool.create_schema_from_function(\n                    target_function\n                )\n                self._funcs[func_name] = target_function\n                printer.log(\n                    f\"Created tool schema for: {func_name}, schema: {self._tools[func_name]}\"\n                )\n\n            schemas.append(self._tools[func_name])\n\n        return schemas\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/__init__",
        "src/criticsearch/workflow.py/WorkflowExecutor/__init__"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/tools/models.py/Tool",
        "src/criticsearch/tools/models.py/Tool/create_schema_from_function"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "register_tool",
      "md_content": [
        "**register_tool**: The function of register_tool is to manually register a tool (Function Call or MCP) by providing its name, execution function, description, and optional parameters.\n\n**parameters**: The parameters of this Function.\n· name: A string representing the name of the MCP tool or function.  \n· func: A Callable that is the actual Python function executing the tool.  \n· description: A string that describes the tool.  \n· parameters: An optional dictionary representing the function parameters in JSON Schema format. If not provided, it will be automatically generated from the func signature.\n\n**Code Description**: The register_tool function is designed to facilitate the manual registration of tools within the ToolRegistry class. It accepts four parameters: `name`, `func`, `description`, and an optional `parameters`. The primary purpose of this function is to create a structured schema for the tool being registered.\n\nWhen the function is called, it first checks if the `parameters` argument is provided. If it is not provided, the function utilizes the `Tool.create_schema_from_function(func)` method to generate a schema based on the provided function's signature and documentation. This method extracts the function's name, description, and parameter details, ensuring that the schema is comprehensive and accurately reflects the function's capabilities.\n\nIf the `parameters` argument is supplied, the function constructs a schema dictionary that includes the `name`, `description`, and the provided `parameters`, along with a type set to \"function\". This structured schema is then stored in the `_tools` dictionary of the ToolRegistry instance, using the `name` as the key.\n\nThe function also logs the registration process using the `printer.log` method, which outputs a styled log message to the console, indicating that the tool has been successfully registered along with its schema. This logging is crucial for tracking the registration of tools and ensuring that developers can monitor the tools available within the registry.\n\nFinally, the function returns the constructed schema, which is formatted to comply with the OpenAI function call/Tool format. This return value can be utilized by other components of the system that require access to the registered tool's metadata.\n\n**Note**: It is important to ensure that the `func` parameter is a well-defined Python function with appropriate documentation, as the schema generation relies on the function's signature and docstring. Additionally, the `name` parameter must be unique within the ToolRegistry to avoid overwriting existing tool registrations.\n\n**Output Example**: A possible return value from the register_tool function might look like this:\n```json\n{\n  \"name\": \"example_tool\",\n  \"description\": \"This tool serves as an example.\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"param1\": {\n        \"type\": \"int\",\n        \"description\": \"An integer parameter.\"\n      },\n      \"param2\": {\n        \"type\": \"str\",\n        \"description\": \"A string parameter.\"\n      }\n    },\n    \"required\": [\"param1\"],\n    \"additionalProperties\": false\n  },\n  \"type\": \"function\"\n}\n```",
        "### Function Documentation: `register_tool`\n\n#### Description:\nThe `register_tool` function is responsible for manually registering a tool (either a Function Call or an MCP) into a registry. It creates a structured schema for the tool and stores it alongside its corresponding function. This function is a critical part of the tool management system, ensuring that tools can be easily registered and accessed in a standardized format.\n\n#### Arguments:\n- **name** (`str`): The name of the MCP tool or function. This serves as the identifier for the tool in the registry.\n- **func** (`Callable`): The Python function that implements the tool's logic. This function will be executed when the tool is called.\n- **description** (`str`): A textual description of the tool's purpose or functionality.\n- **parameters** (`Dict[str, Any]`, optional): A dictionary representing the JSON schema for the function's parameters. If not provided, the schema will be automatically generated from the function signature.\n\n#### Returns:\n- **schema** (`Dict[str, Any]`): A dictionary representing the tool schema, formatted according to the OpenAI function call/Tool standard. This schema includes the tool's name, description, parameters, and type.\n\n#### Functionality:\n1. **Schema Creation**: \n   If the `parameters` argument is not provided, the function utilizes the `Tool.create_schema_from_function` method to generate a schema from the provided function (`func`). If the `parameters` are supplied, a schema is manually constructed with the provided values for `name`, `description`, and `parameters`.\n   \n2. **Tool Registration**: \n   The function registers the created schema in the internal registry (`self._tools`) under the provided `name`. Additionally, the function itself is stored in `self._funcs` under the same `name`.\n\n3. **Logging**: \n   A log message is generated to confirm the registration of the tool, including the tool's name and its schema.\n\n#### Example Usage:\n```python\ndef sample_tool_function(param1: int, param2: str):\n    \"\"\"A sample tool function.\"\"\"\n    return f\"Received {param1} and {param2}\"\n\ntool_registry.register_tool(\n    name=\"sample_tool\",\n    func=sample_tool_function,\n    description=\"A sample tool function that demonstrates tool registration.\",\n    parameters=None\n)\n```\n\nIn the above example, the `register_tool` function registers a new tool named `sample_tool`. The tool's schema is automatically created from the `sample_tool_function` signature, and the tool is added to the registry.\n\n#### Important Notes:\n- If `parameters` are not provided, the function relies on the `Tool.create_schema_from_function` method to automatically generate the schema based on the function's signature and docstring.\n- The logging feature provides visibility into the tool registration process, helping to track the tools registered in the system.\n- It is important to ensure that the `func` provided is a valid Python callable function and that its signature is well-defined for proper schema generation."
      ],
      "code_start_line": 63,
      "code_end_line": 94,
      "params": [
        "self",
        "name",
        "func",
        "description",
        "parameters"
      ],
      "have_return": true,
      "code_content": "    def register_tool(\n        self,\n        name: str,\n        func: Callable,\n        description: str,\n        parameters: Dict[str, Any] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        手动注册一个工具（Function Call 或 MCP）。\n        Args:\n            name: MCP 工具名或函数名\n            func: 实际执行该工具的 Python 函数\n            description: 工具描述\n            parameters: 可选，函数参数 JSON Schema，若不传则自动从 func 签名生成\n        Returns:\n            schema: 一个符合 OpenAI function call/Tool 格式的 dict\n        \"\"\"\n        # 创建 schema\n        if parameters is None:\n            schema = Tool.create_schema_from_function(func)\n        else:\n            schema = {\n                \"name\": name,\n                \"description\": description,\n                \"parameters\": parameters,\n                \"type\": \"function\"\n            }\n        # 存入注册表\n        self._tools[name] = schema\n        self._funcs[name] = func\n        printer.log(f\"Registered tool: {name}, schema: {schema}\")\n        return schema\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/tools/models.py/Tool",
        "src/criticsearch/tools/models.py/Tool/create_schema_from_function"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "invoke_tool",
      "md_content": [
        "**invoke_tool**: The function of invoke_tool is to invoke a registered tool by name with provided arguments and return its result.\n\n**parameters**: The parameters of this Function.\n· name: A string representing the name of the tool to be invoked.\n· arguments: A dictionary containing the arguments to be passed to the tool.\n\n**Code Description**: The invoke_tool function is designed to execute a tool that has been previously registered within the ToolRegistry class. It first retrieves the function associated with the provided tool name from the internal dictionary `_funcs`. If the tool name does not exist in the registry, it raises a KeyError, indicating that the requested tool is not available. \n\nOnce the function is retrieved, it is called with the unpacked arguments provided in the `arguments` dictionary. The function checks if the result of the function call is an awaitable (i.e., a coroutine). If it is, the function uses `asyncio.run()` to execute the coroutine synchronously and return the result. If the result is not awaitable, it simply returns the result directly.\n\nThis function is called within the `step` method of the WorkflowExecutor class. In this context, the `step` method processes an action that may involve invoking a tool. It extracts the tool's name and arguments from the action string, then calls `invoke_tool` to execute the tool and handle its output. The results of the tool invocation are formatted into XML and appended to the history for tracking purposes. If any exceptions occur during the tool invocation, they are caught, and an error message is returned in a structured format.\n\n**Note**: It is important to ensure that the tool name provided to invoke_tool corresponds to a registered tool; otherwise, a KeyError will be raised. Additionally, the arguments must be structured correctly as a dictionary to avoid runtime errors.\n\n**Output Example**: An example of a possible return value from invoke_tool could be a dictionary representing the result of the tool's execution, such as:\n```json\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"result\": \"Tool executed successfully\"\n    }\n}\n```"
      ],
      "code_start_line": 96,
      "code_end_line": 107,
      "params": [
        "self",
        "name",
        "arguments"
      ],
      "have_return": true,
      "code_content": "    def invoke_tool(self, name: str, arguments: Dict[str, Any]) -> Any:\n        \"\"\"\n        Invoke a registered tool by name with provided arguments and return its result.\n        \"\"\"\n        func = self._funcs.get(name)\n        if func is None:\n            raise KeyError(f\"No such tool registered: {name}\")\n        # Call the tool; if it returns an awaitable, run it synchronously\n        result = func(**arguments)\n        if inspect.isawaitable(result):\n            return asyncio.run(result)\n        return result",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/workflow.py/WorkflowExecutor/step"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tools/__init__.py": [],
  "src/criticsearch/tools/models.py": [
    {
      "type": "FunctionDef",
      "name": "get_list_type_annotation",
      "md_content": [
        "**get_list_type_annotation**: The function of get_list_type_annotation is to determine the type of elements in a list for constructing the \"items\" field in a JSON Schema.\n\n**parameters**: The parameters of this Function.\n· param_type: A type annotation representing the parameter to inspect. It should typically be a generic list type like `List[str]` or `List[int]`.\n\n**Code Description**: The `get_list_type_annotation` function is designed to extract the type of elements contained within a list. It is primarily used to help generate JSON Schema definitions, specifically for the \"items\" field, which describes the type of elements within an array. \n\nThe function first checks if the provided `param_type` is a list (using the `get_origin` function to check for a generic list type). If `param_type` is indeed a list, the function proceeds to extract the type of the elements within that list by calling `get_args(param_type)`. The first argument returned by `get_args` represents the element type, and if it is a type (i.e., `isinstance(args[0], type)`), the function returns a dictionary with the element's type name (e.g., `\"string\"` or `\"int\"`) for use in a JSON Schema definition.\n\nIf the `param_type` is not a list or the element type cannot be determined, the function defaults to returning a dictionary with the value `\"type\": \"string\"`, signifying that the list elements are assumed to be of string type.\n\nThis function is called in the `create_schema_from_function` method to help construct the schema of function parameters when the function contains a parameter of list type. Specifically, within `create_schema_from_function`, when a parameter is detected to be a list, the `get_list_type_annotation` function is invoked to determine the type of the items in that list, which is then added to the parameter schema as the `\"items\"` field.\n\n**Note**: It is important that the parameter passed to `get_list_type_annotation` be a valid generic list type (such as `List[str]` or `List[int]`). If the type is not a list or cannot be determined, the function will return a default type of `\"string\"`.\n\n**Output Example**: \nFor a `param_type` of `List[str]`, the function would return:\n```json\n{\n  \"type\": \"str\"\n}\n```\n\nFor a `param_type` of `List[int]`, the function would return:\n```json\n{\n  \"type\": \"int\"\n}\n```\n\nFor a `param_type` of an unsupported or non-list type, the function would return:\n```json\n{\n  \"type\": \"string\"\n}\n```"
      ],
      "code_start_line": 8,
      "code_end_line": 20,
      "params": [
        "param_type"
      ],
      "have_return": true,
      "code_content": "def get_list_type_annotation(param_type):\n    \"\"\"\n    获取列表中元素的类型，用于构造 JSON Schema 的 items 字段。\n    支持 List[str]、List[int] 等类型。\n    \"\"\"\n    # 检查是否为泛型 List 类型\n    if get_origin(param_type) is list or get_origin(param_type) is list:\n        # 获取列表的元素类型\n        args = get_args(param_type)\n        if args and isinstance(args[0], type):\n            return {\"type\": args[0].__name__}\n    # 默认返回字符串类型（未明确指定类型时）\n    return {\"type\": \"string\"}\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/models.py/Tool/create_schema_from_function"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "serialize_type",
      "md_content": [
        "**serialize_type**: The function of serialize_type is to map a given string representing a data type to its corresponding serialized name.\n\n**parameters**: The parameters of this Function.\n· value: str - A string representing a data type to be serialized.\n\n**Code Description**:  \nThe `serialize_type` function takes in a string value representing a data type (e.g., \"str\", \"int\", etc.). The function performs the following steps:\n1. A dictionary named `type_mapping` is defined. This dictionary contains key-value pairs where the key is a string representing a Python data type, and the value is the corresponding serialized name in a more generalized format.\n   - `\"str\"` maps to `\"string\"`\n   - `\"int\"` maps to `\"integer\"`\n   - `\"float\"` maps to `\"number\"`\n   - `\"bool\"` maps to `\"boolean\"`\n   - `\"list\"` maps to `\"array\"`\n   - `\"dict\"` maps to `\"object\"`\n   - `\"None\"` maps to `\"null\"`\n   \n2. The input value is converted to lowercase to ensure that the function is case-insensitive.\n\n3. The function attempts to find the serialized name by checking if the lowercase version of the `value` exists in the `type_mapping` dictionary.\n\n4. If a match is found, the corresponding serialized value is returned.\n\n5. If no match is found, the function defaults to returning `\"null\"`, which is the fallback value defined in the dictionary.\n\n**Note**: \n- The function is case-insensitive due to the use of the `lower()` method on the input string.\n- It assumes that the input is a valid Python type name. If the input string does not match any of the predefined keys, it returns `\"null\"`.\n- The function can be used for serializing Python data types to a format suitable for data serialization or communication in various systems.\n\n**Output Example**:\n- For an input value of `\"str\"`, the function will return `\"string\"`.\n- For an input value of `\"int\"`, the function will return `\"integer\"`.\n- For an input value of `\"boolean\"`, the function will return `\"null\"` since the value does not match any of the keys in the `type_mapping` dictionary."
      ],
      "code_start_line": 23,
      "code_end_line": 33,
      "params": [
        "value"
      ],
      "have_return": true,
      "code_content": "def serialize_type(value: str) -> str:\n    type_mapping = {\n        \"str\": \"string\",\n        \"int\": \"integer\",\n        \"float\": \"number\",\n        \"bool\": \"boolean\",\n        \"list\": \"array\",\n        \"dict\": \"object\",\n        \"None\": \"null\",\n    }\n    return type_mapping.get(value.lower(), \"null\")\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "Item",
      "md_content": [
        "**Item**: The function of Item is to represent a list item with a specific type.\n\n**attributes**: The attributes of this Class.\n· type: str - The type of the list item, which is a required field.\n\n**Code Description**: The Item class is a subclass of BaseModel, which indicates that it is likely part of a data modeling framework that includes validation and serialization capabilities. The primary attribute of this class is `type`, which is a string that specifies the type of the list item. This attribute is defined using the Field function, which enforces that it is a required field (denoted by the ellipsis `...`). \n\nThe class also includes a method called `serialize_type`, which is decorated with `@field_serializer`. This method is responsible for customizing the serialization of the `type` attribute. When the `type` attribute is serialized, this method is invoked, allowing for any specific transformations or formatting to be applied to the value before it is returned. The actual transformation is handled by the `serialize_type` function, which is assumed to be defined elsewhere in the project.\n\nThe Item class is utilized within the ParameterProperty class, where it is defined as an optional attribute named `items`. This indicates that a ParameterProperty can have a list of items, each of which is represented by an instance of the Item class. The relationship between Item and ParameterProperty is significant, as it allows for the encapsulation of item types within the broader context of parameter properties, enhancing the structure and organization of the data model.\n\n**Note**: When using the Item class, ensure that the `type` attribute is always provided, as it is a required field. Additionally, be aware that the serialization behavior of the `type` attribute can be customized through the `serialize_type` method.\n\n**Output Example**: An instance of the Item class might return a serialized representation like this:\n```json\n{\n  \"type\": \"string\"\n}\n``` \nThis output indicates that the type of the list item is a string, demonstrating how the class encapsulates and manages item type information."
      ],
      "code_start_line": 36,
      "code_end_line": 41,
      "params": [],
      "have_return": true,
      "code_content": "class Item(BaseModel):\n    type: str = Field(..., description=\"The type of the list item\")\n\n    @field_serializer(\"type\")\n    def serialize_type(self, value: str, _info) -> str:\n        return serialize_type(value)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/models.py/ParameterProperty"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "serialize_type",
      "md_content": [
        "**serialize_type**: The function of serialize_type is to convert a given string value into a serialized format.\n\n**parameters**: The parameters of this Function.\n· parameter1: value (str) - The string input that needs to be serialized.\n· parameter2: _info - An additional parameter that may contain contextual information, though it is not utilized within the function.\n\n**Code Description**: The serialize_type function takes a string input, referred to as 'value', and passes it to another function named serialize_type. This indicates that the function is likely designed to handle serialization tasks, converting the input string into a specific serialized format. The second parameter, _info, is included in the function signature but is not used within the function body, suggesting that it may be intended for future use or for compatibility with a broader interface. The function returns the result of the serialization process, which is expected to be a string.\n\n**Note**: It is important to ensure that the input 'value' is a valid string that can be serialized. The behavior of the function will depend on the implementation of the serialize_type function that it calls. Additionally, since the _info parameter is not utilized, developers should be aware that it may not affect the serialization process.\n\n**Output Example**: If the input value is \"example\", the function might return a serialized representation such as \"serialized_example\"."
      ],
      "code_start_line": 40,
      "code_end_line": 41,
      "params": [
        "self",
        "value",
        "_info"
      ],
      "have_return": true,
      "code_content": "    def serialize_type(self, value: str, _info) -> str:\n        return serialize_type(value)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "ParameterProperty",
      "md_content": [
        "**ParameterProperty**: The function of ParameterProperty is to define the properties of a parameter, including its data type, description, and optional items.\n\n**attributes**: The attributes of this Class.\n· type: str - The data type of the parameter, which is a required field.  \n· description: Optional[str] - A description of the parameter, which is an optional field.  \n· items: Optional[Item] - An optional attribute that represents a list of items associated with the parameter.\n\n**Code Description**: The ParameterProperty class is a subclass of BaseModel, indicating that it is part of a data modeling framework that provides validation and serialization capabilities. The primary attribute of this class is `type`, which is a string that specifies the data type of the parameter. This attribute is defined using the Field function, which enforces that it is a required field (denoted by the ellipsis `...`). \n\nThe `description` attribute is an optional string that provides additional information about the parameter, enhancing the understanding of its purpose and usage. The `items` attribute is also optional and is defined as an instance of the Item class, which allows for the encapsulation of item types within the context of parameter properties. This relationship is significant as it enables ParameterProperty to manage a collection of items, each represented by the Item class, thereby enhancing the structure and organization of the data model.\n\nThe ParameterProperty class is utilized within the Parameters class, where it is defined as a value in a dictionary that maps parameter names to their respective properties. This indicates that each parameter in the Parameters class can have its own set of properties defined by the ParameterProperty class. The Parameters class also includes attributes such as `required`, which specifies a list of required parameter names, and `additionalProperties`, which indicates whether additional properties are allowed.\n\nThe integration of ParameterProperty within the Parameters class allows for a comprehensive representation of parameters, including their types, descriptions, and associated items. This structured approach facilitates the management of complex parameter configurations in applications.\n\n**Note**: When using the ParameterProperty class, ensure that the `type` attribute is always provided, as it is a required field. The `description` and `items` attributes can be included as needed to provide further context and detail about the parameter.\n\n**Output Example**: An instance of the ParameterProperty class might return a serialized representation like this:\n```json\n{\n  \"type\": \"string\",\n  \"description\": \"A parameter that accepts string values.\",\n  \"items\": {\n    \"type\": \"string\"\n  }\n}\n```\nThis output indicates that the parameter is of type string, includes a description, and has associated items, demonstrating how the class encapsulates and manages parameter property information."
      ],
      "code_start_line": 44,
      "code_end_line": 53,
      "params": [],
      "have_return": true,
      "code_content": "class ParameterProperty(BaseModel):\n    type: str = Field(..., description=\"The data type of the parameter.\")\n    description: Optional[str] = Field(\n        None, description=\"A description of the parameter.\"\n    )\n    items: Optional[Item] = None\n\n    @field_serializer(\"type\")\n    def serialize_type(self, value: str, _info) -> str:\n        return serialize_type(value)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/models.py/Parameters"
      ],
      "reference_who": [
        "src/criticsearch/tools/models.py/Item"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "serialize_type",
      "md_content": [
        "**serialize_type**: The function of serialize_type is to convert a given string value into a serialized format.\n\n**parameters**: The parameters of this Function.\n· parameter1: value (str) - The string input that needs to be serialized.\n· parameter2: _info - Additional information that may be used during serialization, though it is not utilized in the current implementation.\n\n**Code Description**: The serialize_type function is designed to take a string input, referred to as 'value', and pass it to another function named serialize_type for processing. The function does not perform any operations on the input itself; instead, it directly calls the serialize_type function, which is presumably defined elsewhere in the codebase. The purpose of this function is to ensure that the input string is transformed into a serialized format, which is often necessary for data storage or transmission. The second parameter, _info, is included in the function signature but is not used within the function body, indicating that it may be intended for future use or for compatibility with a specific interface.\n\n**Note**: It is important to ensure that the input string is valid and that the serialize_type function being called is properly defined and accessible within the scope of this function. Additionally, the behavior of the function may depend on the implementation of the serialize_type function it calls.\n\n**Output Example**: If the input value is \"example\", and the serialize_type function processes it to return a serialized version, the output might look like \"serialized_example\"."
      ],
      "code_start_line": 52,
      "code_end_line": 53,
      "params": [
        "self",
        "value",
        "_info"
      ],
      "have_return": true,
      "code_content": "    def serialize_type(self, value: str, _info) -> str:\n        return serialize_type(value)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "Parameters",
      "md_content": [
        "**Parameters**: The function of Parameters is to define a schema for function parameters, including their properties and requirements.\n\n**attributes**: The attributes of this Class.\n· type: str - The type of the parameter object, which defaults to \"object\".  \n· properties: Dict[str, ParameterProperty] - A dictionary where keys are parameter names and values are their properties, which is a required field.  \n· required: List[str] - A list of required parameter names, which is also a required field.  \n· additionalProperties: bool - A boolean indicating whether additional properties are allowed, defaulting to False.\n\n**Code Description**: The Parameters class is a subclass of BaseModel, which indicates that it is part of a data modeling framework that provides validation and serialization capabilities. This class is designed to encapsulate the structure and requirements of parameters used in functions.\n\nThe `type` attribute specifies the nature of the parameter object and is set to \"object\" by default. This attribute is essential as it establishes the context for the parameters being defined.\n\nThe `properties` attribute is a dictionary that maps parameter names (as strings) to their respective properties, which are defined by the ParameterProperty class. This relationship allows for a detailed specification of each parameter's characteristics, such as its data type and description.\n\nThe `required` attribute is a list that enumerates the names of parameters that must be provided when the function is called. This ensures that any function utilizing this schema adheres to its defined requirements, promoting robustness and reducing the likelihood of errors.\n\nThe `additionalProperties` attribute indicates whether parameters not explicitly defined in the `properties` dictionary are permitted. By default, this is set to False, enforcing a strict schema that only allows the specified parameters.\n\nThe Parameters class is utilized within the Function class, where it is defined as the `parameters` attribute. This integration signifies that every function can have a well-defined set of parameters, enhancing the clarity and maintainability of the code. Additionally, the Parameters class is referenced in the `create_schema_from_function` method, which constructs a Tool schema from a target function. This method extracts function metadata, including parameter information, and organizes it into the Parameters structure, thereby facilitating the creation of a comprehensive schema for the function.\n\n**Note**: When using the Parameters class, ensure that the `properties` and `required` attributes are always provided, as they are essential for defining the parameter schema. The `additionalProperties` attribute can be adjusted based on the desired flexibility of the parameter definitions."
      ],
      "code_start_line": 56,
      "code_end_line": 65,
      "params": [],
      "have_return": false,
      "code_content": "class Parameters(BaseModel):\n    type: str = Field(\"object\", description=\"The type of the parameter object.\")\n    properties: Dict[str, ParameterProperty] = Field(\n        ...,\n        description=\"A dictionary where keys are parameter names and values are their properties.\",\n    )\n    required: List[str] = Field(..., description=\"A list of required parameter names.\")\n    additionalProperties: bool = Field(\n        False, description=\"Whether additional properties are allowed.\"\n    )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/models.py/Function",
        "src/criticsearch/tools/models.py/Tool/create_schema_from_function"
      ],
      "reference_who": [
        "src/criticsearch/tools/models.py/ParameterProperty"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "Function",
      "md_content": [
        "**Function**: The function of Function is to define a structured representation of a function, including its name, description, and parameters.\n\n**attributes**: The attributes of this Class.\n· name: str - The name of the function, which is a required field.  \n· description: str - A description of what the function does, which is also a required field.  \n· parameters: Parameters - An instance of the Parameters class that defines the schema for the function's parameters, which is a required field.  \n\n**Code Description**: The Function class is a subclass of BaseModel, indicating its role within a data modeling framework that provides validation and serialization capabilities. This class is designed to encapsulate the essential details of a function, including its name, description, and a structured schema for its parameters.\n\nThe `name` attribute is a string that holds the name of the function. This is crucial for identifying the function within the broader context of the application or system.\n\nThe `description` attribute is a string that provides a detailed explanation of the function's purpose and behavior. This attribute is important for documentation and clarity, allowing developers to understand the function's role without needing to inspect its implementation.\n\nThe `parameters` attribute is an instance of the Parameters class, which defines the schema for the function's parameters. This integration ensures that every function represented by the Function class has a well-defined set of parameters, promoting clarity and maintainability in the code. The Parameters class specifies the types, requirements, and additional properties of the parameters, thereby enforcing a structured approach to function definitions.\n\nThe Function class is utilized within the Tool class, where it is defined as the `function` attribute. This relationship signifies that each Tool instance can encapsulate a function definition, allowing for the creation of tools that are based on specific functions. The Tool class also includes a class method, `create_schema_from_function`, which constructs a Tool schema from a target function. This method extracts metadata from the target function, including its name, description, and parameters, and organizes this information into the Function and Parameters structures. This process enhances the clarity and usability of the function definitions within the application.\n\n**Note**: When using the Function class, ensure that all attributes (name, description, and parameters) are provided, as they are essential for defining a complete function representation. The integration with the Parameters class is critical for maintaining a structured approach to function parameter definitions."
      ],
      "code_start_line": 68,
      "code_end_line": 75,
      "params": [],
      "have_return": false,
      "code_content": "class Function(BaseModel):\n    name: str = Field(..., description=\"The name of the function.\")\n    description: str = Field(\n        ..., description=\"A description of what the function does.\"\n    )\n    parameters: Parameters = Field(\n        ..., description=\"The parameters schema for the function.\"\n    )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/models.py/Tool",
        "src/criticsearch/tools/models.py/Tool/create_schema_from_function"
      ],
      "reference_who": [
        "src/criticsearch/tools/models.py/Parameters"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "Tool",
      "md_content": [
        "**Tool**: The function of Tool is to create a structured representation of a function, encapsulating its metadata and schema.\n\n**attributes**: The attributes of this Class.\n· type: str - The type of the tool, typically set to 'function'.  \n· function: Function - An instance of the Function class that defines the function's name, description, and parameters.\n\n**Code Description**: The Tool class is a subclass of BaseModel, designed to encapsulate the details of a function within a structured schema. It includes two primary attributes: `type`, which indicates the nature of the tool (defaulting to 'function'), and `function`, which is an instance of the Function class. The Function class itself represents a structured definition of a function, including its name, description, and a schema for its parameters.\n\nA key feature of the Tool class is the class method `create_schema_from_function`. This method takes a target function as an argument and extracts essential metadata, such as the function's name and documentation string. It parses the documentation to generate sections that include a description and a list of parameters. The method utilizes the inspect module to retrieve the function's signature, allowing it to identify required parameters and their types.\n\nThe extracted information is then organized into a Function instance, which is returned as part of the Tool instance. This process facilitates the creation of a comprehensive schema that can be used to represent the function in a structured manner.\n\nThe Tool class is utilized within the ToolRegistry's `get_or_create_tool_schema` method. This method retrieves or creates tool schemas for specified functions. If a function's schema is not already registered, it invokes `Tool.create_schema_from_function` to generate the schema and add it to the registry. This integration ensures that function definitions are consistently represented and easily accessible within the application.\n\n**Note**: When using the Tool class, ensure that the `function` attribute is properly defined as an instance of the Function class, as this is critical for maintaining a structured representation of the function's metadata.\n\nA possible appearance of the code's return value when invoking `Tool.create_schema_from_function` might look like this:\n```json\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"example_function\",\n        \"description\": \"This function serves as an example.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"param1\": {\n                    \"type\": \"int\",\n                    \"description\": \"The first parameter.\"\n                },\n                \"param2\": {\n                    \"type\": \"str\",\n                    \"description\": \"The second parameter.\"\n                }\n            },\n            \"required\": [\"param1\"],\n            \"additionalProperties\": false\n        }\n    }\n}\n```",
        "**Tool**: The function of Tool is to encapsulate a function definition along with its metadata, enabling the creation of structured tool schemas from Python functions.\n\n**attributes**: The attributes of this Class.\n· type: str - The type of the tool, typically set to 'function'.  \n· function: Function - An instance of the Function class that defines the schema for the tool's function, including its name, description, and parameters.\n\n**Code Description**: The Tool class is a subclass of BaseModel, which indicates its role in a data modeling framework that provides validation and serialization capabilities. This class is designed to represent a tool that is based on a specific function, encapsulating essential details such as the function's name, description, and a structured schema for its parameters.\n\nThe `type` attribute is a string that specifies the type of the tool. In this case, it is typically set to 'function', indicating that the Tool instance is associated with a callable function.\n\nThe `function` attribute is an instance of the Function class, which holds the structured representation of the function. This includes the function's name, a detailed description of its purpose, and a schema for its parameters defined by the Parameters class. This integration ensures that each Tool instance has a well-defined function representation, promoting clarity and maintainability in the code.\n\nThe Tool class includes a class method, `create_schema_from_function`, which is responsible for generating a Tool schema from a provided target function. This method extracts metadata from the target function, such as its name and documentation string, and organizes this information into the Function and Parameters structures. The method also handles the extraction of parameter details, including types and descriptions, ensuring that the resulting schema is comprehensive and accurate.\n\nThe Tool class is utilized within the ToolRegistry class, specifically in methods like `get_or_create_tool_schema` and `register_tool`. In `get_or_create_tool_schema`, the Tool class is called to create a schema for functions that are not already registered, ensuring that the necessary tool schemas are available for the provided functions. The `register_tool` method also leverages the Tool class to create a schema from a function if no parameters are provided, thereby facilitating the manual registration of tools.\n\n**Note**: When using the Tool class, it is essential to ensure that the function provided to `create_schema_from_function` is well-documented and follows the expected format, as the method relies on the function's docstring and signature to generate the schema accurately.\n\n**Output Example**: A possible return value from the `create_schema_from_function` method might look like this:\n```json\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"example_function\",\n    \"description\": \"This function serves as an example.\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"param1\": {\n          \"type\": \"int\",\n          \"description\": \"An integer parameter.\"\n        },\n        \"param2\": {\n          \"type\": \"str\",\n          \"description\": \"A string parameter.\"\n        }\n      },\n      \"required\": [\"param1\"],\n      \"additionalProperties\": false\n    }\n  }\n}\n```"
      ],
      "code_start_line": 78,
      "code_end_line": 147,
      "params": [],
      "have_return": true,
      "code_content": "class Tool(BaseModel):\n    type: str = Field(\n        \"function\", description=\"The type of the tool, typically 'function'.\"\n    )\n    function: Function = Field(..., description=\"The function definition for the tool.\")\n\n    @classmethod\n    def create_schema_from_function(cls, target_function):\n        \"\"\"Create a Tool schema from a target function.\"\"\"\n\n        # 提取函数名称和文档字符串\n        func_name = target_function.__name__\n        func_doc = inspect.getdoc(target_function) or \"No description provided.\"\n\n        # 解析文档字符串，生成 sections\n        docstring = Docstring(func_doc)\n        # NOTE: Only support  Google-style right now.\n        sections = docstring.parse(\"google\")\n\n        # 提取描述信息\n        description = \"\"\n        parameters = []\n\n        for section in sections:\n            if section.kind == DocstringSectionKind.text:\n                description = section.value.strip()\n            elif section.kind == DocstringSectionKind.parameters:\n                parameters = section.value\n\n        # 提取参数信息\n        signature = inspect.signature(target_function)\n        required = []\n        properties = {}\n\n        for param_name, param in signature.parameters.items():\n            param_type = param.annotation if param.annotation != inspect._empty else Any\n            param_default = param.default if param.default != inspect._empty else ...\n\n            # 从解析的参数部分提取描述\n            param_description = None\n            for param_info in parameters:\n                if param_info.name == param_name:  # 使用属性访问\n                    param_description = param_info.description\n                    break\n\n            if param_default is ...:\n                required.append(param_name)\n\n            properties[param_name] = {\n                \"type\": param_type.__name__,\n                \"description\": param_description or f\"The {param_name} parameter.\",\n            }\n\n            if get_origin(param_type) is list:\n                properties[param_name][\"items\"] = get_list_type_annotation(param_type)\n\n        # Build the final Function and Tool schema\n        function_schema = Function(\n            name=func_name,\n            description=description,\n            parameters=Parameters(\n                type=\"object\",\n                properties=properties,\n                required=required,\n                additionalProperties=False,\n            ),\n        )\n        return cls(type=\"function\", function=function_schema).model_dump(\n            exclude_none=True\n        )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/tool_registry.py",
        "src/criticsearch/tools/tool_registry.py/ToolRegistry/get_or_create_tool_schema",
        "src/criticsearch/tools/tool_registry.py/ToolRegistry/register_tool"
      ],
      "reference_who": [
        "src/criticsearch/tools/models.py/Function"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "create_schema_from_function",
      "md_content": [
        "### `create_schema_from_function`\n\n#### Overview\n\nThe `create_schema_from_function` method is a class method designed to generate a schema for a tool based on a target function. This schema includes the function's name, description, and parameters, providing a structured format for the tool. The method processes the function's docstring to extract relevant information and constructs a JSON-like schema, which can be used for further automation or documentation purposes.\n\n#### Parameters\n- **cls** (`type`): The class that is calling the method. This is implicitly passed when the method is invoked.\n- **target_function** (`function`): The function from which the schema will be created. The method inspects the function's name, docstring, and parameters to build the schema.\n\n#### Method Description\n\n1. **Extracting Function Metadata**: \n   - The method starts by extracting the function's name and docstring using the `__name__` attribute and `inspect.getdoc()`, respectively.\n   - If the docstring is empty, a default message of \"No description provided.\" is used.\n\n2. **Parsing the Docstring**: \n   - The docstring is parsed using the `Docstring` class, which is assumed to support Google-style docstrings.\n   - Sections of the docstring are identified and classified into text and parameters sections.\n\n3. **Processing the Parameters**: \n   - The method iterates through the parsed sections to extract the description of the function and its parameters.\n   - It then inspects the function's signature using the `inspect.signature()` method to identify the types, default values, and required status of each parameter.\n   - Parameters are classified into required and optional based on whether they have default values or not.\n\n4. **Building the Schema**: \n   - A `Function` schema is constructed using the extracted information. The function's parameters are detailed in the schema with their types and descriptions. If any parameter is of list type, the `get_list_type_annotation` function is used to determine the type of elements in the list.\n   - The `Parameters` class is utilized to define the structure of the parameters, including their types, descriptions, required status, and whether additional properties are allowed.\n\n5. **Returning the Schema**: \n   - Finally, the schema is returned as a serialized model, excluding any `None` values.\n\n#### Return Value\nThe method returns a serialized model of the function schema, including:\n- **name**: The name of the function.\n- **description**: The description extracted from the function's docstring.\n- **parameters**: A structured schema of the function's parameters, which includes their names, types, descriptions, and required status.\n\n#### Example\n\nAssume a function `example_function` with the following signature:\n\n```python\ndef example_function(arg1: int, arg2: List[str] = []):\n    \"\"\"\n    Example function to demonstrate schema creation.\n\n    Args:\n        arg1 (int): The first argument.\n        arg2 (List[str], optional): The second argument, which is a list of strings.\n    \"\"\"\n    pass\n```\n\nThe `create_schema_from_function` method would generate a schema with the following structure:\n\n```json\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"example_function\",\n    \"description\": \"Example function to demonstrate schema creation.\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"arg1\": {\n          \"type\": \"int\",\n          \"description\": \"The first argument.\"\n        },\n        \"arg2\": {\n          \"type\": \"list\",\n          \"items\": {\n            \"type\": \"str\"\n          },\n          \"description\": \"The second argument, which is a list of strings.\"\n        }\n      },\n      \"required\": [\"arg1\"],\n      \"additionalProperties\": false\n    }\n  }\n}\n```\n\n#### Notes\n- The method currently supports Google-style docstrings and expects parameters to be described in a specific format within the docstring.\n- It relies on the `get_list_type_annotation` function to determine the type of elements in list-type parameters.\n- The `Parameters` class is used to define the structure of the parameters, ensuring a consistent schema for function input validation.",
        "### `create_schema_from_function` Method Documentation\n\n#### Overview\nThe `create_schema_from_function` method is designed to generate a schema for a function, which includes the function's name, description, and parameters. It extracts this information from the provided target function and returns it in a structured format suitable for use in a tool schema.\n\n#### Parameters\n- `cls` (Type): The class that calls this method. Typically, it would be a subclass of a model class.\n- `target_function` (Function): The function for which the schema is being created. The method inspects the function's name, docstring, and parameters.\n\n#### Returns\n- Returns a dictionary representing the function schema. This schema includes the function's name, description, and parameters, formatted as per the JSON Schema standard.\n\n#### Method Behavior\n1. **Extract Function Name and Docstring:**\n   The method first extracts the name of the target function using `target_function.__name__`. It also retrieves the docstring associated with the function using `inspect.getdoc(target_function)`. If no docstring is provided, a default message \"No description provided.\" is used.\n\n2. **Parse the Docstring:**\n   The docstring is parsed using the `Docstring` class, specifically following the Google-style format. The parsed sections are analyzed to extract textual descriptions and parameters. The method supports Google-style docstrings and divides the docstring into sections such as text (for description) and parameters.\n\n3. **Extract Description and Parameters:**\n   - **Description:** The method extracts the general description from the docstring.\n   - **Parameters:** The method gathers information on each parameter defined in the function's signature by examining the parsed sections and the docstring.\n\n4. **Inspect Function Signature:**\n   The function signature is inspected using the `inspect.signature(target_function)`, which allows the method to extract details about each parameter. The method checks if each parameter has a default value and whether it is required or optional.\n\n5. **Build Schema for Parameters:**\n   - For each parameter, the method determines its type (either from the function signature or from the docstring).\n   - It identifies whether a parameter is required or optional based on the presence of a default value.\n   - If the parameter is of a list type, the method calls `get_list_type_annotation()` to determine the type of items in the list.\n\n6. **Build the Final Schema:**\n   After processing the function’s name, description, and parameters, the method constructs a final schema using the `Function` and `Parameters` classes. The schema includes:\n   - **Name:** The function's name.\n   - **Description:** The function's description extracted from the docstring.\n   - **Parameters:** A detailed list of parameters, including their types, descriptions, and required status.\n\n7. **Return Schema:**\n   The function schema is returned as a model, with `exclude_none=True` ensuring that any optional attributes with `None` values are omitted from the final output.\n\n#### Example Usage\n\n```python\nfrom some_module import Tool\n\ndef example_function(param1: str, param2: int = 10, param3: List[str] = None):\n    \"\"\"\n    This is an example function.\n\n    Args:\n        param1 (str): A required string parameter.\n        param2 (int, optional): An optional integer parameter. Defaults to 10.\n        param3 (List[str], optional): A list of strings. Defaults to an empty list.\n\n    Returns:\n        str: A sample return value.\n    \"\"\"\n    return str(param1)\n\n# Create schema from the example function\nschema = Tool.create_schema_from_function(example_function)\n```\n\nIn this example, the `create_schema_from_function` method processes the `example_function` to generate a schema that includes the function's name, description, and parameter details.\n\n#### Notes\n- The method currently supports only Google-style docstrings. If the docstring format differs, it may not parse correctly.\n- The function also processes the parameter type annotations, extracting and formatting them to conform to the JSON Schema specification. For list-type parameters, it uses the helper function `get_list_type_annotation()` to determine the type of items in the list.\n- The generated schema is returned in a format that can be used directly for tool integration or validation."
      ],
      "code_start_line": 85,
      "code_end_line": 147,
      "params": [
        "cls",
        "target_function"
      ],
      "have_return": true,
      "code_content": "    def create_schema_from_function(cls, target_function):\n        \"\"\"Create a Tool schema from a target function.\"\"\"\n\n        # 提取函数名称和文档字符串\n        func_name = target_function.__name__\n        func_doc = inspect.getdoc(target_function) or \"No description provided.\"\n\n        # 解析文档字符串，生成 sections\n        docstring = Docstring(func_doc)\n        # NOTE: Only support  Google-style right now.\n        sections = docstring.parse(\"google\")\n\n        # 提取描述信息\n        description = \"\"\n        parameters = []\n\n        for section in sections:\n            if section.kind == DocstringSectionKind.text:\n                description = section.value.strip()\n            elif section.kind == DocstringSectionKind.parameters:\n                parameters = section.value\n\n        # 提取参数信息\n        signature = inspect.signature(target_function)\n        required = []\n        properties = {}\n\n        for param_name, param in signature.parameters.items():\n            param_type = param.annotation if param.annotation != inspect._empty else Any\n            param_default = param.default if param.default != inspect._empty else ...\n\n            # 从解析的参数部分提取描述\n            param_description = None\n            for param_info in parameters:\n                if param_info.name == param_name:  # 使用属性访问\n                    param_description = param_info.description\n                    break\n\n            if param_default is ...:\n                required.append(param_name)\n\n            properties[param_name] = {\n                \"type\": param_type.__name__,\n                \"description\": param_description or f\"The {param_name} parameter.\",\n            }\n\n            if get_origin(param_type) is list:\n                properties[param_name][\"items\"] = get_list_type_annotation(param_type)\n\n        # Build the final Function and Tool schema\n        function_schema = Function(\n            name=func_name,\n            description=description,\n            parameters=Parameters(\n                type=\"object\",\n                properties=properties,\n                required=required,\n                additionalProperties=False,\n            ),\n        )\n        return cls(type=\"function\", function=function_schema).model_dump(\n            exclude_none=True\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/tool_registry.py/ToolRegistry/get_or_create_tool_schema",
        "src/criticsearch/tools/tool_registry.py/ToolRegistry/register_tool"
      ],
      "reference_who": [
        "src/criticsearch/tools/models.py/get_list_type_annotation",
        "src/criticsearch/tools/models.py/Parameters",
        "src/criticsearch/tools/models.py/Function"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_delivery_date",
      "md_content": [
        "**get_delivery_date**: The function of get_delivery_date is to retrieve the delivery date for a customer's order based on the provided order ID and delivery type.\n\n**parameters**: The parameters of this function:\n· order_id (str): A string representing the unique ID of the order for which the delivery date is being requested.\n· delivery_type (str): A string specifying the type of delivery (e.g., \"standard\" or \"express\"). The default value is \"standard\".\n\n**Code Description**: \nThe `get_delivery_date` function is designed to fetch the delivery date associated with a given customer order. It accepts two parameters:\n1. `order_id` (str): This is a required parameter, which uniquely identifies the order whose delivery date is to be determined. \n2. `delivery_type` (str): This is an optional parameter that defines the delivery method for the order. The default value is \"standard\", but other types such as \"express\" could also be provided. The specific functionality regarding how the delivery type impacts the retrieval of the delivery date is not yet implemented, as the function currently has no logic inside it.\n\nAs the function is currently not implemented (indicated by the `pass` statement), no actions are taken within it, and it does not return any values at the moment.\n\n**Note**: \n- This function currently does not contain any operational logic or return values, meaning it needs to be implemented with appropriate business logic to fetch the delivery date based on the order ID and delivery type.\n- The function signature suggests that it will likely interact with a system (e.g., a database or API) to determine the delivery date based on the provided inputs, but this functionality is not present in the current state."
      ],
      "code_start_line": 152,
      "code_end_line": 160,
      "params": [
        "order_id",
        "delivery_type"
      ],
      "have_return": false,
      "code_content": "    def get_delivery_date(order_id: str, delivery_type: str = \"standard\"):\n        \"\"\"\n        Get the delivery date for a customer's order.\n\n        Parameters:\n            order_id (str): The unique ID of the order.\n            delivery_type (str): The type of delivery (e.g., standard or express).\n        \"\"\"\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tools/image_analyzer.py": [
    {
      "type": "ClassDef",
      "name": "ImageAnalyzer",
      "md_content": [
        "**ImageAnalyzer**: The function of ImageAnalyzer is to analyze an image using a specified vision model.\n\n**attributes**:\n· model: Specifies the vision model used for image analysis, defaulting to \"gpt-4o-mini\".\n\n**Code Description**:  \nThe `ImageAnalyzer` class is designed to facilitate image analysis by processing image data through a specified model. The class contains the following key components:\n\n1. **`__init__(self, model=\"gpt-4o-mini\")`**: This is the initializer method for the `ImageAnalyzer` class. It accepts one optional parameter `model`, which determines the vision model used for image analysis. The default value for this parameter is set to \"gpt-4o-mini\". This attribute is stored within the class as `self.model`.\n\n2. **`analyze_image(self, image_data: str) -> Dict`**: This is the main method responsible for analyzing an image. It accepts `image_data`, which can either be a URL, a file path, or base64-encoded image data. The method follows these steps:\n    - If `image_data` is a string:\n        - If it starts with \"http\", the method assumes the string represents a URL. A request is sent to the URL to retrieve the image, which is then processed by opening it using the `PIL.Image.open()` method after the image content is fetched.\n        - If the string is not a URL, it is treated as a file path and the image is opened directly from the specified location.\n    - The image is then converted to base64 format using the `base64` module, enabling the image to be handled as a string representation. This is done by saving the image into a `BytesIO` buffer, which is then base64-encoded.\n    - If `image_data` is already base64-encoded, the method directly assigns it to `image_base64`.\n    - A dictionary is returned containing a description of the analysis, the model used, and the format of the image data (which is \"base64\").\n\n3. The `analyze_image` method handles exceptions that might occur during the image processing. If an error occurs at any stage, the method returns a dictionary with the error message.\n\n**Note**: \n- The method expects the input image data to either be a file path, URL, or base64-encoded string. It does not handle non-image data or invalid formats.\n- The image analysis itself is not fully implemented in the code; the return statement provides a placeholder response indicating that the analysis was completed.\n- The choice of model for image analysis is configurable but defaults to \"gpt-4o-mini\". However, the analysis logic based on the specified model is not included in this snippet.\n\n**Output Example**:  \nA possible return value from the `analyze_image` method might look like this:\n\n```json\n{\n  \"description\": \"Image analysis completed\",\n  \"model_used\": \"gpt-4o-mini\",\n  \"image_format\": \"base64\"\n}\n```"
      ],
      "code_start_line": 9,
      "code_end_line": 37,
      "params": [],
      "have_return": true,
      "code_content": "class ImageAnalyzer:\n    def __init__(self, model=\"gpt-4o-mini\"):\n        self.model = model\n\n    def analyze_image(self, image_data: str) -> Dict:\n        \"\"\"Analyze an image using the specified vision model.\"\"\"\n        try:\n            # Convert image data to base64 if it's a URL or file path\n            if isinstance(image_data, str):\n                if image_data.startswith(\"http\"):\n                    response = requests.get(image_data)\n                    image = Image.open(BytesIO(response.content))\n                else:\n                    image = Image.open(image_data)\n\n                buffered = BytesIO()\n                image.save(buffered, format=\"PNG\")\n                image_base64 = base64.b64encode(buffered.getvalue()).decode()\n            else:\n                image_base64 = image_data\n\n            # TODO this is a stand in for later use\n            return {\n                \"description\": \"Image analysis completed\",\n                \"model_used\": self.model,\n                \"image_format\": \"base64\",\n            }\n        except Exception as e:\n            return {\"error\": str(e)}\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the object with a specific model configuration.\n\n**parameters**:\n· model: The model to be used, which is set to a default value of \"gpt-4o-mini\".\n\n**Code Description**: \nThe `__init__` method is a constructor that initializes an object of the class. It takes a single parameter, `model`, which defines the model to be used by the object. If no argument is provided during the object creation, it defaults to \"gpt-4o-mini\". The value passed for `model` is then stored in the instance variable `self.model`, which will allow the object to refer to this model later in its usage.\n\nThis method ensures that each object of the class starts with a specific configuration, defined by the `model` parameter. The default value \"gpt-4o-mini\" can be overridden when creating an object if a different model is needed.\n\n**Note**: The `__init__` method does not perform any complex operations or return any value. It only sets up the initial state of the object by assigning the provided `model` value to the instance."
      ],
      "code_start_line": 10,
      "code_end_line": 11,
      "params": [
        "self",
        "model"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, model=\"gpt-4o-mini\"):\n        self.model = model\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "analyze_image",
      "md_content": [
        "**analyze_image**: The function of analyze_image is to analyze an image using the specified vision model.\n\n**parameters**: The parameters of this Function.\n· image_data: A string representing the image data, which can be a URL or a file path to an image.\n\n**Code Description**: The analyze_image function is designed to process an image for analysis. It accepts a single parameter, image_data, which can either be a URL pointing to an image or a local file path. The function first checks if the provided image_data is a string. If it is a URL (indicated by the string starting with \"http\"), the function makes an HTTP GET request to retrieve the image. The image is then opened using the PIL library's Image module. If the image_data is a local file path, the function directly opens the image from that path.\n\nOnce the image is successfully opened, the function converts the image into a PNG format and encodes it into a base64 string. This base64 representation is useful for transmitting image data over the web or embedding it in JSON responses. If the image_data is already in base64 format, it is used directly without further processing.\n\nThe function is currently set up to return a dictionary containing a description of the analysis completion, the model used for analysis (which is an attribute of the class), and the format of the image (base64). In the event of an error during the image processing, the function captures the exception and returns a dictionary with an error message.\n\n**Note**: It is important to ensure that the image_data provided is either a valid URL or a correct file path. The function does not perform extensive validation on the input and assumes that the provided data is in the correct format. Additionally, the actual image analysis logic is not implemented yet, as indicated by the placeholder comment in the code.\n\n**Output Example**: \n{\n    \"description\": \"Image analysis completed\",\n    \"model_used\": \"VisionModelXYZ\",\n    \"image_format\": \"base64\"\n}"
      ],
      "code_start_line": 13,
      "code_end_line": 37,
      "params": [
        "self",
        "image_data"
      ],
      "have_return": true,
      "code_content": "    def analyze_image(self, image_data: str) -> Dict:\n        \"\"\"Analyze an image using the specified vision model.\"\"\"\n        try:\n            # Convert image data to base64 if it's a URL or file path\n            if isinstance(image_data, str):\n                if image_data.startswith(\"http\"):\n                    response = requests.get(image_data)\n                    image = Image.open(BytesIO(response.content))\n                else:\n                    image = Image.open(image_data)\n\n                buffered = BytesIO()\n                image.save(buffered, format=\"PNG\")\n                image_base64 = base64.b64encode(buffered.getvalue()).decode()\n            else:\n                image_base64 = image_data\n\n            # TODO this is a stand in for later use\n            return {\n                \"description\": \"Image analysis completed\",\n                \"model_used\": self.model,\n                \"image_format\": \"base64\",\n            }\n        except Exception as e:\n            return {\"error\": str(e)}\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tools/content_scraper/tavily_extract.py": [
    {
      "type": "ClassDef",
      "name": "TavilyExtract",
      "md_content": [
        "**TavilyExtract**: The function of TavilyExtract is to interact with the Tavily API to extract content from a list of provided URLs.\n\n**attributes**: The attributes of this Class.\n· base_url: The base URL for the Tavily API endpoint used for content extraction.\n· _api_key: The API key used for authentication with the Tavily API.\n· headers: The headers required for making requests to the Tavily API, including content type and authorization.\n\n**Code Description**: The TavilyExtract class is designed to facilitate the extraction of content from specified URLs using the Tavily API. Upon initialization, it requires an API key, which is stored as a private attribute (_api_key). The class constructs the necessary headers for API requests, ensuring that the content type is set to JSON and that the API key is included in the authorization header.\n\nThe primary method of the class, `extract_content`, takes a list of URLs as input. It constructs a JSON payload containing these URLs and makes an asynchronous POST request to the Tavily API using the httpx library. The method handles potential HTTP errors by raising exceptions for any 4xx or 5xx responses, and it captures request errors as well. If the request is successful, the method parses the JSON response and returns the data. In case of errors, it returns a dictionary containing the error message.\n\nThis class is utilized within the `scrape` function of the ContentScraper module. The `scrape` function first initializes an instance of TavilyExtract with the API key retrieved from the settings. It then calls the `extract_content` method, passing the list of URLs to be scraped. The results from the Tavily API are checked for errors; if any errors are present, the function logs the error and falls back to a custom web scraping method. If the extraction is successful, it processes the results, extracting the necessary data and merging successful and failed results into a final output.\n\n**Note**: Ensure that the API key is valid and has the necessary permissions to access the Tavily API. The URLs provided should be accessible and valid to avoid unnecessary errors during the extraction process.\n\n**Output Example**: A possible appearance of the code's return value could be:\n{\n  \"results\": [\n    {\n      \"url\": \"https://example.com\",\n      \"raw_content\": \"<html>...</html>\"\n    }\n  ],\n  \"failed_results\": [\n    {\n      \"url\": \"https://failed-url.com\"\n    }\n  ]\n}",
        "**TavilyExtract**: The function of TavilyExtract is to facilitate the extraction of content from a list of URLs using the Tavily API.\n\n**attributes**: The attributes of this Class.\n· api_key: str - The API key used for authenticating requests to the Tavily API.  \n· base_url: str - The base URL for the Tavily API endpoint used for content extraction.  \n· headers: dict - The headers required for making requests to the Tavily API, including content type and authorization.\n\n**Code Description**: The TavilyExtract class is designed to interact with the Tavily API for the purpose of extracting content from specified URLs. Upon initialization, the class requires an API key, which is stored as a private attribute. The base URL for the Tavily API is set to \"https://api.tavily.com/extract\", and the necessary headers for making requests are constructed, including the content type and authorization using the provided API key.\n\nThe primary method of this class is `extract_content`, which accepts a list of URLs as its parameter. This method is asynchronous and utilizes the httpx library to send a POST request to the Tavily API. The request payload consists of the list of URLs, which is sent in JSON format. Upon receiving a response, the method attempts to parse the JSON data returned by the API. If the request is successful, the parsed data is returned. In the event of a request error, the method captures the exception and returns a structured error message indicating the nature of the issue.\n\nThe TavilyExtract class is utilized within the `scrape` function found in the ContentScraper module. In this context, an instance of TavilyExtract is created using an API key sourced from the application settings. The `scrape` function calls the `extract_content` method of the TavilyExtract instance to retrieve content from the provided URLs. If the Tavily API returns an error, the function logs the error and resorts to a fallback web scraping method to ensure that content extraction continues. This integration allows for a seamless workflow where the TavilyExtract class serves as the primary means of content extraction, while also providing a backup mechanism in case of failure.\n\n**Note**: When using this class, ensure that the API key is valid and that the URLs provided are accessible. Additionally, be mindful of the legal and ethical considerations surrounding web scraping, including compliance with the target website's terms of service.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n{\n    \"results\": [\n        {\"url\": \"http://example.com\", \"raw_content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"raw_content\": \"No content available\"}\n    ],\n    \"failed_results\": []\n}\n```",
        "**TavilyExtract**: The function of TavilyExtract is to interact with the Tavily API to asynchronously extract content from a list of URLs.\n\n**attributes**:\n· `base_url`: A string representing the base URL for the Tavily API (`\"https://api.tavily.com/extract\"`).\n· `_api_key`: A string storing the API key used for authorization with the Tavily API.\n· `headers`: A dictionary containing the HTTP headers for making requests to the Tavily API, including `Content-Type` and `Authorization`.\n\n**Code Description**:  \nThe `TavilyExtract` class is designed to interact with the Tavily API to extract content from a given list of URLs. It provides an asynchronous method, `extract_content`, which sends a POST request to the Tavily API for content extraction.\n\n1. **Initialization (`__init__` method)**:\n   - This method accepts an `api_key` parameter, which is required to authenticate requests to the Tavily API.\n   - The `base_url` is set to the Tavily API endpoint (`\"https://api.tavily.com/extract\"`), and the `headers` dictionary is populated with the appropriate authentication headers using the provided `api_key`.\n  \n2. **Content Extraction (`extract_content` method)**:\n   - The `extract_content` method is an asynchronous function that accepts a list of URLs (`urls: List[str]`) and sends a POST request to the Tavily API to extract content from these URLs.\n   - A `payload` is created, containing the list of URLs, and a request is made using `httpx.AsyncClient` with HTTP/2 support for optimal performance.\n   - The method awaits the response, processes the returned data, and returns the parsed JSON content from the Tavily API response.\n   - If any errors occur during the request (e.g., network issues, invalid API response), a dictionary is returned containing an error message with the details of the issue.\n\n**Relationship with Other Project Components**:\n- The `TavilyExtract` class is utilized by the `scrape` function in the `ContentScraper` module, which is designed to scrape content from multiple URLs.\n- The `scrape` function leverages the `extract_content` method of `TavilyExtract` to retrieve content from the Tavily API. If the API call is successful, the results are processed, and a `ScrapedDataList` is returned. If the Tavily API fails to extract content, the function falls back to using a secondary web scraping method via the `FallbackWebScraper` class.\n- The `scrape` function handles the aggregation of both successful and failed extraction results and ensures that content from URLs is collected despite any errors with the Tavily API.\n\n**Note**:  \n- Ensure that a valid API key is provided when instantiating the `TavilyExtract` class, as this key is required for authorization with the Tavily API.\n- The method `extract_content` is asynchronous and should be called within an asynchronous context (e.g., using `await`).\n- The content extracted from the URLs will be returned as a JSON object. If the extraction fails, an error message will be returned instead of the expected data.\n\n**Output Example**:  \nA possible appearance of the code's return value could be:\n```\n{\n    \"data\": [\n        {\"url\": \"http://example.com\", \"title\": \"Example Title\", \"content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"title\": \"Another Example\", \"content\": \"No content available\"}\n    ]\n}\n```",
        "**TavilyExtract**: The function of TavilyExtract is to interact with the Tavily API to extract content from a list of URLs.\n\n**attributes**: The attributes of this Class.\n· api_key: str - The API key used for authenticating requests to the Tavily API.  \n· base_url: str - The base URL for the Tavily API endpoint used for content extraction.  \n· headers: dict - The headers required for the API request, including content type and authorization.  \n· _client: AsyncClient - An instance of the AsyncClient from the httpx library, configured to support HTTP/2 for making asynchronous requests.\n\n**Code Description**: The TavilyExtract class is designed to facilitate the extraction of content from web pages using the Tavily API. Upon initialization, it requires an API key, which is essential for authenticating requests to the API. The class sets up a base URL pointing to the Tavily API's extract endpoint and prepares the necessary headers, including the content type and authorization token.\n\nThe primary method of this class is `extract_content`, which is an asynchronous function that accepts a list of URLs. This method is decorated with a retry mechanism that allows it to automatically retry the request in case of failures, up to a maximum number of retries defined in the settings. The method constructs a payload containing the URLs and sends a POST request to the Tavily API. It handles various exceptions, including request errors and HTTP status errors, logging warnings as necessary.\n\nIf the response from the API is successful, the method attempts to parse the JSON response. If the JSON parsing fails, it triggers a retry. The method is designed to return the parsed JSON data as a dictionary. In the event of a network request error or an HTTP status error, the method raises the appropriate exceptions after logging the warnings.\n\nThe TavilyExtract class is utilized within the `ReverseUpgradeWorkflow` class, where it serves as a scraper for extracting content from URLs. The `ReverseUpgradeWorkflow` initializes an instance of TavilyExtract with a predefined API key, allowing it to leverage the content extraction capabilities provided by Tavily. Additionally, the TavilyExtract class is called within the `scrape` method of the ContentScraper class, which orchestrates the scraping process by first attempting to extract content using Tavily and falling back to a secondary scraping method if necessary.\n\n**Note**: It is crucial to ensure that the API key provided during the initialization of the TavilyExtract instance is valid and has the necessary permissions to access the Tavily API. Proper error handling is implemented to manage cases where the API requests fail or return unexpected results.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n{\n    \"results\": [\n        {\"url\": \"http://example.com\", \"raw_content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"raw_content\": \"No content available\"}\n    ],\n    \"failed_results\": []\n}\n```",
        "**TavilyExtract**: The function of TavilyExtract is to interact with the Tavily API to extract content from specified URLs.\n\n**attributes**: The attributes of this Class.\n· api_key: str - The API key used for authenticating requests to the Tavily API.  \n· base_url: str - The base URL for the Tavily API endpoint used for content extraction.  \n· headers: dict - A dictionary containing the headers required for the API request, including content type and authorization.  \n· _client: AsyncClient - An instance of the AsyncClient from the httpx library, configured to support HTTP/2 for making asynchronous requests.\n\n**Code Description**: The TavilyExtract class is designed to facilitate content extraction from web pages using the Tavily API. Upon initialization, it requires an API key, which is essential for authenticating requests to the API. The class sets up the base URL for the API endpoint and prepares the necessary headers, including the content type and authorization token.\n\nThe primary method of this class is `extract_content`, which is an asynchronous method that accepts a list of URLs. This method constructs a payload containing the URLs and additional parameters for the extraction process. It utilizes the AsyncClient to send a POST request to the Tavily API, handling the response in a structured manner.\n\nThe `extract_content` method is decorated with a retry mechanism that allows it to automatically retry the request in case of network errors or if the response cannot be parsed as JSON. This is crucial for ensuring robustness in scenarios where the API may be temporarily unavailable or return unexpected results.\n\nIn the event of a successful response, the method attempts to parse the JSON data returned by the API. If the parsing fails, it triggers a retry, logging a warning message for the developer's awareness. Similarly, if a network request fails or the API returns an HTTP error status, appropriate warnings are logged, and the method raises exceptions to signal the failure.\n\nThe TavilyExtract class is utilized within the ReverseUpgradeWorkflow class, where it serves as a scraper to extract content from URLs provided during the workflow's execution. The integration with the SearchAggregator and BaseAgent classes highlights its role in enhancing the quality of the questions and answers being processed by leveraging external content.\n\nAdditionally, the TavilyExtract class is called within the scrape function of the ContentScraper class, which orchestrates the content extraction process. This function attempts to extract content using the Tavily API and falls back to a secondary scraping method if the Tavily extraction fails, demonstrating the importance of the TavilyExtract class in the overall content scraping workflow.\n\n**Note**: It is essential to ensure that the API key provided during the initialization of the TavilyExtract class is valid and that the URLs passed to the `extract_content` method are accessible. The class is designed to handle errors and retries, but proper input is necessary for optimal performance.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```json\n{\n    \"results\": [\n        {\"url\": \"http://example.com\", \"raw_content\": \"This is the main content of the page.\"}\n    ],\n    \"failed_results\": []\n}\n```"
      ],
      "code_start_line": 9,
      "code_end_line": 44,
      "params": [],
      "have_return": true,
      "code_content": "class TavilyExtract:\n    def __init__(self, api_key: str):\n        self.base_url = \"https://api.tavily.com/extract\"\n        self._api_key = api_key\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": self._api_key,\n        }\n        self._client = httpx.AsyncClient(http2=True)\n\n    @retry(stop=stop_after_attempt(settings.max_retries), wait=wait_fixed(1), reraise=True)\n    async def extract_content(self, urls: List[str]) -> dict:\n        \"\"\"\n        向 Tavily API 发送请求并解析 JSON。若网络请求或解析失败，\n        会自动重试最多 settings.max_retries 次，再失败则抛出。\n        \"\"\"\n        payload = {\n            \"urls\": urls,\n            \"include_images\": False,\n            \"extract_depth\": \"basic\",\n        }\n        try:\n            resp = await self._client.post(self.base_url, headers=self.headers, json=payload)\n            resp.raise_for_status()\n            try:\n                return resp.json()\n            except json.JSONDecodeError as e:\n                printer.print(f\"[WARN] 第一次解析 JSON 失败: {e}，重试模型调用…\")\n                # 触发重试\n                raise\n        except httpx.RequestError as e:\n            printer.print(f\"[WARN] 网络请求错误: {e}，重试…\")\n            raise\n        except httpx.HTTPStatusError as e:\n            printer.print(f\"[WARN] HTTP 状态错误: {e}，重试…\")\n            raise\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/__init__",
        "src/criticsearch/tools/content_scraper/__init__.py",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape",
        "tests/test_tavily_extract.py",
        "tests/test_tavily_extract.py/test_extract_content_success",
        "tests/test_tavily_extract.py/test_extract_content_retry_on_invalid_json",
        "tests/test_tavily_extract.py/test_real_extract_content"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the TavilyExtract object by setting up the base URL for API requests and configuring the necessary headers for authentication.\n\n**parameters**: The parameters of this function.\n· api_key: str - This is the API key required for authenticating requests to the Tavily API.\n\n**Code Description**: \nThe `__init__` function is the constructor method of the `TavilyExtract` class. It is responsible for setting up the essential attributes needed to interact with the Tavily API. Upon initialization:\n\n1. The `base_url` attribute is assigned a fixed value, `\"https://api.tavily.com/extract\"`. This URL is the endpoint used for making API requests.\n  \n2. The `_api_key` attribute is initialized with the provided `api_key` parameter, which is a string containing the user's personal API key. This key is used to authenticate requests made to the Tavily API.\n\n3. The `headers` attribute is a dictionary containing two key-value pairs:\n   - `\"Content-Type\": \"application/json\"`: This specifies that the data being sent in requests will be formatted as JSON.\n   - `\"Authorization\": f\"Bearer {self._api_key}\"`: This header is used for bearer token authentication, where the API key is included in the Authorization header to verify the identity of the requester.\n\nThis constructor ensures that an instance of the `TavilyExtract` class is correctly set up with the necessary API URL and authentication details for subsequent API interactions.\n\n**Note**: \n- The `api_key` parameter is mandatory for initializing the `TavilyExtract` class and must be kept secure to prevent unauthorized access.\n- The `base_url` and `headers` are automatically configured and cannot be modified directly through the constructor. Any changes to these values would require altering the code itself.",
        "**__init__**: The function of __init__ is to initialize an instance of the TavilyExtract class with the necessary API key and set up the required configurations for making API requests.\n\n**parameters**: The parameters of this Function.\n· api_key: A string representing the API key required for authentication when making requests to the Tavily API.\n\n**Code Description**: The __init__ function is a constructor for the TavilyExtract class. It takes a single parameter, api_key, which is essential for authenticating requests to the Tavily API. Upon instantiation of the class, the function performs the following actions:\n\n1. It sets the base URL for the Tavily API to \"https://api.tavily.com/extract\". This URL will be used as the endpoint for all subsequent API calls made by the instance.\n\n2. The provided api_key is stored in a private attribute, _api_key, which ensures that the key is not directly accessible from outside the class. This is a common practice to enhance security by encapsulating sensitive information.\n\n3. The function initializes a headers dictionary that contains two key-value pairs:\n   - \"Content-Type\": This is set to \"application/json\", indicating that the requests sent to the API will be in JSON format.\n   - \"Authorization\": This is set to \"Bearer {self._api_key}\", which is the standard way to include the API key in the request headers for authorization purposes.\n\n4. Finally, an asynchronous HTTP client is created using httpx.AsyncClient with HTTP/2 support enabled. This client will be used for making asynchronous requests to the Tavily API, allowing for efficient handling of multiple requests without blocking the execution of the program.\n\n**Note**: It is important to ensure that the api_key provided is valid and has the necessary permissions to access the Tavily API. Additionally, since the HTTP client is asynchronous, the methods that utilize this client should be defined as asynchronous functions to take full advantage of its capabilities.",
        "**__init__**: The function of __init__ is to initialize an instance of the TavilyExtract class with the necessary API key and set up the required parameters for making API requests.\n\n**parameters**: The parameters of this Function.\n· api_key: A string that represents the API key required for authenticating requests to the Tavily API.\n\n**Code Description**: The __init__ function is a constructor for the TavilyExtract class. It takes a single parameter, api_key, which is expected to be a string. This API key is essential for authenticating requests made to the Tavily API. Upon instantiation of the class, the function sets the base URL for the API to \"https://api.tavily.com/extract\", which will be used for all subsequent API calls. The function also initializes an instance variable, _api_key, with the provided api_key to store the key securely within the class instance.\n\nAdditionally, the function constructs a headers dictionary that includes the \"Content-Type\" set to \"application/json\" and the \"Authorization\" set to the provided API key. This headers dictionary will be used in HTTP requests to ensure that the requests are properly formatted and authenticated.\n\nFurthermore, the function creates an asynchronous HTTP client instance using httpx.AsyncClient with HTTP/2 support enabled. This client will facilitate making asynchronous requests to the Tavily API, allowing for efficient handling of multiple requests without blocking the execution of the program.\n\n**Note**: It is important to ensure that the provided API key is valid and has the necessary permissions to access the Tavily API. Users should also be aware that the httpx library must be installed in the environment to utilize the asynchronous client functionality."
      ],
      "code_start_line": 10,
      "code_end_line": 17,
      "params": [
        "self",
        "api_key"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, api_key: str):\n        self.base_url = \"https://api.tavily.com/extract\"\n        self._api_key = api_key\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": self._api_key,\n        }\n        self._client = httpx.AsyncClient(http2=True)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_content",
      "md_content": [
        "**extract_content**: The function of extract_content is to asynchronously request and retrieve content data from an external service based on a list of provided URLs.\n\n**parameters**:\n· urls: List[str]  \n  - A list of URLs for which content needs to be extracted.\n\n**Code Description**:  \nThe `extract_content` function is an asynchronous method that facilitates the extraction of content from a specified external service. The method is designed to take a list of URLs as input and then make an HTTP POST request to a remote API to retrieve the content associated with these URLs.\n\n1. **Payload Creation**:  \n   The function first constructs a payload dictionary containing the `urls` parameter passed to the function. This payload is intended to be sent as the body of the POST request.\n\n2. **Making the API Call**:  \n   Using the `httpx.AsyncClient` with HTTP/2 support enabled, the function sends an asynchronous POST request to the service’s base URL (stored in `self.base_url`). The headers (stored in `self.headers`) and the payload (containing the URLs) are included in the request. This is performed within an asynchronous context to ensure non-blocking behavior during the request.\n\n3. **Handling Responses**:  \n   Upon receiving the response, the function checks whether the request was successful by calling `response.raise_for_status()`. If the status code indicates an error (4xx/5xx), an exception is raised and caught.\n\n4. **Error Handling**:  \n   If an HTTP error occurs (e.g., 404 or 500), the function will return a dictionary with the key `error` and a message indicating that an HTTP error occurred. Similarly, if there’s an issue with the request itself (e.g., network issues), a `RequestError` is caught, and an error message is returned.\n\n5. **Returning Data**:  \n   If the request is successful, the function parses the response as JSON and returns the resulting data. This JSON data is typically structured to contain content related to the requested URLs.\n\nIn terms of its usage, the `extract_content` method is invoked within the `scrape` function found in the `src/criticsearch/tools/content_scraper/__init__.py` file. The `scrape` function calls `extract_content` to attempt content extraction using the Tavily API. If successful, it processes the returned results into a list of `ScrapedData` objects. If the extraction fails or encounters errors, it falls back to a custom web scraping mechanism via `FallbackWebScraper`. This integration ensures the robustness of the scraping process, allowing for alternative methods when the primary API fails.\n\n**Note**:  \n- The function is asynchronous and requires an `await` keyword when calling it.  \n- The `httpx` library should be installed and configured correctly to ensure successful HTTP requests.  \n- Proper error handling is implemented to catch both HTTP-specific and general request errors, returning meaningful error messages for each type of failure.\n\n**Output Example**:  \nA possible response when the request is successful could look like the following:\n\n```json\n{\n  \"results\": [\n    {\n      \"url\": \"http://example.com/page1\",\n      \"raw_content\": \"This is the content of the first page.\"\n    },\n    {\n      \"url\": \"http://example.com/page2\",\n      \"raw_content\": \"This is the content of the second page.\"\n    }\n  ],\n  \"failed_results\": []\n}\n```\n\nIf there is an error, the returned dictionary may look like this:\n\n```json\n{\n  \"error\": \"HTTP error occurred: 404 Client Error: Not Found for url: http://example.com/page1\"\n}\n```",
        "**extract_content**: The function of extract_content is to asynchronously extract content from a list of provided URLs using the Tavily API.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs to scrape content from.\n\n**Code Description**: The `extract_content` function is an asynchronous method that takes a list of URLs as input and sends a POST request to the Tavily API to extract content from those URLs. The function begins by constructing a payload that includes the provided URLs. It then utilizes the `httpx.AsyncClient` to create an asynchronous HTTP client capable of handling HTTP/2 requests.\n\nWithin a try-except block, the function attempts to send the POST request to the API endpoint specified by `self.base_url`, including necessary headers and the JSON payload. Upon receiving a response, the function parses the JSON data from the response and returns it. If a request error occurs during this process, the function catches the exception and returns a structured error message indicating the nature of the request error.\n\nThis function is called by the `scrape` function within the `ContentScraper` class. The `scrape` function orchestrates the content extraction process by first invoking `extract_content` to retrieve data from the Tavily API. If the API call is successful, the `scrape` function processes the results, creating `ScrapedData` objects for each successful extraction. In cases where the Tavily API returns an error or fails to extract content from certain URLs, the `scrape` function logs the error and may fall back to an alternative web scraping method.\n\nThe integration of `extract_content` within the `scrape` function highlights its role as a critical component in the content extraction workflow, enabling seamless interaction with the Tavily API to facilitate data retrieval from multiple URLs.\n\n**Note**: When using this function, ensure that the URLs provided are valid and accessible. Additionally, be aware of the legal and ethical considerations surrounding web scraping, including compliance with the target website's terms of service.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n{\n    \"results\": [\n        {\"url\": \"http://example.com\", \"raw_content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"raw_content\": \"No content available\"}\n    ],\n    \"failed_results\": []\n}\n```",
        "**extract_content**: The function of extract_content is to asynchronously send a request to an external API to extract content from a list of provided URLs.\n\n**parameters**: The parameters of this function.\n· urls: List[str] - A list of URLs from which the content needs to be extracted.\n\n**Code Description**: The `extract_content` function is an asynchronous method responsible for extracting content from a list of URLs. It takes a list of URLs as an argument, constructs a payload with the URLs, and sends a POST request to a specified base URL using the HTTPX client with support for HTTP/2. This operation is performed inside a context manager to ensure proper resource management.\n\nThe function begins by defining the payload containing the list of URLs, which is then sent in a POST request to the API endpoint. The request includes the appropriate headers, which are passed from the instance's configuration. The response from the API is expected to be in JSON format. Upon receiving the response, the function parses the JSON data and returns it to the caller.\n\nIf an error occurs during the request (e.g., network issues, server errors), the function catches the `httpx.RequestError` exception. In this case, it returns a dictionary with an error message indicating the nature of the issue.\n\nThis function is typically invoked in a larger process, where the content extracted from the URLs is further processed. For example, the `search_validator` method calls `extract_content` to retrieve content from search results. It uses the URLs of the top search results to fetch the content, then processes it by joining the raw content into a corpus, which is passed to an LLM model for further use.\n\nIn summary, `extract_content` is a utility function designed to interact with an external content extraction API and handle errors gracefully by returning structured error information if needed.\n\n**Note**: Ensure that the URLs provided are valid and accessible. Network issues or incorrect configurations may result in a request error. The function is built to handle such cases by returning a structured error message.\n\n**Output Example**:\n```\n{\n    \"data\": [\n        {\"url\": \"http://example.com\", \"title\": \"Example Title\", \"content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"title\": \"Another Example\", \"content\": \"No content available\"}\n    ]\n}\n```",
        "**extract_content**: The function of extract_content is to send requests to the Tavily API and parse the JSON response, with automatic retries in case of network or parsing failures.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs that need to be processed by the Tavily API.\n\n**Code Description**: The extract_content function is an asynchronous method designed to interact with the Tavily API. It takes a list of URLs as input and constructs a payload to send to the API. The function initiates a POST request to the Tavily API using an HTTP client. Upon receiving the response, it checks for any HTTP errors and attempts to parse the JSON content.\n\nIf the JSON parsing fails due to a JSONDecodeError, the function logs a warning message indicating the failure and triggers a retry mechanism. Similarly, if there is a network request error or an HTTP status error, it logs the corresponding warning and raises the error to initiate a retry. The function is designed to handle these errors gracefully, allowing for a specified number of retries as defined in the settings.\n\nThe extract_content function is called by other components within the project, such as the scrape function in the ContentScraper class. The scrape function utilizes extract_content to retrieve content from the specified URLs, handling both successful and failed extraction attempts. This highlights the role of extract_content as a critical component in the content scraping workflow, ensuring that data is fetched reliably from the Tavily API.\n\nAdditionally, the extract_content function is invoked by the search_validator function, which validates the model's response against the content retrieved from the URLs. This further emphasizes the importance of extract_content in providing accurate and relevant data for subsequent processing and validation steps.\n\n**Note**: When using the extract_content function, it is essential to ensure that the URLs provided are valid and accessible. The function is built to handle errors and retries, but the initial input must be correct for optimal performance.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n{\n    \"results\": [\n        {\"url\": \"http://example.com\", \"raw_content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"raw_content\": \"No content available\"}\n    ],\n    \"failed_results\": []\n}\n```",
        "**extract_content**: The function of extract_content is to send requests to the Tavily API to extract content from a list of provided URLs and return the results in a structured format.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs from which content needs to be extracted.\n\n**Code Description**: The extract_content function is an asynchronous method designed to interact with the Tavily API for content extraction. It accepts a list of URLs as input and constructs a payload that specifies the URLs to be processed. The function is structured to handle potential errors during the API request and JSON parsing, implementing a retry mechanism to ensure robustness.\n\nUpon invocation, the function first prepares a payload containing the URLs and additional parameters such as `include_images` set to False and `extract_depth` set to \"basic\". It then attempts to send a POST request to the Tavily API using an asynchronous HTTP client. The response from the API is checked for successful status; if the request fails due to network issues or an HTTP error, the function logs a warning message and raises the corresponding exception.\n\nIn the case of a successful response, the function attempts to parse the JSON content returned by the API. If the JSON parsing fails, it logs a warning and raises an exception to trigger a retry. This retry mechanism is crucial for ensuring that transient issues do not lead to permanent failures in content extraction.\n\nThe extract_content function is called by other components within the project, such as the scrape function in the ContentScraper class. The scrape function utilizes extract_content to retrieve content from URLs obtained through search results. Additionally, it is invoked by the search_validator function, which validates the correctness of a model's answer by leveraging the content extracted from the URLs.\n\nThis function plays a critical role in the overall content extraction workflow, ensuring that data is retrieved accurately and efficiently from external sources. Its design emphasizes error handling and resilience, making it a vital component of the TavilyExtract class.\n\n**Note**: It is important to ensure that the URLs provided are valid and accessible. The function is built to handle errors and retries, but the initial input must be correct for optimal performance.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```json\n{\n    \"data\": [\n        {\"url\": \"http://example.com\", \"raw_content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"raw_content\": \"No content available\"}\n    ]\n}\n```"
      ],
      "code_start_line": 20,
      "code_end_line": 44,
      "params": [
        "self",
        "urls"
      ],
      "have_return": true,
      "code_content": "    async def extract_content(self, urls: List[str]) -> dict:\n        \"\"\"\n        向 Tavily API 发送请求并解析 JSON。若网络请求或解析失败，\n        会自动重试最多 settings.max_retries 次，再失败则抛出。\n        \"\"\"\n        payload = {\n            \"urls\": urls,\n            \"include_images\": False,\n            \"extract_depth\": \"basic\",\n        }\n        try:\n            resp = await self._client.post(self.base_url, headers=self.headers, json=payload)\n            resp.raise_for_status()\n            try:\n                return resp.json()\n            except json.JSONDecodeError as e:\n                printer.print(f\"[WARN] 第一次解析 JSON 失败: {e}，重试模型调用…\")\n                # 触发重试\n                raise\n        except httpx.RequestError as e:\n            printer.print(f\"[WARN] 网络请求错误: {e}，重试…\")\n            raise\n        except httpx.HTTPStatusError as e:\n            printer.print(f\"[WARN] HTTP 状态错误: {e}，重试…\")\n            raise\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/search_validator",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape",
        "tests/test_tavily_extract.py/test_extract_content_success",
        "tests/test_tavily_extract.py/test_extract_content_retry_on_invalid_json",
        "tests/test_tavily_extract.py/test_real_extract_content"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "src/criticsearch/tools/content_scraper/__init__.py": [
    {
      "type": "ClassDef",
      "name": "ContentScraper",
      "md_content": [
        "**ContentScraper**: The function of ContentScraper is to scrape content from the provided URLs by using an external API (Tavily) with a fallback mechanism for custom scraping.\n\n**attributes**: \n- urls: List[str] - A list of URLs from which content is to be scraped.\n\n**Code Description**: \nThe `ContentScraper` class provides a static method, `scrape`, that is used to extract content from a list of URLs. The method follows these main steps:\n\n1. **API Key and Initialization**: The method starts by retrieving an API key from the project settings (`settings.tavily.api_key`) and initializes an instance of `TavilyExtract` with the API key.\n   \n2. **Tavily Extraction**: The method attempts to scrape content from the provided URLs using the `TavilyExtract` instance by calling `extract_content` on the `tavily` object. This method returns results that are either successful or failed.\n\n3. **Error Handling**: If the Tavily extraction results contain an error (indicated by the presence of the \"error\" key), the method logs the error and proceeds to fall back to a custom web scraping solution by invoking `FallbackWebScraper.scrape`. This is done to ensure that content scraping continues even if the Tavily service fails.\n\n4. **Processing Successful Results**: If the Tavily extraction is successful, the results (contained in the `results` key) are iterated through. For each result, the URL and raw content are extracted and stored as `ScrapedData` objects, which are then added to a list of successful results.\n\n5. **Handling Failed Results**: If any URLs are marked as failed in the Tavily results (`failed_results`), the method logs a warning and retries scraping those URLs using the custom fallback scraper.\n\n6. **Final Results**: After processing both successful and failed results, the method merges these into a final list and returns them encapsulated within a `ScrapedDataList` object, which is serialized using the `model_dump` method before being returned.\n\nThis approach ensures a robust content scraping process with error handling and fallback mechanisms for cases when the primary extraction method (Tavily) fails.\n\nThe `scrape` method is used within the `BaseAgent` class (located in `src/criticsearch/base_agent.py`), which initializes an instance of `ContentScraper` and adds it to the list of available tools for the conversation manager. This allows the agent to call the `scrape` method when it requires content extraction from a set of URLs.\n\n**Note**: \n- The `scrape` method is asynchronous, meaning it must be awaited when called.\n- The method handles both successful and failed scraping attempts by using a combination of the Tavily API and a custom fallback scraper.\n- Ensure the API key for Tavily is correctly set in the project settings for successful integration.\n\n**Output Example**: \nThe `scrape` method will return a serialized `ScrapedDataList` object that contains a list of `ScrapedData` objects. Each `ScrapedData` object includes the URL and the raw content extracted from the source. Here's a mock example:\n\n```json\n{\n  \"data\": [\n    {\n      \"url\": \"https://example.com/page1\",\n      \"content\": \"This is the raw content from page 1.\"\n    },\n    {\n      \"url\": \"https://example.com/page2\",\n      \"content\": \"This is the raw content from page 2.\"\n    }\n  ]\n}\n```",
        "**ContentScraper**: The function of ContentScraper is to scrape content from provided URLs using a combination of API-based extraction and fallback web scraping.\n\n**attributes**:  \n· urls: List of strings containing the URLs to be scraped.  \n\n**Code Description**:  \nThe `ContentScraper` class contains a single static method `scrape`, which is designed to scrape content from a list of URLs. This method handles the process of scraping through an external service (Tavily API) and, in case of failure, falls back to web scraping using another method. The `scrape` method performs the following key operations:\n\n1. **Tavily API Extraction**:  \n   The method begins by extracting an API key from the settings configuration. It initializes an instance of `TavilyExtract` with this key. The `TavilyExtract` class is responsible for interacting with the Tavily API, which is used to attempt content extraction from the provided URLs. The `scrape` method asynchronously calls `tavily.extract_content(urls)` to perform the extraction.\n\n2. **Error Handling and Fallback**:  \n   If the Tavily API response contains an error (as indicated by the presence of a \"detail\" key in the response), the method logs an error message and proceeds with a fallback scraping approach. The fallback scraping is performed by the `FallbackWebScraper`, which scrapes content directly from the web.\n\n3. **Processing Successful Results**:  \n   In cases where Tavily extraction is successful, the results are processed. The method iterates over the successful results returned by Tavily. For each result, the URL and raw content are extracted, and a `ScrapedData` object is created to store these values. These successful results are then stored in a list.\n\n4. **Handling Failed Results**:  \n   If some URLs fail in the Tavily extraction process, the method logs this issue and proceeds to scrape those URLs using the fallback web scraper. The failed results are processed in a similar way to the successful ones, resulting in another list of `ScrapedData`.\n\n5. **Merging Results**:  \n   After handling both successful and failed results, the method merges these two sets of results into a final list and wraps it in a `ScrapedDataList` object. This final result is then returned by the method after invoking `model_dump()` to serialize the data for further processing or storage.\n\nThis class is called within the context of the `BaseAgent` class, where an instance of `ContentScraper` is created and used to scrape content as part of the agent's overall functionality. Specifically, `BaseAgent` utilizes the `scrape` method to extract relevant content from URLs during its execution flow. It relies on the `ContentScraper` for content extraction, which is an essential part of the agent’s toolset, enabling the agent to gather and process web data.\n\n**Note**:  \nWhen using the `ContentScraper` class, ensure that the settings are correctly configured, particularly the API key for Tavily, and that the fallback web scraper is available. The success of the content scraping process is dependent on both the Tavily API's performance and the fallback web scraper's ability to handle failed cases. The `scrape` method is asynchronous and should be called within an asynchronous context.\n\n**Output Example**:  \nA possible return value from the `scrape` method could be a serialized list of `ScrapedData` objects, each containing the URL and the corresponding scraped content. The serialized structure may look like:\n\n```json\n{\n  \"data\": [\n    {\n      \"url\": \"https://example.com/article1\",\n      \"content\": \"This is the scraped content from article 1.\"\n    },\n    {\n      \"url\": \"https://example.com/article2\",\n      \"content\": \"This is the scraped content from article 2.\"\n    }\n  ]\n}\n```"
      ],
      "code_start_line": 10,
      "code_end_line": 67,
      "params": [],
      "have_return": true,
      "code_content": "class ContentScraper:\n    @staticmethod\n    async def scrape(urls: List[str]) -> ScrapedDataList:\n        \"\"\"\n        Scrapes content using the provided URLs.\n\n        Args:\n            urls (List[str]): A list of URLs to scrape content from.\n        \"\"\"\n        api_key = settings.tavily.api_key\n        tavily = TavilyExtract(api_key)\n\n        # Try to extract using Tavily\n        tavily_results = await tavily.extract_content(urls)\n        final_results = []\n\n        # Check for errors or failed results in the Tavily response\n        if \"detail\" in tavily_results:\n            error_message = tavily_results[\"detail\"].get(\"error\", \"Unknown error\")\n\n            printer.log(\n                f\"Tavily API extraction failed: {error_message}, falling back to web scraping.\",\n                style=\"bold red\",\n            )\n\n            final_results = await FallbackWebScraper.scrape(urls)\n        else:\n            # Process successful results from Tavily\n            successful_results = []\n\n            results = tavily_results.get(\"results\", [])\n            for result in results:\n                # Extract the necessary data from the Tavily API response\n                url = result.get(\"url\")\n                raw_content = result.get(\"raw_content\")\n\n                successful_results.append(\n                    ScrapedData(\n                        url=url,\n                        content=raw_content,\n                    )\n                )\n\n            failed_results = tavily_results.get(\"failed_results\", [])\n\n            # If Tavily has failed results, log them and proceed with fallback\n            if failed_results:\n                printer.log(\n                    f\"Some URLs failed in Tavily extraction. Using fallback web scraping for these URLs.\",\n                    style=\"bold yellow\",\n                )\n                failed_urls = [result.get(\"url\") for result in failed_results]\n                failed_results = await FallbackWebScraper.scrape(failed_urls)\n\n            # Merge both successful and failed results into ScrapedDataList\n            final_results = successful_results + failed_results\n\n        return ScrapedDataList(data=final_results).model_dump()\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py",
        "src/criticsearch/base_agent.py/BaseAgent/__init__",
        "src/criticsearch/tools/__init__.py"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "scrape",
      "md_content": [
        "**scrape**: The function of scrape is to asynchronously scrape content from a list of provided URLs, utilizing the Tavily API for extraction and falling back to a custom web scraping method if necessary.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs to scrape content from.\n\n**Code Description**: The `scrape` function is an asynchronous method designed to extract content from a list of URLs. It first initializes an instance of the `TavilyExtract` class using an API key retrieved from the settings. This instance is responsible for interacting with the Tavily API to perform the content extraction.\n\nThe function begins by calling the `extract_content` method of the `TavilyExtract` instance, passing the list of URLs. This method sends an asynchronous POST request to the Tavily API, which returns a JSON response containing the results of the extraction. If the response includes an error, the function logs the error message and proceeds to invoke the `FallbackWebScraper.scrape` method, which serves as a backup scraping mechanism.\n\nIn the case of successful extraction, the function processes the results returned by the Tavily API. It iterates through the successful results, extracting the URL and raw content from each result. For each successful extraction, it creates a `ScrapedData` object, which encapsulates the URL and the corresponding content.\n\nAdditionally, if there are any failed results in the Tavily API response, the function logs a warning and attempts to scrape the failed URLs using the `FallbackWebScraper.scrape` method. The results from both successful and fallback scraping are merged into a final list of `ScrapedData` objects.\n\nFinally, the function returns a `ScrapedDataList` object, which contains all the scraped data, including both successful and failed attempts. This object is serialized using the `model_dump` method, providing a structured output of the scraping results.\n\nThe `scrape` function is called within the `BaseAgent` class, specifically in the `search_and_browse` method. This method orchestrates the overall search and scraping process, first performing a search using the `SearchAggregator` and then invoking the `scrape` function to gather additional content from the URLs identified during the search. This integration ensures that the scraping functionality is seamlessly incorporated into the broader search and retrieval workflow of the application.\n\n**Note**: When using this function, ensure that the URLs provided are valid and accessible. Additionally, be aware of the legal and ethical considerations surrounding web scraping, including compliance with the target website's terms of service.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n[\n    ScrapedData(url=\"http://example.com\", title=\"Example Title\", content=\"This is the main content of the page.\"),\n    ScrapedData(url=\"http://anotherexample.com\", title=\"Another Example\", content=\"No content available\")\n]\n```",
        "**scrape**: The function of scrape is to asynchronously scrape content from a list of provided URLs, utilizing the Tavily API for extraction and falling back to a web scraping method if necessary.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs to scrape content from.\n\n**Code Description**: The `scrape` function is an asynchronous method designed to extract content from a list of URLs. It begins by initializing an instance of the `TavilyExtract` class using an API key sourced from the application settings. This instance is responsible for interacting with the Tavily API to retrieve content.\n\nThe function then calls the `extract_content` method of the `TavilyExtract` instance, passing the provided list of URLs. This method sends a POST request to the Tavily API, which returns a structured response containing either the extracted content or error details.\n\nUpon receiving the results from the Tavily API, the function checks for any errors indicated in the response. If an error is present, it logs the error message using the `log` method from the `RichPrinter` class, indicating that the Tavily API extraction has failed. In this case, the function falls back to the `FallbackWebScraper` class, invoking its `scrape` method to attempt content extraction from the original list of URLs.\n\nIf the Tavily API returns successful results, the function processes these results by iterating through the list of extracted data. For each successful extraction, it creates a `ScrapedData` object, which encapsulates the URL and the raw content retrieved. The function also checks for any failed results returned by the Tavily API. If there are failed results, it logs a warning message and attempts to scrape content from the failed URLs using the `FallbackWebScraper`.\n\nFinally, the function aggregates both successful and failed results into a `ScrapedDataList` object, which is returned after being serialized. This structured approach ensures that the function handles both successful and failed scraping attempts effectively, providing a comprehensive output that includes all relevant data.\n\nThe `scrape` function is called within the `web_scrape_results` method of the `BaseAgent` class. This method orchestrates the overall scraping process by rendering a prompt and interacting with the model. It collects the URLs from the tool calls and invokes the `scrape` function to retrieve content, ensuring that the agent can effectively extract information from web sources.\n\n**Note**: When using this function, ensure that the URLs provided are valid and accessible. Additionally, be aware of the legal and ethical considerations surrounding web scraping, including compliance with the target website's terms of service.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n{\n    \"data\": [\n        {\"url\": \"http://example.com\", \"title\": \"Example Title\", \"content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"title\": \"Another Example\", \"content\": \"No content available\"}\n    ]\n}\n```",
        "**scrape**: The function of scrape is to scrape content using the provided URLs.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs to scrape content from.\n\n**Code Description**: The `scrape` function is an asynchronous method designed to extract content from a list of URLs. It begins by retrieving the API key from the settings and initializing an instance of the `TavilyExtract` class, which is responsible for interacting with the Tavily API. The function then attempts to extract content using the `extract_content` method of the `TavilyExtract` instance, passing the provided URLs.\n\nUpon receiving the results from the Tavily API, the function checks for any errors indicated in the response. If an error is detected, it logs the error message using the `log` method from the `RichPrinter` class, indicating that the Tavily API extraction has failed and that the function will fall back to a secondary scraping method. In this case, it invokes the `scrape` method of the `FallbackWebScraper` class to handle the URLs that failed during the Tavily extraction.\n\nIf the Tavily API returns successful results, the function processes these results by extracting the necessary data, such as the URL and raw content, and constructs `ScrapedData` objects for each successful extraction. It also checks for any failed results from the Tavily API and logs them, subsequently attempting to scrape the failed URLs using the fallback method.\n\nFinally, the function aggregates both successful and failed results into a `ScrapedDataList` object, which is returned after being serialized through the `model_dump` method. This structured approach ensures that the function can handle both successful and unsuccessful scraping attempts, providing a comprehensive output of the scraping operation.\n\nThe `scrape` function is called by the `web_scrape_results` method within the `BaseAgent` class, which orchestrates the process of searching for information and subsequently scraping content based on the search results. This highlights the function's role in enhancing the agent's ability to provide accurate and relevant responses by leveraging real-time data from web sources.\n\n**Note**: When using this function, ensure that the URLs provided are valid and accessible. Additionally, proper error handling is implemented to manage cases where the scraping operations do not yield results.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n{\n    \"data\": [\n        {\"url\": \"http://example.com\", \"title\": \"Example Title\", \"content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"title\": \"Another Example\", \"content\": \"No content available\"}\n    ]\n}\n```",
        "**scrape**: The function of scrape is to scrape content using the provided URLs.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs to scrape content from.\n\n**Code Description**: The scrape function is an asynchronous method designed to extract content from a list of URLs. It begins by retrieving the API key from the settings configuration, which is essential for authenticating requests to the Tavily API. An instance of the TavilyExtract class is then created using this API key, enabling interaction with the Tavily API for content extraction.\n\nThe function attempts to extract content from the provided URLs by calling the extract_content method of the TavilyExtract instance. This method sends a request to the Tavily API and returns a response containing the results of the extraction. If the Tavily API response indicates an error (identified by the presence of a \"detail\" key), the function logs the error message using the RichPrinter's log method and falls back to a secondary scraping method provided by the FallbackWebScraper class.\n\nIn the case of a successful extraction from the Tavily API, the function processes the results by iterating through the returned data. For each successful result, it constructs a ScrapedData object containing the URL and the raw content extracted from that URL. If there are any failed results in the Tavily response, these URLs are logged, and the function calls the FallbackWebScraper's scrape method to attempt to retrieve content from those URLs as well.\n\nFinally, the function merges both successful and failed results into a ScrapedDataList object, which is returned after being serialized using the model_dump method. This structured approach ensures that the scrape function effectively handles both successful and failed content extraction attempts, providing a comprehensive output that includes all relevant data.\n\nThe scrape function is called within the BaseAgent class, specifically in the web_scrape_results method, which orchestrates the web scraping process based on search results. It is also invoked in the _action_router function, where it plays a critical role in the decision-making process of the intelligent agent, allowing it to gather content from various sources as needed.\n\n**Note**: It is important to ensure that the URLs provided are valid and accessible. The function is built to handle errors and retries, but the initial input must be correct for optimal performance.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n{\n    \"data\": [\n        {\"url\": \"http://example.com\", \"content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"content\": \"No content available\"}\n    ]\n}\n```",
        "**scrape**: The function of scrape is to scrape content using the provided URLs.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs to scrape content from.\n\n**Code Description**: The scrape function is an asynchronous method designed to extract content from a list of URLs. It begins by retrieving an API key from the settings, which is essential for authenticating requests to the Tavily API. An instance of the TavilyExtract class is then created using this API key.\n\nThe function attempts to extract content using the Tavily API by calling the extract_content method of the TavilyExtract instance. If the extraction is successful, the results are processed to create a list of ScrapedData objects, which encapsulate the URL and the raw content retrieved.\n\nIn the event that the Tavily API returns an error or if there are failed results, the function logs an appropriate message using the RichPrinter's log method, indicating the failure and the fallback to web scraping. It then invokes the scrape method of the FallbackWebScraper class to handle the URLs that failed during the Tavily extraction.\n\nThe final results, which may include both successfully scraped data and any fallback results, are aggregated into a ScrapedDataList object. This object is then serialized and returned, providing a structured output of the scraping operation.\n\nThe scrape function is called within the ContentScraper class, which is instantiated in the BaseAgent class. The BaseAgent class initializes the ContentScraper and registers its schema for use in the agent's operations. Additionally, the scrape function is invoked by the web_scrape_results method in the BaseAgent class, which facilitates web content extraction based on search results.\n\nThis function plays a critical role in the overall content scraping workflow, ensuring that data is retrieved accurately and efficiently from external sources. Its design emphasizes error handling and resilience, making it a vital component of the content extraction process.\n\n**Note**: It is essential to ensure that the URLs provided are valid and accessible. The function is built to handle errors and retries, but the initial input must be correct for optimal performance.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```json\n{\n    \"data\": [\n        {\"url\": \"http://example.com\", \"content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"content\": \"No content available\"}\n    ]\n}\n```"
      ],
      "code_start_line": 12,
      "code_end_line": 67,
      "params": [
        "urls"
      ],
      "have_return": true,
      "code_content": "    async def scrape(urls: List[str]) -> ScrapedDataList:\n        \"\"\"\n        Scrapes content using the provided URLs.\n\n        Args:\n            urls (List[str]): A list of URLs to scrape content from.\n        \"\"\"\n        api_key = settings.tavily.api_key\n        tavily = TavilyExtract(api_key)\n\n        # Try to extract using Tavily\n        tavily_results = await tavily.extract_content(urls)\n        final_results = []\n\n        # Check for errors or failed results in the Tavily response\n        if \"detail\" in tavily_results:\n            error_message = tavily_results[\"detail\"].get(\"error\", \"Unknown error\")\n\n            printer.log(\n                f\"Tavily API extraction failed: {error_message}, falling back to web scraping.\",\n                style=\"bold red\",\n            )\n\n            final_results = await FallbackWebScraper.scrape(urls)\n        else:\n            # Process successful results from Tavily\n            successful_results = []\n\n            results = tavily_results.get(\"results\", [])\n            for result in results:\n                # Extract the necessary data from the Tavily API response\n                url = result.get(\"url\")\n                raw_content = result.get(\"raw_content\")\n\n                successful_results.append(\n                    ScrapedData(\n                        url=url,\n                        content=raw_content,\n                    )\n                )\n\n            failed_results = tavily_results.get(\"failed_results\", [])\n\n            # If Tavily has failed results, log them and proceed with fallback\n            if failed_results:\n                printer.log(\n                    f\"Some URLs failed in Tavily extraction. Using fallback web scraping for these URLs.\",\n                    style=\"bold yellow\",\n                )\n                failed_urls = [result.get(\"url\") for result in failed_results]\n                failed_results = await FallbackWebScraper.scrape(failed_urls)\n\n            # Merge both successful and failed results into ScrapedDataList\n            final_results = successful_results + failed_results\n\n        return ScrapedDataList(data=final_results).model_dump()\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/__init__",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/main.py/_action_router",
        "src/criticsearch/session.py/Session/browse",
        "src/criticsearch/workflow.py/WorkflowExecutor/__init__"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/tools/content_scraper/tavily_extract.py/TavilyExtract",
        "src/criticsearch/tools/content_scraper/tavily_extract.py/TavilyExtract/extract_content",
        "src/criticsearch/tools/content_scraper/fallback_web_scraper.py/FallbackWebScraper",
        "src/criticsearch/tools/content_scraper/models.py/ScrapedData",
        "src/criticsearch/tools/content_scraper/models.py/ScrapedDataList"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        true
      ]
    }
  ],
  "src/criticsearch/tools/content_scraper/fallback_web_scraper.py": [
    {
      "type": "ClassDef",
      "name": "FallbackWebScraper",
      "md_content": [
        "**FallbackWebScraper**: The function of FallbackWebScraper is to scrape content from a list of webpages asynchronously, serving as a fallback mechanism when the Tavily API fails.\n\n**attributes**: The attributes of this Class.\n· urls: List[str] - A list of URLs to scrape content from.\n\n**Code Description**: The FallbackWebScraper class contains a static method named `scrape`, which is designed to handle the asynchronous scraping of web content from a provided list of URLs. This method is particularly useful in scenarios where the primary content extraction method, the Tavily API, encounters issues or fails to return valid results. \n\nThe `scrape` method begins by defining a set of HTTP headers to mimic a standard web browser request, enhancing the likelihood of successful content retrieval. It then defines an inner asynchronous function `fetch_url`, which takes a single URL as an argument. This function is responsible for making the actual HTTP GET request using the `httpx` library, which supports asynchronous operations and HTTP/2.\n\nWithin `fetch_url`, the method attempts to fetch the content of the specified URL. If the response status code is not 200 (indicating a successful request), it returns a `ScrapedData` object containing the URL and an error message detailing the HTTP status code and reason. If the request is successful, the HTML content is parsed using BeautifulSoup, where unwanted elements such as scripts and styles are removed to focus on the main content.\n\nThe main content is extracted by searching for common HTML structures like `<main>`, `<article>`, or `<div>` elements that typically contain the primary content of a webpage. If no main content is found, a default message indicating \"No content available\" is returned. Otherwise, the text from paragraph tags is concatenated and cleaned up to form the final content string. The method then returns a `ScrapedData` object containing the URL, the page title, and the extracted content.\n\nThe `scrape` method utilizes `asyncio.gather` to concurrently fetch content from all provided URLs, improving efficiency and reducing overall scraping time.\n\nThis class is called by the `scrape` method of the `ContentScraper` class, which first attempts to extract content using the Tavily API. If the Tavily API fails, the `FallbackWebScraper.scrape` method is invoked to handle the fallback scraping process. This relationship ensures that the FallbackWebScraper serves as a reliable alternative for content extraction when the primary method is unavailable.\n\n**Note**: When using this code, ensure that the URLs provided are valid and accessible. Additionally, be aware of the legal and ethical considerations surrounding web scraping, including compliance with the target website's terms of service.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n[\n    ScrapedData(url=\"http://example.com\", title=\"Example Title\", content=\"This is the main content of the page.\"),\n    ScrapedData(url=\"http://anotherexample.com\", title=\"Another Example\", content=\"No content available\")\n]\n```"
      ],
      "code_start_line": 11,
      "code_end_line": 67,
      "params": [],
      "have_return": true,
      "code_content": "class FallbackWebScraper:\n    @staticmethod\n    async def scrape(urls: List[str]) -> List[ScrapedData]:\n        \"\"\"\n        Scrapes content from a list of webpages asynchronously.\n        If Tavily API fails, fall back to scraping.\n        \"\"\"\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n        }\n\n        async def fetch_url(url: str) -> ScrapedData:\n            try:\n                async with httpx.AsyncClient(\n                    http2=True, follow_redirects=True\n                ) as client:\n                    response = await client.get(url, headers=headers, timeout=10)\n\n                    if response.status_code != 200:\n                        return ScrapedData(\n                            url=url,\n                            error=f\"HTTP {response.status_code}: {response.reason_phrase}\",\n                        )\n\n                    html = response.text\n                    soup = BeautifulSoup(html, \"html.parser\")\n\n                    # Remove unwanted elements\n                    for script in soup([\"script\", \"style\", \"meta\", \"noscript\"]):\n                        script.decompose()\n\n                    # Extract content\n                    main_content = (\n                        soup.find(\"main\")\n                        or soup.find(\"article\")\n                        or soup.find(\"div\", class_=re.compile(r\"content|main|article\"))\n                    )\n                    content = (\n                        \"No content available\"\n                        if not main_content\n                        else \"\\n\".join(\n                            [\n                                re.sub(r\"\\s+\", \" \", p.get_text(strip=True))\n                                for p in main_content.find_all(\"p\")\n                            ]\n                        )\n                    )\n\n                    return ScrapedData(\n                        url=url,\n                        title=soup.title.string if soup.title else \"Untitled\",\n                        content=content,\n                    )\n            except Exception as e:\n                return ScrapedData(url=url, error=f\"Error: {str(e)}\")\n\n        return await asyncio.gather(*(fetch_url(url) for url in urls))\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/content_scraper/__init__.py",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "scrape",
      "md_content": [
        "**scrape**: The function of scrape is to asynchronously scrape content from a list of webpages, utilizing a fallback mechanism if the Tavily API fails.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs from which the content will be scraped.\n\n**Code Description**: The scrape function is designed to handle the asynchronous scraping of multiple webpages. It takes a list of URLs as input and returns a list of ScrapedData objects, which encapsulate the results of the scraping operation for each URL. The function begins by defining a headers dictionary that includes a User-Agent string to mimic a typical web browser request, which helps avoid potential blocks from the target websites.\n\nWithin the scrape function, an inner asynchronous function named fetch_url is defined. This function is responsible for making the actual HTTP requests to each URL. It utilizes the httpx library's AsyncClient to send GET requests with the specified headers and a timeout of 10 seconds. If the response status code is not 200 (indicating a successful request), the function constructs a ScrapedData object containing the URL and an error message detailing the HTTP status code and reason.\n\nIf the request is successful, the function processes the HTML content of the response using BeautifulSoup. It removes unwanted elements such as scripts, styles, and meta tags to focus on the main content of the page. The function then attempts to locate the main content area by searching for common HTML tags like <main>, <article>, or <div> with class names that suggest they contain the primary content. If no main content is found, it defaults to a message indicating that no content is available.\n\nThe content is extracted from the identified main content area by joining the text of all <p> tags, ensuring that excessive whitespace is removed. A ScrapedData object is then created with the URL, the title of the page (if available), and the extracted content.\n\nThe scrape function ultimately calls asyncio.gather to execute the fetch_url function concurrently for all URLs in the input list. This allows for efficient scraping of multiple pages at once, significantly reducing the time required compared to synchronous requests.\n\nFrom a functional perspective, the scrape function is integral to the FallbackWebScraper module, providing a mechanism to retrieve webpage content when other methods, such as the Tavily API, are unavailable. The results are structured in ScrapedData objects, which standardize the output format, making it easier for other components of the system to handle both successful and failed scraping attempts.\n\n**Note**: It is important to handle the ScrapedData objects carefully, particularly by checking the error attribute to determine if the scraping was successful. The content and title attributes may be None if the scraping process encounters issues or if the content is not available on the page.\n\n**Output Example**: \n[\n    ScrapedData(url=\"http://example.com\", title=\"Example Domain\", content=\"This domain is for use in illustrative examples in documents.\", error=None),\n    ScrapedData(url=\"http://nonexistent.com\", title=None, content=None, error=\"HTTP 404: Not Found\")\n]"
      ],
      "code_start_line": 13,
      "code_end_line": 67,
      "params": [
        "urls"
      ],
      "have_return": true,
      "code_content": "    async def scrape(urls: List[str]) -> List[ScrapedData]:\n        \"\"\"\n        Scrapes content from a list of webpages asynchronously.\n        If Tavily API fails, fall back to scraping.\n        \"\"\"\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n        }\n\n        async def fetch_url(url: str) -> ScrapedData:\n            try:\n                async with httpx.AsyncClient(\n                    http2=True, follow_redirects=True\n                ) as client:\n                    response = await client.get(url, headers=headers, timeout=10)\n\n                    if response.status_code != 200:\n                        return ScrapedData(\n                            url=url,\n                            error=f\"HTTP {response.status_code}: {response.reason_phrase}\",\n                        )\n\n                    html = response.text\n                    soup = BeautifulSoup(html, \"html.parser\")\n\n                    # Remove unwanted elements\n                    for script in soup([\"script\", \"style\", \"meta\", \"noscript\"]):\n                        script.decompose()\n\n                    # Extract content\n                    main_content = (\n                        soup.find(\"main\")\n                        or soup.find(\"article\")\n                        or soup.find(\"div\", class_=re.compile(r\"content|main|article\"))\n                    )\n                    content = (\n                        \"No content available\"\n                        if not main_content\n                        else \"\\n\".join(\n                            [\n                                re.sub(r\"\\s+\", \" \", p.get_text(strip=True))\n                                for p in main_content.find_all(\"p\")\n                            ]\n                        )\n                    )\n\n                    return ScrapedData(\n                        url=url,\n                        title=soup.title.string if soup.title else \"Untitled\",\n                        content=content,\n                    )\n            except Exception as e:\n                return ScrapedData(url=url, error=f\"Error: {str(e)}\")\n\n        return await asyncio.gather(*(fetch_url(url) for url in urls))\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/content_scraper/models.py/ScrapedData"
      ],
      "special_reference_type": [
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "fetch_url",
      "md_content": [
        "**fetch_url**: The function of fetch_url is to asynchronously retrieve and scrape content from a specified URL, returning structured data about the page, including its title and main content, or an error message if the retrieval fails.\n\n**parameters**: The parameters of this Function.\n· url: str - The URL from which the content is to be scraped.\n\n**Code Description**: The fetch_url function is designed to perform an asynchronous HTTP GET request to a specified URL using the httpx library. It initiates an asynchronous context with an HTTP client that supports HTTP/2 and follows redirects. The function attempts to retrieve the content of the page within a timeout period of 10 seconds.\n\nUpon receiving a response, the function checks the HTTP status code. If the status code is not 200 (indicating a successful request), it constructs and returns a ScrapedData object containing the URL and an error message that specifies the HTTP status code and reason phrase.\n\nIf the response is successful, the function proceeds to parse the HTML content using BeautifulSoup. It removes unwanted elements such as scripts, styles, meta tags, and noscript tags to clean the document. The function then attempts to extract the main content of the page by searching for specific HTML tags, such as <main>, <article>, or <div> elements with class names that include \"content\", \"main\", or \"article\".\n\nThe extracted content is processed to ensure that it is clean and well-formatted, specifically by stripping unnecessary whitespace from paragraph elements. If no main content is found, a default message \"No content available\" is returned.\n\nFinally, the function returns a ScrapedData object populated with the URL, the title of the page (if available), and the extracted content. If any exceptions occur during the process, the function captures the exception and returns a ScrapedData object with the URL and an error message indicating the nature of the error.\n\nThis function is integral to the web scraping process as it provides a structured way to handle both successful and failed scraping attempts. The results are encapsulated in ScrapedData objects, which can be further processed or aggregated by other components of the system, such as the FallbackWebScraper.\n\n**Note**: It is important to handle the returned ScrapedData object with care, particularly by checking the error attribute to determine if the scraping was successful. The title and content attributes may be optional and could be None if the information is not available.\n\n**Output Example**: \nA possible return value from the fetch_url function could look like this:\n```\nScrapedData(\n    url=\"https://example.com\",\n    title=\"Example Domain\",\n    content=\"This domain is for use in illustrative examples in documents.\"\n)\n```\nOr in the case of an error:\n```\nScrapedData(\n    url=\"https://example.com\",\n    error=\"HTTP 404: Not Found\"\n)\n```"
      ],
      "code_start_line": 22,
      "code_end_line": 65,
      "params": [
        "url"
      ],
      "have_return": true,
      "code_content": "        async def fetch_url(url: str) -> ScrapedData:\n            try:\n                async with httpx.AsyncClient(\n                    http2=True, follow_redirects=True\n                ) as client:\n                    response = await client.get(url, headers=headers, timeout=10)\n\n                    if response.status_code != 200:\n                        return ScrapedData(\n                            url=url,\n                            error=f\"HTTP {response.status_code}: {response.reason_phrase}\",\n                        )\n\n                    html = response.text\n                    soup = BeautifulSoup(html, \"html.parser\")\n\n                    # Remove unwanted elements\n                    for script in soup([\"script\", \"style\", \"meta\", \"noscript\"]):\n                        script.decompose()\n\n                    # Extract content\n                    main_content = (\n                        soup.find(\"main\")\n                        or soup.find(\"article\")\n                        or soup.find(\"div\", class_=re.compile(r\"content|main|article\"))\n                    )\n                    content = (\n                        \"No content available\"\n                        if not main_content\n                        else \"\\n\".join(\n                            [\n                                re.sub(r\"\\s+\", \" \", p.get_text(strip=True))\n                                for p in main_content.find_all(\"p\")\n                            ]\n                        )\n                    )\n\n                    return ScrapedData(\n                        url=url,\n                        title=soup.title.string if soup.title else \"Untitled\",\n                        content=content,\n                    )\n            except Exception as e:\n                return ScrapedData(url=url, error=f\"Error: {str(e)}\")\n",
      "name_column": 18,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/content_scraper/models.py/ScrapedData"
      ],
      "special_reference_type": [
        true
      ]
    }
  ],
  "src/criticsearch/tools/content_scraper/models.py": [
    {
      "type": "ClassDef",
      "name": "ScrapedData",
      "md_content": [
        "**ScrapedData**: The function of ScrapedData is to represent a data object that holds information scraped from a given URL, including content, title, and potential errors.\n\n**attributes**: The attributes of this Class.\n· url: str - The URL from which the data is scraped.\n· title: Optional[str] - The title of the scraped page, which can be None if unavailable.\n· content: Optional[str] - The main content of the scraped page, which can be None if not found or unavailable.\n· error: Optional[str] - An error message, if there was an issue during scraping.\n\n**Code Description**: The ScrapedData class is a data container that is designed to store and structure the information extracted from web scraping operations. It inherits from `BaseModel`, which likely provides data validation and serialization functionality. The class includes four attributes:\n\n1. **url** (str): This is the URL from which content is scraped, making it an essential identifier for the scraped data.\n2. **title** (Optional[str]): This attribute holds the title of the webpage or content if available, providing a reference to the type or subject of the page. It is optional and can be None if the title is not present.\n3. **content** (Optional[str]): The main content that was extracted from the webpage is stored in this field. It is optional, as some pages might not have meaningful content or may not be accessible.\n4. **error** (Optional[str]): This attribute is used to store any error message that might have occurred during the scraping process. If an error was encountered, this field will contain the error message; otherwise, it will be None.\n\nThe ScrapedData class plays a crucial role in both successful and failed scraping scenarios. When content is scraped successfully, the `url` and `content` attributes are populated. If there are any issues during scraping, such as an HTTP error or a failure to retrieve meaningful content, the `error` attribute will contain details about what went wrong. This class is used by the `scrape` functions found in the `FallbackWebScraper` and `ContentScraper` modules.\n\nFrom a functional perspective, ScrapedData objects are instantiated during the scraping process to store the results of individual scraping attempts. If the scraping process is successful, the object is populated with data like the content of the page and its title. If scraping fails, the object includes an error message to provide insight into the issue. These instances are then collected in lists, such as `ScrapedDataList`, which is used to return the aggregated results.\n\nThe `scrape` function in `ContentScraper` first tries to extract content using the Tavily API and constructs a ScrapedData object for each URL it processes. In case of failure, it uses `FallbackWebScraper.scrape`, which also returns ScrapedData objects. These instances are used throughout the system to ensure that scraping results are handled consistently, even when some URLs result in errors.\n\n**Note**: The `ScrapedData` class should always be handled with care, especially when dealing with failed scrapes. The presence of the `error` attribute should be checked to ensure that the scraping process was successful. Additionally, it is important to understand that content and title are optional attributes and may be missing in certain cases."
      ],
      "code_start_line": 6,
      "code_end_line": 10,
      "params": [],
      "have_return": false,
      "code_content": "class ScrapedData(BaseModel):\n    url: str\n    title: Optional[str] = None\n    content: Optional[str] = None\n    error: Optional[str] = None\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/content_scraper/__init__.py",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape",
        "src/criticsearch/tools/content_scraper/fallback_web_scraper.py",
        "src/criticsearch/tools/content_scraper/fallback_web_scraper.py/FallbackWebScraper/scrape",
        "src/criticsearch/tools/content_scraper/fallback_web_scraper.py/FallbackWebScraper/scrape/fetch_url",
        "src/criticsearch/tools/content_scraper/models.py/ScrapedDataList"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "ScrapedDataList",
      "md_content": [
        "**ScrapedDataList**: The function of ScrapedDataList is to represent a collection of ScrapedData objects, holding the results of web scraping, including both successful and failed attempts, with serialization capabilities to output the data in a readable format.\n\n**attributes**: The attributes of this Class.\n· data: List[ScrapedData] - A list of ScrapedData objects, representing the individual scraping results for each URL processed. It contains both successful and failed results.\n· max_content_length: int - The maximum allowed length for individual scraped content. If the content exceeds this length, it will be truncated.\n· max_output_length: int - The maximum length of the serialized result that can be returned. If the output exceeds this length, it will be truncated.\n\n**Code Description**: \nThe ScrapedDataList class is designed to manage a collection of ScrapedData objects, which store the results of scraping attempts for multiple URLs. It inherits from BaseModel, providing data validation and serialization capabilities. The class contains three key attributes:\n\n1. **data** (List[ScrapedData]): This attribute holds a list of ScrapedData instances, which represent the individual results of the scraping process. Each ScrapedData object can contain information like the scraped URL, title, content, and any errors encountered during scraping. This list forms the core of the class, containing all the data gathered during scraping.\n\n2. **max_content_length** (int): This attribute defines the maximum allowable length for the content of each scraped data object. If any scraped content exceeds this length, it will be truncated and appended with the string \"[TOO LONG, END]\" to indicate that the content has been cut off. The default value is set to 10,000 characters.\n\n3. **max_output_length** (int): This attribute controls the maximum length of the entire serialized output. If the concatenated result of all scraped data exceeds this length, the output will be truncated with the message \"[OUTPUT TOO LONG, TRUNCATED]\" to avoid excessive output. The default value is set to 100,000 characters.\n\nThe class includes the method `ser_model`, which performs the serialization of the ScrapedDataList object into a human-readable string format. The function iterates over each ScrapedData object in the `data` attribute and processes it as follows:\n\n- If the ScrapedData object contains an error, the method appends an error message to the result, specifying the URL and the associated error.\n- If no error is found, the content is checked against the `max_content_length`. If the content exceeds this length, it is truncated to the specified limit, with the string \"[TOO LONG, END]\" appended to signal the truncation.\n- The method then constructs a string for each ScrapedData object, including the URL, title, and content.\n- All these strings are concatenated with a separator (\"---\") between each entry.\n\nFinally, the complete serialized string is checked against the `max_output_length`. If the overall string exceeds the defined length, it will be truncated with the message \"[OUTPUT TOO LONG, TRUNCATED]\".\n\nFrom a functional perspective, ScrapedDataList is typically used to aggregate multiple ScrapedData objects that represent the results of a scraping operation. For example, the `scrape` function in the `ContentScraper` module calls this class to collect the results of web scraping and returns a serialized version of the results as a string.\n\nThe ScrapedDataList class provides an efficient way to manage and output scraped data, ensuring that even large datasets are handled appropriately by applying length restrictions at both the individual content level and the overall output level. This ensures that the serialized results remain within acceptable limits for processing or logging.\n\n**Note**: When using ScrapedDataList, ensure that the content length and output length are managed effectively, especially when handling large sets of data, to avoid data truncation. Additionally, be aware that the `error` field in ScrapedData objects may be present, and it should be checked to handle failed scraping attempts appropriately.\n\n**Output Example**:\nHere is an example of a serialized output that might be returned by the `ser_model` method:\n\n```\nURL: https://example.com/page1\nTitle: Example Page 1\nContent:\nThis is the content of the first page. It contains relevant information about the topic.\n\n---\nURL: https://example.com/page2\nTitle: Example Page 2\nContent:\nThis page has too much content. It is truncated here: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. [TOO LONG, END]\n\n[OUTPUT TOO LONG, TRUNCATED]\n```"
      ],
      "code_start_line": 13,
      "code_end_line": 46,
      "params": [],
      "have_return": true,
      "code_content": "class ScrapedDataList(BaseModel):\n    data: List[ScrapedData] = Field(default_factory=list)\n    max_content_length: int = 10000  # Max length for individual content\n    max_output_length: int = 100000  # Max length for the entire serialized result\n\n    @model_serializer\n    def ser_model(self) -> str:\n        # List to store concatenated strings\n        result = []\n\n        for data in self.data:\n            if data.error:\n                result.append(f\"Error for URL {data.url}: {data.error}\\n\")\n                continue  # Skip further processing if there's an error\n\n            assert data.content is not None\n\n            # Truncate content to ensure it does not exceed max_content_length\n            if len(data.content) > self.max_content_length:\n                data.content = (\n                    data.content[: self.max_content_length] + \"[TOO LONG, END]\"\n                )\n\n            result.append(\n                f\"URL: {data.url}\\nTitle: {data.title}\\nContent:\\n{data.content}\\n\"\n            )\n\n        output = \"\\n---\\n\".join(result)\n\n        # Apply final length truncation to the overall result\n        if len(output) > self.max_output_length:\n            output = output[: self.max_output_length] + \"\\n[OUTPUT TOO LONG, TRUNCATED]\"\n\n        return output\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/content_scraper/__init__.py",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape"
      ],
      "reference_who": [
        "src/criticsearch/tools/content_scraper/models.py/ScrapedData"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "ser_model",
      "md_content": [
        "**ser_model**: The function of ser_model is to generate a formatted string representation of scraped data, including error handling and content truncation.\n\n**parameters**: The parameters of this Function.\n· self: An instance of the class that contains the scraped data and configuration settings.\n\n**Code Description**: The ser_model function processes a collection of scraped data stored in the instance variable `self.data`. It initializes an empty list called `result` to accumulate formatted strings for each data entry. The function iterates over each item in `self.data`, checking for any errors associated with the data. If an error is found, it appends an error message to the `result` list and skips further processing for that entry.\n\nFor valid entries, the function asserts that the content is not None. It then checks if the length of the content exceeds a predefined maximum length (`self.max_content_length`). If it does, the content is truncated, and a suffix indicating truncation is added. The function constructs a formatted string that includes the URL, title, and content of the data entry, which is then appended to the `result` list.\n\nAfter processing all entries, the function joins the strings in the `result` list with a separator (\"---\") to create a single output string. It then checks if the total length of this output exceeds another predefined maximum length (`self.max_output_length`). If it does, the output is truncated, and a message indicating truncation is appended.\n\nFinally, the function returns the formatted output string, which contains the processed information of all the scraped data entries.\n\n**Note**: It is important to ensure that `self.data` is properly populated with valid data objects before calling this function. Additionally, the maximum lengths for content and output should be set appropriately to avoid unintended truncation.\n\n**Output Example**: \n```\nURL: http://example.com/page1\nTitle: Example Page 1\nContent:\nThis is the content of the first example page.\n\n---\nURL: http://example.com/page2\nTitle: Example Page 2\nContent:\nError for URL http://example.com/page2: Page not found\n\n---\nURL: http://example.com/page3\nTitle: Example Page 3\nContent:\nThis is the content of the third example page, which is quite informative and exceeds the maximum length set for content. [TOO LONG, END]\n```"
      ],
      "code_start_line": 19,
      "code_end_line": 46,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def ser_model(self) -> str:\n        # List to store concatenated strings\n        result = []\n\n        for data in self.data:\n            if data.error:\n                result.append(f\"Error for URL {data.url}: {data.error}\\n\")\n                continue  # Skip further processing if there's an error\n\n            assert data.content is not None\n\n            # Truncate content to ensure it does not exceed max_content_length\n            if len(data.content) > self.max_content_length:\n                data.content = (\n                    data.content[: self.max_content_length] + \"[TOO LONG, END]\"\n                )\n\n            result.append(\n                f\"URL: {data.url}\\nTitle: {data.title}\\nContent:\\n{data.content}\\n\"\n            )\n\n        output = \"\\n---\\n\".join(result)\n\n        # Apply final length truncation to the overall result\n        if len(output) > self.max_output_length:\n            output = output[: self.max_output_length] + \"\\n[OUTPUT TOO LONG, TRUNCATED]\"\n\n        return output\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tools/search_adapter/tavily_client.py": [
    {
      "type": "ClassDef",
      "name": "TavilyClient",
      "md_content": [
        "**TavilyClient**: The function of TavilyClient is to serve as an API client for interacting with the Tavily search service.\n\n**attributes**: The attributes of this Class.\n· base_url: str - This attribute stores the base URL for the Tavily API.\n· _api_key: str - This attribute holds the API key required for authenticating requests to the Tavily API.\n· headers: dict - This attribute contains the headers that will be sent with each API request, specifically setting the content type to JSON.\n\n**Code Description**: The TavilyClient class inherits from the BaseSearchClient class, establishing itself as a specific implementation for interacting with the Tavily API. Upon initialization, it sets the base URL for the Tavily API and stores the provided API key for use in subsequent requests. The class is designed to facilitate asynchronous search operations through the `search` method, which allows users to query the Tavily API with various parameters.\n\nThe `search` method is decorated with a retry mechanism, which attempts to resend the request up to five times in case of a rate limit exception (RatelimitException). This method accepts several parameters: `query`, `search_depth`, `topic`, `days`, and `max_results`, allowing for flexible search configurations. The method constructs a JSON payload containing these parameters along with the API key and sends an asynchronous POST request to the Tavily API's search endpoint.\n\nUpon receiving a response, the method checks the status code to determine the outcome of the request. A successful response (HTTP status code 200) results in the parsing of the JSON response into a SearchResponse object. If the response indicates a rate limit has been exceeded (HTTP status code 429), it raises a UsageLimitExceededError or RatelimitException based on the details provided in the response. An unauthorized access attempt (HTTP status code 401) raises an InvalidAPIKeyError. For any other unexpected status codes, the method returns a SearchResponse object containing an error message.\n\nThe TavilyClient is utilized within the SearchAggregator class, where it is instantiated if a valid Tavily API key is available. This integration allows the SearchAggregator to manage multiple search clients, including TavilyClient and BingClient, enabling it to perform searches across different services seamlessly.\n\n**Note**: It is essential to ensure that the API key provided to the TavilyClient is valid and that the application handles exceptions appropriately, particularly those related to rate limits and unauthorized access. Proper error handling will enhance the robustness of the application when interacting with the Tavily API.\n\nA possible appearance of the code's return value from the `search` method could be:\n```json\n{\n    \"query\": \"example search\",\n    \"results\": [\n        {\n            \"title\": \"Example Result 1\",\n            \"link\": \"https://example.com/result1\",\n            \"snippet\": \"This is a snippet of the first result.\"\n        },\n        {\n            \"title\": \"Example Result 2\",\n            \"link\": \"https://example.com/result2\",\n            \"snippet\": \"This is a snippet of the second result.\"\n        }\n    ],\n    \"error_message\": null\n}\n```",
        "**TavilyClient**: The function of TavilyClient is to serve as an API client for interacting with the Tavily search service.\n\n**attributes**: The attributes of this Class.\n· base_url: str - This attribute stores the base URL for the Tavily API, which is \"https://api.tavily.com\".\n· _api_key: str - This attribute holds the API key required for authenticating requests to the Tavily API.\n· headers: dict - This attribute contains the HTTP headers to be sent with API requests, specifically setting the \"Content-Type\" to \"application/json\".\n\n**Code Description**: The TavilyClient class inherits from BaseSearchClient and is designed to facilitate asynchronous search operations through the Tavily API. Upon initialization, the class requires an API key, which is stored in the _api_key attribute. The base URL for the API is predefined, and the headers for the requests are set to indicate that the content type is JSON.\n\nThe primary functionality of the TavilyClient is encapsulated in the asynchronous search method. This method accepts several parameters: a query string, search depth, topic, the number of days to look back, and the maximum number of results to return. The method constructs a JSON payload with these parameters, including the API key, and sends an asynchronous POST request to the Tavily API's search endpoint.\n\nThe search method is equipped with retry logic, allowing it to attempt the request up to five times in the event of a rate limit exception (indicated by a 429 status code). It employs an exponential backoff strategy combined with random jitter to manage retries effectively. If the API responds with a 200 status code, the method processes the response and returns a validated SearchResponse object. If a 429 status code is encountered, it checks for specific error details and raises a UsageLimitExceededError if applicable. For a 401 status code, it raises an InvalidAPIKeyError, indicating that the provided API key is invalid. Any other unexpected status codes result in the method returning a SearchResponse object that includes an error message detailing the issue.\n\nThe TavilyClient is utilized within the SearchAggregator class, which initializes instances of various search clients based on available API keys. If a valid Tavily API key is found in the settings, an instance of TavilyClient is created and added to the clients dictionary. This design allows the SearchAggregator to manage multiple search clients, including TavilyClient, enabling it to perform searches across different services seamlessly.\n\n**Note**: It is essential to ensure that the API key for Tavily is correctly configured in the settings. If the key is missing or invalid, the TavilyClient will not be instantiated, which may limit the functionality of the SearchAggregator. Proper error handling should be implemented when using the TavilyClient to manage search requests effectively.\n\nA possible appearance of the code's return value from the search method could be:\n```json\n{\n    \"query\": \"example search\",\n    \"results\": [\n        {\n            \"title\": \"Example Result 1\",\n            \"url\": \"https://example.com/result1\",\n            \"snippet\": \"This is a snippet of the first result.\"\n        },\n        {\n            \"title\": \"Example Result 2\",\n            \"url\": \"https://example.com/result2\",\n            \"snippet\": \"This is a snippet of the second result.\"\n        }\n    ],\n    \"error_message\": null\n}\n```"
      ],
      "code_start_line": 20,
      "code_end_line": 88,
      "params": [],
      "have_return": true,
      "code_content": "class TavilyClient(BaseSearchClient):\n    \"\"\"\n    Tavily API client class.\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        self.base_url = \"https://api.tavily.com\"\n        self._api_key = api_key\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n        }\n\n    @retry(\n        stop=stop_after_attempt(5),  # 重试最多5次\n        wait=wait_exponential(multiplier=1, min=4, max=30)\n        + wait_random(min=1, max=5),  # 指数退避 + 随机抖动\n        retry=retry_if_exception_type(RatelimitException),\n    )\n    async def search(\n        self,\n        query: str,\n        search_depth: Literal[\"basic\", \"advanced\"] = \"basic\",\n        topic: Literal[\"general\", \"news\"] = \"general\",\n        days: int = 7,\n        max_results: int = getattr(settings, \"max_results\", 10) or 10,\n    ) -> SearchResponse:\n        \"\"\"\n        异步搜索方法\n        \"\"\"\n\n        # 发起异步请求\n        data = {\n            \"query\": query,\n            \"search_depth\": search_depth,\n            \"topic\": topic,\n            \"days\": days,\n            \"max_results\": max_results,\n            \"api_key\": self._api_key,\n        }\n\n        async with httpx.AsyncClient(timeout=30, http2=True) as client:\n            response = await client.post(\n                self.base_url + \"/search\", json=data, headers=self.headers\n            )\n\n        if response.status_code == 200:\n            return SearchResponse.model_validate(response.json())\n        elif response.status_code == 429:\n            try:\n                detail = response.json().get(\"detail\", {}).get(\"error\")\n                if detail:\n                    raise UsageLimitExceededError(detail)  # 抛出后直接传播，不被捕获\n            except UsageLimitExceededError:\n                raise  # 直接传播 UsageLimitExceededError，避免被后续捕获\n            except Exception:\n                # 捕获其他异常并记录日志\n                printer.print_exception(\n                    f\"Failed to process 429 response. Response: {response.text}\"\n                )\n                raise RatelimitException()  # 抛出通用限流异常\n\n            raise RatelimitException()\n        elif response.status_code == 401:\n            raise InvalidAPIKeyError()\n        else:\n            return SearchResponse(\n                query=query,\n                error_message=f\"Unexpected status code: {response.status_code}. Response: {response.text}\",\n            )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/__init__"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/base_search_client.py/BaseSearchClient",
        "src/criticsearch/tools/search_adapter/exceptions.py/RatelimitException"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the TavilyClient object with the required API key and configure default settings for the instance.\n\n**parameters**: The parameters of this Function.\n- api_key: A string representing the API key used for authentication with the Tavily API.\n\n**Code Description**: The `__init__` method is the constructor of the `TavilyClient` class. It initializes an instance of the class by setting up default values and configurations necessary for interacting with the Tavily API.\n  \n1. **self.base_url**: This attribute is set to the string \"https://api.tavily.com\", which represents the base URL of the Tavily API. It is used as the root URL for all API requests made by this client.\n  \n2. **self._api_key**: This attribute is initialized with the value of the `api_key` parameter. It stores the API key provided when the instance of the client is created. This key will likely be used in API requests for authentication purposes.\n\n3. **self.headers**: A dictionary is created with the key `\"Content-Type\"` set to `\"application/json\"`. This header is typically included in HTTP requests to specify that the body of the request is in JSON format. It ensures that the client interacts with the API in a manner that is compatible with its expected input format.\n\nThe method does not return any value. It simply prepares the object with the necessary attributes to interact with the Tavily API.\n\n**Note**: \n- The `api_key` is essential for authentication with the Tavily API and must be provided when creating an instance of the `TavilyClient`.\n- The `base_url` and headers are set to default values, which can be further customized or extended by other methods in the `TavilyClient` class for specific API requests."
      ],
      "code_start_line": 25,
      "code_end_line": 30,
      "params": [
        "self",
        "api_key"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, api_key: str):\n        self.base_url = \"https://api.tavily.com\"\n        self._api_key = api_key\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n        }\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [
        "## Function: `search`\n\n### Overview\nThe `search` function is an asynchronous method designed to perform a search query using the Tavily engine. It allows users to specify search parameters, such as the query string, search depth, topic, date range, and maximum number of results. The function interacts with an API endpoint and returns a structured response encapsulated in the `SearchResponse` class. In case of errors such as rate limiting or invalid API keys, appropriate exceptions are raised.\n\n### Parameters\n- **query** (`str`): The search query string to be used in the search request. This parameter is required.\n- **search_depth** (`Literal[\"basic\", \"advanced\"]`): The depth of the search. The default value is `\"basic\"`, and the available options are:\n  - `\"basic\"`: A standard search depth.\n  - `\"advanced\"`: A more detailed search.\n  \n- **topic** (`Literal[\"general\", \"news\"]`): The topic of the search query. The default value is `\"general\"`, and the options available are:\n  - `\"general\"`: General search results.\n  - `\"news\"`: Results specifically related to news topics.\n  \n- **days** (`int`): The time frame in days for filtering results. The default value is `7`, meaning results from the past 7 days will be included.\n  \n- **max_results** (`int`): The maximum number of search results to return. The default value is `10`, and it can be adjusted as needed.\n\n### Returns\n- **SearchResponse**: The function returns an instance of the `SearchResponse` class, which contains the results of the search query. The `SearchResponse` includes the query string, a list of search results, and any error messages encountered during the process.\n\n### Exceptions\nThe function raises the following exceptions in case of errors:\n- **UsageLimitExceededError**: Raised when the API usage limit is exceeded. This is specifically handled when a `429` status code is returned.\n- **RatelimitException**: Raised when a rate-limiting issue occurs, typically in the event of frequent API requests or when an exception other than `UsageLimitExceededError` is encountered due to rate-limiting issues.\n- **InvalidAPIKeyError**: Raised when the provided API key is invalid, typically corresponding to a `401` status code.\n\n### Behavior\n1. The function sends an asynchronous POST request to the Tavily search API with the specified parameters.\n2. If the request is successful (status code `200`), the function returns a `SearchResponse` object containing the search results.\n3. If the API response returns a `429` status code (rate limiting), the function checks if the response contains a usage limit error. If found, it raises a `UsageLimitExceededError`. If any other error occurs while handling the response, a `RatelimitException` is raised.\n4. If the API response returns a `401` status code (authentication error), the function raises an `InvalidAPIKeyError`.\n5. If the status code is not `200`, `429`, or `401`, the function returns a `SearchResponse` with an error message indicating an unexpected status code.\n\n### Usage Example\n```python\ntavily_client = TavilyClient(api_key=\"your_api_key\")\nresponse = await tavily_client.search(query=\"Python programming\", search_depth=\"advanced\", topic=\"general\", days=7, max_results=5)\nprint(response.ser_model())\n```\n\n### Notes\n- The `SearchResponse` class is used to structure the response from the search API, which includes the query, results, and any error messages.\n- The `UsageLimitExceededError` is raised when the search client has exceeded the API usage limits.",
        "**search**: The function of search is to perform an asynchronous search query based on user-defined parameters and return the results encapsulated in a SearchResponse object.\n\n**parameters**: The parameters of this Function.\n· query: str - A string representing the user's search query. This parameter cannot be empty and is essential for executing the search.  \n· search_depth: Literal[\"basic\", \"advanced\"] - This optional parameter specifies the depth of the search, with a default value of \"basic\".  \n· topic: Literal[\"general\", \"news\"] - This optional parameter defines the topic of the search, defaulting to \"general\".  \n· days: int - This optional parameter indicates the number of days to look back for results, defaulting to 7 days.  \n· max_results: int - This optional parameter sets the maximum number of results to return, defaulting to the value specified in the settings or 10 if not defined.\n\n**Code Description**: The search function is an asynchronous method designed to facilitate search queries against a specified API. It constructs a data payload containing the search parameters, including the query string, search depth, topic, days, maximum results, and the API key. Using the httpx library, it initiates an asynchronous POST request to the search endpoint of the API.\n\nUpon receiving a response, the function checks the status code:\n- If the status code is 200, it validates and returns the response data as a SearchResponse object.\n- If the status code is 429, indicating that the usage limit has been exceeded, it attempts to extract the error detail from the response. If a specific error message is present, it raises a UsageLimitExceededError. If no detail is found, it logs the exception and raises a RatelimitException.\n- If the status code is 401, it raises an InvalidAPIKeyError, indicating that the provided API key is invalid.\n- For any other status codes, it constructs a SearchResponse object containing the original query and an error message indicating the unexpected status code.\n\nThe search function is called by the _search_single_query method within the SearchAggregator class. This method manages the execution of search queries across multiple search engines. It iterates through a list of available engines, invoking the search method for each engine that is operational. The results from these searches are collected and returned as a structured response. The search function is integral to the overall search functionality, providing a consistent interface for querying search engines and handling various error conditions.\n\n**Note**: It is crucial to ensure that the query parameter is a valid and non-empty string. The optional parameters should be set according to the desired search behavior. Proper error handling is implemented for various HTTP status codes, and developers should be aware of the implications of exceeding usage limits or providing invalid API keys.\n\n**Output Example**: A possible return value of the search function could be structured as follows:\n```python\nSearchResponse(\n    query=\"latest technology news\",\n    results=[\n        SearchResult(title=\"Tech Innovations\", url=\"https://example.com/tech-innovations\", content=\"Explore the latest in technology.\"),\n        SearchResult(title=\"Gadget Reviews\", url=\"https://example.com/gadget-reviews\", content=\"Read reviews on the newest gadgets.\")\n    ],\n    error_message=None\n)\n```\nThis output illustrates a successful search response containing the original query, a list of search results, and no error messages."
      ],
      "code_start_line": 38,
      "code_end_line": 88,
      "params": [
        "self",
        "query",
        "search_depth",
        "topic",
        "days",
        "max_results"
      ],
      "have_return": true,
      "code_content": "    async def search(\n        self,\n        query: str,\n        search_depth: Literal[\"basic\", \"advanced\"] = \"basic\",\n        topic: Literal[\"general\", \"news\"] = \"general\",\n        days: int = 7,\n        max_results: int = getattr(settings, \"max_results\", 10) or 10,\n    ) -> SearchResponse:\n        \"\"\"\n        异步搜索方法\n        \"\"\"\n\n        # 发起异步请求\n        data = {\n            \"query\": query,\n            \"search_depth\": search_depth,\n            \"topic\": topic,\n            \"days\": days,\n            \"max_results\": max_results,\n            \"api_key\": self._api_key,\n        }\n\n        async with httpx.AsyncClient(timeout=30, http2=True) as client:\n            response = await client.post(\n                self.base_url + \"/search\", json=data, headers=self.headers\n            )\n\n        if response.status_code == 200:\n            return SearchResponse.model_validate(response.json())\n        elif response.status_code == 429:\n            try:\n                detail = response.json().get(\"detail\", {}).get(\"error\")\n                if detail:\n                    raise UsageLimitExceededError(detail)  # 抛出后直接传播，不被捕获\n            except UsageLimitExceededError:\n                raise  # 直接传播 UsageLimitExceededError，避免被后续捕获\n            except Exception:\n                # 捕获其他异常并记录日志\n                printer.print_exception(\n                    f\"Failed to process 429 response. Response: {response.text}\"\n                )\n                raise RatelimitException()  # 抛出通用限流异常\n\n            raise RatelimitException()\n        elif response.status_code == 401:\n            raise InvalidAPIKeyError()\n        else:\n            return SearchResponse(\n                query=query,\n                error_message=f\"Unexpected status code: {response.status_code}. Response: {response.text}\",\n            )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print_exception",
        "src/criticsearch/tools/search_adapter/exceptions.py/UsageLimitExceededError",
        "src/criticsearch/tools/search_adapter/exceptions.py/InvalidAPIKeyError",
        "src/criticsearch/tools/search_adapter/exceptions.py/RatelimitException",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponse"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/tools/search_adapter/base_search_client.py": [
    {
      "type": "ClassDef",
      "name": "BaseSearchClient",
      "md_content": [
        "**BaseSearchClient**: The function of BaseSearchClient is to define the structure and interface for search clients. \n\n**attributes**: \n- There are no attributes in this class.\n\n**Code Description**: \nThe `BaseSearchClient` class is an abstract base class, designed to be inherited by other search client classes that implement specific search API integrations. It contains a single abstract method, `search`, which must be implemented by any subclass. The method signature specifies that `search` should be an asynchronous method that accepts a `query` string and potentially other keyword arguments (`kwargs`) to allow flexibility. The return type for `search` is expected to be any type, which provides a broad scope for returning various formats of search responses depending on the specific implementation in the subclasses.\n\nThis class does not implement any functionality itself; it serves as a contract for all subclasses that handle search functionality. Subclasses such as `BingClient`, `DuckDuckGoClient`, and `TavilyClient` inherit from `BaseSearchClient` and implement the `search` method to interact with specific search engines. These subclasses are responsible for providing the actual logic for sending search requests, handling responses, and structuring the search results.\n\nThe `BaseSearchClient` class is crucial in this context as it standardizes the interface for search functionality, allowing the rest of the system to interact with any specific search engine implementation in a uniform way, without needing to worry about the underlying details of the API being used. Any object or module in the system that relies on search results can use `BaseSearchClient` as the reference for interacting with any subclass that implements the `search` method.\n\n**Note**: \n- The `BaseSearchClient` class is not intended to be instantiated directly. It is meant to be subclassed, and the `search` method should be implemented in those subclasses to provide the actual search functionality.\n- This class enforces the asynchronous nature of search operations, ensuring that any subclass providing the `search` method must handle asynchronous behavior."
      ],
      "code_start_line": 5,
      "code_end_line": 8,
      "params": [],
      "have_return": false,
      "code_content": "class BaseSearchClient(ABC):\n    @abstractmethod\n    async def search(self, query: str, **kwargs: Any) -> Any:\n        pass\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/bing_client.py",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient",
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py",
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient",
        "src/criticsearch/tools/search_adapter/tavily_client.py",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [
        "**search**: The function of search is to asynchronously process a search query and return the result.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the search query to be executed.\n· kwargs: A variable number of additional keyword arguments, which may provide further customization or configuration for the search operation.\n\n**Code Description**: \nThe `search` function is defined as an asynchronous method, which implies that it will likely handle operations that may take time to complete, such as making network requests or querying a database. The function signature takes in two parameters: `query` and `kwargs`. \n- The `query` parameter is a required string that holds the search term or phrase the function will process.\n- The `kwargs` parameter allows for the inclusion of additional keyword arguments, which could be used for supplementary parameters such as pagination, filters, or sorting criteria in the search operation.\n\nHowever, the function body itself is currently not implemented, which means it lacks the specific logic to execute the search and return results. The `pass` statement indicates that this function is a placeholder or is intended to be overridden in a subclass or future implementation. The function is defined to return a result of type `Any`, indicating that the return type is flexible and could be adjusted based on the actual implementation.\n\n**Note**: \n- The `search` function is expected to be part of an asynchronous workflow, so it should be awaited when called in an asynchronous context.\n- The behavior and utility of the `kwargs` parameter are not defined here and would depend on the actual implementation of the method."
      ],
      "code_start_line": 7,
      "code_end_line": 8,
      "params": [
        "self",
        "query"
      ],
      "have_return": false,
      "code_content": "    async def search(self, query: str, **kwargs: Any) -> Any:\n        pass\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tools/search_adapter/duckduckgo_client.py": [
    {
      "type": "ClassDef",
      "name": "DuckDuckGoClient",
      "md_content": [
        "**DuckDuckGoClient**: The function of DuckDuckGoClient is to implement a search client that interacts with the DuckDuckGo search engine, providing search results based on specified query parameters.\n\n**attributes**: \n- None.\n\n**Code Description**: \nThe `DuckDuckGoClient` class is a subclass of the `BaseSearchClient` class. It is specifically designed to perform search operations via DuckDuckGo's search engine. This class implements the asynchronous `search` method, which retrieves search results by sending a query to DuckDuckGo's API, processes the results, and returns a structured response.\n\nThe class contains the following key components:\n\n1. **_convert_days_to_timelimit**: This helper method converts a specified number of days into DuckDuckGo's internal time limit format. It accepts an integer `days` as input and returns a string corresponding to one of the following time periods:\n   - `\"d\"` for the last 24 hours\n   - `\"w\"` for the last week\n   - `\"m\"` for the last month\n   - `\"y\"` for the last year\n   The method is essential for formatting the time-related parameter when calling the DuckDuckGo search API.\n\n2. **search**: This asynchronous method is responsible for sending a search query to DuckDuckGo's API. It accepts the following parameters:\n   - `query`: A string representing the search query.\n   - `days`: An optional integer that filters results to a specific time frame. It defaults to 7 (last week).\n   - `max_results`: An optional integer specifying the maximum number of results to return. It defaults to 10.\n   - `region`: A string that defines the search region. It can be `\"us-en\"` for the United States or `\"cn-zh\"` for China, with a default value of `\"us-en\"`.\n   \n   The method utilizes the `_convert_days_to_timelimit` helper to determine the time frame for the search, constructs a request to DuckDuckGo, and processes the response into a list of `SearchResult` objects. Each `SearchResult` contains the title, URL, and content of a search result. Finally, the method returns a `SearchResponse` object that contains the query and the processed search results, limited to the specified `max_results`.\n\n3. **retry decorator**: The `search` method is wrapped with a retry decorator that handles retries in case of failures. The retry logic is configured to stop after 5 attempts (`stop_after_attempt(5)`) and apply an exponential backoff strategy with random jitter (`wait_exponential` and `wait_random`). The decorator specifically targets the `RatelimitException`, which is likely raised when the DuckDuckGo API is being rate-limited.\n\nIn terms of functionality, the `DuckDuckGoClient` integrates with the `AsyncDDGS` (asynchronous DuckDuckGo search) client, which is responsible for sending the query and receiving raw search results. These results are then transformed into a structured format suitable for further use in the system. \n\n**Note**: \n- The `search` method is asynchronous, so it must be awaited when called.\n- The method supports time-based filtering, allowing users to retrieve results from the last 24 hours, week, month, or year.\n- The retry logic ensures robustness in case of rate limiting or temporary failures in the API interaction.\n\n**Output Example**:\nA possible return value from the `search` method might look like this:\n\n```json\n{\n  \"query\": \"example search query\",\n  \"results\": [\n    {\n      \"title\": \"Example Result 1\",\n      \"url\": \"https://www.example1.com\",\n      \"content\": \"This is the content of the first search result.\"\n    },\n    {\n      \"title\": \"Example Result 2\",\n      \"url\": \"https://www.example2.com\",\n      \"content\": \"This is the content of the second search result.\"\n    }\n  ]\n}\n```"
      ],
      "code_start_line": 18,
      "code_end_line": 74,
      "params": [],
      "have_return": true,
      "code_content": "class DuckDuckGoClient(BaseSearchClient):\n    def _convert_days_to_timelimit(self, days: int) -> str:\n        \"\"\"\n        Convert days to DuckDuckGo's timelimit format.\n\n        Args:\n            days (int): Number of days to filter results.\n\n        Returns:\n            str: A string representing DuckDuckGo's timelimit format ('d', 'w', 'm', 'y').\n        \"\"\"\n        if days <= 1:\n            return \"d\"  # Last 24 hours\n        elif days <= 7:\n            return \"w\"  # Last week\n        elif days <= 30:\n            return \"m\"  # Last month\n        else:\n            return \"y\"  # Last year\n\n    @retry(\n        stop=stop_after_attempt(5),  # 重试最多5次\n        wait=wait_exponential(multiplier=1, min=4, max=30)\n        + wait_random(min=1, max=5),  # 指数退避 + 随机抖动\n        retry=retry_if_exception_type(RatelimitException),\n    )\n    async def search(\n        self,\n        query: str,\n        days: int = 7,\n        max_results: int = 10,\n        region: Literal[\"us-en\", \"cn-zh\"] = \"us-en\",\n    ) -> SearchResponse:\n        timelimit = self._convert_days_to_timelimit(days)\n\n        logger.debug(f\"Using 'duckduckgo-search-client' for query '{query}'.\")\n\n        raw_results = await AsyncDDGS(timeout=10).atext(\n            query,\n            region=region,\n            safesearch=\"on\",\n            timelimit=timelimit,\n            max_results=max_results,\n        )\n\n        results = [\n            SearchResult(\n                title=result[\"title\"],\n                url=result[\"href\"],\n                content=result[\"body\"],\n            )\n            for result in raw_results\n        ]\n        return SearchResponse(\n            query=query,\n            results=results[:max_results],\n        )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/base_search_client.py/BaseSearchClient"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_convert_days_to_timelimit",
      "md_content": [
        "**_convert_days_to_timelimit**: The function of _convert_days_to_timelimit is to convert a given number of days into a timelimit format used by DuckDuckGo's search client.\n\n**parameters**:\n· days: An integer representing the number of days to filter results.\n\n**Code Description**:  \nThe function _convert_days_to_timelimit takes an integer value for `days` and returns a corresponding string representing DuckDuckGo's timelimit format. This format is used to filter search results based on the specified timeframe.\n\nThe function works as follows:\n1. If the `days` value is less than or equal to 1, the function returns `\"d\"`, which stands for results from the last 24 hours.\n2. If the `days` value is greater than 1 but less than or equal to 7, the function returns `\"w\"`, representing results from the last week.\n3. If the `days` value is greater than 7 but less than or equal to 30, the function returns `\"m\"`, indicating results from the last month.\n4. If the `days` value exceeds 30, the function returns `\"y\"`, indicating results from the last year.\n\nThis function is crucial for the `search` method within the `DuckDuckGoClient` class. Specifically, it is called to determine the appropriate timelimit format when initiating a search request. The `search` method passes the `days` argument to _convert_days_to_timelimit, which then returns the timelimit string to be used in the search query. The timelimit helps filter search results based on recency, allowing the user to retrieve results relevant to a specific timeframe.\n\n**Note**: The `days` parameter should be an integer, and the function will return one of the following string values:\n- `\"d\"` for the last 24 hours,\n- `\"w\"` for the last week,\n- `\"m\"` for the last month,\n- `\"y\"` for the last year.\n\n**Output Example**:\nFor `days = 3`, the output would be `\"w\"`, representing results from the last week.  \nFor `days = 40`, the output would be `\"y\"`, representing results from the last year."
      ],
      "code_start_line": 19,
      "code_end_line": 36,
      "params": [
        "self",
        "days"
      ],
      "have_return": true,
      "code_content": "    def _convert_days_to_timelimit(self, days: int) -> str:\n        \"\"\"\n        Convert days to DuckDuckGo's timelimit format.\n\n        Args:\n            days (int): Number of days to filter results.\n\n        Returns:\n            str: A string representing DuckDuckGo's timelimit format ('d', 'w', 'm', 'y').\n        \"\"\"\n        if days <= 1:\n            return \"d\"  # Last 24 hours\n        elif days <= 7:\n            return \"w\"  # Last week\n        elif days <= 30:\n            return \"m\"  # Last month\n        else:\n            return \"y\"  # Last year\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient/search"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [
        "**search**: The function of search is to perform an asynchronous search query using the DuckDuckGo search client and return the results in a structured format.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the search term to be queried.  \n· days: An integer representing the number of days to filter results, defaulting to 7.  \n· max_results: An integer indicating the maximum number of search results to return, defaulting to 10.  \n· region: A string literal that specifies the region for the search results, defaulting to \"us-en\".  \n\n**Code Description**: The `search` function is an asynchronous method designed to query the DuckDuckGo search engine. It accepts a search term (`query`) and several optional parameters that allow users to customize the search results based on recency and quantity.\n\n1. The method begins by converting the `days` parameter into a timelimit format using the `_convert_days_to_timelimit` method. This conversion is crucial as it determines how recent the search results should be, based on the specified number of days.\n\n2. A debug log statement is executed to indicate the initiation of a search query, providing visibility into the operation being performed.\n\n3. The function then calls the `atext` method of the `AsyncDDGS` class, which is part of the DuckDuckGo search client. This method is awaited, meaning that the function will pause execution until the search results are retrieved. The parameters passed to `atext` include the search `query`, the `region`, a safesearch setting, the `timelimit`, and the maximum number of results to return.\n\n4. Once the raw results are obtained, the function processes these results into a list of `SearchResult` objects. Each `SearchResult` is instantiated with the title, URL, and content extracted from the raw results.\n\n5. Finally, the function returns a `SearchResponse` object that encapsulates the original query and the list of search results, limited to the specified `max_results`.\n\nThe `search` function is integral to the `DuckDuckGoClient` class, facilitating user queries and structuring the response in a way that is easy to consume by other components of the application. It relies on the `_convert_days_to_timelimit` method to filter results based on recency and utilizes the `SearchResult` and `SearchResponse` classes to format the output.\n\n**Note**: \n- The `query` parameter is mandatory and should be a valid search term. \n- The `days`, `max_results`, and `region` parameters are optional and have default values, allowing for flexible usage.\n- The function is asynchronous, which means it should be awaited when called to ensure proper execution flow.\n\n**Output Example**: \nA possible return value of the `search` function could look like this:\n```json\n{\n  \"query\": \"Python programming\",\n  \"results\": [\n    {\n      \"title\": \"Learn Python - Full Course for Beginners\",\n      \"url\": \"https://www.example.com/learn-python\",\n      \"content\": \"This comprehensive course covers Python basics and advanced topics.\"\n    },\n    {\n      \"title\": \"Python Programming Language\",\n      \"url\": \"https://www.example.com/python\",\n      \"content\": \"Python is a popular programming language known for its simplicity.\"\n    }\n  ]\n}\n```"
      ],
      "code_start_line": 44,
      "code_end_line": 74,
      "params": [
        "self",
        "query",
        "days",
        "max_results",
        "region"
      ],
      "have_return": true,
      "code_content": "    async def search(\n        self,\n        query: str,\n        days: int = 7,\n        max_results: int = 10,\n        region: Literal[\"us-en\", \"cn-zh\"] = \"us-en\",\n    ) -> SearchResponse:\n        timelimit = self._convert_days_to_timelimit(days)\n\n        logger.debug(f\"Using 'duckduckgo-search-client' for query '{query}'.\")\n\n        raw_results = await AsyncDDGS(timeout=10).atext(\n            query,\n            region=region,\n            safesearch=\"on\",\n            timelimit=timelimit,\n            max_results=max_results,\n        )\n\n        results = [\n            SearchResult(\n                title=result[\"title\"],\n                url=result[\"href\"],\n                content=result[\"body\"],\n            )\n            for result in raw_results\n        ]\n        return SearchResponse(\n            query=query,\n            results=results[:max_results],\n        )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient/_convert_days_to_timelimit",
        "src/criticsearch/tools/search_adapter/models.py/SearchResult",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponse"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/tools/search_adapter/__init__.py": [],
  "src/criticsearch/tools/search_adapter/__main__.py": [
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "## Function Documentation: `main`\n\n### Overview:\nThe `main` function serves as the entry point for executing a search query using the `SearchAggregator` class. It is an asynchronous function that initializes the search aggregator, performs a search, and prints the results.\n\n### Function Signature:\n```python\nasync def main()\n```\n\n### Description:\nThe `main` function is responsible for initiating the search process by utilizing the `SearchAggregator` class to handle search queries. It performs the following steps:\n\n1. **Initialization**: The function first creates an instance of the `SearchAggregator` class, which is used to manage and execute searches across multiple available search engines.\n\n2. **Search Execution**: It calls the `search` method of the `SearchAggregator` instance, passing a list of queries. In this case, the query is a single string: `\"Who is Leo Messi?\"`.\n\n3. **Results Handling**: After executing the search, the function awaits the response from the `search` method. The results are then printed to the console.\n\n### Purpose:\nThe function provides an example of how to interact with the `SearchAggregator` class to perform searches. It demonstrates the process of query submission and result handling within an asynchronous context.\n\n### Parameters:\nThis function does not accept any parameters.\n\n### Execution Flow:\n1. An instance of the `SearchAggregator` is created.\n2. The `search` method of the `SearchAggregator` is called with a predefined query.\n3. The search results are printed to the console.\n\n### Example Output:\n```json\n{\n  \"responses\": [\n    {\n      \"query\": \"Who is Leo Messi?\",\n      \"results\": [\n        {\n          \"title\": \"Lionel Messi - Wikipedia\",\n          \"url\": \"https://en.wikipedia.org/wiki/Lionel_Messi\",\n          \"snippet\": \"Lionel Andrés Messi is an Argentine professional footballer widely regarded as one of the greatest players of all time.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Notes:\n- The function demonstrates basic usage of the `SearchAggregator` class, performing an asynchronous search and printing the response.\n- The search query used in this example is predefined, but in a real-world scenario, queries could be dynamic, and multiple queries could be passed to the `search` method concurrently.\n"
      ],
      "code_start_line": 6,
      "code_end_line": 12,
      "params": [],
      "have_return": false,
      "code_content": "    async def main():\n        search_aggregator = SearchAggregator()\n\n        # 调用异步搜索方法\n        response = await search_aggregator.search(query=[\"Who is Leo Messi?\"])\n\n        print(response)\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "src/criticsearch/tools/search_adapter/exceptions.py": [
    {
      "type": "ClassDef",
      "name": "SearchClientError",
      "md_content": [
        "**SearchClientError**: The function of SearchClientError is to define a base exception class for errors related to the search client.\n\n**attributes**:\n· message: str - This attribute stores the error message that is associated with the exception.\n\n**Code Description**:  \nThe `SearchClientError` class is a custom exception that extends Python's built-in `Exception` class. It is used as a base class for specific error types related to the search client functionality. The class constructor (`__init__`) accepts a message parameter, which defaults to \"An error occurred in the search client.\" This message is passed to the parent `Exception` class using `super().__init__(message)`, enabling the `SearchClientError` instance to store and propagate the message to whoever handles the exception.\n\nThe class is intended to be subclassed by other specific error classes that need to indicate particular search client-related errors, such as exceeded usage limits, bad requests, or invalid API keys. In the project, we can see that multiple exceptions inherit from `SearchClientError`, such as `UsageLimitExceededError`, `BadRequestError`, `InvalidAPIKeyError`, `RatelimitException`, and `TimeoutException`. Each of these subclasses customizes the default error message to reflect a more specific error condition, but they all share the same base behavior provided by `SearchClientError`. \n\nThe subclasses inherit the core functionality of the `SearchClientError` class, while modifying the error message for clarity and relevance to the particular situation. These subclasses ensure that the search client exceptions are appropriately handled in different cases, providing clear and specific error messages to the developers working with the search client.\n\n**Note**:  \n- The `SearchClientError` class serves as a foundational class for defining more granular exception types.\n- It is important that any custom error related to the search client extend from `SearchClientError` to maintain consistency across the project.\n- The default message can be overridden when raising exceptions, but the base class guarantees that a message is always available, which helps in debugging and troubleshooting."
      ],
      "code_start_line": 4,
      "code_end_line": 6,
      "params": [],
      "have_return": false,
      "code_content": "class SearchClientError(Exception):\n    def __init__(self, message: str = \"An error occurred in the search client.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/exceptions.py/UsageLimitExceededError",
        "src/criticsearch/tools/search_adapter/exceptions.py/BadRequestError",
        "src/criticsearch/tools/search_adapter/exceptions.py/InvalidAPIKeyError",
        "src/criticsearch/tools/search_adapter/exceptions.py/RatelimitException",
        "src/criticsearch/tools/search_adapter/exceptions.py/TimeoutException"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the SearchClientError class with a specified error message.\n\n**parameters**: The parameters of this Function.\n· message: A string that represents the error message to be displayed. It defaults to \"An error occurred in the search client.\"\n\n**Code Description**: This __init__ function is a constructor for the SearchClientError class, which is likely a custom exception class used to handle errors related to a search client in the application. The function takes one optional parameter, `message`, which allows the user to specify a custom error message. If no message is provided when an instance of the class is created, the default message \"An error occurred in the search client.\" will be used. The constructor calls the `__init__` method of its superclass (presumably a built-in exception class) using `super().__init__(message)`, which initializes the base class with the provided message. This ensures that the error message is properly set up for the exception handling mechanism in Python.\n\n**Note**: It is important to provide a meaningful error message when raising this exception to facilitate debugging and error tracking. Users of this class should be aware that the default message may not always convey the specific issue encountered, so customizing the message is recommended when applicable."
      ],
      "code_start_line": 5,
      "code_end_line": 6,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"An error occurred in the search client.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "UsageLimitExceededError",
      "md_content": [
        "**UsageLimitExceededError**: The function of UsageLimitExceededError is to indicate that the usage limit for the search client has been exceeded.\n\n**attributes**:\n· message: str - This attribute stores the error message that is associated with the exception.\n\n**Code Description**: The `UsageLimitExceededError` class is a custom exception that inherits from the `SearchClientError` base class. It is specifically designed to handle scenarios where the usage limit of the search client has been surpassed. The constructor of the `UsageLimitExceededError` class accepts a single parameter, `message`, which defaults to \"Usage limit exceeded.\" This message provides a clear indication of the error condition when the exception is raised.\n\nWhen an instance of `UsageLimitExceededError` is created, it calls the constructor of its parent class, `SearchClientError`, using `super().__init__(message)`. This ensures that the error message is properly initialized and can be accessed by any exception handling mechanisms that catch this specific error.\n\nIn the context of the project, the `UsageLimitExceededError` is utilized within the `search` method of the `TavilyClient` class. When a search request is made and the server responds with a status code of 429, indicating that the usage limit has been exceeded, the `UsageLimitExceededError` is raised. This allows the error to propagate up the call stack, where it can be handled appropriately, such as logging the error and marking the search engine as unavailable.\n\nAdditionally, the `UsageLimitExceededError` is caught in the `_search_single_query` method of the `SearchAggregator` class. When this exception is encountered, it logs a warning message indicating that the engine has failed due to the usage limit being exceeded and subsequently marks the engine as unavailable. This structured approach to error handling ensures that the application can gracefully manage situations where the search client cannot fulfill requests due to usage constraints.\n\n**Note**: \n- The `UsageLimitExceededError` class is intended to be used specifically for signaling that the search client's usage limit has been reached.\n- It is essential to handle this exception in the calling code to maintain robust error management and provide meaningful feedback to users or developers regarding the state of the search client."
      ],
      "code_start_line": 9,
      "code_end_line": 11,
      "params": [],
      "have_return": false,
      "code_content": "class UsageLimitExceededError(SearchClientError):\n    def __init__(self, message: str = \"Usage limit exceeded.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "src/criticsearch/tools/search_adapter/tavily_client.py",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the UsageLimitExceededError class with a specific error message.\n\n**parameters**: The parameters of this Function.\n· message: A string that represents the error message to be displayed when the exception is raised. It defaults to \"Usage limit exceeded.\"\n\n**Code Description**: The __init__ method is a constructor for the UsageLimitExceededError class, which is a custom exception that inherits from the built-in Exception class. When an instance of UsageLimitExceededError is created, this method is called to set up the instance. The method takes one optional parameter, message, which allows the user to specify a custom error message. If no message is provided, it defaults to \"Usage limit exceeded.\" The constructor then calls the constructor of the parent class (Exception) using super().__init__(message), which initializes the base class with the provided message. This ensures that the error message is properly stored and can be accessed when the exception is raised.\n\n**Note**: It is important to provide a meaningful message when raising this exception to ensure that the context of the error is clear to the user. If the default message is used, it may not provide sufficient information about the specific situation that caused the exception."
      ],
      "code_start_line": 10,
      "code_end_line": 11,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Usage limit exceeded.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "BadRequestError",
      "md_content": [
        "**BadRequestError**: The function of BadRequestError is to represent an exception that occurs when a bad request is made to the search client.\n\n**attributes**:\n· message: str - This attribute stores the error message that is associated with the exception. The default message is \"Bad request.\"\n\n**Code Description**:  \nThe `BadRequestError` class is a custom exception that extends the `SearchClientError` base exception class. This class is designed to handle errors that arise specifically from a bad request made to the search client. It inherits the core functionality of `SearchClientError`, which means it can propagate an error message and be caught by any exception handling mechanism designed to manage `SearchClientError` exceptions.\n\nThe constructor of the `BadRequestError` class accepts a message parameter, which has a default value of \"Bad request.\" If no message is provided, this default message will be used when raising the exception. The message is passed to the parent class `SearchClientError` using `super().__init__(message)`, ensuring that the error message is properly stored and can be accessed by the exception handler.\n\nAs a subclass of `SearchClientError`, the `BadRequestError` inherits the attributes and behaviors of its parent, but it customizes the default message to indicate that the error specifically pertains to a bad request. This helps in distinguishing it from other types of errors in the search client, such as those related to invalid API keys, rate limits, or usage limits.\n\nIn terms of functionality, the `BadRequestError` is useful for handling scenarios where the client makes a request that the search service cannot process due to an issue with the request itself (such as invalid parameters or incorrect syntax). The error message provides clarity about the nature of the issue, assisting developers in debugging and resolving the problem.\n\n**Note**:  \n- The `BadRequestError` class is intended to be raised when a bad request is made to the search client, providing a clear and specific error message.\n- This class inherits from `SearchClientError`, so it benefits from the structure and behavior defined in the base class.\n- The default message for this exception can be overridden when raising the exception, but the base class ensures that an appropriate message is always available.\n"
      ],
      "code_start_line": 14,
      "code_end_line": 16,
      "params": [],
      "have_return": false,
      "code_content": "class BadRequestError(SearchClientError):\n    def __init__(self, message: str = \"Bad request.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the BadRequestError class with a specified error message.\n\n**parameters**: The parameters of this Function.\n· message: A string that represents the error message to be associated with the BadRequestError instance. It defaults to \"Bad request.\" if no message is provided.\n\n**Code Description**: The __init__ function is a constructor for the BadRequestError class, which is likely a custom exception class used to indicate that a client has made a bad request to a server or an API. This function takes one optional parameter, message, which allows the user to specify a custom error message. If the user does not provide a message, the default value \"Bad request.\" is used. The function then calls the constructor of its superclass (presumably Exception or a subclass thereof) using super().__init__(message), which initializes the base class with the provided message. This ensures that the error message is properly set up for the exception handling mechanism in Python.\n\n**Note**: It is important to provide meaningful error messages when raising exceptions to facilitate debugging and improve the user experience. Users of this class should be aware that the message parameter is optional, but providing a specific message can help clarify the nature of the error encountered."
      ],
      "code_start_line": 15,
      "code_end_line": 16,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Bad request.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "InvalidAPIKeyError",
      "md_content": [
        "**InvalidAPIKeyError**: The function of InvalidAPIKeyError is to define a specific exception that signals an invalid API key error.\n\n**attributes**:\n· message: str - This attribute stores the error message associated with the exception.\n\n**Code Description**:  \nThe `InvalidAPIKeyError` class is a custom exception that extends from the `SearchClientError` base exception class. It is specifically designed to handle errors related to invalid API keys when interacting with a search engine client. This class inherits the functionality of the `SearchClientError` class, ensuring that it shares the same message handling mechanism, but it customizes the error message to indicate that the error is due to an invalid API key.\n\nThe constructor of the `InvalidAPIKeyError` class accepts an optional `message` parameter that defaults to \"Invalid API key.\" If no specific message is provided when the exception is raised, this default message is used. The constructor calls the parent class's (`SearchClientError`) constructor using `super().__init__(message)`, allowing the error message to be passed to the parent class, which ensures consistent exception handling and message propagation throughout the system.\n\nThis class is primarily used within the project to signal when an API key used for making requests to a search engine is invalid. It is invoked in the `search` methods of different client classes like `BingClient` and `TavilyClient`. For example, in the case of a 401 HTTP status code (Unauthorized), the `InvalidAPIKeyError` is raised to indicate that the API key is incorrect. Once raised, it can be caught and handled appropriately by the surrounding code, such as marking the search engine as unavailable, as seen in the `SearchAggregator` class.\n\nIn the project, this exception serves as a clear and specific indicator of issues related to invalid API keys, providing a more precise understanding of the problem than more generic error messages.\n\n**Note**:  \n- The `InvalidAPIKeyError` class is a subclass of `SearchClientError`, ensuring it follows the same error-handling structure as other search client exceptions.\n- This exception should be used whenever an invalid API key is encountered in the system, providing consistency in error reporting and debugging.\n- The default message can be customized by providing a different `message` value when raising the exception, but the base message is helpful for general use cases."
      ],
      "code_start_line": 19,
      "code_end_line": 21,
      "params": [],
      "have_return": false,
      "code_content": "class InvalidAPIKeyError(SearchClientError):\n    def __init__(self, message: str = \"Invalid API key.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/bing_client.py",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient/search",
        "src/criticsearch/tools/search_adapter/search_aggregator.py",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "src/criticsearch/tools/search_adapter/tavily_client.py",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the InvalidAPIKeyError class, setting a custom error message.\n\n**parameters**:\n· message: str (default value: \"Invalid API key.\") - A string message that is used to describe the error.\n\n**Code Description**:  \nThe __init__ method in this class is responsible for initializing an instance of the InvalidAPIKeyError exception. It inherits from the built-in Exception class, and its purpose is to provide a custom error message when an invalid API key is encountered. \n\nWhen an instance of InvalidAPIKeyError is created, the method first checks if a custom message has been provided by the caller. If no message is given, it defaults to \"Invalid API key.\" This message is then passed to the parent class' constructor using `super().__init__(message)`, which ensures that the Exception class is properly initialized with the message string. This allows the InvalidAPIKeyError to carry the custom or default error message when raised.\n\n**Note**:  \n- The message parameter is optional. If not provided, the default message \"Invalid API key.\" will be used.\n- The super() function is used to call the parent class constructor (Exception) to ensure that the error message is properly handled by the base Exception class."
      ],
      "code_start_line": 20,
      "code_end_line": 21,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Invalid API key.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "RatelimitException",
      "md_content": [
        "**RatelimitException**: The function of RatelimitException is to indicate that the rate limit for API requests has been exceeded.\n\n**attributes**:\n· message: str - This attribute stores the error message that is associated with the exception.\n\n**Code Description**: The `RatelimitException` class is a custom exception that inherits from the `SearchClientError` base class. It is specifically designed to handle scenarios where the rate limit for API requests has been exceeded. The constructor of `RatelimitException` accepts a message parameter, which defaults to \"Rate limit exceeded.\" This message is passed to the parent class `SearchClientError` using `super().__init__(message)`, ensuring that the exception carries a relevant error message when raised.\n\nIn the context of the project, `RatelimitException` is utilized within the `BingClient` and `TavilyClient` classes, which are responsible for interacting with their respective search APIs. When a request to these APIs results in a 429 HTTP status code, which signifies that the rate limit has been reached, the `RatelimitException` is raised. This allows the application to handle the situation appropriately, such as by implementing retry logic or notifying the user of the issue.\n\nThe `RatelimitException` serves as a clear indication to developers that the application has encountered a rate limiting issue, allowing for better error handling and user experience. It is part of a broader hierarchy of exceptions that extend from `SearchClientError`, which includes other specific error types like `UsageLimitExceededError` and `InvalidAPIKeyError`. Each of these exceptions provides a way to manage different error conditions that may arise while interacting with search client functionalities.\n\n**Note**: \n- It is essential to handle `RatelimitException` in the application logic to ensure that the user is informed of the rate limit issue and that appropriate measures are taken, such as retrying the request after a delay.\n- The default message can be overridden if a more specific message is required when raising the exception, but the base functionality ensures that a message is always available for debugging purposes."
      ],
      "code_start_line": 24,
      "code_end_line": 26,
      "params": [],
      "have_return": false,
      "code_content": "class RatelimitException(SearchClientError):\n    def __init__(self, message: str = \"Rate limit exceeded.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/bing_client.py",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient/search",
        "src/criticsearch/tools/search_adapter/tavily_client.py",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize a RatelimitException instance with a specific error message.\n\n**parameters**: The parameters of this Function.\n· message: A string that represents the error message to be displayed when the exception is raised. It defaults to \"Rate limit exceeded.\"\n\n**Code Description**: The __init__ method is a constructor for the RatelimitException class, which is likely a custom exception used to indicate that a rate limit has been exceeded in an application. When an instance of RatelimitException is created, it calls the constructor of its superclass (presumably Exception) using the super() function, passing the message parameter to it. This ensures that the base exception class is properly initialized with the provided message. If no message is specified when the exception is raised, it defaults to \"Rate limit exceeded,\" providing a clear indication of the error condition.\n\n**Note**: It is important to provide a meaningful message when raising this exception to ensure that the context of the error is clear to the developers or users handling the exception."
      ],
      "code_start_line": 25,
      "code_end_line": 26,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Rate limit exceeded.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "TimeoutException",
      "md_content": [
        "**TimeoutException**: The function of TimeoutException is to represent an error that occurs when a timeout condition is encountered in the search client operations.\n\n**attributes**: The attributes of this Class.\n· message: str - This attribute stores the error message that is associated with the exception, defaulting to \"Timeout occurred.\"\n\n**Code Description**: The `TimeoutException` class is a custom exception that extends the `SearchClientError` class. It is specifically designed to handle timeout errors that may arise during the execution of search client operations. When an instance of `TimeoutException` is created, it invokes the constructor of its parent class, `SearchClientError`, passing a default message that indicates a timeout has occurred. This allows the exception to carry a meaningful message that can be used for debugging and error handling.\n\nThe `TimeoutException` class inherits all the properties and methods of the `SearchClientError` class, which serves as a base for various search client-related exceptions. By extending `SearchClientError`, `TimeoutException` ensures that it maintains a consistent structure and behavior with other exceptions in the search client domain, such as `UsageLimitExceededError`, `BadRequestError`, and others. This design promotes a clear hierarchy of exceptions, making it easier for developers to handle specific error cases effectively.\n\nIn practical terms, when a timeout occurs during a search operation, the `TimeoutException` can be raised to signal this specific issue. Developers can catch this exception in their code to implement appropriate error handling strategies, such as retrying the operation, logging the error, or notifying users of the timeout condition.\n\n**Note**: \n- It is essential to use `TimeoutException` in scenarios where a timeout is a relevant error condition, ensuring that the error handling is precise and informative.\n- The default message can be customized when raising the exception, but it is advisable to maintain clarity regarding the nature of the timeout error for effective debugging and user communication."
      ],
      "code_start_line": 29,
      "code_end_line": 31,
      "params": [],
      "have_return": false,
      "code_content": "class TimeoutException(SearchClientError):\n    def __init__(self, message: str = \"Timeout occurred.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the TimeoutException class with a specific error message.\n\n**parameters**: The parameters of this Function.\n· message: A string that represents the error message to be displayed when the exception is raised. It defaults to \"Timeout occurred.\" if no message is provided.\n\n**Code Description**: The __init__ function is a constructor for the TimeoutException class, which is likely a custom exception used to indicate that a timeout has occurred in a process or operation. This function takes one optional parameter, `message`, which allows the user to specify a custom error message. If the user does not provide a message, the default message \"Timeout occurred.\" will be used. The constructor calls the superclass's __init__ method using `super().__init__(message)`, which ensures that the base class (likely Exception or a subclass thereof) is properly initialized with the provided message. This allows the TimeoutException to inherit all the properties and methods of the base exception class, enabling it to be used effectively in exception handling.\n\n**Note**: It is important to provide a meaningful message when raising this exception to ensure that the context of the timeout is clear to the developers or users handling the exception."
      ],
      "code_start_line": 30,
      "code_end_line": 31,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Timeout occurred.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tools/search_adapter/bing_client.py": [
    {
      "type": "ClassDef",
      "name": "BingClient",
      "md_content": [
        "**BingClient**: The function of BingClient is to serve as a client for the Bing Search API, facilitating search queries and handling responses.\n\n**attributes**: The attributes of this Class.\n· base_url: str - This attribute stores the base URL for the Bing Search API endpoint.\n· _api_key: str - This attribute holds the API key required for authenticating requests to the Bing Search API.\n· _client_name: str - This attribute defines the name of the client, which is \"BingClient\".\n\n**Code Description**: The `BingClient` class is an implementation of the `BaseSearchClient`, specifically designed to interact with the Bing Search API. It initializes with an API key, which is essential for authenticating requests to the Bing service. The class defines a method `search`, which is an asynchronous function that takes a search query as input and returns a structured response containing search results.\n\nUpon instantiation, the `BingClient` sets the `base_url` to the Bing Search API endpoint and stores the provided API key. The `search` method is decorated with a retry mechanism that allows it to attempt the request up to five times in case of a `RatelimitException`, which indicates that the rate limit for API requests has been exceeded. The retry logic employs exponential backoff combined with random jitter to manage the timing of retries effectively.\n\nIn the `search` method, an HTTP GET request is made to the Bing API using the `httpx.AsyncClient`. The request includes headers for authentication and parameters that specify the search query and additional options such as the number of results to return, safe search settings, and response filters. The method processes the API response, extracting relevant information from the JSON payload, and constructs a list of `SearchResult` objects that encapsulate the title, URL, and snippet of each search result.\n\nThe `BingClient` is utilized within the `SearchAggregator` class, which manages multiple search clients. When the `SearchAggregator` is initialized, it checks for the presence of the Bing API key in the settings. If the key is available, it creates an instance of `BingClient` and adds it to its collection of clients. This design allows the `SearchAggregator` to leverage the Bing search functionality alongside other search clients, providing a unified interface for executing search queries across different services.\n\n**Note**: It is crucial to handle exceptions such as `RatelimitException` and `InvalidAPIKeyError` when using the `BingClient` to ensure robust error management and user experience. The `search` method should be called asynchronously to comply with its design.\n\nA possible appearance of the code's return value from the `search` method could look like this:\n```json\n{\n    \"query\": \"fishing\",\n    \"results\": [\n        {\n            \"title\": \"Fishing Tips and Techniques\",\n            \"url\": \"https://www.example.com/fishing-tips\",\n            \"content\": \"Learn the best fishing tips and techniques for a successful day on the water.\"\n        },\n        {\n            \"title\": \"Top Fishing Spots\",\n            \"url\": \"https://www.example.com/top-fishing-spots\",\n            \"content\": \"Discover the top fishing spots in your area for a great fishing experience.\"\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 16,
      "code_end_line": 92,
      "params": [],
      "have_return": true,
      "code_content": "class BingClient(BaseSearchClient):\n    \"\"\"\n    Bing Search API client.\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        self.base_url = \"https://api.bing.microsoft.com/v7.0/search\"\n        self._api_key = api_key\n\n        self._client_name = \"BingClient\"\n\n    @retry(\n        stop=stop_after_attempt(5),  # 重试最多5次\n        wait=wait_exponential(multiplier=1, min=4, max=30)\n        + wait_random(min=1, max=5),  # 指数退避 + 随机抖动\n        retry=retry_if_exception_type(RatelimitException),\n    )\n    async def search(\n        self,\n        query: str,\n    ) -> SearchResponse:\n        headers = {\"Ocp-Apim-Subscription-Key\": self._api_key}\n\n        params = {\n            \"q\": query,  # 用户的搜索查询词。不能为空。\n            # 查询词可以包含 Bing 高级操作符，例如使用 site: 操作符限定结果来源于特定域名。\n            # 示例：q=\"fishing+site:fishing.contoso.com\"。\n            # 注意：即使使用了 site: 操作符，结果可能仍会包含其他站点的内容，具体取决于相关结果的数量。\n            \"count\": 2,  # 返回的搜索结果数量。默认值为 10，最大值为 50。\n            # 可以与 offset 参数结合使用来分页结果。\n            # 示例：如果每页显示 10 个搜索结果，第一页设置 count=10 和 offset=0，\n            # 第二页设置 offset=10，以此类推。分页时可能存在部分结果重叠。\n            \"safeSearch\": \"strict\",  # 过滤成人内容的设置。\n            # 可选值：\n            # - \"Off\": 返回包含成人文本和图片但不包括成人视频的内容。\n            # - \"Moderate\": 返回包含成人文本但不包括成人图片或视频的内容。\n            # - \"Strict\": 不返回任何成人文本、图片或视频内容。\n            \"responseFilter\": \"Webpages\",  # 用逗号分隔的答案类型，指定要在响应中包含的内容。\n            # 如果未指定此参数，则响应包含所有相关的数据类型。\n            # 可选值包括：\n            # - Computation, Entities, Images, News, Places, RelatedSearches,\n            #   SpellSuggestions, TimeZone, Translations, Videos, Webpages。\n            # 示例：使用 &responseFilter=-images 排除图片结果。\n            # 注意：若想获得单一答案类型，应优先使用特定的 API 端点。\n        }\n\n        async with httpx.AsyncClient(timeout=10) as client:\n            response = await client.get(self.base_url, headers=headers, params=params)\n\n        if response.status_code == 200:\n            json_response = response.json()\n\n            # 解析 Bing 的响应\n            web_pages = json_response.get(\"webPages\", {})\n            items = web_pages.get(\"value\", [])\n\n            results = []\n            for item in items:\n                results.append(\n                    SearchResult(\n                        title=item.get(\"name\", \"\"),\n                        url=item.get(\"url\", \"\"),\n                        content=item.get(\"snippet\", \"\"),\n                    )\n                )\n\n            return SearchResponse(query=query, results=results)\n        # 处理 HTTP 状态码\n        if response.status_code == 429:\n            raise RatelimitException()\n        elif response.status_code == 401:\n            raise InvalidAPIKeyError()\n        else:\n            return SearchResponse(\n                query=query,\n                error_message=f\"Unexpected status code: {response.status_code}. Response: {response.text}\",\n            )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/__init__"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/base_search_client.py/BaseSearchClient",
        "src/criticsearch/tools/search_adapter/exceptions.py/RatelimitException"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the BingClient object with necessary configuration values.\n\n**parameters**: The parameters of this Function.\n· api_key: A string representing the API key required to authenticate the client with Bing's search API.\n\n**Code Description**:  \nThe `__init__` method is a constructor function for the `BingClient` class. It is called when an instance of the class is created. This method performs the following actions:\n\n1. It assigns a fixed string URL (`\"https://api.bing.microsoft.com/v7.0/search\"`) to the `base_url` attribute, which is the endpoint for Bing's search API. This URL will be used in API requests to interact with Bing's search service.\n  \n2. It stores the provided `api_key` parameter in a private attribute `_api_key`. This API key is essential for authenticating requests to Bing's API. The key is passed to the class upon instantiation, and it is stored for later use in making authorized API calls.\n\n3. It initializes the `self._client_name` attribute with the string value `\"BingClient\"`. This can be used to identify the client or for logging purposes to track requests made by this client.\n\nThe constructor ensures that an instance of `BingClient` is ready for making authenticated API requests to the Bing search service.\n\n**Note**: \n- The `api_key` parameter must be correctly provided when creating an instance of the `BingClient`, as it is required for API authentication.\n- The `base_url` is fixed and does not change during the lifetime of the object, which makes it reusable for all API requests initiated from the same instance."
      ],
      "code_start_line": 21,
      "code_end_line": 25,
      "params": [
        "self",
        "api_key"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, api_key: str):\n        self.base_url = \"https://api.bing.microsoft.com/v7.0/search\"\n        self._api_key = api_key\n\n        self._client_name = \"BingClient\"\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [
        "**search**: The function of search is to perform an asynchronous search query using the Bing search API and return the results encapsulated in a SearchResponse object.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the user's search query. This parameter cannot be empty.\n\n**Code Description**: The `search` function is an asynchronous method that interacts with the Bing search API to retrieve search results based on a specified query. It begins by setting the necessary headers, including the API key required for authentication. The function constructs a parameters dictionary that includes the search query, the number of results to return, safe search settings, and response filters.\n\nThe function utilizes the `httpx.AsyncClient` to send an asynchronous GET request to the Bing API endpoint. The response is awaited, and upon receiving a successful status code (200), the function processes the JSON response. It extracts the relevant search results from the response, specifically focusing on the \"webPages\" section. Each result is then instantiated as a `SearchResult` object, which includes the title, URL, and content snippet of the search result.\n\nIf the response indicates a rate limit error (status code 429), the function raises a `RatelimitException`. If the API key is invalid (status code 401), it raises an `InvalidAPIKeyError`. For any other unexpected status codes, the function returns a `SearchResponse` object that includes the query and an error message detailing the unexpected status.\n\nThis function is called by the `_search_single_query` method in the `SearchAggregator` class. The `_search_single_query` method iterates through a list of available search engines and attempts to execute the `search` function for each engine. If the search is successful, it logs the result and returns it. In case of exceptions such as `RetryError`, `InvalidAPIKeyError`, or `UsageLimitExceededError`, the method handles these by marking the engine as unavailable and logging the appropriate error messages.\n\n**Note**: It is essential to ensure that the query parameter is not empty when calling this function, as it will lead to an error. Additionally, proper handling of the exceptions raised by this function is crucial for maintaining the robustness of the application, especially in scenarios where API limits or invalid keys may affect search operations.\n\n**Output Example**: A possible return value of the `search` function could be a `SearchResponse` object structured as follows:\n```python\nSearchResponse(\n    query=\"fishing\",\n    results=[\n        SearchResult(title=\"Fishing Tips\", url=\"https://example.com/fishing-tips\", content=\"Learn the best tips for fishing.\"),\n        SearchResult(title=\"Fishing Gear\", url=\"https://example.com/fishing-gear\", content=\"Find the best gear for your fishing adventures.\")\n    ],\n    error_message=None\n)\n```"
      ],
      "code_start_line": 33,
      "code_end_line": 92,
      "params": [
        "self",
        "query"
      ],
      "have_return": true,
      "code_content": "    async def search(\n        self,\n        query: str,\n    ) -> SearchResponse:\n        headers = {\"Ocp-Apim-Subscription-Key\": self._api_key}\n\n        params = {\n            \"q\": query,  # 用户的搜索查询词。不能为空。\n            # 查询词可以包含 Bing 高级操作符，例如使用 site: 操作符限定结果来源于特定域名。\n            # 示例：q=\"fishing+site:fishing.contoso.com\"。\n            # 注意：即使使用了 site: 操作符，结果可能仍会包含其他站点的内容，具体取决于相关结果的数量。\n            \"count\": 2,  # 返回的搜索结果数量。默认值为 10，最大值为 50。\n            # 可以与 offset 参数结合使用来分页结果。\n            # 示例：如果每页显示 10 个搜索结果，第一页设置 count=10 和 offset=0，\n            # 第二页设置 offset=10，以此类推。分页时可能存在部分结果重叠。\n            \"safeSearch\": \"strict\",  # 过滤成人内容的设置。\n            # 可选值：\n            # - \"Off\": 返回包含成人文本和图片但不包括成人视频的内容。\n            # - \"Moderate\": 返回包含成人文本但不包括成人图片或视频的内容。\n            # - \"Strict\": 不返回任何成人文本、图片或视频内容。\n            \"responseFilter\": \"Webpages\",  # 用逗号分隔的答案类型，指定要在响应中包含的内容。\n            # 如果未指定此参数，则响应包含所有相关的数据类型。\n            # 可选值包括：\n            # - Computation, Entities, Images, News, Places, RelatedSearches,\n            #   SpellSuggestions, TimeZone, Translations, Videos, Webpages。\n            # 示例：使用 &responseFilter=-images 排除图片结果。\n            # 注意：若想获得单一答案类型，应优先使用特定的 API 端点。\n        }\n\n        async with httpx.AsyncClient(timeout=10) as client:\n            response = await client.get(self.base_url, headers=headers, params=params)\n\n        if response.status_code == 200:\n            json_response = response.json()\n\n            # 解析 Bing 的响应\n            web_pages = json_response.get(\"webPages\", {})\n            items = web_pages.get(\"value\", [])\n\n            results = []\n            for item in items:\n                results.append(\n                    SearchResult(\n                        title=item.get(\"name\", \"\"),\n                        url=item.get(\"url\", \"\"),\n                        content=item.get(\"snippet\", \"\"),\n                    )\n                )\n\n            return SearchResponse(query=query, results=results)\n        # 处理 HTTP 状态码\n        if response.status_code == 429:\n            raise RatelimitException()\n        elif response.status_code == 401:\n            raise InvalidAPIKeyError()\n        else:\n            return SearchResponse(\n                query=query,\n                error_message=f\"Unexpected status code: {response.status_code}. Response: {response.text}\",\n            )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/exceptions.py/InvalidAPIKeyError",
        "src/criticsearch/tools/search_adapter/exceptions.py/RatelimitException",
        "src/criticsearch/tools/search_adapter/models.py/SearchResult",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponse"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/tools/search_adapter/models.py": [
    {
      "type": "ClassDef",
      "name": "SearchResult",
      "md_content": [
        "**SearchResult**: The function of SearchResult is to represent an individual search result, encapsulating details such as the title, URL, and content.\n\n**attributes**: The attributes of this Class.\n· title: A string representing the title of the search result.  \n· url: A string representing the URL of the search result.  \n· content: A string containing a brief description or snippet of the content of the search result.  \n\n**Code Description**:  \nThe `SearchResult` class is a data model that stores information about a single search result retrieved from a search engine API. It inherits from `BaseModel`, suggesting that it is likely used with Pydantic or a similar library for data validation and serialization. \n\n- `title`: This attribute holds the title of the search result. It is represented as a string and typically corresponds to the main heading or name of the webpage or resource returned in the search results.\n  \n- `url`: This attribute contains the URL string that directs to the search result. It is essential for linking directly to the content described in the `title`.\n\n- `content`: This is a string attribute that contains a brief snippet or preview of the content found at the given URL. This snippet is often a portion of text extracted from the webpage to give users a preview of what the page is about.\n\nThe `SearchResult` class is frequently used in the context of search operations, where it is populated with relevant data for each individual search result. This can be seen in the `search` methods of both `BingClient` and `DuckDuckGoClient`. In these classes, after performing a search query, a list of `SearchResult` instances is created, each representing an individual result in the response from the respective search engine API.\n\nIn the `BingClient.search` method, for example, the response from the Bing API is parsed, and each item in the list of results is used to instantiate a `SearchResult` object with the corresponding title, URL, and content snippet. The resulting list of `SearchResult` instances is then included in a `SearchResponse` object, which encapsulates the entire search result for further use.\n\nSimilarly, in the `DuckDuckGoClient.search` method, the search results are processed, and a list of `SearchResult` objects is created, each holding the title, URL, and content for the results returned by DuckDuckGo.\n\n**Note**:  \n- The attributes `title`, `url`, and `content` are expected to be provided when creating an instance of `SearchResult`. Missing or malformed values may lead to errors, especially in systems that rely on proper data validation.\n- The `SearchResult` class is often used as part of a larger response object, such as `SearchResponse`, which groups together multiple `SearchResult` instances along with metadata like the search query and potential error messages.\n- It is important that each `SearchResult` contains relevant and accurate data, as it directly reflects the results returned by external search APIs, which may vary in structure."
      ],
      "code_start_line": 9,
      "code_end_line": 12,
      "params": [],
      "have_return": false,
      "code_content": "class SearchResult(BaseModel):\n    title: str\n    url: str\n    content: str\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/bing_client.py",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient/search",
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py",
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient/search",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponse"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "SearchResponse",
      "md_content": [
        "## Class: `SearchResponse`\n\n### Overview\nThe `SearchResponse` class is a data model used to represent the results of a search query. It encapsulates the query itself, the list of search results, and any potential error messages that may have occurred during the search process. This class is commonly used to structure the response from a search API client, such as a Bing or DuckDuckGo search client.\n\n### Attributes\n- **query** (`str`): The search query string that was submitted by the user.\n- **results** (`List[SearchResult]`): A list of `SearchResult` objects representing the individual search results. If no results are found, this list will be empty.\n- **error_message** (`Optional[str]`): An optional string that contains an error message, if applicable. If an error occurred during the search, this attribute will contain a descriptive error message.\n\n### Method\n\n#### `ser_model() -> str`\nThe `ser_model` method is used to serialize the `SearchResponse` object into a human-readable string. It formats the response based on the availability of results and error messages.\n\n- **Returns**: A formatted string representing the search response, including the query, any error message, and the details of the search results.\n  \n  **Behavior**:\n  - If an `error_message` is provided, the method will include it in the formatted response.\n  - If no results are found (i.e., `results` is an empty list), the method will indicate that no results were found.\n  - If results are available, the method will format the response with the titles, URLs, and content of the search results.\n\n### Usage Example\n```python\nsearch_response = SearchResponse(query=\"Python programming\", results=[...], error_message=None)\nprint(search_response.ser_model())\n```\n\n### Notes\n- The `SearchResult` class is used to structure each individual search result, containing attributes like `title`, `url`, and `content`. These details are displayed when the `ser_model` method is called, allowing for easy inspection of the search results.\n- The `error_message` is optional, and if there are no errors, the response will display the search results or indicate that no results were found.\n\nThis class is useful in the context of search client responses, enabling structured representation and easy presentation of the query and search outcomes.",
        "**SearchResponse**: The function of SearchResponse is to encapsulate the results of a search query, including the query string, a list of search results, and any error messages encountered during the search process.\n\n**attributes**: The attributes of this Class.\n· query: A string representing the user's search query.  \n· results: A list of `SearchResult` objects that represent the individual search results returned from the query.  \n· error_message: An optional string that contains an error message if an error occurred during the search.\n\n**Code Description**: The `SearchResponse` class is a data model that serves to structure the response from search queries. It inherits from `BaseModel`, indicating that it is likely part of a data validation and serialization framework, such as Pydantic. \n\nThe primary attributes of the `SearchResponse` class include:\n- `query`: This attribute holds the search query string that was submitted by the user. It is essential for understanding the context of the results returned.\n- `results`: This attribute is a list of `SearchResult` instances, each representing an individual result from the search query. The `SearchResult` class encapsulates details such as the title, URL, and content snippet of each search result.\n- `error_message`: This optional attribute is used to convey any error messages that may arise during the search process. If no errors occur, this attribute will be `None`.\n\nThe `SearchResponse` class includes a method named `ser_model`, which is responsible for serializing the response into a formatted string. This method constructs a human-readable representation of the search results. It checks for the presence of an error message and formats the output accordingly. If there are no results, it indicates that no results were found. If results are present, it enumerates through each `SearchResult`, appending its details to the formatted string.\n\nThe `SearchResponse` class is utilized by various search client implementations, such as `BingClient` and `DuckDuckGoClient`. In these implementations, after executing a search query, the results are collected and instantiated as `SearchResult` objects. These objects are then aggregated into a `SearchResponse` object, which is returned to the caller. This structured response allows for consistent handling of search results across different search engines.\n\nFor example, in the `BingClient.search` method, after successfully retrieving search results from the Bing API, a list of `SearchResult` instances is created. This list, along with the original query and any potential error messages, is used to construct a `SearchResponse` object that is returned to the user.\n\n**Note**: \n- The `query` attribute must be a valid string representing the user's search input. \n- The `results` attribute will contain a list of `SearchResult` objects, which must be populated with valid data to ensure meaningful output.\n- The `error_message` attribute is optional and should be used to provide feedback in case of errors during the search process.\n- Proper handling of the `SearchResponse` object is crucial for applications that rely on search functionality, as it encapsulates both successful results and error states.\n\n**Output Example**: A possible return value of the `SearchResponse` object could be structured as follows:\n```python\nSearchResponse(\n    query=\"latest technology news\",\n    results=[\n        SearchResult(title=\"Tech Innovations\", url=\"https://example.com/tech-innovations\", content=\"Explore the latest in technology.\"),\n        SearchResult(title=\"Gadget Reviews\", url=\"https://example.com/gadget-reviews\", content=\"Read reviews on the newest gadgets.\")\n    ],\n    error_message=None\n)\n``` \n\nThis output illustrates a successful search response containing the original query, a list of search results, and no error messages."
      ],
      "code_start_line": 15,
      "code_end_line": 37,
      "params": [],
      "have_return": true,
      "code_content": "class SearchResponse(BaseModel):\n    query: str\n    results: List[SearchResult] = Field(default_factory=list)\n    error_message: Optional[str] = None\n\n    @model_serializer\n    def ser_model(self) -> str:\n        if self.error_message:\n            formatted_response = (\n                f\"Query: {self.query}\\nError: {self.error_message}\\n\" + \"-\" * 50\n            )\n        elif self.results == []:\n            formatted_response = (\n                f\"Query: {self.query}\\nError: No results found.\" + \"-\" * 50\n            )\n        else:\n            formatted_response = f\"Query: {self.query}\\nSearch Results:\\n\" + \"-\" * 50\n            for i, res in enumerate(self.results, 1):\n                formatted_response += (\n                    f\"\\n[{i}]:\\nTITLE: {res.title}\\nURL: {res.url}\\nCONTENT: {res.content}\\n\"\n                    + \"-\" * 50\n                )\n        return formatted_response\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/bing_client.py",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient/search",
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py",
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient/search",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponseList",
        "src/criticsearch/tools/search_adapter/search_aggregator.py",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "src/criticsearch/tools/search_adapter/tavily_client.py",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/models.py/SearchResult"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "ser_model",
      "md_content": [
        "**ser_model**: The function of ser_model is to generate a formatted string representation of the search response based on the query, error messages, and results.\n\n**parameters**: The parameters of this Function.\n· self: An instance of the class that contains the attributes query, error_message, and results.\n\n**Code Description**: The ser_model function constructs a formatted response string that summarizes the outcome of a search operation. It first checks if there is an error message present in the instance. If an error message exists, it formats the response to include the query and the error message, followed by a line of dashes for separation. If there are no results (i.e., the results list is empty), it similarly formats the response to indicate that no results were found. In the case where there are valid search results, the function constructs a response that includes the query and iterates through the results list. For each result, it appends the index, title, URL, and content of the result to the formatted response, again followed by a line of dashes for clarity. Finally, the function returns the complete formatted response string.\n\n**Note**: It is important to ensure that the attributes query, error_message, and results are properly initialized in the class before calling this function. The results should be a list of objects that contain title, url, and content attributes for the function to work correctly.\n\n**Output Example**: \n```\nQuery: \"How to use Python for data analysis?\"\nError: No results found.--------------------------------------------------\n```\nor \n```\nQuery: \"How to use Python for data analysis?\"\nSearch Results:\n--------------------------------------------------\n[1]:\nTITLE: \"Data Analysis with Python\"\nURL: \"https://example.com/data-analysis-python\"\nCONTENT: \"This article provides an overview of data analysis techniques using Python.\"\n--------------------------------------------------\n[2]:\nTITLE: \"Python for Data Science\"\nURL: \"https://example.com/python-data-science\"\nCONTENT: \"Learn how Python is used in data science and analytics.\"\n--------------------------------------------------\n```",
        "**ser_model**: The function of ser_model is to generate a formatted string representation of the search response, including the query, any errors, and the results.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**:  \nThe `ser_model` function generates a formatted string that presents the search results or error messages associated with a search query. It checks for the presence of an error message or results in the following order:\n\n1. If there is an error message, the function creates a formatted response containing the query and the error message, followed by a separator line.\n2. If the `results` list is empty, it generates a formatted response indicating that no results were found, including the query and an appropriate error message, followed by a separator line.\n3. If there are search results, the function formats the response with the query and a header indicating search results. It then iterates over the results, adding details such as the title, URL, and content of each result, followed by a separator line after each entry.\n\nThe final formatted string is returned to the caller. This function is useful for presenting a search summary in a readable format, including any potential errors or results from a search operation.\n\n**Note**:  \n- This function does not take any input parameters, as it operates on the attributes of the object it is a part of. \n- The function relies on the `query`, `error_message`, and `results` attributes of the object to generate the formatted response. It assumes that `results` is a list of objects that have `title`, `url`, and `content` attributes.\n\n**Output Example**:  \nHere’s an example of the returned string based on different scenarios:\n\n1. **Error with a message:**\n\n```\nQuery: example query\nError: Something went wrong.\n--------------------------------------------------\n```\n\n2. **No results found:**\n\n```\nQuery: example query\nError: No results found.\n--------------------------------------------------\n```\n\n3. **With search results:**\n\n```\nQuery: example query\nSearch Results:\n--------------------------------------------------\n[1]:\nTITLE: Example Title 1\nURL: http://example.com/1\nCONTENT: Example content for result 1.\n--------------------------------------------------\n[2]:\nTITLE: Example Title 2\nURL: http://example.com/2\nCONTENT: Example content for result 2.\n--------------------------------------------------\n```"
      ],
      "code_start_line": 21,
      "code_end_line": 37,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def ser_model(self) -> str:\n        if self.error_message:\n            formatted_response = (\n                f\"Query: {self.query}\\nError: {self.error_message}\\n\" + \"-\" * 50\n            )\n        elif self.results == []:\n            formatted_response = (\n                f\"Query: {self.query}\\nError: No results found.\" + \"-\" * 50\n            )\n        else:\n            formatted_response = f\"Query: {self.query}\\nSearch Results:\\n\" + \"-\" * 50\n            for i, res in enumerate(self.results, 1):\n                formatted_response += (\n                    f\"\\n[{i}]:\\nTITLE: {res.title}\\nURL: {res.url}\\nCONTENT: {res.content}\\n\"\n                    + \"-\" * 50\n                )\n        return formatted_response\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "SearchResponseList",
      "md_content": [
        "**SearchResponseList**: The function of SearchResponseList is to represent a list of search response objects and provide functionality to serialize them, ensuring that duplicate content across queries is removed.\n\n**attributes**: The attributes of this Class.\n· responses: A list of `SearchResponse` objects that represent the responses from multiple search queries.\n\n**Code Description**: \nThe `SearchResponseList` class is used to hold a collection of `SearchResponse` objects, each representing an individual search query's response. It provides a method, `ser_model()`, which serializes these responses into a formatted string while ensuring the removal of duplicate content across the search results.\n\n- The class inherits from `BaseModel`, indicating that it is part of a data model system, likely designed for serialization or structured data handling.\n- The primary attribute, `responses`, is a list that holds instances of `SearchResponse`. This attribute holds the individual responses from search queries and is initialized as an empty list by default.\n  \nThe key functionality of this class is in the `ser_model()` method. This method iterates over the `SearchResponse` objects in the `responses` list and performs several actions:\n1. It maintains a set of `global_seen_contents` to ensure that duplicate search results are removed across queries.\n2. For each `SearchResponse`, if there is an error message (i.e., `error_message` is not `None`), the method will log the error and skip processing that response.\n3. For each result in a valid `SearchResponse`, it checks if the content of the result has been seen before across all responses. If the content is unique, it is added to a list of unique results, and this result is serialized.\n4. After processing, the method updates the `SearchResponse` object to reflect only unique results and keeps track of the total number of results and the unique results count. It then generates a string representation of the serialized results.\n5. The method logs the number of duplicates removed and returns the final serialized string.\n\nThe `SearchResponseList` class is primarily used in the context of aggregating and serializing multiple search responses in a way that filters out redundant results. It plays a crucial role in handling multiple search queries, especially when working with multiple search engines or sources. \n\nFrom a functional perspective, this class is invoked in the `search` method of the `SearchAggregator` class. The `search` method performs concurrent searches for a list of queries, collects the responses, and returns them as an instance of `SearchResponseList`. This allows for streamlined processing and serialization of the search results, ensuring that the final output only includes unique search results from the queries.\n\n**Note**: \n- The `ser_model()` method only includes `SearchResponse` objects that do not contain error messages in its serialization.\n- It ensures that any duplicate content across multiple responses is filtered out based on the content of the search results, which helps in returning cleaner and more relevant data.\n- This class uses logging to provide feedback on the serialization process, including information on skipped responses due to errors and the number of duplicates removed.\n\n**Output Example**:\nAn example output of the `ser_model()` method might look like this:\n\n```\nQuery: \"Python programming\"\nSearch Results:\n--------------------------------------------------\n[1]:\nTITLE: Introduction to Python\nURL: https://example.com/python\nCONTENT: Python is a high-level programming language.\n--------------------------------------------------\n[2]:\nTITLE: Python Tutorials\nURL: https://example.com/tutorials\nCONTENT: Learn Python programming with these tutorials.\n--------------------------------------------------\nSerialization completed. Total results: 5, Unique results: 3, Duplicates removed: 2.\n``` \n\nThis output shows the results of the search query, including the serialized format of unique results, and a summary of the serialization process.",
        "### Class: `SearchResponseList`\n\n#### Overview:\nThe `SearchResponseList` class is designed to represent a collection of `SearchResponse` objects. It provides functionality to serialize the list of search responses, ensuring unique content across all search queries by removing duplicates.\n\n#### Attributes:\n- **responses** (`List[SearchResponse]`): A list containing `SearchResponse` objects. Each `SearchResponse` encapsulates the results of a single search query, including the query string, a list of search results, and any error messages encountered during the search process. The list is initialized with an empty list by default.\n\n#### Methods:\n- **ser_model()** -> `str`:\n  The `ser_model` method serializes the list of `SearchResponse` objects into a string format. This method ensures that any duplicate content across the search results is removed. The serialization process proceeds as follows:\n  1. The method iterates over each `SearchResponse` in the `responses` list.\n  2. If the `SearchResponse` contains an error message, it logs the error and skips serialization for that particular response.\n  3. For each valid `SearchResponse`, the method checks for unique search results by comparing the `content` of each result to a global set of seen contents.\n  4. The results are filtered to retain only unique entries, and the count of unique results is updated.\n  5. After filtering, the method appends the serialized content of the `SearchResponse` to the result string.\n  6. A summary log is generated, indicating the total number of results, the number of unique results, and the number of duplicates removed.\n  \n  The method returns a formatted string containing the serialized results of the search responses.\n\n#### Example Usage:\n```python\nsearch_response_list = SearchResponseList(responses=[response1, response2])\nserialized_results = search_response_list.ser_model()\n```\n\n#### Notes:\n- The `responses` attribute must be populated with instances of the `SearchResponse` class.\n- The method `ser_model` performs de-duplication across all responses and presents the results in a human-readable format.\n- If any `SearchResponse` contains an error message, it will be skipped during serialization, ensuring that the final output only contains valid data.",
        "**SearchResponseList**: The `SearchResponseList` class is designed to encapsulate and manage a list of search responses, specifically handling the serialization and filtering of search results. It provides functionality to ensure unique content across multiple search responses while also allowing for the removal of irrelevant results, such as Wikipedia links.\n\n### Detailed Analysis\n\nThe `SearchResponseList` class inherits from `BaseModel`, indicating that it is part of a data model system that provides validation and serialization features, likely provided by frameworks such as Pydantic.\n\n#### Attributes:\n- **responses** (`List[SearchResponse]`): This attribute holds a list of `SearchResponse` objects. Each `SearchResponse` corresponds to a search query and contains the results or errors associated with that query.\n\n#### Methods:\n\n1. **_is_wiki_url** (`url: str`) -> `bool`:  \n   This private method checks if a given URL belongs to a Wikipedia-related domain. It scans the URL for known Wikipedia-related domain names (e.g., \"wikipedia.org\", \"wiki.\", \"fandom.com\", etc.) and returns `True` if the URL matches any of these domains, and `False` otherwise. This method is used later in the class to filter out Wikipedia-related results from the search responses.\n\n2. **ser_model** () -> `str`:  \n   This method is responsible for serializing the search responses into a structured string format. It processes the list of `SearchResponse` objects, ensuring that only unique search results are included, while also filtering out Wikipedia-related URLs.  \n   \n   The serialization process involves the following key steps:\n   - **Global Deduplication**: The method maintains a `global_seen_contents` set to track and eliminate duplicate search results across all responses. If a result has already been encountered (based on its content), it is skipped.\n   - **Wiki Filtering**: The method calls `_is_wiki_url` to identify and exclude any search results that link to Wikipedia-related domains, effectively filtering out irrelevant results.\n   - **Error Handling**: If any search response contains an error message (i.e., `response.error_message` is not `None`), it is logged and skipped during serialization.\n   - **Result Counting**: The method also keeps track of the total number of results, the number of unique results, and the number of filtered Wikipedia results.\n   - **Final Serialization**: Once the filtering and deduplication processes are complete, the method constructs a serialized string by calling `model_dump()` on each `SearchResponse` object. This method is assumed to generate a string representation of the `SearchResponse` in a standardized format.\n   \n   At the end of the process, the method logs important statistics, including the total number of results, the number of unique results, the number of duplicates removed, and the number of Wikipedia links filtered. This information is logged using the `printer.log` method, which is likely a utility for logging output messages.\n\n   The final result of the method is a string representation of the serialized search responses.\n\n#### Usage Context:\n\nThe `SearchResponseList` class is primarily used in the `search` method of the `SearchAggregator` class. This method performs concurrent searches using multiple search engines and aggregates the results into a `SearchResponseList` object. Once all search tasks are completed, the method calls `SearchResponseList.ser_model()` to serialize the results into a structured string.\n\nThe serialized string is returned as the output of the `search` method. This output provides a clear, formatted representation of the search results, which can be used for further processing or presentation.\n\n### Key Notes:\n- The `responses` attribute must be a list of valid `SearchResponse` objects. Each `SearchResponse` should contain a properly formatted search query, a list of `SearchResult` objects, and optionally, an error message.\n- The `ser_model` method performs global deduplication and wiki filtering, which is crucial for ensuring that the final output contains only unique and relevant search results.\n- The `ser_model` method logs important statistics about the filtering process, which helps in tracking the effectiveness of deduplication and filtering steps.\n- The class depends on the `SearchResponse` class for individual search query responses and the `SearchResult` class for individual search results. Proper handling of these classes is essential for the correct functioning of `SearchResponseList`.\n\n### Example Output:\nThe `ser_model` method might return a serialized string like this:\n```json\n{\n  \"responses\": [\n    {\n      \"query\": \"latest technology news\",\n      \"results\": [\n        {\n          \"title\": \"Tech Innovations\",\n          \"url\": \"https://example.com/tech-innovations\",\n          \"content\": \"Explore the latest in technology.\"\n        },\n        {\n          \"title\": \"Gadget Reviews\",\n          \"url\": \"https://example.com/gadget-reviews\",\n          \"content\": \"Read reviews on the newest gadgets.\"\n        }\n      ],\n      \"error_message\": null\n    }\n  ]\n}\n```\nThis serialized output represents a list of search responses, each containing the query, the list of unique results, and any associated error messages.",
        "**SearchResponseList**: The function of SearchResponseList is to manage and serialize a list of search responses, ensuring unique content across queries while filtering out unwanted results.\n\n**attributes**: The attributes of this Class.\n· responses: A list of `SearchResponse` objects that encapsulate the results of individual search queries.\n\n**Code Description**: The `SearchResponseList` class inherits from `BaseModel` and is designed to handle multiple `SearchResponse` objects, which represent the results of search queries. The primary attribute of this class is `responses`, which is a list that stores instances of `SearchResponse`. Each `SearchResponse` contains details about a specific search query, including the query string, a list of search results, and any error messages encountered during the search process.\n\nThe class includes a private method `_is_wiki_url`, which checks if a given URL belongs to a Wikipedia or related domain. This method is utilized within the `ser_model` method to filter out search results that are deemed irrelevant, specifically those that link to Wikipedia or similar sites. The filtering is based on a predefined list of wiki-related domains.\n\nThe `ser_model` method is responsible for serializing the list of `SearchResponse` objects into a formatted string representation. It ensures that the content across different queries is unique by maintaining a global set of seen contents. The method iterates through each `SearchResponse` in the `responses` list, checking for errors and filtering out results that are identified as wiki URLs. It counts the total number of results, the number of unique results, and the number of filtered wiki pages, logging this information for user feedback.\n\nThe `SearchResponseList` class is called by the `search` method in the `SearchAggregator` class, which performs multiple search queries concurrently. After executing the search queries, the responses are collected and passed to the `SearchResponseList` for serialization. This integration allows for efficient handling of search results, providing a structured output that can be utilized for further processing or presentation.\n\n**Note**: It is essential to ensure that the `responses` attribute contains valid `SearchResponse` objects. The `ser_model` method relies on the proper structure of these objects to generate meaningful output. Additionally, the filtering mechanism is crucial for excluding unwanted results, which enhances the relevance of the search output.\n\n**Output Example**: A possible return value from the `ser_model` method could be structured as follows:\n```json\n{\n  \"results\": [\n    {\n      \"query\": \"latest technology news\",\n      \"results\": [\n        {\n          \"title\": \"Tech Innovations\",\n          \"url\": \"https://example.com/tech-innovations\",\n          \"content\": \"Explore the latest in technology.\"\n        },\n        {\n          \"title\": \"Gadget Reviews\",\n          \"url\": \"https://example.com/gadget-reviews\",\n          \"content\": \"Read reviews on the newest gadgets.\"\n        }\n      ],\n      \"error_message\": null\n    }\n  ]\n}\n``` \nThis output illustrates a successful serialization of search responses, containing the original query, a list of relevant search results, and no error messages."
      ],
      "code_start_line": 40,
      "code_end_line": 109,
      "params": [],
      "have_return": true,
      "code_content": "class SearchResponseList(BaseModel):\n    responses: List[SearchResponse] = Field(default_factory=list)\n\n    def _is_wiki_url(self, url: str) -> bool:\n        \"\"\"检查URL是否为维基百科或wiki相关网站\"\"\"\n        wiki_domains = [\n            \"wikipedia.org\",\n            \"wiki.\",\n            \"fandom.com\",\n            \"wikimedia.org\",\n            \"wiktionary.org\",\n            \"wikidata.org\",\n            \"wikiwand\",\n            \"wikihow.com\",\n        ]\n        return any(domain in url.lower() for domain in wiki_domains)\n    \n\n    @model_serializer\n    def ser_model(self) -> str:\n        \"\"\"\n        Serialize the list of SearchResponse objects into a dictionary,\n        ensuring unique content across queries.\n\n        Returns:\n            Dict[str, str]: A dictionary where the key is the query,\n                            and the value is a formatted string representation of the search response.\n        \"\"\"\n        global_seen_contents = set()  # 全局去重逻辑\n        total_results = 0\n        unique_results_count = 0\n        filtered_wiki_count = 0\n        result_str = \"\"\n\n        for response in self.responses:\n            if response.error_message:\n                printer.log(\n                    f\"Skipping serialize query '{response.query}' due to error: {response.error_message}\"\n                )\n                continue  # 跳过有 error_message 的响应\n\n            unique_results = []\n            for res in response.results:\n                total_results += 1\n\n                # 过滤wiki页面\n                if self._is_wiki_url(res.url):\n                    filtered_wiki_count += 1\n                    continue\n\n                if res.content not in global_seen_contents:\n                    global_seen_contents.add(res.content)\n                    unique_results.append(res)\n\n            # 将去重后的结果更新到当前 response\n            response.results = unique_results\n            unique_results_count += len(unique_results)\n            result_str += response.model_dump()  # type: ignore\n\n        # 打印提示信息\n        duplicates_removed = total_results - unique_results_count\n        printer.log(\n            f\"Serialization completed. Total results: {total_results}, \"\n            f\"Unique results: {unique_results_count}, \"\n            f\"Duplicates removed: {duplicates_removed},\"\n            f\"Wiki pages filtered: {filtered_wiki_count}.\",\n            style=\"bold green\",\n        )\n\n        return result_str\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/models.py/SearchResponse"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_is_wiki_url",
      "md_content": [
        "**_is_wiki_url**: The function of _is_wiki_url is to check if a given URL belongs to Wikipedia or related wiki sites.\n\n**parameters**: The parameters of this Function.\n· url: str - The URL string that needs to be checked for its affiliation with wiki-related domains.\n\n**Code Description**: The _is_wiki_url method is designed to determine whether a specified URL is associated with Wikipedia or other wiki-related websites. It achieves this by checking the provided URL against a predefined list of wiki domains. The method converts the URL to lowercase to ensure that the comparison is case-insensitive. \n\nThe method utilizes a list called `wiki_domains`, which contains the following entries:\n- \"wikipedia.org\"\n- \"wiki.\"\n- \"fandom.com\"\n- \"wikimedia.org\"\n- \"wiktionary.org\"\n\nThe core functionality of the method is implemented using a generator expression within the `any()` function. This expression iterates over each domain in the `wiki_domains` list and checks if any of these domains are present in the lowercase version of the provided URL. If at least one domain is found, the method returns `True`, indicating that the URL is indeed a wiki-related URL. If none of the domains match, it returns `False`.\n\nThis method is called within the `ser_model` function of the `SearchResponseList` class. In that context, it is used to filter out search results that link to wiki pages before performing serialization. Specifically, when processing each search response, the `ser_model` function checks each result's URL using the `_is_wiki_url` method. If the URL is identified as a wiki URL, that result is skipped, ensuring that only non-wiki content is included in the final serialized output. This filtering is crucial for maintaining the relevance and uniqueness of the search results being processed.\n\n**Note**: It is important to ensure that the input URL is a valid string format. The method does not handle exceptions or errors related to malformed URLs; it simply checks for the presence of specified domains.\n\n**Output Example**: \nFor a URL input such as \"https://en.wikipedia.org/wiki/Python_(programming_language)\", the method would return `True`, while for \"https://www.example.com\", it would return `False`.",
        "**_is_wiki_url**: The function of _is_wiki_url is to check if a given URL belongs to Wikipedia or related wiki sites.\n\n**parameters**: The parameters of this Function.\n· url: str - The URL string that needs to be checked for wiki-related domains.\n\n**Code Description**: The `_is_wiki_url` function is a method designed to determine whether a specified URL is associated with Wikipedia or other wiki-related websites. It achieves this by maintaining a predefined list of wiki domains, which includes popular sites such as \"wikipedia.org\", \"fandom.com\", and \"wikihow.com\". \n\nThe function operates as follows:\n1. It accepts a single parameter, `url`, which is expected to be a string representing the URL to be evaluated.\n2. The method converts the URL to lowercase to ensure that the comparison is case-insensitive.\n3. It then checks if any of the domains in the `wiki_domains` list are present in the provided URL using a generator expression combined with the `any()` function. This approach efficiently evaluates each domain against the URL.\n4. If any domain from the list is found within the URL, the function returns `True`, indicating that the URL is indeed a wiki-related link. If none of the domains match, it returns `False`.\n\nThe `_is_wiki_url` function is called within the `ser_model` method of the `SearchResponseList` class. In this context, it plays a crucial role in filtering search results. Specifically, during the serialization process of search responses, the `ser_model` method utilizes `_is_wiki_url` to identify and exclude any results that are links to wiki pages. This filtering ensures that the final serialized output contains only unique and relevant content, thereby enhancing the quality of the search results presented to the user.\n\n**Note**: The function is designed to be straightforward and efficient, focusing solely on the presence of specific domains within the URL. It is essential for maintaining the integrity of the search results by preventing wiki-related content from being included in the final output.\n\n**Output Example**: The function will return a boolean value, such as `True` for a URL like \"https://en.wikipedia.org/wiki/Example\" and `False` for a URL like \"https://example.com\"."
      ],
      "code_start_line": 43,
      "code_end_line": 55,
      "params": [
        "self",
        "url"
      ],
      "have_return": true,
      "code_content": "    def _is_wiki_url(self, url: str) -> bool:\n        \"\"\"检查URL是否为维基百科或wiki相关网站\"\"\"\n        wiki_domains = [\n            \"wikipedia.org\",\n            \"wiki.\",\n            \"fandom.com\",\n            \"wikimedia.org\",\n            \"wiktionary.org\",\n            \"wikidata.org\",\n            \"wikiwand\",\n            \"wikihow.com\",\n        ]\n        return any(domain in url.lower() for domain in wiki_domains)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/models.py/SearchResponseList/ser_model"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "ser_model",
      "md_content": [
        "**ser_model**: The function of ser_model is to serialize a list of SearchResponse objects into a string, ensuring uniqueness in the content across different queries.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**:  \nThe `ser_model` method is responsible for serializing the list of `SearchResponse` objects contained within the instance. It processes each `SearchResponse` object to ensure that the search results are unique by comparing their content against a global set of previously seen content.\n\n1. **Global Uniqueness Tracking**:  \n   A set named `global_seen_contents` is used to store the content of the results encountered so far, ensuring that each result is unique across all queries processed by this method.\n\n2. **Response Processing**:  \n   The method loops over each `SearchResponse` object in `self.responses`. For each `response`, if it contains an error message, it is skipped, and the serialization continues with the next response.\n\n3. **Result Deduplication**:  \n   For each `response`, the method iterates through its `results` and checks whether the `content` of each result has already been encountered (using the `global_seen_contents` set). If a result’s content is unique (i.e., not found in the set), it is added to the list `unique_results` and the content is added to the set to prevent future duplicates. The total number of results and unique results are tracked during this process.\n\n4. **Updating the Response**:  \n   After deduplication, the `results` attribute of the `response` is updated with the `unique_results`. The count of unique results is accumulated in `unique_results_count`.\n\n5. **Serialization**:  \n   The `model_dump()` method is called on each `response` to generate its serialized string representation, which is appended to `result_str`. This `result_str` will contain the serialized data of all responses, with duplicates removed.\n\n6. **Logging**:  \n   After processing all responses, the method logs a summary message, indicating the total number of results processed, the number of unique results, and the number of duplicates removed.\n\n7. **Return Value**:  \n   Finally, the method returns the `result_str`, which contains the serialized data of the unique results.\n\n**Note**: \n- The method ensures that only unique search results are included in the serialized output, removing any duplicates based on content.\n- The method relies on the `model_dump()` method of the `SearchResponse` object to generate its string representation, which may vary depending on the implementation of that method.\n- If any `SearchResponse` contains an error message, it will be skipped entirely, and no results from that response will be included in the final serialized output.\n\n**Output Example**:  \nAssuming `self.responses` contains two `SearchResponse` objects with some duplicate results, the returned `result_str` might look like this:\n\n```\n\"SearchResponse(query='query1', results=[{'content': 'unique content 1'}, {'content': 'unique content 2'}])SearchResponse(query='query2', results=[{'content': 'unique content 3'}, {'content': 'unique content 1'}])\"\n```\n\nIn this example, 'unique content 1' is only included once, even though it appeared in multiple responses.",
        "**ser_model**: The function of ser_model is to serialize a list of SearchResponse objects into a formatted string, ensuring unique content across different queries.\n\n**parameters**: The parameters of this Function.\n- None\n\n**Code Description**:  \nThe `ser_model` method is designed to process a list of `SearchResponse` objects and serialize the relevant data into a string representation. The method ensures that content within the search responses is unique across all queries by removing duplicate results based on their content.\n\n1. **Global Deduplication Logic**:  \n   The function initializes a set called `global_seen_contents` to store content that has already been encountered. This set helps track which results have been seen across all queries, ensuring that duplicates are eliminated.\n\n2. **Processing Search Responses**:  \n   The method then iterates over each `response` in the `self.responses` list. If a response contains an `error_message`, it is skipped, and a log message is generated using the `printer.log` method to notify that the response was ignored due to an error. This ensures that only valid responses are processed.\n\n3. **Result Deduplication**:  \n   For each valid response, the method further iterates over the `results` in the response. It checks whether the `content` of each result has already been encountered by comparing it against the `global_seen_contents` set. If a result's content is unique, it is added to the list `unique_results`, and its content is stored in the set to prevent future duplicates. The method keeps track of both the total number of results and the number of unique results.\n\n4. **Updating the Response**:  \n   Once duplicates are removed, the `results` of the current `response` are updated to include only the unique results. The count of unique results is then updated.\n\n5. **Result Serialization**:  \n   After processing all responses, the `model_dump` method is called on each `response` to obtain a serialized string representation of the response, which is concatenated into the `result_str` variable.\n\n6. **Logging the Summary**:  \n   After completing the serialization process, a log message is printed using the `printer.log` method to summarize the operation. The message includes the total number of results, the number of unique results, and the number of duplicates removed.\n\n7. **Return Value**:  \n   The method returns the concatenated string (`result_str`) representing all serialized and deduplicated search responses.\n\n**Note**:  \n- The `model_dump` method is assumed to serialize each response object into a string format, and the exact format depends on its implementation.\n- The `printer.log` function is used for logging, which formats the messages in a styled manner to enhance readability in the console output.\n- It is important to ensure that all responses in `self.responses` are well-formed and contain the expected attributes, especially `error_message` and `results`. This method does not handle any unexpected data formats or missing attributes.\n\n**Output Example**:  \nAssuming a scenario where there are two search responses with some duplicate content, the output might look like:\n\n```\nSerialization completed. Total results: 10, Unique results: 7, Duplicates removed: 3.\n<Serialized String of the Responses>\n```\n\nThe string returned would contain the serialized representations of the unique search responses, with any duplicates removed based on the content comparison.",
        "**ser_model**: The function of ser_model is to serialize a list of SearchResponse objects into a formatted string representation, ensuring unique content across queries.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: \nThe `ser_model` function is a method designed to process and serialize a collection of `SearchResponse` objects, ensuring that the serialized output contains only unique content across all queries. The function operates in the following steps:\n\n1. **Initialization**: \n   - A `global_seen_contents` set is created to track the content that has already been processed, ensuring uniqueness. This set will hold the content of search results, preventing duplicates.\n   - Counters such as `total_results`, `unique_results_count`, and `filtered_wiki_count` are initialized to track the number of total search results, unique results, and results filtered out due to being wiki-related content.\n\n2. **Processing Each Response**: \n   - The method iterates over the `responses` list, which contains the search results of a query. For each response, it checks if an error message is present. If an error exists, it logs the issue using the `printer.log` method and skips further processing for that particular response.\n   - If no error is present, it proceeds to process each result in the `response.results` list.\n\n3. **Filtering and Deduplication**: \n   - For each search result (`res`), the method checks if the URL corresponds to a wiki-related page using the `_is_wiki_url` method. If the result is a wiki URL, it is skipped, and the `filtered_wiki_count` is incremented.\n   - For non-wiki results, the function checks if the result's content has been encountered previously (via the `global_seen_contents` set). If the content is unique (not found in the set), it is added to the set and appended to the list of `unique_results`.\n\n4. **Serialization**: \n   - After processing all results for a given response, the filtered and unique results are stored back in the response's `results` list.\n   - The method then increments the `unique_results_count` by the number of unique results found and appends the serialized output of the current response (using `response.model_dump()`) to the `result_str`.\n\n5. **Logging and Summary**: \n   - After all responses have been processed, a summary of the serialization process is logged using `printer.log`. This summary includes the total number of results processed, the number of unique results, the number of duplicates removed, and the number of wiki pages filtered.\n\n6. **Return Value**: \n   - The method returns a concatenated string (`result_str`), which contains the serialized representation of all the processed and filtered search results.\n\n**Note**: \n- It is important to note that the method relies on the `_is_wiki_url` method for filtering out wiki-related content. This filtering ensures that only non-wiki content is included in the final serialized output.\n- The `printer.log` function is used to provide logs regarding the serialization process, including skipped queries due to errors and a summary of the results after serialization.\n- The method does not perform any error handling for potential issues in the `response.model_dump()` function, assuming that the response objects are properly structured and serializable.\n\n**Output Example**:\nAn example of the serialized string returned by the method could look like this:\n\n```\n\"Response 1 serialized data... Response 2 serialized data... Response 3 serialized data...\"\n```\n\nThis string will include the serialized data of each `SearchResponse` object, formatted according to the `model_dump()` method of the respective `SearchResponse`."
      ],
      "code_start_line": 59,
      "code_end_line": 109,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def ser_model(self) -> str:\n        \"\"\"\n        Serialize the list of SearchResponse objects into a dictionary,\n        ensuring unique content across queries.\n\n        Returns:\n            Dict[str, str]: A dictionary where the key is the query,\n                            and the value is a formatted string representation of the search response.\n        \"\"\"\n        global_seen_contents = set()  # 全局去重逻辑\n        total_results = 0\n        unique_results_count = 0\n        filtered_wiki_count = 0\n        result_str = \"\"\n\n        for response in self.responses:\n            if response.error_message:\n                printer.log(\n                    f\"Skipping serialize query '{response.query}' due to error: {response.error_message}\"\n                )\n                continue  # 跳过有 error_message 的响应\n\n            unique_results = []\n            for res in response.results:\n                total_results += 1\n\n                # 过滤wiki页面\n                if self._is_wiki_url(res.url):\n                    filtered_wiki_count += 1\n                    continue\n\n                if res.content not in global_seen_contents:\n                    global_seen_contents.add(res.content)\n                    unique_results.append(res)\n\n            # 将去重后的结果更新到当前 response\n            response.results = unique_results\n            unique_results_count += len(unique_results)\n            result_str += response.model_dump()  # type: ignore\n\n        # 打印提示信息\n        duplicates_removed = total_results - unique_results_count\n        printer.log(\n            f\"Serialization completed. Total results: {total_results}, \"\n            f\"Unique results: {unique_results_count}, \"\n            f\"Duplicates removed: {duplicates_removed},\"\n            f\"Wiki pages filtered: {filtered_wiki_count}.\",\n            style=\"bold green\",\n        )\n\n        return result_str\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponseList/_is_wiki_url"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "src/criticsearch/tools/search_adapter/search_aggregator.py": [
    {
      "type": "ClassDef",
      "name": "SearchAggregator",
      "md_content": [
        "**SearchAggregator**: The function of SearchAggregator is to aggregate and execute search queries across multiple search engines while handling various error conditions and API limitations.\n\n**attributes**:\n- clients: A dictionary holding instances of different search engine clients, such as TavilyClient and BingClient, which are used to execute search queries.\n- available_clients: A set that tracks the search engines that are currently available for use, excluding any that are marked as unavailable due to errors or limitations.\n\n**Code Description**:  \nThe `SearchAggregator` class is responsible for managing multiple search engines and executing queries across them. Upon initialization, it checks for API keys for both Tavily and Bing search engines from the settings. If valid keys are found, it initializes corresponding clients for these engines and stores them in the `clients` attribute. The `available_clients` set is then populated with the keys (names) of the initialized clients. \n\nThe class includes the following key methods:\n\n- `mark_engine_unavailable(engine: str)`: This method marks a specified search engine as unavailable by removing it from the `available_clients` set. This is used when an error or limitation (such as an invalid API key, retry limit reached, or usage limit exceeded) occurs with a particular search engine.\n  \n- `_search_single_query(query: str, engines: List[str])`: This asynchronous method attempts to search a given query across the provided list of search engines. For each engine, it checks if the engine is available and then tries to perform the search. If the search is successful, the result is returned. If an exception occurs, the engine is marked as unavailable, and the method moves on to the next engine in the list. Various exceptions are handled, including `RetryError`, `InvalidAPIKeyError`, and `UsageLimitExceededError`.\n\n- `search(query: List[str])`: This is the primary method that allows the user to perform searches using a list of queries. It first checks if there are any available search engines. If none are available, it raises an exception. It then creates asynchronous tasks for each query and executes them concurrently. The method returns the aggregated search results as a list of responses.\n\nThe `SearchAggregator` class is used in other components of the project to facilitate search functionality. For instance, in `BaseAgent`, it is instantiated and its search method is called to perform searches with specific queries. This enables the broader application to conduct searches in a robust and fault-tolerant manner by leveraging multiple search engines. Additionally, in the main entry point (`src/criticsearch/tools/search_adapter/__main__.py/main`), an asynchronous search is triggered, and the results are printed.\n\n**Note**:  \n- Ensure that valid API keys for the search engines (Tavily and Bing) are provided in the settings, as the functionality depends on these keys to initialize the respective clients.\n- The `search` method handles multiple queries simultaneously, making it efficient in handling concurrent searches.\n- If all search engines are marked unavailable, the method returns a `SearchResponse` indicating the failure to execute the search, along with an appropriate error message.\n  \n**Output Example**:\nA possible return value of the `search` method might look like this:\n\n```json\n{\n  \"responses\": [\n    {\n      \"query\": \"Who is Leo Messi?\",\n      \"results\": [\n        {\n          \"title\": \"Lionel Messi - Wikipedia\",\n          \"url\": \"https://en.wikipedia.org/wiki/Lionel_Messi\",\n          \"snippet\": \"Lionel Andrés Messi is an Argentine professional footballer widely regarded as one of the greatest players of all time.\"\n        }\n      ]\n    }\n  ]\n}\n```",
        "**SearchAggregator**: The function of SearchAggregator is to manage and execute search queries across multiple search engines.\n\n**attributes**: The attributes of this Class.\n· clients: A dictionary that holds instances of search engine clients, specifically TavilyClient and BingClient, keyed by their respective engine names.  \n· available_clients: A set that contains the names of currently available search engines that can be used for querying.\n\n**Code Description**: The SearchAggregator class is designed to facilitate searching through multiple search engines by managing their respective clients. Upon initialization, the class checks for the presence of API keys for Tavily and Bing. If the keys are available, it creates instances of TavilyClient and BingClient and stores them in the clients dictionary. The available_clients set is initialized to keep track of which search engines are currently available for use.\n\nThe class provides two main functionalities: marking a search engine as unavailable and performing searches. The `mark_engine_unavailable` method allows the user to remove a search engine from the available_clients set if it encounters issues during a search operation. This is particularly useful for handling errors such as invalid API keys or usage limits being exceeded.\n\nThe `_search_single_query` method is an asynchronous function that attempts to execute a search query against the specified engines. It iterates through the provided list of engines, checking if they are available. If an engine is available, it calls its search method and logs the result. If an error occurs, it logs the error and marks the engine as unavailable. If all specified engines fail, it returns a SearchResponse indicating that no available search engines could handle the query.\n\nThe `search` method allows users to perform searches using a list of queries. It gathers the available engines and creates tasks for concurrent execution of searches for each query. The results are awaited and returned as a SearchResponseList, which encapsulates the responses from the search engines.\n\nThe SearchAggregator class is utilized in the BaseAgent class, where an instance of SearchAggregator is created to manage search queries. This integration allows the BaseAgent to leverage the capabilities of multiple search engines, enhancing its ability to gather information. Additionally, the SearchAggregator is instantiated in the process_single_task function, where it is used to execute search queries based on user tasks.\n\n**Note**: When using the SearchAggregator, ensure that the necessary API keys for the search engines are configured correctly in the settings. Additionally, be aware that if all search engines become unavailable, the search operation will fail, and an appropriate error message will be returned.\n\n**Output Example**: A possible appearance of the code's return value after executing a search query could look like this:\n```json\n{\n  \"responses\": [\n    {\n      \"query\": \"Who is Leo Messi?\",\n      \"error_message\": \"Search failed: No available search engines for this query.\"\n    }\n  ]\n}\n```",
        "**SearchAggregator**: The function of SearchAggregator is to manage and aggregate search queries across multiple search engines, handling the initialization of clients and executing searches asynchronously.\n\n**attributes**: The attributes of this Class.\n· clients: A dictionary that holds instances of search engine clients (TavilyClient and BingClient) keyed by their respective engine names.  \n· available_clients: A set that maintains the names of currently available search engines that can be used for querying.\n\n**Code Description**: The SearchAggregator class is designed to facilitate the execution of search queries across multiple search engines by managing the initialization of client instances and handling search requests. Upon instantiation, the constructor initializes the clients for Tavily and Bing if their respective API keys are available in the settings. This ensures that only valid clients are available for use.\n\nThe class provides a method `mark_engine_unavailable`, which allows marking a specific search engine as unavailable. This is particularly useful in scenarios where a search engine fails to respond or encounters an error during a search attempt. The method checks if the engine is in the set of available clients and removes it if it is.\n\nThe core functionality of the class is encapsulated in the `_search_single_query` method, which performs an asynchronous search for a given query using the specified search engines. It iterates through the list of engines, checking their availability before attempting to execute a search. If a search engine fails due to various reasons (such as a RetryError, InvalidAPIKeyError, or UsageLimitExceededError), it logs the error and marks the engine as unavailable. If all specified engines are unavailable, it returns a SearchResponse indicating the failure.\n\nThe `search` method allows for performing searches using a list of queries. It first checks the availability of search engines and raises a ValueError if none are available. It then creates asynchronous tasks for each query and gathers the responses. The results are returned as a SearchResponseList, which encapsulates the responses from the search engines.\n\nThe SearchAggregator class is utilized by the BaseAgent class, which serves as a foundational component for intelligent agents in the project. The BaseAgent initializes an instance of SearchAggregator, allowing it to perform searches as part of its operations. This integration highlights the importance of the SearchAggregator in enabling the agent to gather information from multiple sources effectively.\n\n**Note**: When using the SearchAggregator, it is essential to ensure that the API keys for the search engines are correctly configured in the settings. Additionally, the handling of unavailable engines is crucial for maintaining the reliability of search operations.\n\n**Output Example**: A possible appearance of the code's return value when executing a search might look like this:\n```json\n{\n  \"responses\": [\n    {\n      \"query\": \"Who is Leo Messi?\",\n      \"error_message\": \"Search failed: No available search engines for this query.\"\n    }\n  ]\n}\n```"
      ],
      "code_start_line": 14,
      "code_end_line": 97,
      "params": [],
      "have_return": true,
      "code_content": "class SearchAggregator:\n    def __init__(self):\n        self.clients: Dict[str, TavilyClient | BingClient] = {}\n\n        # 如果 Tavily 的 API key 存在，初始化客户端\n        tavily_api_key = settings.tavily.api_key\n        if tavily_api_key:\n            self.clients[\"tavily\"] = TavilyClient(tavily_api_key)\n\n        # 如果 Bing 的 API key 存在，初始化客户端\n        bing_api_key = settings.search_engine.bing.api_key\n        if bing_api_key:\n            self.clients[\"bing\"] = BingClient(bing_api_key)\n\n        self.available_clients = set(self.clients.keys())\n\n    def mark_engine_unavailable(self, engine: str):\n        \"\"\"\n        Mark a specific search engine as unavailable.\n\n        Args:\n            engine (str): The name of the engine to mark as unavailable.\n        \"\"\"\n        if engine in self.available_clients:\n            self.available_clients.remove(engine)\n\n    async def _search_single_query(\n        self, query: str, engines: List[str]\n    ) -> SearchResponse:\n        for engine in engines:\n            if engine in self.available_clients:\n                try:\n                    # Call the asynchronous search method for the specified engine\n                    result = await self.clients[engine].search(query)\n                    printer.log(f\"{result.model_dump()}\")\n                    return result\n                except RetryError:\n                    printer.log(\n                        f\"Engine '{engine}' for query: {query} failed after multiple retries. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except InvalidAPIKeyError:\n                    printer.log(\n                        f\"Engine '{engine}' for query: {query} failed because of wrong api key. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except UsageLimitExceededError:\n                    printer.log(\n                        f\"Engine '{engine}' for query: {query} failed because of usage limit exceeded. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except Exception:\n                    printer.print_exception(\n                        f\"Engine '{engine}' encountered error. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n\n        printer.log(\"All specified search engines are unavailable.\", style=\"bold red\")\n        return SearchResponse(\n            query=query,\n            error_message=\"Search failed: No available search engines for this query.\",\n        )\n\n    async def search(self, query: List[str]) -> str:\n        \"\"\"\n        Performs a search using the provided query.\n        Supports various search techniques using special syntax.\n\n        Args:\n            query (List[str]): A list of search queries.\n        \"\"\"\n        # Get the list of currently available search engines\n        engines = list(self.available_clients)\n        if not engines:\n            raise ValueError(\"No available engines to perform the search.\")\n\n        # Create tasks for concurrent search\n        tasks = [self._search_single_query(q, engines) for q in query]\n\n        # Execute all tasks and gather the responses\n        responses = await gather(*tasks)\n\n        # Return the search responses as a dictionary\n        return SearchResponseList(responses=responses).model_dump()  # type: ignore\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py",
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/__init__",
        "src/criticsearch/base_agent.py",
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/__init__",
        "src/criticsearch/main.py",
        "src/criticsearch/tools/__init__.py",
        "src/criticsearch/tools/search_adapter/__init__.py",
        "src/criticsearch/tools/search_adapter/__main__.py",
        "src/criticsearch/tools/search_adapter/__main__.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the SearchAggregator class, setting up the necessary search clients based on available API keys.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The __init__ method is the constructor for the SearchAggregator class. It initializes an empty dictionary named `clients`, which is intended to hold instances of search clients that can be used for executing search queries. The method first checks for the presence of an API key for the Tavily search service by accessing `settings.tavily.api_key`. If a valid API key is found, an instance of the TavilyClient is created and added to the `clients` dictionary with the key \"tavily\". \n\nSimilarly, the method checks for the Bing search service API key using `settings.search_engine.bing.api_key`. If this API key is available, an instance of the BingClient is created and added to the `clients` dictionary with the key \"bing\". \n\nAfter initializing the clients, the method creates a set called `available_clients`, which contains the keys of the clients that have been successfully instantiated. This allows the SearchAggregator to keep track of which search clients are available for use.\n\nThe SearchAggregator class is designed to manage multiple search clients, enabling it to perform searches across different services. By initializing the clients in the constructor, the class ensures that it can leverage the functionalities of both TavilyClient and BingClient, provided that the necessary API keys are available. This design promotes flexibility and extensibility, allowing for easy integration of additional search clients in the future.\n\n**Note**: It is important to ensure that the API keys for both Tavily and Bing are correctly configured in the settings. If the keys are missing or invalid, the corresponding clients will not be initialized, which may affect the functionality of the SearchAggregator. Proper error handling should be implemented when using the SearchAggregator to manage search requests."
      ],
      "code_start_line": 15,
      "code_end_line": 28,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.clients: Dict[str, TavilyClient | BingClient] = {}\n\n        # 如果 Tavily 的 API key 存在，初始化客户端\n        tavily_api_key = settings.tavily.api_key\n        if tavily_api_key:\n            self.clients[\"tavily\"] = TavilyClient(tavily_api_key)\n\n        # 如果 Bing 的 API key 存在，初始化客户端\n        bing_api_key = settings.search_engine.bing.api_key\n        if bing_api_key:\n            self.clients[\"bing\"] = BingClient(bing_api_key)\n\n        self.available_clients = set(self.clients.keys())\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "mark_engine_unavailable",
      "md_content": [
        "**mark_engine_unavailable**: The function of mark_engine_unavailable is to mark a specific search engine as unavailable.\n\n**parameters**:\n· engine: str - The name of the engine to mark as unavailable.\n\n**Code Description**:  \nThe `mark_engine_unavailable` method is part of a class and is responsible for updating the status of a specified search engine by marking it as unavailable. This is achieved by removing the engine's name from the `available_clients` collection. The parameter `engine` is expected to be a string, which represents the name of the search engine that needs to be marked as unavailable.\n\nIn terms of its usage, this function is typically called within error-handling blocks. Specifically, it is invoked whenever an exception is raised during a search operation, indicating that a particular engine is no longer functional for the query being processed. The method ensures that once an engine fails due to errors like retries, invalid API keys, or usage limits, it is effectively removed from the list of available search engines, preventing any further attempts to use that engine in future queries.\n\n**Note**:  \n- The `mark_engine_unavailable` function operates by directly modifying the `available_clients` list. Therefore, it is essential that `available_clients` is a mutable collection that supports the `remove` operation, like a list or set.\n- The engine's unavailability is handled automatically through exception handling in related functions (like `_search_single_query`), ensuring that engines that fail during the search process are excluded from subsequent attempts. \n- It is important to note that this function does not handle the re-inclusion of the engine once it has been marked as unavailable; this would need to be managed separately if required."
      ],
      "code_start_line": 30,
      "code_end_line": 38,
      "params": [
        "self",
        "engine"
      ],
      "have_return": false,
      "code_content": "    def mark_engine_unavailable(self, engine: str):\n        \"\"\"\n        Mark a specific search engine as unavailable.\n\n        Args:\n            engine (str): The name of the engine to mark as unavailable.\n        \"\"\"\n        if engine in self.available_clients:\n            self.available_clients.remove(engine)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_search_single_query",
      "md_content": [
        "**_search_single_query**: The function of _search_single_query is to perform an asynchronous search query using specified search engines and return the results encapsulated in a SearchResponse object.\n\n**parameters**: The parameters of this Function.\n· query: str - A string representing the user's search query. This parameter cannot be empty.\n· engines: List[str] - A list of search engine names that are available for performing the search.\n\n**Code Description**: The `_search_single_query` function is an asynchronous method designed to execute a search query against a list of specified search engines. It iterates through each engine in the provided `engines` list and checks if the engine is available in the `available_clients` collection. If the engine is available, it attempts to call the asynchronous `search` method of the corresponding client.\n\nThe function handles various exceptions that may arise during the search process:\n- If a `RetryError` occurs, it logs a warning indicating that the engine has failed after multiple retries and marks the engine as unavailable using the `mark_engine_unavailable` method.\n- If an `InvalidAPIKeyError` is raised, it logs an error message indicating that the search engine's API key is invalid and marks the engine as unavailable.\n- If a `UsageLimitExceededError` is encountered, it logs a warning about the usage limit being exceeded and marks the engine as unavailable.\n- For any other exceptions, it logs the exception details and marks the engine as unavailable.\n\nIf all specified search engines are unavailable, the function logs an error message and returns a `SearchResponse` object containing the original query and an error message indicating that no available search engines could fulfill the request.\n\nThis function is called by the `search` method of the `SearchAggregator` class. The `search` method gathers a list of currently available search engines and creates tasks for concurrent execution of `_search_single_query` for each query in the provided list. The results from these tasks are then collected and returned as a structured response.\n\n**Note**: It is essential to ensure that the engines passed to this function are valid and that the `available_clients` collection is properly maintained to reflect the current state of search engines. The function does not handle the re-inclusion of engines marked as unavailable; this must be managed separately if required.\n\n**Output Example**: A possible return value of the `_search_single_query` function could be a `SearchResponse` object structured as follows:\n```python\nSearchResponse(\n    query=\"latest technology news\",\n    results=[\n        SearchResult(title=\"Tech Innovations\", url=\"https://example.com/tech-innovations\", content=\"Explore the latest in technology.\"),\n        SearchResult(title=\"Gadget Reviews\", url=\"https://example.com/gadget-reviews\", content=\"Read reviews on the newest gadgets.\")\n    ],\n    error_message=None\n)\n```",
        "**_search_single_query**: The function of _search_single_query is to execute a search query against multiple search engines and return the results encapsulated in a SearchResponse object.\n\n**parameters**: The parameters of this Function.\n· query: str - A string representing the search query to be executed.  \n· engines: List[str] - A list of search engine names that are available for executing the query.\n\n**Code Description**: The _search_single_query function is an asynchronous method designed to perform a search operation using a specified query across multiple search engines. It iterates through the provided list of engines, checking if each engine is available for use. If an engine is found to be available, the function attempts to execute the search by calling the search method of the corresponding client for that engine.\n\nUpon calling the search method, the function awaits the result. If the search is successful, it logs the result using the printer.log method and returns the result encapsulated in a SearchResponse object. The logging provides visibility into the search results, which can be useful for debugging and monitoring purposes.\n\nIn the event of an error during the search operation, the function handles several specific exceptions:\n- RetryError: Indicates that the search engine failed after multiple retry attempts. The engine is marked as unavailable using the mark_engine_unavailable method.\n- InvalidAPIKeyError: Raised when the API key used for the search is invalid. The engine is also marked as unavailable in this case.\n- UsageLimitExceededError: This exception is raised when the search engine has exceeded its usage limits. The engine is marked as unavailable as well.\n- General Exception: Any other unexpected errors are caught, and the function logs the exception details using the printer.print_exception method, marking the engine as unavailable.\n\nIf all specified search engines are found to be unavailable after iterating through the list, the function logs a message indicating that no available search engines could be used for the query. It then returns a SearchResponse object containing the original query and an error message stating that the search failed due to the unavailability of search engines.\n\nThe _search_single_query function is called by the search method within the SearchAggregator class. The search method is responsible for executing multiple search queries concurrently by creating tasks for each query and utilizing the asyncio.gather function to run them. This design allows for efficient handling of multiple queries, leveraging the asynchronous capabilities of the underlying framework.\n\n**Note**: It is important to ensure that the engines parameter contains valid search engine names that are currently available. The function relies on proper exception handling to manage errors effectively, ensuring that any issues encountered during the search process are logged and handled gracefully.\n\n**Output Example**: A possible return value of the _search_single_query function could be structured as follows:\n```python\nSearchResponse(\n    query=\"latest technology news\",\n    results=[\n        SearchResult(title=\"Tech Innovations\", url=\"https://example.com/tech-innovations\", content=\"Explore the latest in technology.\"),\n        SearchResult(title=\"Gadget Reviews\", url=\"https://example.com/gadget-reviews\", content=\"Read reviews on the newest gadgets.\")\n    ],\n    error_message=None\n)\n``` \nThis output illustrates a successful search response containing the original query, a list of search results, and no error messages."
      ],
      "code_start_line": 40,
      "code_end_line": 75,
      "params": [
        "self",
        "query",
        "engines"
      ],
      "have_return": true,
      "code_content": "    async def _search_single_query(\n        self, query: str, engines: List[str]\n    ) -> SearchResponse:\n        for engine in engines:\n            if engine in self.available_clients:\n                try:\n                    # Call the asynchronous search method for the specified engine\n                    result = await self.clients[engine].search(query)\n                    printer.log(f\"{result.model_dump()}\")\n                    return result\n                except RetryError:\n                    printer.log(\n                        f\"Engine '{engine}' for query: {query} failed after multiple retries. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except InvalidAPIKeyError:\n                    printer.log(\n                        f\"Engine '{engine}' for query: {query} failed because of wrong api key. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except UsageLimitExceededError:\n                    printer.log(\n                        f\"Engine '{engine}' for query: {query} failed because of usage limit exceeded. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except Exception:\n                    printer.print_exception(\n                        f\"Engine '{engine}' encountered error. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n\n        printer.log(\"All specified search engines are unavailable.\", style=\"bold red\")\n        return SearchResponse(\n            query=query,\n            error_message=\"Search failed: No available search engines for this query.\",\n        )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/rich_output.py/RichPrinter/print_exception",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient/search",
        "src/criticsearch/tools/search_adapter/exceptions.py/UsageLimitExceededError",
        "src/criticsearch/tools/search_adapter/exceptions.py/InvalidAPIKeyError",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient/search",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponse",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/mark_engine_unavailable"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [
        "**search**: The function of search is to perform a search using the provided query and return the results as a serialized response.\n\n**parameters**: The parameters of this Function.\n· query: List[str] - A list of search queries that the user wants to search for.\n\n**Code Description**: The `search` method is an asynchronous function designed to execute multiple search queries concurrently using available search engines. It takes a list of search queries as input and returns a serialized string representation of the search results.\n\nThe method begins by retrieving the list of currently available search engines from the `available_clients` attribute. If there are no available engines, it raises a `ValueError`, indicating that the search cannot be performed. This ensures that the function does not attempt to execute a search when there are no resources to handle it.\n\nNext, the method creates a list of tasks, where each task corresponds to a single search query. This is done by calling the `_search_single_query` method for each query in the provided list, passing the list of available engines to it. The `_search_single_query` method is responsible for executing the search against each engine and returning the results encapsulated in a `SearchResponse` object.\n\nOnce the tasks are created, the method uses the `gather` function from the `asyncio` library to execute all the search tasks concurrently. This allows for efficient handling of multiple queries, as it does not block the execution while waiting for each individual search to complete.\n\nAfter all tasks are executed, the method collects the responses and constructs an instance of `SearchResponseList`, which is designed to hold and serialize the search results. The `model_dump()` method of `SearchResponseList` is called to generate a formatted string representation of the results, ensuring that any duplicate content across the responses is removed.\n\nThe `search` method is called by various components in the project, including the `search_and_browse` method in the `BaseAgent` class and the `main` function in the `__main__.py` module. In `search_and_browse`, it is invoked to handle user prompts for search queries, while in `main`, it demonstrates a simple use case of performing a search for a specific query.\n\n**Note**: It is important to ensure that the search engines are properly configured and available before calling this method. The method handles the serialization of results, ensuring that duplicates are filtered out, which enhances the relevance of the returned data.\n\n**Output Example**: A possible return value of the `search` function could be a serialized string representing the search results, structured as follows:\n```\n{\n  \"results\": [\n    {\n      \"title\": \"Introduction to Python\",\n      \"url\": \"https://example.com/python\",\n      \"content\": \"Python is a high-level programming language.\"\n    },\n    {\n      \"title\": \"Python Tutorials\",\n      \"url\": \"https://example.com/tutorials\",\n      \"content\": \"Learn Python programming with these tutorials.\"\n    }\n  ],\n  \"summary\": {\n    \"total_results\": 5,\n    \"unique_results\": 3,\n    \"duplicates_removed\": 2\n  }\n}\n``` \nThis output illustrates the results of the search queries, including a summary of the total results and the number of unique results returned.",
        "**search**: The function of search is to perform a search using the provided query, supporting various search techniques through special syntax.\n\n**parameters**: The parameters of this Function.\n· query: List[str] - A list of search queries to be executed.\n\n**Code Description**: The search function is an asynchronous method within the SearchAggregator class that facilitates searching across multiple search engines based on the provided list of queries. The function begins by retrieving the currently available search engines from the instance's available_clients attribute. If no engines are available, it raises a ValueError, indicating that the search cannot proceed.\n\nTo execute the search, the function creates a list of tasks, where each task corresponds to a single search query. These tasks are generated by calling the _search_single_query method, which is responsible for executing a search against the available engines. The search_single_query method is called for each query in the provided list, allowing for concurrent execution of searches.\n\nOnce the tasks are created, the function uses the asyncio.gather method to run all tasks concurrently and await their results. This approach enhances efficiency by allowing multiple searches to be processed simultaneously rather than sequentially.\n\nAfter gathering the responses from all search tasks, the function constructs a SearchResponseList object, which encapsulates the results of the searches. This object is then serialized and returned as a string representation, providing a structured output of the search results.\n\nThe search method is called by various components within the project, including the search_and_browse method in the BaseAgent class. This method utilizes the search function to perform searches based on user prompts and integrates the results into a broader workflow that may involve web scraping and content generation.\n\n**Note**: It is essential to ensure that the query parameter contains valid search terms and that the available_clients attribute is populated with active search engines. Proper error handling is implemented to manage cases where no engines are available, ensuring that the function fails gracefully.\n\n**Output Example**: A possible return value of the search function could be structured as follows:\n```json\n{\n  \"responses\": [\n    {\n      \"query\": \"latest technology news\",\n      \"results\": [\n        {\n          \"title\": \"Tech Innovations\",\n          \"url\": \"https://example.com/tech-innovations\",\n          \"content\": \"Explore the latest in technology.\"\n        },\n        {\n          \"title\": \"Gadget Reviews\",\n          \"url\": \"https://example.com/gadget-reviews\",\n          \"content\": \"Read reviews on the newest gadgets.\"\n        }\n      ],\n      \"error_message\": null\n    }\n  ]\n}\n``` \nThis output illustrates a successful search response containing the original query, a list of search results, and no error messages.",
        "**search**: The function of search is to perform a search using the provided query.\n\n**parameters**: The parameters of this Function.\n· query: List[str] - A list of search queries to be executed.\n\n**Code Description**: The search function is an asynchronous method that facilitates the execution of multiple search queries concurrently using available search engines. It begins by retrieving a list of currently available search engines from the instance's `available_clients` attribute. If no search engines are available, it raises a ValueError, indicating that the search cannot be performed.\n\nNext, the function constructs a list of tasks, where each task corresponds to a single query being processed by the `_search_single_query` method. This method is responsible for executing the search against the specified engines and returning the results encapsulated in a `SearchResponse` object.\n\nThe function then utilizes the `gather` method from the asyncio library to execute all the search tasks concurrently. This allows for efficient handling of multiple queries, leveraging the asynchronous capabilities of the framework. Once all tasks are completed, the responses are collected and passed to the `SearchResponseList` class, which is designed to manage and serialize the search results.\n\nFinally, the serialized search responses are returned as a string representation, providing a structured output that can be utilized for further processing or presentation. The search function is called by various components within the project, including the `search_validator` method, which uses it to obtain search results based on a user-provided question. This integration highlights the function's role in enabling real-time search capabilities within the broader application context.\n\n**Note**: It is essential to ensure that the `query` parameter is a list of valid search strings. The function relies on the availability of search engines and proper handling of exceptions to manage scenarios where no engines are available or if errors occur during the search process.\n\n**Output Example**: A possible return value from the search function could be structured as follows:\n```json\n{\n  \"responses\": [\n    {\n      \"query\": \"latest technology news\",\n      \"results\": [\n        {\n          \"title\": \"Tech Innovations\",\n          \"url\": \"https://example.com/tech-innovations\",\n          \"content\": \"Explore the latest in technology.\"\n        },\n        {\n          \"title\": \"Gadget Reviews\",\n          \"url\": \"https://example.com/gadget-reviews\",\n          \"content\": \"Read reviews on the newest gadgets.\"\n        }\n      ],\n      \"error_message\": null\n    }\n  ]\n}\n``` \nThis output illustrates a successful search response containing the original query, a list of search results, and no error messages.",
        "**search**: The function of search is to perform a search using the provided query.\n\n**parameters**: The parameters of this Function.\n· query: List[str] - A list of search queries.\n\n**Code Description**: The search function is an asynchronous method designed to execute search queries across multiple available search engines. It begins by retrieving the list of currently available search engines from the instance's `available_clients` attribute. If no search engines are available, the function raises a ValueError, indicating that a search cannot be performed.\n\nNext, the function creates a list of tasks, where each task corresponds to a single search query. This is achieved by calling the `_search_single_query` method for each query in the provided list, passing the list of available engines as an argument. The tasks are then executed concurrently using the `gather` function from the asyncio library, which allows for efficient handling of multiple asynchronous operations.\n\nOnce all tasks are completed, the function collects the responses and constructs a `SearchResponseList` object, which is responsible for managing and serializing the search results. The `model_dump` method of the `SearchResponseList` is called to return the serialized output, which is a structured representation of the search results.\n\nThe search function is called by various components within the project, including the `search_validator` function in the `ReverseUpgradeWorkflow` class. This function utilizes the search method to validate the correctness of a model's answer by performing a search based on the input question and comparing the results against the expected answer. Additionally, the search method is invoked in the `_action_router` function, which manages the decision-making process for the intelligent agent, guiding it through actions such as searching and browsing based on the context and results.\n\n**Note**: It is essential to ensure that the `query` parameter is a well-formed list of strings, as the function relies on valid input to perform searches effectively. The proper functioning of the search engines is also crucial for obtaining meaningful results.\n\n**Output Example**: A possible return value from the search function could be structured as follows:\n```json\n{\n  \"responses\": [\n    {\n      \"query\": \"latest technology news\",\n      \"results\": [\n        {\n          \"title\": \"Tech Innovations\",\n          \"url\": \"https://example.com/tech-innovations\",\n          \"content\": \"Explore the latest in technology.\"\n        },\n        {\n          \"title\": \"Gadget Reviews\",\n          \"url\": \"https://example.com/gadget-reviews\",\n          \"content\": \"Read reviews on the newest gadgets.\"\n        }\n      ],\n      \"error_message\": null\n    }\n  ]\n}\n``` \nThis output illustrates a successful serialization of search responses, containing the original query, a list of relevant search results, and no error messages."
      ],
      "code_start_line": 77,
      "code_end_line": 97,
      "params": [
        "self",
        "query"
      ],
      "have_return": true,
      "code_content": "    async def search(self, query: List[str]) -> str:\n        \"\"\"\n        Performs a search using the provided query.\n        Supports various search techniques using special syntax.\n\n        Args:\n            query (List[str]): A list of search queries.\n        \"\"\"\n        # Get the list of currently available search engines\n        engines = list(self.available_clients)\n        if not engines:\n            raise ValueError(\"No available engines to perform the search.\")\n\n        # Create tasks for concurrent search\n        tasks = [self._search_single_query(q, engines) for q in query]\n\n        # Execute all tasks and gather the responses\n        responses = await gather(*tasks)\n\n        # Return the search responses as a dictionary\n        return SearchResponseList(responses=responses).model_dump()  # type: ignore\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/abstract_substitution/abs_exp_origin.py/ReverseUpgradeWorkflow/search_validator",
        "src/criticsearch/base_agent.py/BaseAgent/__init__",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/main.py/_action_router",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/tools/search_adapter/__main__.py/main",
        "src/criticsearch/workflow.py/WorkflowExecutor/__init__"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/models.py/SearchResponseList",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "src/criticsearch/reportbench/read_wikiItem_hierachy.py": [
    {
      "type": "FunctionDef",
      "name": "traverse",
      "md_content": [
        "**traverse**: The function of traverse is to recursively traverse a nested structure of dictionaries and lists and print the \"title\" key from any dictionary encountered.\n\n**parameters**: \n· parameter1: data (Required) – This parameter is the data structure to be traversed, which can be a dictionary, list, or any other nested combination of these types.\n· parameter2: level (Optional) – An integer indicating the current level of recursion. It is used for indentation purposes to visually represent the depth of traversal.\n\n**Code Description**: \nThe traverse function is a recursive function designed to navigate through a nested data structure, which may consist of dictionaries, lists, or combinations of both. The function begins by checking the type of the `data` parameter to determine how to process it.\n\n1. If `data` is a dictionary, it first checks if the dictionary contains a key named \"title.\" If this key exists, its value is printed with indentation that corresponds to the current `level`. The indentation is achieved by multiplying four spaces (\"    \") by the `level` parameter.\n   \n2. After checking for the \"title\" key, the function then iterates over all the key-value pairs in the dictionary. If any of the values are another dictionary or a list, the function calls itself recursively with the value and an incremented `level` (i.e., `level + 1`).\n\n3. If `data` is a list, the function iterates over each item in the list and recursively calls itself on each item.\n\nThe recursive nature of this function allows it to handle arbitrarily nested dictionaries and lists. The indentation helps to visualize the depth of each level of recursion when printing the \"title\" values.\n\n**Note**: \n- The `level` parameter is optional and defaults to 0, representing the top level of the data structure. If provided, it should be an integer.\n- The function expects the input data to be a dictionary or list; other data types are not processed.\n- If dictionaries or lists are nested inside each other, the function will process them in a depth-first manner, meaning it explores each item fully before moving to the next.\n- The printed output will display the \"title\" value for each dictionary that contains it, indented according to its level in the structure."
      ],
      "code_start_line": 4,
      "code_end_line": 14,
      "params": [
        "data",
        "level"
      ],
      "have_return": false,
      "code_content": "def traverse(data, level=0):\n    indent = \"    \" * level\n    if isinstance(data, dict):\n        if \"title\" in data:\n            print(indent + data[\"title\"])\n        for key, value in data.items():\n            if isinstance(value, (dict, list)):\n                traverse(value, level + 1)\n    elif isinstance(data, list):\n        for item in data:\n            traverse(item, level)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/reportbench/report_benchmark.py": [
    {
      "type": "FunctionDef",
      "name": "safe_chat_and_parse",
      "md_content": [
        "**safe_chat_and_parse**: The function of safe_chat_and_parse is to invoke a conversational model and parse the returned JSON, retrying up to a specified maximum number of attempts in case of parsing failure.\n\n**parameters**: The parameters of this Function.\n· agent: An instance of the conversational agent responsible for interacting with the model.  \n· prompt: A string that contains the user prompt to be sent to the conversational model for processing.  \n· model: A string that specifies the model to be used for generating the response.\n\n**Code Description**: The safe_chat_and_parse function is designed to facilitate communication with a conversational model and ensure that the response received is valid JSON. It begins by calling the chat method of the provided agent, passing the user prompt and the specified model. This method is responsible for sending the prompt to the conversational model and retrieving the response.\n\nOnce the response is obtained, the function proceeds to extract and validate the JSON content using the extract_and_validate_json function. This function attempts to parse the response, handling various scenarios such as the presence of Markdown-style fences around the JSON content. If the parsing fails, safe_chat_and_parse will retry the process up to a maximum number of times defined in the settings, ensuring robustness in obtaining a valid response.\n\nThe safe_chat_and_parse function is called by other functions within the project, such as process_with_model and verify_qa_format. In the process_with_model function, it is used to handle the interaction with the AI model for processing a section of text and obtaining structured data. The response from safe_chat_and_parse is then further processed to ensure it meets the expected format. Similarly, in the verify_qa_format function, it validates the format of question-answer pairs by interacting with the conversational model and ensuring that the responses are correctly structured as JSON.\n\nThis function plays a critical role in the overall workflow of the application, as it acts as a bridge between user prompts and the conversational model, ensuring that the data returned is usable for subsequent processing steps.\n\n**Note**: It is essential to ensure that the prompt parameter is well-formed and relevant to the context of the conversation. The function relies on the proper configuration of the conversational model to generate accurate and meaningful responses. Additionally, the settings must define the maximum number of retries appropriately to handle potential parsing failures effectively.\n\n**Output Example**: A possible return value from the safe_chat_and_parse function could be a structured JSON object such as:\n```json\n{\n    \"response\": {\n        \"answer\": \"The capital of France is Paris.\",\n        \"sources\": [\"Geography textbook\", \"Wikipedia\"]\n    }\n}\n```"
      ],
      "code_start_line": 39,
      "code_end_line": 44,
      "params": [
        "agent",
        "prompt",
        "model"
      ],
      "have_return": true,
      "code_content": "def safe_chat_and_parse(agent, prompt: str, model: str):\n    \"\"\"\n    调用 LLM 并解析返回的 JSON。解析失败会重试至 settings.max_retries。\n    \"\"\"\n    response = agent.chat(usr_prompt=prompt, model=model)\n    return extract_and_validate_json(response)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section_with_models/process_with_model",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/verify_qa_format"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)",
        "src/criticsearch/utils.py/extract_and_validate_json"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "ReportBenchmark",
      "md_content": [
        "**ReportBenchmark**: The function of ReportBenchmark is to generate report evaluations by building ground truths and performing fact extraction and FactualQA evaluations.\n\n**attributes**: The attributes of this Class.\n· json_path: The path to the JSON input file containing the report data.\n· agent: An instance of BaseAgent used for interacting with the model.\n· breadth_gt: The ground truth for report breadth extracted from the JSON input.\n· article_content: The content of the article extracted from the Markdown file.\n· sections: The sections of the article extracted from the Markdown content.\n· section_content_pairs: Pairs of section titles and their corresponding content.\n· user_query: The user-defined query or a generated query based on the breadth ground truth.\n· cache_dir: The directory path for storing cached benchmark results.\n\n**Code Description**: The ReportBenchmark class is designed to facilitate the evaluation of reports by generating ground truths and extracting relevant facts from the provided report data. Upon initialization, it takes a JSON input path and an optional user query. It extracts the necessary information from the JSON file, including the breadth ground truth, article content, and sections, which are essential for generating comprehensive reports.\n\nThe class includes several key methods:\n- **_get_cache_key**: Generates a unique cache key based on the JSON path and user query, which is used to store and retrieve cached results.\n- **_load_from_cache**: Loads previously cached results if available, allowing for efficient reuse of data.\n- **_save_to_cache**: Saves the results to the cache for future reference.\n- **sliding_window_pairing**: Creates a sliding window of section content, merging sections while respecting a specified token limit. This method ensures that the content is organized and manageable for processing.\n- **run_fact_extraction**: Executes fact extraction on each section of the report using the BaseAgent's common_chat method. It handles retries in case of failures, ensuring robustness in the extraction process.\n- **run_factualqa**: Conducts a FactualQA evaluation using the user query and the ground truths, returning the evaluation results.\n- **process_window_content**: Processes the content of a single window, retrying if the result is empty.\n- **generate_benchmark_item**: Generates benchmark items with caching support, combining the results of the sliding window pairing and fact extraction.\n- **process_section**: A helper method that encapsulates the logic for processing a section of the report, similar to the run_fact_extraction method.\n- **parse_tagged_data_to_table**: Parses the extracted data into a structured format, specifically a table, which can be useful for further analysis.\n- **verify_extraction_meaningful**: A placeholder method intended to check the meaningfulness of the fact extraction results.\n\nThe ReportBenchmark class is called within the process_single_task function in the main.py file. It initializes an instance of ReportBenchmark with a JSON file path and generates benchmark items, which are then used to guide the conversation and content generation process. This integration highlights the class's role in facilitating the overall report generation workflow.\n\n**Note**: When using the ReportBenchmark class, ensure that the input JSON file is correctly formatted and contains the necessary data for extraction. The caching mechanism can significantly improve performance by avoiding redundant computations.\n\n**Output Example**: A possible return value from the generate_benchmark_item method could be a list of dictionaries, each containing the path of the section, the merged content of the section, and the extracted facts, structured as follows:\n```json\n[\n    {\n        \"path\": \"Section 1 -> Subsection 1.1\",\n        \"merged_section_window_content\": \"Content of Subsection 1.1...\",\n        \"extracted_facts\": [\n            {\"question\": \"What is the main topic?\", \"format\": \"text\", \"answer\": \"The main topic is...\"},\n            {\"question\": \"What are the key points?\", \"format\": \"list\", \"answer\": \"1. Point one\\n2. Point two\"}\n        ]\n    },\n    ...\n]\n```",
        "**ReportBenchmark**: The function of ReportBenchmark is to generate report evaluations by building ground truths for report breadth and depth, and to perform factual evaluations using various models.\n\n**attributes**: The attributes of this Class.\n· json_path: A string representing the path to the JSON input file containing report data.\n· agent: An instance of the BaseAgent class used for interacting with the model and performing tasks.\n· breadth_gt: A dictionary representing the ground truth for the breadth of the report, extracted from the JSON input.\n· article_content: A string containing the content of the article extracted from the input JSON file.\n· sections: A list of sections extracted from the article content.\n· section_content_pairs: A list of section-content pairs extracted from the JSON input.\n· user_query: A string representing the user’s query or task for generating the report.\n· cache_dir: A Path object representing the directory where cache files for benchmark results are stored.\n\n**Code Description**: The ReportBenchmark class is designed to facilitate the generation of report evaluations by leveraging ground truths derived from a specified JSON input file. Upon initialization, it extracts relevant data such as breadth ground truths, article content, and sections from the provided JSON file. The class includes methods for caching results, managing section content through a sliding window approach, and processing sections with multiple models for fact extraction.\n\nThe class is closely integrated with the BaseAgent, which is responsible for executing tasks such as fact extraction and outline generation. The ReportBenchmark class utilizes the BaseAgent to render templates and interact with models, enabling it to perform evaluations like FactualQA and process sections of the report concurrently using multiple models.\n\nThe ReportBenchmark class is invoked within the process_single_task function found in the src/criticsearch/main.py file. This function initializes an instance of ReportBenchmark with the path to a JSON file containing benchmark data. It subsequently generates a benchmark item, which includes the evaluation of the report based on the user’s task. The process_single_task function orchestrates the interaction between the agent, the report benchmark, and other components to produce a comprehensive report.\n\n**Note**: When utilizing the ReportBenchmark class, ensure that the input JSON file is correctly formatted and contains the necessary data for generating accurate evaluations. The caching mechanism is also important for optimizing performance by avoiding redundant computations.\n\n**Output Example**: A possible return value from the generate_benchmark_item method could be a list of dictionaries, each containing the path to a section, the merged content of that section, and the extracted facts, such as:\n```\n[\n    {\n        \"path\": \"Introduction -> Background\",\n        \"merged_section_window_content\": \"## Background\\nThis section discusses the historical context...\",\n        \"extracted_facts\": [\n            {\"question\": \"What is the historical context?\", \"format\": \"text\", \"answer\": \"The historical context is...\"}\n        ]\n    },\n    ...\n]\n```",
        "**ReportBenchmark**: The function of ReportBenchmark is to generate report evaluations by building ground truths and facilitating fact extraction through various models.\n\n**attributes**: The attributes of this Class.\n· json_path: A string representing the path to the JSON input file containing the data for report evaluation.  \n· agent: An instance of the BaseAgent class, which is responsible for interacting with the models and handling prompts.  \n· breadth_gt: A dictionary that holds the ground truth data extracted from the JSON file, specifically for report breadth.  \n· article_content: A string containing the markdown content extracted from the JSON file.  \n· sections: A list of sections extracted from the markdown content.  \n· section_content_pairs: A list of section content pairs derived from the JSON input.  \n· user_query: A string that represents the user's query for generating a comprehensive report.  \n· cache_dir: A Path object pointing to the directory where benchmark results will be cached.\n\n**Code Description**: The ReportBenchmark class is designed to facilitate the evaluation of reports by generating ground truths and processing content through various models. Upon initialization, it takes a JSON input path and an optional user query. It extracts necessary data from the JSON file, including the breadth ground truth, article content, and sections. The user query is constructed based on the title of the breadth ground truth if not provided.\n\nThe class includes methods for caching results, loading from cache, and saving to cache, which optimize performance by avoiding redundant computations. The sliding_window_pairing method creates a sliding window of section content, merging sections while respecting token limits, which is crucial for processing large reports efficiently.\n\nThe run_factualqa method evaluates the factual accuracy of the generated content by utilizing a template and the BaseAgent's chat capabilities. The process_section_with_models method allows for parallel processing of sections using multiple models, enhancing the efficiency of fact extraction.\n\nThe aggregate_model_results method consolidates results from different models, ensuring uniqueness and providing a comprehensive output. The generate_benchmark_item method incorporates caching support, generating benchmark items while checking for previously cached results to improve performance.\n\nThe ReportBenchmark class is called within the process_single_task function found in the src/criticsearch/main.py file. This function initializes an instance of ReportBenchmark with the loaded JSON file, generates a benchmark item, and orchestrates the overall task execution workflow. The generated benchmark item is then used to guide the report generation process, ensuring that the content aligns with the ground truths extracted from the JSON file.\n\n**Note**: It is essential to ensure that the input JSON file is correctly formatted and contains valid data, as this directly impacts the functionality of the ReportBenchmark class. Additionally, the caching mechanism should be managed to optimize performance and avoid redundant computations.\n\n**Output Example**: A possible return value from the generate_benchmark_item method could look like this:\n```json\n[\n    {\n        \"path\": \"Chapter 1 -> Section 1.1\",\n        \"merged_section_window_content\": \"## Section 1.1 Content\\nThis section discusses...\",\n        \"extracted_facts\": [\n            {\n                \"question\": \"What is the main topic of Section 1.1?\",\n                \"format\": \"Answer in one sentence.\",\n                \"answer\": \"The main topic of Section 1.1 is...\"\n            }\n        ]\n    },\n    ...\n]\n```",
        "**ReportBenchmark**: The function of ReportBenchmark is to generate report evaluations by building ground truths and facilitating fact extraction through various models.\n\n**attributes**: The attributes of this Class.\n· json_path: A string representing the path to the JSON input file containing the report data.  \n· agent: An instance of the BaseAgent class used for interacting with models and templates.  \n· breadth_gt: A dictionary containing the ground truth data extracted from the JSON file, specifically focusing on the breadth of the report.  \n· article_content: A string containing the markdown content extracted from the input JSON file.  \n· sections: A list of sections extracted from the markdown content.  \n· section_content_pairs: A list of pairs containing section titles and their corresponding content.  \n· user_query: A string representing the user's query or a generated query for report generation.  \n· cache_dir: A Path object representing the directory where cached results are stored.\n\n**Code Description**: The ReportBenchmark class is designed to facilitate the evaluation of reports by generating ground truths and processing content through various models. Upon initialization, it takes a JSON input path and an optional user query. It extracts relevant data from the JSON file, including the breadth ground truth, article content, and section details. The class also manages caching of results to optimize performance.\n\nThe class includes several key methods:\n- **_get_cache_key**: Generates a unique cache key based on the input file path and user query, ensuring that cached results can be accurately retrieved.\n- **_load_from_cache**: Loads results from the cache if available and non-empty, providing a mechanism to avoid redundant processing.\n- **_save_to_cache**: Saves results to the cache for future use, enhancing efficiency.\n- **sliding_window_pairing**: Creates sliding windows of section content, merging sections while respecting token limits, which is essential for processing large reports.\n- **run_factualqa**: Executes a FactualQA evaluation using the BaseAgent, leveraging the generated ground truths.\n- **process_section_with_models**: Processes a section of text using multiple models in parallel, allowing for efficient fact extraction.\n- **aggregate_model_results**: Aggregates results from multiple models, ensuring uniqueness and providing a comprehensive output.\n- **generate_benchmark_item**: Generates benchmark items while optionally utilizing cached results, facilitating the evaluation process.\n\nThe ReportBenchmark class is called by the process_single_task function in the main module of the project. This function initializes an instance of ReportBenchmark with the path to the JSON file containing ground truth data. It then generates benchmark items that guide the report generation process. The generated items are used to evaluate the agent's performance and ensure the accuracy of the generated content against the ground truths.\n\n**Note**: When using the ReportBenchmark class, it is crucial to ensure that the input JSON file is correctly formatted and contains the necessary data for generating ground truths. Proper management of the cache directory is also important to avoid unnecessary processing and to improve performance.\n\n**Output Example**: A possible appearance of the code's return value when executing the generate_benchmark_item method could look like this:\n```json\n[\n    {\n        \"path\": \"Chapter 1 -> Section 1\",\n        \"merged_section_window_content\": \"## Section 1 Title\\nContent of section 1...\",\n        \"extracted_facts\": [\n            {\"question\": \"What is the main topic?\", \"format\": \"text\", \"answer\": \"The main topic is...\"},\n            {\"question\": \"What are the key points?\", \"format\": \"list\", \"answer\": \"1. Point one\\n2. Point two\"}\n        ]\n    },\n    {\n        \"path\": \"Chapter 1 -> Section 2\",\n        \"merged_section_window_content\": \"## Section 2 Title\\nContent of section 2...\",\n        \"extracted_facts\": [\n            {\"question\": \"What is discussed in this section?\", \"format\": \"text\", \"answer\": \"This section discusses...\"}\n        ]\n    }\n]\n```",
        "**ReportBenchmark**: The function of ReportBenchmark is to generate report evaluations and benchmark items based on input JSON data, utilizing various models for fact extraction and evaluation.\n\n**attributes**: The attributes of this Class.\n· json_path: A string representing the path to the JSON input file containing the report data.\n· agent: An instance of the BaseAgent class used for interacting with the model and handling prompts.\n· breadth_gt: A dictionary representing the ground truth extracted from the JSON file, specifically for the breadth of the report.\n· article_content: A string containing the markdown content extracted from the JSON file.\n· sections: A list of sections extracted from the article content.\n· section_content_pairs: A list of section-content pairs extracted from the JSON input.\n· user_query: A string representing the user query for generating the report, defaulting to a comprehensive report request if not provided.\n· cache_dir: A Path object representing the directory for storing cached benchmark results.\n\n**Code Description**: The ReportBenchmark class is designed to facilitate the evaluation and generation of reports by processing input data from a specified JSON file. Upon initialization, it extracts necessary information such as the breadth ground truth, article content, and sections from the provided JSON input. It also sets up a user query that guides the report generation process.\n\nThe class includes several methods that contribute to its functionality:\n\n- **_get_cache_key**: Generates a unique cache key based on the JSON path and user query, which is used for caching results.\n- **_load_from_cache**: Loads previously cached results if they exist and are non-empty, providing a mechanism to avoid redundant processing.\n- **_save_to_cache**: Saves the generated results to a cache file for future use, enhancing efficiency.\n- **sliding_window_pairing**: Creates sliding windows of section content, merging sections while adhering to a specified token limit. This method ensures that the content is processed in manageable chunks.\n- **run_factualqa**: Loads a template for FactualQA evaluation and processes the user query along with the ground truth data to generate a response.\n- **process_section_with_models**: Utilizes multiple models to process a given section of text in parallel, enhancing the extraction of relevant information.\n- **aggregate_model_results**: Combines and deduplicates results from various models, ensuring that the final output is unique and informative.\n- **generate_benchmark_item**: Generates benchmark items for the report, optionally using cached results to improve performance.\n- **verify_qa_format**: Validates the format of question-answer pairs to ensure they meet specified constraints.\n- **parse_tagged_data_to_table**: Parses the data returned by models into a structured format, extracting relevant fields such as questions and answers.\n- **generate_for_folder**: A class method that processes all JSON files in a specified folder, generating benchmark items for each file while utilizing caching.\n\nThe ReportBenchmark class is called by various components within the project, including the process_single_task function in the main.py file and the iterate_traj function in the workflow.py file. In process_single_task, an instance of ReportBenchmark is created to generate benchmark items based on the specified JSON file, which are then used to guide the report generation process. Similarly, in iterate_traj, ReportBenchmark is utilized to generate benchmark items for sections based on user prompts, facilitating the evaluation of factual accuracy.\n\n**Note**: It is important to ensure that the input JSON file is correctly formatted and contains the necessary data for the ReportBenchmark class to function effectively. Additionally, caching mechanisms should be properly managed to optimize performance and avoid unnecessary reprocessing of data.\n\n**Output Example**: A possible appearance of the code's return value when executing the generate_benchmark_item method could look like this:\n```json\n[\n    {\n        \"path\": \"# Section 1\",\n        \"merged_section_window_content\": \"Content of section 1...\",\n        \"extracted_facts\": [\n            {\"question\": \"What is the main topic?\", \"answer\": \"The main topic is...\"},\n            {\"question\": \"What are the key points?\", \"answer\": \"The key points include...\"}\n        ]\n    },\n    {\n        \"path\": \"# Section 2\",\n        \"merged_section_window_content\": \"Content of section 2...\",\n        \"extracted_facts\": [\n            {\"question\": \"What is the conclusion?\", \"answer\": \"The conclusion is...\"},\n            {\"question\": \"What are the implications?\", \"answer\": \"The implications are...\"}\n        ]\n    }\n]\n```"
      ],
      "code_start_line": 51,
      "code_end_line": 484,
      "params": [],
      "have_return": true,
      "code_content": "class ReportBenchmark:\n    \"\"\"\n    A benchmarking class for generating report evaluations.\n    Builds ground truths for report breadth & depth using two modules,\n    and calls prompts (fact_extraction, outline_generation) via BaseAgent's common_chat.\n    Also includes a method for FactualQA evaluation using a model (e.g., GPT-4o).\n    \"\"\"\n\n    def __init__(self, json_input_path, user_query=None):\n        self.json_path = json_input_path\n        self.agent = BaseAgent()\n        self.breadth_gt = extractDirectoryTree(\n            self.json_path\n        )  # Extract breadth ground truth，得到一个json结构的广度树\n        self.article_content = extractMarkdownContent(self.json_path)\n        self.sections = extract_markdown_sections(self.article_content)\n        self.section_content_pairs = extractSectionContentPairs(json_input_path)\n        self.user_query = (\n            f\"Generate a comprehensive long report about {self.breadth_gt.get('title', '')}\"\n            if user_query is None\n            else user_query\n        )\n        # 添加缓存相关的属性\n        self.cache_dir = Path(\"cache/benchmark_results\")\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n    def _get_cache_key(self):\n        \"\"\"生成缓存文件的唯一标识\"\"\"\n        # 使用输入文件路径和查询作为缓存key的基础\n        content = f\"{self.json_path}_{self.user_query}\"\n        return hashlib.md5(content.encode()).hexdigest()\n\n    def _load_from_cache(self) -> Optional[Any]:\n        \"\"\"Load results from cache if available and non-empty.\"\"\"\n        cache_file = self.cache_dir / f\"{self._get_cache_key()}.json\"\n        if not cache_file.exists() or cache_file.stat().st_size == 0:\n            printer.print(f\"Cache file {cache_file} is missing or empty.\")\n            return None\n        with cache_file.open('r', encoding='utf-8') as f:\n            data = json.load(f)\n        # 如果缓存内容是空列表，也视为无效\n        if isinstance(data, list) and not data:\n            printer.print(f\"Cache file {cache_file} contains an empty list.\")\n            return None\n        return data\n\n    def _save_to_cache(self, results: Any) -> None:\n        \"\"\"Save results to cache.\"\"\"\n        cache_file = self.cache_dir / f\"{self._get_cache_key()}.json\"\n        with cache_file.open('w', encoding='utf-8') as f:\n            json.dump(results, f, ensure_ascii=False, indent=2)\n        with open(cache_file, 'w', encoding='utf-8') as f:\n            json.dump(results, f, ensure_ascii=False, indent=2)\n\n    def sliding_window_pairing(self, max_token_length=2000):\n        \"\"\"\n        创建 section 内容的滑动窗口，尽可能在 token 限制内合并更多的 section。\n        \n        Args:\n            max_token_length: 每个窗口的最大 token 数量\n            \n        Returns:\n            List[Dict]: 一个包含合并窗口的列表，每个窗口包含最高级标题、合并内容、路径等信息\n        \"\"\"\n        # 首先提取所有 section 的标题和内容\n        sections = []\n        \n        def extract_sections(data, path=[], depth=0):\n            \"\"\"递归提取所有 section 的标题、内容、深度和路径\"\"\"\n            if isinstance(data, dict):\n                # 如果是直接的字典对象\n                title = data.get('title', '')\n                content = data.get('content', '')\n                \n                # 获取正确的层级深度\n                current_depth = depth + 1  # 默认深度加1\n                # 如果数据中有明确的id，我们可以从id解析深度\n                section_id = data.get('id', '')\n                if section_id and '.' in section_id:\n                    # 例如 \"4.3.5\" 表示这是第4个主标题下的第3个子标题下的第5个小节\n                    # 那么深度应该是3（4是1级，4.3是2级，4.3.5是3级）\n                    current_depth = len(section_id.split('.'))\n                \n                if title and content:\n                    current_path = path + [{\"title\": title, \"depth\": current_depth}]\n                    sections.append({\n                        \"id\": section_id,                \n                        \"title\": title,\n                        \"content\": content,\n                        \"depth\": current_depth,\n                        \"path\": current_path.copy(),\n                        \"tokens\": count_tokens(content)\n                    })\n                \n                # 检查是否有 children\n                if 'children' in data and data['children']:\n                    current_path = path + [{\"title\": title, \"depth\": current_depth}]\n                    # 递归处理 children，子节点的深度加1\n                    for child in data['children']:\n                        extract_sections(child, current_path, current_depth)\n        \n        # 从 section_content_pairs 中提取所有 section\n        extract_sections(self.section_content_pairs)\n        \n        # 改为按 id 数值序列排序\n        def path_sort_key(section):\n            try:\n                # \"3.2.1\" -> (3,2,1)，\"5\" -> (5,)\n                return tuple(int(x) for x in section.get(\"id\", \"0\").split(\".\"))\n            except ValueError:\n                return (0,)\n            \n        sections.sort(key=path_sort_key)\n        printer.rule(f\"Extracted {len(sections)} sections from the report.\")\n        printer.print(json.dumps(sections, indent=2))\n        \n        \n        # 创建滑动窗口\n        windows = []\n        i = 0\n        while i < len(sections):\n            current_section = sections[i]\n            highest_title = current_section['title']\n            highest_depth = current_section['depth']\n            current_path = current_section['path']\n            \n            # 使用原始深度来设置标题级别\n            merged_content = f\"{'#' * current_section['depth']} {highest_title}\\n{current_section['content']}\"\n            window_tokens = current_section['tokens'] + count_tokens(f\"{'#' * current_section['depth']} {highest_title}\\n\")\n            window_path = [current_section['path'][-1]]\n            \n            # 尝试添加后续的 section，如果它们是当前 section 的子节点或平级节点\n            j = i + 1\n            while j < len(sections) and window_tokens < max_token_length:\n                next_section = sections[j]\n                next_path = next_section['path']\n                \n                # 检查路径关系时使用标题字符串进行比较\n                current_path_titles = [p[\"title\"] for p in current_path]\n                next_path_titles = [p[\"title\"] for p in next_path]\n                \n                # 检查是否为子节点或平级节点\n                is_subsection_or_sibling = False\n                \n                # 如果下一个 section 是当前 section 的子节点\n                if len(next_path_titles) > len(current_path_titles) and all(next_path_titles[k] == current_path_titles[k] for k in range(len(current_path_titles))):\n                    is_subsection_or_sibling = True\n                \n                # 如果下一个 section 是当前 section 的平级节点\n                elif len(next_path_titles) == len(current_path_titles) and all(next_path_titles[k] == current_path_titles[k] for k in range(len(current_path_titles) - 1)):\n                    is_subsection_or_sibling = True\n                \n                if is_subsection_or_sibling:\n                    # 使用原始深度来设置标题级别，不再计算相对深度\n                    section_header = '#' * next_section['depth'] + ' ' + next_section['title'] + '\\n'\n                    additional_tokens = count_tokens(section_header + next_section['content'])\n                    \n                    if window_tokens + additional_tokens <= max_token_length:\n                        # 可以添加到当前窗口\n                        merged_content += f\"\\n{section_header}{next_section['content']}\"\n                        window_tokens += additional_tokens\n                        window_path.append({\"title\": next_section['title'], \"depth\": next_section['depth']})\n                        j += 1\n                    else:\n                        # 超出 token 限制，不能再添加\n                        break\n                else:\n                    # 不是子节点或平级节点，跳过\n                    break\n            \n            def format_path_with_depth(path_nodes):\n                formatted_titles = []\n                for node in path_nodes:\n                    depth = node[\"depth\"]\n                    title = node[\"title\"]\n                    formatted_title = '#' * depth + ' ' + title\n                    formatted_titles.append(formatted_title)\n                return ' -> '.join(formatted_titles)\n            \n            path_text = format_path_with_depth(window_path)\n            window = {\n                \"highest_title\": highest_title,\n                \"merged_section_window_content\": merged_content,\n                \"section_window_path\": window_path,\n                \"section_window_path_text\": path_text,\n                \"section_window_tokens\": window_tokens\n            }\n            \n            windows.append(window)\n            \n            # 下一个窗口的起始位置\n            i = j if j > i + 1 else i + 1\n        \n        printer.rule(f\"Created {len(windows)} sliding windows.\")\n        printer.print(json.dumps(windows, indent=2))\n                \n        return windows\n\n    def run_factualqa(self):\n        # Load and render a template \"factual_qa.txt\" for FactualQA evaluation.\n        # Pass Query, BreadthGT (converted to JSON string) and DepthGT.\n        template_str = self.agent.load_template(\"factual_qa.txt\")\n        data = {\n            \"Query\": self.user_query,  # Updated from self.query to self.user_query\n            \"BreadthGT\": json.dumps(self.breadth_gt),\n            \"DepthGT\": self.depth_gt,\n        }\n        prompt = self.agent.render_template(template_str, data)\n        response = self.agent.chat(usr_prompt=prompt)\n        return response\n\n    def process_section_with_models(self, section_text: str, models: list) -> list:\n        \"\"\"使用多个模型并行处理同一段文本\"\"\"\n\n        def process_with_model(model: str):\n            try:\n                template_str = self.agent.load_template(\"fact_extraction_new.txt\")\n                data = {\n                    \"wiki_text\": section_text,\n                    \"UserQuery\": self.user_query,\n                }\n                prompt = self.agent.render_template(template_str, data)\n\n                printer.print(f\"Model: {model}\")\n                # 通过 safe_chat_and_parse 完成模型调用 + JSON 解析\n                candidate = safe_chat_and_parse(self.agent, prompt, model)\n                # 如果拿到的是 list，则继续处理\n                if isinstance(candidate, list):\n                    parsed_data = self.parse_tagged_data_to_table(candidate)\n                    return {\"model\": model, \"data\": parsed_data}\n                else:\n                    printer.print(f\"[WARN] 模型 {model} 返回非列表结构: {candidate}\")\n                    return None\n\n            except Exception as e:\n                printer.print(f\"[ERROR] model={model} failed after {settings.max_retries} retries:\\n{e}\")\n                return None\n\n        results = []\n        with ThreadPoolExecutor(max_workers=50) as executor:\n            # submit + as_completed，方便捕获每个 future 的异常\n            futures = {executor.submit(process_with_model, m): m for m in models}\n            for fut in as_completed(futures):\n                model = futures[fut]\n                try:\n                    res = fut.result()  # 这里会抛出未被捕获的异常\n                    results.append(res)\n                except Exception as e:\n                    printer.print(f\"[ERROR] task for {model} failed: {e}\")\n\n        # 过滤掉 None\n        results = [r for r in results if r is not None]\n\n        # 对每个结果进行格式验证\n        verified_results = []\n        for result in results:\n            verified_data = []\n            for item in result[\"data\"]:\n                print(f\"\\n\\nVerifying item: {item}\\n\\n\")\n                if self.verify_qa_format(item):\n                    verified_data.append(item)\n            if verified_data:\n                result[\"data\"] = verified_data\n                verified_results.append(result)\n\n        # 聚合并去重\n        return self.aggregate_model_results(verified_results)\n\n    def aggregate_model_results(self, results: list) -> list:\n        \"\"\"聚合多个模型的结果并去重\"\"\"\n        seen_items = set()\n        unique_results = []\n        \n        for result in results:\n            for item in result[\"data\"]:\n                # 创建问题和答案的组合哈希值\n                item_hash = hashlib.md5(\n                    f\"{item['question'].strip().lower()}_{item['answer'].strip().lower()}\".encode()\n                ).hexdigest()\n                \n                if item_hash not in seen_items:\n                    seen_items.add(item_hash)\n                    item[\"source_model\"] = result[\"model\"]  # 添加来源模型信息\n                    unique_results.append(item)\n        \n        return unique_results\n\n    def process_window_content(self, content, max_retries=10):\n        # 从settings中直接获取extract_models配置\n        models = settings.extract_models if hasattr(settings, 'extract_models') else [\"gpt-4o\"]\n        \n        for attempt in range(max_retries):\n            try:\n                # 使用多个模型并行处理\n                results = self.process_section_with_models(content, models)\n                if results:  # 如果有结果则返回\n                    return results\n            except Exception:\n                continue\n        \n        return []  # 如果所有尝试都失败则返回空列表\n\n    def generate_benchmark_item(self, use_cache=True, max_window_tokens=1):\n        \"\"\"添加缓存支持的基准测试项生成方法\"\"\"\n        if use_cache:\n            cached_results = self._load_from_cache()\n            if cached_results is not None:\n                printer.print(\"Loading results from cache...\")\n                return cached_results\n\n        windows = self.sliding_window_pairing(max_token_length=max_window_tokens)\n\n        # 并发处理所有窗口\n        results = []\n        with ThreadPoolExecutor(max_workers=50) as executor:\n            futures = {\n                executor.submit(self.process_window_content, w[\"merged_section_window_content\"]): w\n                for w in windows\n            }\n            for fut in tqdm(as_completed(futures),\n                            total=len(futures),\n                            desc=\"⏳ Processing windows\",\n                            unit=\"win\"):\n                w = futures[fut]\n                parsed = fut.result()\n                if parsed:\n                    results.append({\n                        \"path\": w[\"section_window_path_text\"],\n                        \"merged_section_window_content\": w[\"merged_section_window_content\"],\n                        \"extracted_facts\": parsed\n                    })\n\n        if use_cache:\n            self._save_to_cache(results)\n\n        return results\n\n    def verify_qa_format(self, item: dict) -> bool:\n        \"\"\"验证问答对是否符合格式约束\"\"\"\n        data = {\n            \"question\": item[\"question\"],\n            \"format\": item[\"format\"],\n            \"answer\": extract_boxed_content(item[\"answer\"]),\n        }\n        try:\n            template_str = self.agent.load_template(\"verify_qa_format.txt\")\n            prompt = self.agent.render_template(template_str, data)\n            # 同样使用 safe_chat_and_parse 重试模型验证\n            result = safe_chat_and_parse(self.agent, prompt, model=\"gpt-4o\")\n            printer.log(f\"\\nVerification Result: \\n{result}\\n\")\n            return result.get(\"result\") is True\n        except Exception as e:\n            printer.print(f\"[ERROR] 验证失败 after {settings.max_retries} retries: {e}\")\n            return False\n\n    def parse_tagged_data_to_table(self, entries, csv_path=None):\n        \"\"\"解析模型返回的数据并添加来源模型字段\"\"\"\n        parsed_data = []\n        for entry in entries:\n            # 使用通用方法提取 question 和 format\n            q = extract_tag_content(entry, \"question\")\n            fmt = extract_tag_content(entry, \"constrained_format\")\n            # 提取 answer 并解析 boxed 内容\n            raw_ans = extract_answer_from_response(entry)\n            ans = extract_boxed_content(raw_ans)\n            if q and fmt and ans:\n                parsed_data.append({\n                    \"question\": q,\n                    \"format\": fmt,\n                    \"answer\": ans,\n                    \"source_model\": None,  # aggregate_model_results 会填充\n                })\n        return parsed_data\n\n    def verify_extraction_meaningful(self):\n        # Check if the fact extraction result is meaningful enough and correct.\n        pass\n\n    @classmethod\n    def generate_for_folder(\n        cls,\n        folder_path: str,\n        use_cache: bool = True,\n        max_workers: int = 50,\n        max_window_tokens: int = 1\n    ) -> dict:\n        \"\"\"批量为文件夹中所有 JSON 生成基准测试项，使用缓存并行执行\"\"\"\n        import os\n        from pathlib import Path\n        from tqdm import tqdm\n        from concurrent.futures import ThreadPoolExecutor, as_completed\n\n        if not os.path.isdir(folder_path):\n            printer.print(f\"[ERROR] {folder_path} 不是有效的文件夹路径。\")\n            return {}\n        json_files = sorted(Path(folder_path).glob(\"*.json\"))\n        if not json_files:\n            printer.print(f\"[WARN] 文件夹 {folder_path} 不包含任何 json 文件。\")\n            return {}\n\n        results = {}\n        to_process = []\n        for fp in json_files:\n            bench = cls(str(fp))\n            if use_cache:\n                cached = bench._load_from_cache()\n                if cached is not None:\n                    printer.print(f\"[CACHE] 加载 {fp.name} 缓存\")\n                    results[fp.name] = cached\n                    continue\n            to_process.append(fp)\n\n        def _proc(fp: Path):\n            printer.rule(f\"Processing {fp.name}\")\n            bench = cls(str(fp))\n            return fp.name, bench.generate_benchmark_item(\n                use_cache=use_cache,\n                max_window_tokens=max_window_tokens\n            )\n\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = {executor.submit(_proc, fp): fp for fp in to_process}\n            for fut in tqdm(as_completed(futures),\n                            total=len(futures),\n                            desc=\"⏳ Processing files\",\n                            unit=\"file\"):\n                fp = futures[fut]\n                try:\n                    name, res = fut.result()\n                    results[name] = res\n                except Exception as e:\n                    printer.print(f\"[ERROR] 处理 {fp.name} 失败：{e}\")\n\n        return results\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/workflow.py",
        "src/criticsearch/workflow.py/iterate_traj"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the ReportBenchmark class, setting up necessary attributes and processing input data.\n\n**parameters**: The parameters of this Function.\n· json_input_path: A string representing the path to the input JSON file that contains the data to be processed.\n· user_query: An optional string that allows the user to specify a custom query for generating a report.\n\n**Code Description**: The __init__ method of the ReportBenchmark class is responsible for initializing an instance of the class with specific attributes and processing the input data provided through the json_input_path parameter. \n\nUpon invocation, the method begins by storing the provided json_input_path in the instance variable self.json_path. It then creates an instance of the BaseAgent class, which serves as a foundational component for managing interactions and functionalities related to intelligent agents. The BaseAgent instance is assigned to the self.agent attribute.\n\nNext, the method calls the extractDirectoryTree function, passing the json_input_path to extract a breadth ground truth representation of the data. This function reads the JSON file, filters its contents, and constructs a hierarchical tree structure, which is crucial for understanding the relationships within the data. The resulting tree structure is stored in the self.breadth_gt attribute.\n\nFollowing this, the method utilizes the extractMarkdownContent function to read the JSON file and convert its content into a markdown format. The generated markdown content is stored in the self.article_content attribute. The extract_markdown_sections function is then called to split the markdown content into distinct sections based on header lines, and the resulting sections are stored in the self.sections attribute.\n\nAdditionally, the method calls the extractSectionContentPairs function to extract structured section-content pairs from the JSON file. This structured data is stored in the self.section_content_pairs attribute, which is essential for organizing the report content.\n\nThe user_query attribute is set based on the provided user_query parameter. If no custom query is specified, a default query is generated using the title from the breadth ground truth. This query is intended to guide the report generation process.\n\nLastly, the method initializes a cache directory for storing benchmark results. The cache directory is created if it does not already exist, ensuring that the application has a designated location for caching results.\n\nOverall, the __init__ method establishes the foundational setup for the ReportBenchmark class, ensuring that all necessary data is processed and stored for subsequent operations related to report generation.\n\n**Note**: It is important to ensure that the input JSON file adheres to the expected structure, as the various extraction functions rely on specific keys and formats to process the data correctly. Any deviations from this structure may lead to errors or unexpected results during execution."
      ],
      "code_start_line": 59,
      "code_end_line": 75,
      "params": [
        "self",
        "json_input_path",
        "user_query"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, json_input_path, user_query=None):\n        self.json_path = json_input_path\n        self.agent = BaseAgent()\n        self.breadth_gt = extractDirectoryTree(\n            self.json_path\n        )  # Extract breadth ground truth，得到一个json结构的广度树\n        self.article_content = extractMarkdownContent(self.json_path)\n        self.sections = extract_markdown_sections(self.article_content)\n        self.section_content_pairs = extractSectionContentPairs(json_input_path)\n        self.user_query = (\n            f\"Generate a comprehensive long report about {self.breadth_gt.get('title', '')}\"\n            if user_query is None\n            else user_query\n        )\n        # 添加缓存相关的属性\n        self.cache_dir = Path(\"cache/benchmark_results\")\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/reportbench/extract_ground_truth.py/extractDirectoryTree",
        "src/criticsearch/reportbench/extract_ground_truth.py/extractMarkdownContent",
        "src/criticsearch/reportbench/extract_ground_truth.py/extractSectionContentPairs",
        "src/criticsearch/reportbench/extract_ground_truth.py/extract_markdown_sections"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_get_cache_key",
      "md_content": [
        "**_get_cache_key**: The function of _get_cache_key is to generate a unique identifier for a cache file based on the input file path and user query.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The _get_cache_key function constructs a unique cache key by concatenating the instance's json_path and user_query attributes. This concatenated string is then encoded and hashed using the MD5 algorithm to produce a hexadecimal digest, which serves as a unique identifier for the cache file. The resulting cache key is crucial for ensuring that cached results are correctly associated with specific input parameters, thereby preventing conflicts and ensuring data integrity.\n\nThis function is called by two other methods within the ReportBenchmark class: _load_from_cache and _save_to_cache. \n\n- In _load_from_cache, the cache key generated by _get_cache_key is used to construct the filename of the cache file from which results are loaded. If the cache file exists, the method reads the JSON data from the file and returns it. If the file does not exist, it returns None, indicating that there are no cached results available.\n\n- In _save_to_cache, the cache key is similarly used to create the filename for storing results. The method writes the provided results to a JSON file named after the cache key, ensuring that the results can be retrieved later using the same key.\n\nThe relationship between _get_cache_key and its callers is essential for the caching mechanism implemented in the ReportBenchmark class. By generating a consistent cache key based on the input parameters, the class can efficiently save and retrieve results, optimizing performance and reducing redundant computations.\n\n**Note**: It is important to ensure that both json_path and user_query are properly set before calling _get_cache_key, as they directly influence the uniqueness of the generated cache key.\n\n**Output Example**: A possible return value of the _get_cache_key function could be a string like \"d41d8cd98f00b204e9800998ecf8427e\", which represents the MD5 hash of the concatenated json_path and user_query."
      ],
      "code_start_line": 77,
      "code_end_line": 81,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _get_cache_key(self):\n        \"\"\"生成缓存文件的唯一标识\"\"\"\n        # 使用输入文件路径和查询作为缓存key的基础\n        content = f\"{self.json_path}_{self.user_query}\"\n        return hashlib.md5(content.encode()).hexdigest()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_load_from_cache",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_save_to_cache"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_load_from_cache",
      "md_content": [
        "**_load_from_cache**: The function of _load_from_cache is to load the results from a cached file if available.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**:  \nThe _load_from_cache method is designed to retrieve previously stored data from a cache, if it exists. It constructs the path to the cache file using the cache directory (accessible via `self.cache_dir`) and a cache key generated by the _get_cache_key method. This cache key is used to create the filename of the cache file in JSON format.\n\n1. **Cache File Path**: The function first constructs the path to the cache file by calling `self._get_cache_key()` to generate a unique key, then appends this key to the cache directory path. The final file path is represented as `cache_file = self.cache_dir / f\"{self._get_cache_key()}.json\"`.\n   \n2. **Checking Cache Existence**: The function checks if the cache file exists using the `exists()` method. If the file exists, the cache data is read.\n\n3. **Reading the Cache**: If the cache file is found, the function opens the file in read mode (`'r'`) with UTF-8 encoding and loads the contents of the file using the `json.load()` function. The loaded JSON data is then returned.\n\n4. **Return None if No Cache**: If the cache file does not exist, the method returns `None`, indicating that no cached data is available for loading.\n\nThe method is used by other functions in the `ReportBenchmark` class, such as `generate_benchmark_item`. In `generate_benchmark_item`, the method is called with the `use_cache` argument set to `True`. If caching is enabled and valid cached results are found, they are returned immediately, bypassing the need to regenerate the results. If no cache is found, the function proceeds to generate new results and optionally saves them to the cache.\n\n**Note**: This method relies on the successful generation of a cache key by the _get_cache_key method. It is crucial that the cache directory (`self.cache_dir`) is properly initialized and that a valid cache key is generated for this function to operate correctly.\n\n**Output Example**: A possible return value of the _load_from_cache function could be a Python object, such as a list of dictionaries containing cached results, or `None` if no cached data exists. For instance:\n\n```json\n[\n    {\n        \"path\": \"/path/to/section\",\n        \"merged_section_window_content\": \"Example content\",\n        \"extracted_facts\": {\"fact1\": \"value1\", \"fact2\": \"value2\"}\n    }\n]\n```",
        "**_load_from_cache**: The function of _load_from_cache is to load results from a cache file if it is available and contains valid data.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The _load_from_cache function is responsible for retrieving cached results from a JSON file located in the cache directory. It constructs the cache file path using a cache key generated by the _get_cache_key method, which uniquely identifies the cache file based on the instance's input parameters. The cache file is expected to be named in the format \"{cache_key}.json\".\n\nThe function first checks if the cache file exists and whether it is non-empty by examining its size. If the file does not exist or is empty, a message is printed to the console indicating the cache file's status, and the function returns None, signifying that no valid cached results are available.\n\nIf the cache file is present and contains data, the function opens the file in read mode with UTF-8 encoding and loads the JSON content into a variable named data. It then performs an additional check to determine if the loaded data is an empty list. If the data is indeed an empty list, it prints a message indicating that the cache file contains an empty list and returns None.\n\nIf the loaded data passes all checks, it is returned as the output of the function. This function is called by the generate_benchmark_item method within the ReportBenchmark class. In generate_benchmark_item, if caching is enabled, _load_from_cache is invoked to attempt to retrieve previously cached results before proceeding to generate new benchmark items. This mechanism optimizes performance by avoiding redundant computations when valid cached data is available.\n\n**Note**: It is essential to ensure that the cache directory is correctly set up and that the cache key generation method (_get_cache_key) functions properly to facilitate the effective operation of this caching mechanism.\n\n**Output Example**: A possible return value from the _load_from_cache function could be a list of dictionaries representing cached benchmark items, such as:\n\n```json\n[\n    {\n        \"path\": \"/path/to/section\",\n        \"merged_section_window_content\": \"Cached content example\",\n        \"extracted_facts\": [{\"fact1\": \"value1\"}, {\"fact2\": \"value2\"}]\n    }\n]\n```",
        "### Function: `_load_from_cache`\n\n#### Description:\nThe `_load_from_cache` function is responsible for loading cached results from a file, if available and valid. It checks the existence of the cache file and its size to determine if it contains valid data. If the cache file is missing, empty, or contains an empty list, the function returns `None`. Otherwise, it reads the cache file, loads the data from it, and returns the content.\n\n#### Parameters:\nThis function does not accept any parameters.\n\n#### Return Value:\n- **Optional[Any]**: The function returns the cached data if it is valid. If the cache is missing, empty, or contains invalid content (such as an empty list), it returns `None`.\n\n#### Functionality:\n1. **Cache File Identification**: The function first constructs the path to the cache file using the cache directory and the cache key, which is generated by the `_get_cache_key` function.\n   \n2. **File Existence and Size Check**: It checks if the cache file exists and whether its size is greater than 0. If the file is missing or empty, a message is printed, and the function returns `None`.\n\n3. **Loading Cache Data**: If the file exists and is non-empty, the function opens the cache file in read mode, loads the JSON data from it, and performs an additional check to ensure the data is not an empty list.\n\n4. **Empty List Handling**: If the loaded data is an empty list, the function considers it invalid, prints a message, and returns `None`.\n\n5. **Return Data**: If the data is valid, it is returned to the caller.\n\n#### Example Usage:\n```python\ncache_data = self._load_from_cache()\nif cache_data is not None:\n    # Process the loaded cache data\nelse:\n    # Handle the case where cache is not available or invalid\n```\n\n#### Related Functions:\n- **`_get_cache_key`**: This function is used to generate a unique cache key, which is part of the cache file name. The cache key ensures that results are correctly associated with specific inputs.\n- **`generate_benchmark_item`**: This function may call `_load_from_cache` to retrieve cached results when generating benchmark items."
      ],
      "code_start_line": 83,
      "code_end_line": 95,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _load_from_cache(self) -> Optional[Any]:\n        \"\"\"Load results from cache if available and non-empty.\"\"\"\n        cache_file = self.cache_dir / f\"{self._get_cache_key()}.json\"\n        if not cache_file.exists() or cache_file.stat().st_size == 0:\n            printer.print(f\"Cache file {cache_file} is missing or empty.\")\n            return None\n        with cache_file.open('r', encoding='utf-8') as f:\n            data = json.load(f)\n        # 如果缓存内容是空列表，也视为无效\n        if isinstance(data, list) and not data:\n            printer.print(f\"Cache file {cache_file} contains an empty list.\")\n            return None\n        return data\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_benchmark_item",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_for_folder"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_get_cache_key"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_save_to_cache",
      "md_content": [
        "**__save_to_cache**: The function of _save_to_cache is to save the results to a cache file.\n\n**parameters**: The parameters of this Function.\n· results: The data that needs to be stored in the cache file.\n\n**Code Description**: The _save_to_cache function is responsible for saving the results of a computation or process to a cache file. It constructs a file path using the cache directory defined in the instance (`self.cache_dir`), appending a filename generated by the _get_cache_key method. This method ensures that the file is uniquely identified based on the current context. The file is named with the cache key followed by the \".json\" extension.\n\nOnce the cache file path is determined, the function opens the file in write mode with UTF-8 encoding and uses the `json.dump` function to serialize the `results` parameter into a JSON format. The `ensure_ascii=False` option is set to handle non-ASCII characters properly, and `indent=2` is used for pretty-printing the JSON data to make it human-readable.\n\nThis method is called by other parts of the class, particularly in scenarios where data needs to be saved for future use or to avoid redundant computations. One of the main functions calling _save_to_cache is `generate_benchmark_item`. In `generate_benchmark_item`, after generating the benchmark results, the method checks whether caching is enabled. If it is, the results are passed to _save_to_cache for storage.\n\n**Note**: It is important that the cache directory (`self.cache_dir`) is properly defined and accessible before calling this method. Additionally, the cache file name depends on the correct functioning of the _get_cache_key method, which generates the unique identifier for the file. Therefore, the cache key must be correctly generated for the caching process to work as intended.",
        "**_save_to_cache**: The function of _save_to_cache is to save results to a cache file in JSON format.\n\n**parameters**: The parameters of this Function.\n· results: This parameter holds the data (of any type) that needs to be saved to the cache file.\n\n**Code Description**: \nThe `_save_to_cache` function is responsible for saving the given results to a cache file in JSON format. The cache file's name is determined dynamically by calling the `_get_cache_key` method, which generates a unique identifier based on specific input parameters such as the file path and user query. This unique identifier ensures that the cache file is appropriately linked to the corresponding dataset, preventing any conflicts between different sets of results.\n\nThe method constructs the file path by concatenating the cache directory with the cache key, followed by the \".json\" extension. It then opens the cache file in write mode with UTF-8 encoding and uses the `json.dump()` function to write the results to the file. The `ensure_ascii=False` argument ensures that any non-ASCII characters are correctly encoded, while the `indent=2` argument formats the JSON content with an indentation level of 2 spaces for readability.\n\nOnce the data is written to the cache file, the cache file is closed. This functionality supports the efficient storage of benchmark results, ensuring that they can be reloaded later using the same cache key, thereby reducing redundant computations when the same data is needed again.\n\nThe `_save_to_cache` function is called by the `generate_benchmark_item` method, which is designed to generate benchmark items from content windows. After the benchmark items are generated (or retrieved from cache if caching is enabled), they are saved to the cache to improve future performance. \n\nThe cache mechanism is critical in scenarios where performance optimization is necessary, as it avoids reprocessing the same content repeatedly. The integration with `_get_cache_key` ensures that each cache file is uniquely associated with a specific set of input parameters, preventing conflicts between different data sets.\n\n**Note**: It is essential that the cache directory exists and is properly set up before calling this function. Additionally, the correct functioning of `_get_cache_key` is crucial, as it determines the uniqueness of the cache file's name. If either the cache directory or the cache key generation logic is misconfigured, the function may not work as expected."
      ],
      "code_start_line": 97,
      "code_end_line": 103,
      "params": [
        "self",
        "results"
      ],
      "have_return": false,
      "code_content": "    def _save_to_cache(self, results: Any) -> None:\n        \"\"\"Save results to cache.\"\"\"\n        cache_file = self.cache_dir / f\"{self._get_cache_key()}.json\"\n        with cache_file.open('w', encoding='utf-8') as f:\n            json.dump(results, f, ensure_ascii=False, indent=2)\n        with open(cache_file, 'w', encoding='utf-8') as f:\n            json.dump(results, f, ensure_ascii=False, indent=2)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_benchmark_item"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_get_cache_key"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "sliding_window_pairing",
      "md_content": [
        "## `sliding_window_pairing` Function Documentation\n\n### Purpose\nThe `sliding_window_pairing` function is designed to create sliding windows of section content, merging as many sections as possible within a specified token limit. This function ensures that sections are grouped in a way that maximizes content within the constraints of a token limit, which is essential for processing and inputting into models that have token limitations.\n\n### Arguments\n\n- **max_token_length (int, optional)**: \n  - The maximum number of tokens allowed in each window. The default value is 2000 tokens. This argument determines the size of each content window created by the function. Sections will be combined until the token limit is reached.\n\n### Returns\n\n- **List[Dict]**: \n  - A list of dictionaries, where each dictionary represents a merged window of sections. Each window contains:\n    - **highest_title** (str): The title of the highest-level section within the window.\n    - **merged_section_window_content** (str): The combined content of the sections within the window.\n    - **section_window_path** (list): A list of dictionaries representing the hierarchical path of titles and their respective depths within the window.\n    - **section_window_path_text** (str): A string representation of the section path, formatted with depth information.\n    - **section_window_tokens** (int): The total number of tokens in the window, calculated based on the combined content of the sections.\n\n### Functionality\n\n1. **Section Extraction**: \n   The function starts by recursively extracting the titles and content of all sections from the provided data structure (`self.section_content_pairs`). For each section, it records the title, content, depth, path, and token count.\n\n2. **Section Sorting**: \n   The sections are sorted by their depth and title path to ensure that parent sections appear before their children. This sorting helps in maintaining a logical structure when creating the sliding windows.\n\n3. **Sliding Window Creation**: \n   The function then iterates through the sorted sections, creating windows of sections that fit within the specified token limit. It merges sections if the total token count of the merged content does not exceed the `max_token_length`.\n\n4. **Path Formatting**: \n   Each window includes a formatted path that displays the hierarchical structure of the sections within that window. The path is represented as a sequence of titles, with each title preceded by a number of hash (`#`) symbols indicating its depth level.\n\n5. **Token Counting**: \n   The function uses the `count_tokens` function to calculate the number of tokens in each section's content and the combined content of each window. This is crucial for ensuring that the windows respect the token limit.\n\n6. **Window Finalization**: \n   For each window, the function compiles the content and metadata (title, path, and token count) into a dictionary and adds it to the list of windows. The function continues to the next window, ensuring that the token limit is adhered to at all times.\n\n### Example Use Case\n\nThis function is particularly useful in scenarios where content needs to be preprocessed into manageable chunks for model input, such as when dealing with natural language processing models that have token limits (e.g., GPT-3 or GPT-4). The sliding windows allow for efficient content grouping, ensuring that the model receives input without exceeding token constraints.\n\n### Dependencies\n\n- **count_tokens**: The function relies on `count_tokens` to calculate the number of tokens in a section's content. The tokenization is model-specific and ensures that the content remains within acceptable limits for the processing model.\n\n---\n\nThis function provides an efficient method for handling and processing large sets of hierarchical content, especially when working with token-limited systems. By ensuring that content is grouped and tokenized appropriately, it helps optimize the preparation of content for further processing or analysis.",
        "### `sliding_window_pairing` Function Documentation\n\n#### Overview\n\nThe `sliding_window_pairing` function is designed to create sliding windows of section content, aiming to merge as many sections as possible while respecting a maximum token length. This is particularly useful in scenarios where content needs to be processed in manageable chunks, ensuring that each chunk does not exceed a predefined token limit.\n\n#### Parameters\n\n- **`max_token_length`** (int, default = 2000):\n  - This parameter specifies the maximum number of tokens allowed in each window. The function will attempt to group sections together without exceeding this limit.\n\n#### Returns\n\n- **List[Dict]**:\n  - The function returns a list of dictionaries, where each dictionary represents a window. Each window includes the following keys:\n    - **`id`** (str): A unique identifier for the section.\n    - **`title`** (str): The title of the section.\n    - **`content`** (str): The content of the section.\n    - **`depth`** (int): The depth of the section in the hierarchical structure.\n    - **`path`** (list): A list representing the path to the section, with each element containing a dictionary with the title and depth of the section.\n    - **`tokens`** (int): The number of tokens in the section content.\n\n#### Functionality\n\n1. **Section Extraction**:\n   The function recursively extracts section titles, content, and other relevant metadata, such as depth and path. Sections are identified as either dictionaries with specific titles and content or as children of other sections.\n\n2. **Depth Calculation**:\n   The depth of each section is dynamically calculated based on its `id` (if available). If the section has an `id` with a dot notation (e.g., \"4.3.5\"), the function determines its depth by counting the number of segments in the `id`.\n\n3. **Token Counting**:\n   The number of tokens in each section's content is calculated using the `count_tokens` function. This ensures that the function respects the `max_token_length` limit.\n\n4. **Sliding Window Creation**:\n   The sections are grouped into sliding windows, attempting to maximize the number of sections in each window without exceeding the `max_token_length` constraint. This process continues until all sections have been grouped into windows.\n\n#### Example Use Case\n\nThis function can be used when processing large documents that need to be split into smaller, token-limited chunks for further analysis, such as text processing tasks or API calls that have token limits.\n\n#### Notes\n\n- The function handles hierarchical structures by recursively processing children sections and calculating their depth based on their `id`.\n- It ensures that the content does not exceed the specified token length by adjusting the number of sections grouped together in each window.",
        "## Function: `sliding_window_pairing`\n\n### Description\nThe `sliding_window_pairing` function creates sliding windows of section content, attempting to merge as many sections as possible within a specified token limit. This process helps efficiently manage content within a defined size, ensuring the number of tokens does not exceed a predefined threshold.\n\n### Arguments\n\n- **max_token_length** (`int`, optional): The maximum number of tokens allowed per window. By default, this is set to `2000`.\n\n### Returns\n\n- **List[Dict]**: A list of dictionaries, each representing a merged window. Each dictionary contains information about the highest-level title, the merged content, and associated paths.\n\n### Functionality\n1. **Extract Sections**: The function first extracts all available section titles and their respective content.\n   \n2. **Recursive Section Extraction**: A recursive helper function, `extract_sections`, is utilized to traverse and extract the section data, including:\n   - Section title\n   - Section content\n   - The hierarchical depth of the section\n   - The section path for reference\n   \n3. **Depth Calculation**: The depth of each section is determined based on the section ID. For example, a section ID like \"4.3.5\" would imply the section is part of a hierarchical structure, with depth calculated from the number of levels in the ID.\n   \n4. **Window Creation**: Once sections are extracted, the function attempts to merge sections into windows while respecting the `max_token_length` constraint.\n\nThis function is useful in scenarios where large content needs to be broken into manageable sections, such as processing large documents with token limitations for models or other systems that handle textual data."
      ],
      "code_start_line": 105,
      "code_end_line": 247,
      "params": [
        "self",
        "max_token_length"
      ],
      "have_return": true,
      "code_content": "    def sliding_window_pairing(self, max_token_length=2000):\n        \"\"\"\n        创建 section 内容的滑动窗口，尽可能在 token 限制内合并更多的 section。\n        \n        Args:\n            max_token_length: 每个窗口的最大 token 数量\n            \n        Returns:\n            List[Dict]: 一个包含合并窗口的列表，每个窗口包含最高级标题、合并内容、路径等信息\n        \"\"\"\n        # 首先提取所有 section 的标题和内容\n        sections = []\n        \n        def extract_sections(data, path=[], depth=0):\n            \"\"\"递归提取所有 section 的标题、内容、深度和路径\"\"\"\n            if isinstance(data, dict):\n                # 如果是直接的字典对象\n                title = data.get('title', '')\n                content = data.get('content', '')\n                \n                # 获取正确的层级深度\n                current_depth = depth + 1  # 默认深度加1\n                # 如果数据中有明确的id，我们可以从id解析深度\n                section_id = data.get('id', '')\n                if section_id and '.' in section_id:\n                    # 例如 \"4.3.5\" 表示这是第4个主标题下的第3个子标题下的第5个小节\n                    # 那么深度应该是3（4是1级，4.3是2级，4.3.5是3级）\n                    current_depth = len(section_id.split('.'))\n                \n                if title and content:\n                    current_path = path + [{\"title\": title, \"depth\": current_depth}]\n                    sections.append({\n                        \"id\": section_id,                \n                        \"title\": title,\n                        \"content\": content,\n                        \"depth\": current_depth,\n                        \"path\": current_path.copy(),\n                        \"tokens\": count_tokens(content)\n                    })\n                \n                # 检查是否有 children\n                if 'children' in data and data['children']:\n                    current_path = path + [{\"title\": title, \"depth\": current_depth}]\n                    # 递归处理 children，子节点的深度加1\n                    for child in data['children']:\n                        extract_sections(child, current_path, current_depth)\n        \n        # 从 section_content_pairs 中提取所有 section\n        extract_sections(self.section_content_pairs)\n        \n        # 改为按 id 数值序列排序\n        def path_sort_key(section):\n            try:\n                # \"3.2.1\" -> (3,2,1)，\"5\" -> (5,)\n                return tuple(int(x) for x in section.get(\"id\", \"0\").split(\".\"))\n            except ValueError:\n                return (0,)\n            \n        sections.sort(key=path_sort_key)\n        printer.rule(f\"Extracted {len(sections)} sections from the report.\")\n        printer.print(json.dumps(sections, indent=2))\n        \n        \n        # 创建滑动窗口\n        windows = []\n        i = 0\n        while i < len(sections):\n            current_section = sections[i]\n            highest_title = current_section['title']\n            highest_depth = current_section['depth']\n            current_path = current_section['path']\n            \n            # 使用原始深度来设置标题级别\n            merged_content = f\"{'#' * current_section['depth']} {highest_title}\\n{current_section['content']}\"\n            window_tokens = current_section['tokens'] + count_tokens(f\"{'#' * current_section['depth']} {highest_title}\\n\")\n            window_path = [current_section['path'][-1]]\n            \n            # 尝试添加后续的 section，如果它们是当前 section 的子节点或平级节点\n            j = i + 1\n            while j < len(sections) and window_tokens < max_token_length:\n                next_section = sections[j]\n                next_path = next_section['path']\n                \n                # 检查路径关系时使用标题字符串进行比较\n                current_path_titles = [p[\"title\"] for p in current_path]\n                next_path_titles = [p[\"title\"] for p in next_path]\n                \n                # 检查是否为子节点或平级节点\n                is_subsection_or_sibling = False\n                \n                # 如果下一个 section 是当前 section 的子节点\n                if len(next_path_titles) > len(current_path_titles) and all(next_path_titles[k] == current_path_titles[k] for k in range(len(current_path_titles))):\n                    is_subsection_or_sibling = True\n                \n                # 如果下一个 section 是当前 section 的平级节点\n                elif len(next_path_titles) == len(current_path_titles) and all(next_path_titles[k] == current_path_titles[k] for k in range(len(current_path_titles) - 1)):\n                    is_subsection_or_sibling = True\n                \n                if is_subsection_or_sibling:\n                    # 使用原始深度来设置标题级别，不再计算相对深度\n                    section_header = '#' * next_section['depth'] + ' ' + next_section['title'] + '\\n'\n                    additional_tokens = count_tokens(section_header + next_section['content'])\n                    \n                    if window_tokens + additional_tokens <= max_token_length:\n                        # 可以添加到当前窗口\n                        merged_content += f\"\\n{section_header}{next_section['content']}\"\n                        window_tokens += additional_tokens\n                        window_path.append({\"title\": next_section['title'], \"depth\": next_section['depth']})\n                        j += 1\n                    else:\n                        # 超出 token 限制，不能再添加\n                        break\n                else:\n                    # 不是子节点或平级节点，跳过\n                    break\n            \n            def format_path_with_depth(path_nodes):\n                formatted_titles = []\n                for node in path_nodes:\n                    depth = node[\"depth\"]\n                    title = node[\"title\"]\n                    formatted_title = '#' * depth + ' ' + title\n                    formatted_titles.append(formatted_title)\n                return ' -> '.join(formatted_titles)\n            \n            path_text = format_path_with_depth(window_path)\n            window = {\n                \"highest_title\": highest_title,\n                \"merged_section_window_content\": merged_content,\n                \"section_window_path\": window_path,\n                \"section_window_path_text\": path_text,\n                \"section_window_tokens\": window_tokens\n            }\n            \n            windows.append(window)\n            \n            # 下一个窗口的起始位置\n            i = j if j > i + 1 else i + 1\n        \n        printer.rule(f\"Created {len(windows)} sliding windows.\")\n        printer.print(json.dumps(windows, indent=2))\n                \n        return windows\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_benchmark_item"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/count_tokens"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "extract_sections",
      "md_content": [
        "**extract_sections**: The function of extract_sections is to recursively extract the titles, content, depth, and path of all sections in a given hierarchical data structure.\n\n**parameters**:\n· data: The input data which can be a dictionary containing the details of a section, including title, content, id, and potentially nested child sections.\n· path: A list used to track the hierarchical path of sections (default is an empty list).\n· depth: An integer representing the current depth of recursion within the section hierarchy (default is 0).\n\n**Code Description**: \nThe `extract_sections` function is designed to recursively traverse a hierarchical data structure, typically representing sections of a document or report, extracting specific details about each section such as its title, content, depth, and hierarchical path. It processes the data in a depth-first manner, identifying and handling nested sections (children) as it proceeds.\n\n1. **Base Case (Dictionary Check)**: The function first checks if the input `data` is a dictionary. This is necessary because each section is expected to be represented as a dictionary, containing specific keys such as `'title'`, `'content'`, and `'id'`. If the data is not a dictionary, the function does not process it, and the recursion moves back.\n\n2. **Extracting Section Information**: For each valid section dictionary, the function extracts the title and content of the section. If the title and content are present, the function then calculates the depth of the section in the hierarchy. The depth is incremented by 1 for each recursive call, which represents moving one level deeper into the section structure. Additionally, if the section has an `id`, the depth can be recalculated based on the format of the `id`. For example, an `id` like \"4.3.5\" indicates the section is at depth 3, since \"4\" is level 1, \"4.3\" is level 2, and \"4.3.5\" is level 3.\n\n3. **Storing Section Data**: If both the title and content of the section are found, the function stores the section's information in the `sections` list. Each entry in this list includes the title, content, calculated depth, the path of the section (a list of parent sections leading up to this one), and the number of tokens in the content. The `count_tokens` function is called to determine the token count of the section's content, which can be used for various purposes, such as ensuring that sections do not exceed token limits when processed by AI models.\n\n4. **Processing Children Sections**: After handling the current section, the function checks if the section contains any child sections under the `'children'` key. If there are children, the function recursively calls `extract_sections` for each child, passing along the updated path and depth. This allows the function to extract and process nested sections, ensuring that all levels of the hierarchy are traversed and their details are captured.\n\n5. **Path and Depth**: The `path` parameter is used to keep track of the section’s place in the hierarchy. As the function recurses, it appends the current section's title and depth to the path. This helps in maintaining a record of where in the document structure the current section resides. The depth provides insight into how deep the current section is in the hierarchy.\n\n**Note**:\n- The `extract_sections` function relies on the presence of certain keys (`'title'`, `'content'`, `'id'`, and `'children'`) in the section data. If these keys are missing or structured differently, the function may not behave as expected.\n- The function is recursive and can handle deeply nested structures. However, it assumes that the input `data` is a dictionary and that the sections are consistently formatted with the expected fields.\n- The function uses the `count_tokens` function to calculate the number of tokens in each section's content. The `count_tokens` function itself relies on the `tiktoken` library, and the results may vary depending on the model used for tokenization.\n- The `sections` list, which stores the extracted data, is assumed to be defined outside of this function. It holds the processed sections for further use or analysis.",
        "**extract_sections**: The function of extract_sections is to recursively extract the title, content, depth, and path of all sections from a given data structure.\n\n**parameters**: The parameters of this function.\n· data: A dictionary representing the section data, which may include fields like 'title', 'content', and 'children'. This is the primary data structure from which sections are extracted.\n· path: A list representing the current path of section titles and their respective depths in the recursive structure. It is used to keep track of the path traversed so far. The default is an empty list.\n· depth: An integer representing the current depth in the hierarchical structure of sections. The default is 0.\n\n**Code Description**: The `extract_sections` function is designed to recursively process a nested dictionary structure containing hierarchical section data. The function performs the following steps:\n\n1. **Initial Checks and Data Extraction**: \n   - The function begins by verifying if the current data is a dictionary.\n   - It then attempts to extract the 'title' and 'content' fields from the dictionary. If these fields are present, they represent the title and content of the current section.\n   \n2. **Depth Calculation**: \n   - The depth of the current section is determined. Initially, it is set to the given depth value incremented by one.\n   - If the section contains an 'id' field and that ID follows a dot-separated format (e.g., \"4.3.5\"), the depth is recalculated based on the number of parts in the ID. This ensures that the section's depth reflects its position in the hierarchy accurately.\n\n3. **Section Extraction and Recording**:\n   - If both the 'title' and 'content' are present, the section’s information is recorded. A dictionary containing the section's 'id', 'title', 'content', 'depth', and the current path is appended to the `sections` list. Additionally, the number of tokens in the 'content' is calculated by calling the `count_tokens` function (as described in the related `count_tokens` documentation).\n\n4. **Recursive Traversal of Children**:\n   - The function checks if the current section has any children by verifying if a 'children' field exists and is non-empty.\n   - If children exist, the function recursively calls `extract_sections` on each child, passing the updated path and incremented depth to ensure the path and depth are properly tracked for each child section.\n\nThis function is integral for processing hierarchical section data in a structured manner, extracting detailed information about each section, including its title, content, depth, and path. The result is a comprehensive list of sections, which can then be used for further processing or analysis. \n\nThe `extract_sections` function relies on the `count_tokens` function to calculate the number of tokens in the content of each section, a crucial step for handling text-based data within token-limited environments, such as those involving machine learning models or document processing pipelines. The function efficiently handles nested structures by leveraging recursion, ensuring all sections and their sub-sections are processed appropriately.\n\n**Note**:\n- The function assumes that the input data is structured in a specific way, where each section is a dictionary with optional fields such as 'title', 'content', 'id', and 'children'.\n- The depth calculation based on the section ID (e.g., \"4.3.5\") is only applicable when the ID field follows a specific hierarchical format. If no ID is provided, the function uses a default depth calculation.\n- The recursive nature of the function means that it is well-suited for processing deeply nested section data, but care must be taken with very large or deeply nested structures to avoid potential recursion limits.\n",
        "**extract_sections**: The function of extract_sections is to recursively extract all section titles, content, depth, and paths from a given data structure.\n\n**parameters**: The parameters of this Function.\n· data: A dictionary representing a section, which may contain keys such as 'title', 'content', 'id', and 'children'.\n· path: A list that tracks the path of titles and their respective depths as the function recurses through the sections. It defaults to an empty list.\n· depth: An integer representing the current depth in the hierarchy of sections. It defaults to 0.\n\n**Code Description**: The extract_sections function is designed to navigate through a hierarchical data structure, typically representing sections of a document or report. It extracts relevant information such as titles, content, and their respective paths and depths. The function begins by checking if the input data is a dictionary, which is expected for a section. If so, it retrieves the 'title' and 'content' from the dictionary. \n\nThe function calculates the current depth of the section. By default, it increments the depth by one for each level of recursion. If the section has an 'id' that includes periods (e.g., \"4.3.5\"), the function determines the depth based on the number of segments in the 'id'. This allows for a more accurate representation of the section's position within the hierarchy.\n\nIf both 'title' and 'content' are present, the function constructs a current path that includes the title and its depth, and appends a dictionary containing the section's 'id', 'title', 'content', 'depth', 'path', and the token count of the content (calculated using the count_tokens function). The count_tokens function is crucial here as it provides the number of tokens in the content, which is important for managing input sizes for models that have token limits.\n\nThe function then checks for any 'children' within the current section. If children exist, it recursively calls itself for each child section, passing along the updated path and incremented depth. This recursive approach allows the function to traverse the entire structure, collecting all relevant sections and their details.\n\nThe relationship with the count_tokens function is significant; it ensures that the token count for each section's content is accurately calculated and stored. This is particularly important in contexts where token limits are a concern, such as when preparing data for processing by machine learning models.\n\n**Note**: It is essential to ensure that the input data is structured correctly as a dictionary with the expected keys. The function relies on the presence of 'title', 'content', and 'children' to operate effectively. Additionally, the count_tokens function must be available in the environment for the token counting to function correctly."
      ],
      "code_start_line": 118,
      "code_end_line": 150,
      "params": [
        "data",
        "path",
        "depth"
      ],
      "have_return": false,
      "code_content": "        def extract_sections(data, path=[], depth=0):\n            \"\"\"递归提取所有 section 的标题、内容、深度和路径\"\"\"\n            if isinstance(data, dict):\n                # 如果是直接的字典对象\n                title = data.get('title', '')\n                content = data.get('content', '')\n                \n                # 获取正确的层级深度\n                current_depth = depth + 1  # 默认深度加1\n                # 如果数据中有明确的id，我们可以从id解析深度\n                section_id = data.get('id', '')\n                if section_id and '.' in section_id:\n                    # 例如 \"4.3.5\" 表示这是第4个主标题下的第3个子标题下的第5个小节\n                    # 那么深度应该是3（4是1级，4.3是2级，4.3.5是3级）\n                    current_depth = len(section_id.split('.'))\n                \n                if title and content:\n                    current_path = path + [{\"title\": title, \"depth\": current_depth}]\n                    sections.append({\n                        \"id\": section_id,                \n                        \"title\": title,\n                        \"content\": content,\n                        \"depth\": current_depth,\n                        \"path\": current_path.copy(),\n                        \"tokens\": count_tokens(content)\n                    })\n                \n                # 检查是否有 children\n                if 'children' in data and data['children']:\n                    current_path = path + [{\"title\": title, \"depth\": current_depth}]\n                    # 递归处理 children，子节点的深度加1\n                    for child in data['children']:\n                        extract_sections(child, current_path, current_depth)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/utils.py/count_tokens"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "path_sort_key",
      "md_content": [
        "**path_sort_key**: The function of path_sort_key is to generate a sorting key based on the length and titles of a given path in a section.\n\n**parameters**: The parameters of this Function.\n· section: A dictionary containing a key 'path', which is a list of dictionaries, each having a 'title' key.\n\n**Code Description**: The path_sort_key function takes a single parameter, section, which is expected to be a dictionary. This dictionary must contain a key named 'path', which should be a list of dictionaries. Each dictionary in this list must have a 'title' key. The function first calculates the length of the path by determining the number of elements in the section['path'] list. It then constructs a list of titles by extracting the 'title' from each dictionary in the path list. Finally, the function returns a tuple consisting of the path length and a tuple of the titles. This tuple can be used as a sorting key, allowing for comparison based on the length of the path and the lexicographical order of the titles.\n\n**Note**: It is important to ensure that the 'path' key exists in the section dictionary and that it contains a list of dictionaries with 'title' keys. If these conditions are not met, the function may raise an error.\n\n**Output Example**: An example return value of the function could be (3, ('Title A', 'Title B', 'Title C')) if the section['path'] contained three dictionaries with those titles.",
        "**path_sort_key**: The function of path_sort_key is to convert a section identifier string into a tuple of integers for sorting.\n\n**parameters**: The parameters of this function.\n· section: This is a dictionary object that must contain a key named \"id\". The value associated with this key should be a string representing a path identifier, which may include multiple segments separated by periods (e.g., \"3.2.1\" or \"5\").\n\n**Code Description**: The `path_sort_key` function is used to generate a tuple of integers from a section identifier string (which is expected to be in the \"id\" field of the input dictionary). The function first retrieves the \"id\" value from the provided `section` dictionary. If the \"id\" is not found, the function defaults to \"0\". The string value of the \"id\" is then split by the period (\".\") separator, and each resulting segment is converted into an integer. The function returns a tuple consisting of these integers. If there is an error during the conversion (e.g., a non-numeric value), the function returns a default tuple `(0,)`.\n\nThe main purpose of this function is to convert hierarchical path strings like \"3.2.1\" or \"5\" into a format suitable for sorting, where each segment of the path corresponds to an integer value in the tuple. This enables numerical sorting of path components.\n\n**Note**: \n- The function assumes that the `section` parameter is a dictionary and that the \"id\" key exists within it, although a default value of \"0\" is used if it is missing.\n- If any part of the \"id\" string is not a valid integer, the function will catch the exception and return the default value `(0,)`.\n- The function is intended to support path-like strings consisting of numeric segments separated by periods.\n\n**Output Example**: \n- For an input `section = {\"id\": \"3.2.1\"}`, the output will be `(3, 2, 1)`.\n- For an input `section = {\"id\": \"5\"}`, the output will be `(5,)`.\n- For an input `section = {\"id\": \"invalid.id\"}`, the output will be `(0,)` due to the ValueError exception being caught."
      ],
      "code_start_line": 156,
      "code_end_line": 161,
      "params": [
        "section"
      ],
      "have_return": true,
      "code_content": "        def path_sort_key(section):\n            try:\n                # \"3.2.1\" -> (3,2,1)，\"5\" -> (5,)\n                return tuple(int(x) for x in section.get(\"id\", \"0\").split(\".\"))\n            except ValueError:\n                return (0,)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "format_path_with_depth",
      "md_content": [
        "**format_path_with_depth**: The function of format_path_with_depth is to format a list of path nodes into a string representation that visually indicates the depth of each node.\n\n**parameters**: The parameters of this Function.\n· path_nodes: A list of dictionaries, where each dictionary represents a node containing a \"depth\" key indicating the node's depth level and a \"title\" key representing the node's title.\n\n**Code Description**: The format_path_with_depth function takes a list of path nodes as input. Each node in the list is expected to be a dictionary with two specific keys: \"depth\" and \"title\". The function initializes an empty list called formatted_titles to store the formatted titles of each node. It then iterates over each node in the path_nodes list. For each node, it retrieves the depth and title values. The depth value is used to determine the number of '#' characters to prepend to the title, effectively creating a visual representation of the node's depth in a hierarchical structure. The formatted title is constructed by concatenating the '#' characters, a space, and the title. This formatted title is then appended to the formatted_titles list. After processing all nodes, the function joins the formatted titles using ' -> ' as a separator and returns the resulting string. This output provides a clear and structured representation of the nodes, indicating their relative depths.\n\n**Note**: It is important to ensure that each node in the path_nodes list contains both the \"depth\" and \"title\" keys to avoid KeyError exceptions during execution. The depth value should be a non-negative integer to maintain the intended formatting.\n\n**Output Example**: For an input of path_nodes = [{'depth': 1, 'title': 'Node A'}, {'depth': 2, 'title': 'Node B'}, {'depth': 1, 'title': 'Node C'}], the function would return the string: \"# Node A -> ## Node B -> # Node C\"."
      ],
      "code_start_line": 221,
      "code_end_line": 228,
      "params": [
        "path_nodes"
      ],
      "have_return": true,
      "code_content": "            def format_path_with_depth(path_nodes):\n                formatted_titles = []\n                for node in path_nodes:\n                    depth = node[\"depth\"]\n                    title = node[\"title\"]\n                    formatted_title = '#' * depth + ' ' + title\n                    formatted_titles.append(formatted_title)\n                return ' -> '.join(formatted_titles)\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "run_factualqa",
      "md_content": [
        "## Function Documentation: `run_factualqa`\n\n### Purpose:\nThe `run_factualqa` function is responsible for executing the FactualQA evaluation. It loads a template for FactualQA, populates it with relevant data (such as a user query and ground truth values), and then interacts with a chat system to generate a response based on the template.\n\n### Parameters:\nThis function does not accept any parameters.\n\n### Method Overview:\n1. **Template Loading**: The function begins by loading a template file named `\"factual_qa.txt\"` using the `load_template` method from the `agent` object.\n2. **Data Preparation**: It prepares a dictionary containing the following key-value pairs:\n   - `Query`: This is the user query, fetched from the `user_query` attribute.\n   - `BreadthGT`: The breadth ground truth, converted to a JSON string using `json.dumps`.\n   - `DepthGT`: The depth ground truth, directly retrieved from the `depth_gt` attribute.\n3. **Template Rendering**: The template loaded in step 1 is then rendered with the data dictionary using the `render_template` method.\n4. **Chat Interaction**: The rendered template is passed to the `common_chat` method, which processes the prompt and returns a response.\n5. **Return Value**: The function returns the response generated from the chat interaction.\n\n### Key Functions Involved:\n- **`load_template`**: Loads the template file `\"factual_qa.txt\"` from the predefined prompts directory.\n- **`render_template`**: Renders the loaded template by replacing placeholders with values from the data dictionary.\n- **`common_chat`**: Interacts with the chat system using the rendered template and returns the system's response.\n\n### Return:\nThe function returns the response generated from the `common_chat` method, which is the result of processing the prompt for the FactualQA evaluation.\n\n### Example:\n```python\nresponse = run_factualqa()\n```\n\nIn this example, `response` will contain the output from the chat system after the factual QA evaluation. The response will be based on the user query, breadth ground truth, and depth ground truth provided in the function.",
        "**run_factualqa**: The function of run_factualqa is to load a template for FactualQA evaluation, render it with specific data, and generate a response from a conversational model.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The run_factualqa function is a method within the ReportBenchmark class that is designed to facilitate the evaluation of FactualQA by preparing a prompt for a conversational model. The function does not take any parameters directly, as it relies on instance variables defined within the class.\n\nThe function begins by loading a template file named \"factual_qa.txt\" using the load_template method from the BaseAgent class. This method retrieves the content of the specified template file, which is expected to be located in a predefined directory. The loaded template serves as a basis for constructing the prompt that will be sent to the conversational model.\n\nNext, the function constructs a data dictionary that includes:\n- \"Query\": This is populated with the value of self.user_query, which represents the user's input query for the evaluation.\n- \"BreadthGT\": This is a JSON string representation of self.breadth_gt, which likely contains ground truth data related to the breadth of the query.\n- \"DepthGT\": This is directly assigned the value of self.depth_gt, which presumably contains ground truth data related to the depth of the query.\n\nThe constructed data dictionary is then used to render the template string using the render_template method from the BaseAgent class. This method replaces placeholders in the template with the actual values from the data dictionary, resulting in a fully formatted prompt.\n\nAfter rendering the prompt, the function calls the chat method from the BaseAgent class, passing the generated prompt as the usr_prompt argument. This method is responsible for sending the prompt to the conversational model and receiving a response based on the input provided.\n\nFinally, the function returns the response generated by the chat method, which encapsulates the output from the conversational model based on the FactualQA evaluation prompt.\n\nThe run_factualqa function is integral to the evaluation process, as it effectively bridges the gap between user queries, template rendering, and conversational model interaction.\n\n**Note**: It is important to ensure that the template file \"factual_qa.txt\" exists in the specified directory, and that the instance variables self.user_query, self.breadth_gt, and self.depth_gt are properly initialized before invoking this function to avoid runtime errors.\n\n**Output Example**: A possible return value from the run_factualqa function could be a string containing the model's response, such as:\n\"Based on the provided query, the relevant information is as follows: [details].\""
      ],
      "code_start_line": 249,
      "code_end_line": 260,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def run_factualqa(self):\n        # Load and render a template \"factual_qa.txt\" for FactualQA evaluation.\n        # Pass Query, BreadthGT (converted to JSON string) and DepthGT.\n        template_str = self.agent.load_template(\"factual_qa.txt\")\n        data = {\n            \"Query\": self.user_query,  # Updated from self.query to self.user_query\n            \"BreadthGT\": json.dumps(self.breadth_gt),\n            \"DepthGT\": self.depth_gt,\n        }\n        prompt = self.agent.render_template(template_str, data)\n        response = self.agent.chat(usr_prompt=prompt)\n        return response\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "process_section_with_models",
      "md_content": [
        "### `process_section_with_models`\n\n#### Description:\nThe `process_section_with_models` function is responsible for processing a section of text using multiple models in parallel. It interacts with an agent to perform fact extraction by submitting a request to each model and receiving a response. The function ensures that all responses are properly parsed and verified, before aggregating and returning the final results.\n\n#### Parameters:\n- **section_text** (`str`): A string containing the section of text to be processed by the models.\n- **models** (`list`): A list of model identifiers (strings). These models will be used to process the provided `section_text`.\n\n#### Returns:\n- **List**: A list of dictionaries where each dictionary contains the model name and the processed data, ensuring that the data is correctly parsed and verified.\n\n#### Functionality:\n1. **Model Processing**: \n   The function defines a helper function, `process_with_model`, which is responsible for handling the interaction with a single model. For each model in the `models` list, it loads a template, prepares a prompt, and sends it to the model for a response.\n   \n2. **Error Handling**:\n   If any error occurs during the model processing (such as template rendering or communication with the model), it is caught, and an error message is printed, logging the exception details.\n\n3. **Parallel Execution**:\n   The processing is performed in parallel using a `ThreadPoolExecutor`, which allows multiple models to process the same text concurrently. This is achieved by submitting each model processing task to the executor and using `as_completed` to handle the results as they become available.\n\n4. **Results Filtering**:\n   The results from the models are collected and filtered to exclude any `None` values (i.e., unsuccessful model responses).\n\n5. **Data Verification**:\n   Each result is further verified by checking if the parsed data adheres to the expected format. If any data does not pass the verification, it is excluded from the final results.\n\n6. **Aggregation and Deduplication**:\n   The results are aggregated and duplicates are removed, ensuring that only unique data, based on question and answer hashes, is retained. This is achieved through the `aggregate_model_results` function.\n\n#### Usage:\nThis function is useful when you need to gather responses from multiple models for the same text and require parallel processing to optimize performance. It is typically used in scenarios where multiple perspectives on the same content are needed, and the data needs to be verified and aggregated into a final, unified result.\n\n#### Example:\n\n```python\n# Example of usage\nsection_text = \"This is the section of text that needs to be processed.\"\nmodels = [\"model_1\", \"model_2\", \"model_3\"]\nresults = report_benchmark.process_section_with_models(section_text, models)\n```\n\nIn the example above, the function processes the `section_text` using three models (`model_1`, `model_2`, and `model_3`). The function returns a list of results, where each result is associated with one model's output data, properly verified and aggregated.\n\n#### Notes:\n- The function uses a template file `\"fact_extraction_new.txt\"` for rendering the prompt, so ensure this template is available and properly configured in the environment.\n- The `ThreadPoolExecutor` uses a maximum of 50 workers to process the models concurrently, but this can be adjusted based on system capacity.\n- The `aggregate_model_results` function is critical for removing duplicate responses and ensuring that only unique results are returned.\n\n",
        "### Function: `process_section_with_models`\n\n#### Description:\nThe `process_section_with_models` function processes a given text using multiple models concurrently. The function executes the model processing in parallel and handles potential errors gracefully, ensuring robust performance in extracting and verifying data.\n\n#### Parameters:\n- **section_text** (`str`): The input text or section of the document that will be processed by the models.\n- **models** (`list`): A list of models to be used for processing the provided `section_text`.\n\n#### Returns:\n- **list**: A list of dictionaries containing the processed results from each model. Each dictionary includes the model name and the associated data.\n\n#### Detailed Behavior:\n1. **Model Processing**:\n   The function uses a helper function `process_with_model` to handle the processing of `section_text` for each model. This function:\n   - Loads a template for fact extraction from the file `fact_extraction_new.txt`.\n   - Renders the template with the provided `section_text` and user query.\n   - Prints the model being processed.\n   - Makes a call to the model using `safe_chat_and_parse`, which sends the rendered prompt to the model and parses the response.\n   - If the response is a list, it is parsed into a structured table format.\n   - If the response is not a list, a warning is logged, and the model's result is discarded.\n\n2. **Parallel Execution**:\n   The function utilizes a `ThreadPoolExecutor` to process multiple models concurrently. The `max_workers` parameter is set to 50, allowing up to 50 models to be processed in parallel. Each model's processing result is captured using the `as_completed` method to handle any exceptions that may occur during execution.\n\n3. **Error Handling**:\n   - If an exception occurs during the processing of a model, the error is caught, and an error message is printed with details of the failure.\n   - The function ensures that only successfully processed results are kept, filtering out `None` values.\n\n4. **Data Verification**:\n   After processing the models, the results are verified. Each item in the results is checked using the `verify_qa_format` method to ensure that the data follows the expected structure. If any item does not conform, it is excluded from the final output.\n\n5. **Aggregation and Deduplication**:\n   Once the data is verified, the function aggregates the results from all models, removing any duplicates to ensure a clean and consolidated output.\n\n#### Example Workflow:\n1. The `section_text` and `models` are provided to the function.\n2. Each model processes the text in parallel, with the results being captured and errors handled.\n3. The results are filtered and verified to ensure they conform to the required format.\n4. The function returns a list of unique, verified results from all models.\n\n#### Usage:\nThis function is particularly useful in scenarios where multiple models are available for processing a single piece of text. By running the models concurrently and handling potential failures, the function ensures efficient and reliable text processing, making it suitable for use in automated data extraction pipelines or systems that leverage multiple AI models."
      ],
      "code_start_line": 262,
      "code_end_line": 317,
      "params": [
        "self",
        "section_text",
        "models"
      ],
      "have_return": true,
      "code_content": "    def process_section_with_models(self, section_text: str, models: list) -> list:\n        \"\"\"使用多个模型并行处理同一段文本\"\"\"\n\n        def process_with_model(model: str):\n            try:\n                template_str = self.agent.load_template(\"fact_extraction_new.txt\")\n                data = {\n                    \"wiki_text\": section_text,\n                    \"UserQuery\": self.user_query,\n                }\n                prompt = self.agent.render_template(template_str, data)\n\n                printer.print(f\"Model: {model}\")\n                # 通过 safe_chat_and_parse 完成模型调用 + JSON 解析\n                candidate = safe_chat_and_parse(self.agent, prompt, model)\n                # 如果拿到的是 list，则继续处理\n                if isinstance(candidate, list):\n                    parsed_data = self.parse_tagged_data_to_table(candidate)\n                    return {\"model\": model, \"data\": parsed_data}\n                else:\n                    printer.print(f\"[WARN] 模型 {model} 返回非列表结构: {candidate}\")\n                    return None\n\n            except Exception as e:\n                printer.print(f\"[ERROR] model={model} failed after {settings.max_retries} retries:\\n{e}\")\n                return None\n\n        results = []\n        with ThreadPoolExecutor(max_workers=50) as executor:\n            # submit + as_completed，方便捕获每个 future 的异常\n            futures = {executor.submit(process_with_model, m): m for m in models}\n            for fut in as_completed(futures):\n                model = futures[fut]\n                try:\n                    res = fut.result()  # 这里会抛出未被捕获的异常\n                    results.append(res)\n                except Exception as e:\n                    printer.print(f\"[ERROR] task for {model} failed: {e}\")\n\n        # 过滤掉 None\n        results = [r for r in results if r is not None]\n\n        # 对每个结果进行格式验证\n        verified_results = []\n        for result in results:\n            verified_data = []\n            for item in result[\"data\"]:\n                print(f\"\\n\\nVerifying item: {item}\\n\\n\")\n                if self.verify_qa_format(item):\n                    verified_data.append(item)\n            if verified_data:\n                result[\"data\"] = verified_data\n                verified_results.append(result)\n\n        # 聚合并去重\n        return self.aggregate_model_results(verified_results)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_window_content"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/aggregate_model_results",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/verify_qa_format"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "process_with_model",
      "md_content": [
        "**process_with_model**: The function of process_with_model is to interact with a specified AI model to process a section of text and return structured data based on the model's response.\n\n**parameters**: The parameters of this Function.\n· model: A string representing the name of the AI model to be used for generating a response.\n\n**Code Description**: The process_with_model function is designed to facilitate the interaction between the application and an AI model, specifically for processing a section of text. The function begins by attempting to load a template from a file named \"fact_extraction_new.txt\" using the agent's load_template method. This template serves as a basis for constructing a prompt that will be sent to the AI model.\n\nOnce the template is loaded, the function prepares a data dictionary containing the section text and a user query. This data is then used to render the template into a prompt using the agent's render_template method. The rendered prompt is subsequently sent to the AI model via the agent's chat method, which handles the communication with the model and returns the model's response.\n\nThe function then prints the model name and the response received from the model using the printer's print method. It checks the response to determine if it is a valid list. If the response is not a list and is empty after stripping whitespace, the function returns None. If the response is not empty, it attempts to extract and validate JSON content from the response using the extract_and_validate_json function. If the extraction is successful and the result is a list, the function parses the data into a structured format using the parse_tagged_data_to_table method and returns a dictionary containing the model name and the parsed data.\n\nIn the event of any exceptions during the process, the function captures the exception and prints a detailed error message, including the traceback, to assist in debugging. If the response cannot be parsed or is invalid, the function returns None.\n\nThis function is integral to the overall workflow of the application, as it connects the user queries and section texts with the AI model's capabilities, ensuring that the responses are appropriately structured for further processing.\n\n**Note**: It is essential to ensure that the model parameter corresponds to a valid AI model that the application can interact with. Additionally, the function expects the response from the model to be in a format that can be parsed as JSON. If the response does not meet these criteria, the function may return None or raise an error.\n\n**Output Example**: A possible return value from the process_with_model function could be:\n{\n    \"model\": \"gpt-3.5-turbo\",\n    \"data\": [\n        {\n            \"question\": \"What is the capital of France?\",\n            \"format\": \"text\",\n            \"answer\": \"Paris\",\n            \"source_model\": null\n        }\n    ]\n}",
        "**process_with_model**: The function of process_with_model is to process a given model with a user-defined prompt and return structured data based on the model's response.\n\n**parameters**: The parameters of this Function.\n· model: A string representing the name of the model to be used for generating the response.\n\n**Code Description**: The process_with_model function is designed to interact with a conversational model by utilizing a template for generating prompts. It begins by loading a specific template file named \"fact_extraction_new.txt\" using the load_template method from the BaseAgent class. This template is then populated with data, including the section text and user query, which are passed as a dictionary to the render_template method. The rendered prompt is prepared for the model.\n\nOnce the prompt is generated, the function prints the model name using the print method from the RichPrinter class, providing feedback to the user about which model is being utilized. The function then calls safe_chat_and_parse, passing the agent, the rendered prompt, and the specified model. This function is responsible for invoking the conversational model and parsing the returned JSON response, ensuring that it adheres to the expected format.\n\nIf the response from safe_chat_and_parse is a list, the function proceeds to parse this data into a structured format using the parse_tagged_data_to_table method. This method extracts specific tagged content from the list of entries and organizes it into a table format, returning the parsed data.\n\nIn cases where the response is not a list, a warning message is printed to indicate that the model returned an unexpected structure. If any exceptions occur during the execution of the function, an error message is printed, detailing the model name and the exception encountered.\n\nThe process_with_model function is integral to the workflow of the application, as it bridges the interaction between user queries, template rendering, and model responses. It ensures that the data returned from the model is properly structured for further processing.\n\n**Note**: It is important to ensure that the template file \"fact_extraction_new.txt\" exists in the expected directory. The function also assumes that the model specified is valid and that the user query and section text are appropriately defined. Any exceptions raised during execution should be handled to maintain the robustness of the application.\n\n**Output Example**: A possible return value from the process_with_model function could be a structured dictionary such as:\n```json\n{\n    \"model\": \"gpt-3.5\",\n    \"data\": [\n        {\n            \"question\": \"What is the capital of France?\",\n            \"format\": \"text\",\n            \"answer\": \"The capital of France is Paris.\",\n            \"source_model\": null\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 265,
      "code_end_line": 287,
      "params": [
        "model"
      ],
      "have_return": true,
      "code_content": "        def process_with_model(model: str):\n            try:\n                template_str = self.agent.load_template(\"fact_extraction_new.txt\")\n                data = {\n                    \"wiki_text\": section_text,\n                    \"UserQuery\": self.user_query,\n                }\n                prompt = self.agent.render_template(template_str, data)\n\n                printer.print(f\"Model: {model}\")\n                # 通过 safe_chat_and_parse 完成模型调用 + JSON 解析\n                candidate = safe_chat_and_parse(self.agent, prompt, model)\n                # 如果拿到的是 list，则继续处理\n                if isinstance(candidate, list):\n                    parsed_data = self.parse_tagged_data_to_table(candidate)\n                    return {\"model\": model, \"data\": parsed_data}\n                else:\n                    printer.print(f\"[WARN] 模型 {model} 返回非列表结构: {candidate}\")\n                    return None\n\n            except Exception as e:\n                printer.print(f\"[ERROR] model={model} failed after {settings.max_retries} retries:\\n{e}\")\n                return None\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/reportbench/report_benchmark.py/safe_chat_and_parse",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/parse_tagged_data_to_table"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "aggregate_model_results",
      "md_content": [
        "**aggregate_model_results**: The function of aggregate_model_results is to aggregate results from multiple models, ensuring uniqueness by removing duplicates based on a combination of question and answer hashes.\n\n**parameters**:\n- results: A list of dictionaries, where each dictionary contains the model's output data. Each dictionary must include a \"model\" key identifying the model name and a \"data\" key containing a list of result items, each with a \"question\" and \"answer\".\n\n**Code Description**:\nThe `aggregate_model_results` function processes a list of results obtained from multiple models. It ensures that the returned list contains only unique results by generating a hash for each question-answer pair. This hash is based on the lowercased, stripped strings of the question and answer, which is then used to check if the combination has already been encountered.\n\n1. **Initialization**: The function initializes two variables:\n   - `seen_items`: A set used to track the hashes of the processed question-answer combinations.\n   - `unique_results`: A list to store the final, unique results.\n\n2. **Iterating through Results**: It then iterates through each result in the `results` list:\n   - For each model's result, it accesses the `data` field, which is assumed to be a list of dictionaries containing \"question\" and \"answer\" pairs.\n   - For each item in the result's data, a hash is computed by concatenating the question and answer, followed by creating an MD5 hash from the concatenated string. The hash serves as a unique identifier for that question-answer pair.\n\n3. **Check for Uniqueness**: If the hash of the current item has not been encountered before (i.e., it's not in the `seen_items` set), the item is added to the `unique_results` list, and the hash is added to the `seen_items` set. Additionally, the source model information (model name) is added to the item under the \"source_model\" key.\n\n4. **Return Unique Results**: After processing all items in the results list, the function returns the `unique_results` list, which contains only unique question-answer pairs from the different models.\n\nThis function is directly invoked by the `process_section_with_models` function. After multiple models process the same text, `process_section_with_models` compiles their results and passes them to `aggregate_model_results` to ensure that the returned results are unique. The `process_section_with_models` method handles multiple models running in parallel, collects their outputs, and validates the format before delegating to `aggregate_model_results` for deduplication. Therefore, this function plays a crucial role in ensuring that only distinct, non-redundant results are returned after aggregating the outputs from various models.\n\n**Note**: It is important to ensure that the items in the results list have a consistent structure, with each item containing \"question\" and \"answer\" keys. If any result has an invalid or missing \"data\" field, the function may not behave as expected.\n\n**Output Example**:\nFor example, if the input `results` list contains the following data:\n\n```json\n[\n  {\n    \"model\": \"ModelA\",\n    \"data\": [\n      {\"question\": \"What is AI?\", \"answer\": \"Artificial Intelligence\"},\n      {\"question\": \"What is ML?\", \"answer\": \"Machine Learning\"}\n    ]\n  },\n  {\n    \"model\": \"ModelB\",\n    \"data\": [\n      {\"question\": \"What is AI?\", \"answer\": \"Artificial Intelligence\"},\n      {\"question\": \"What is DL?\", \"answer\": \"Deep Learning\"}\n    ]\n  }\n]\n```\n\nThe function will return:\n\n```json\n[\n  {\"question\": \"What is AI?\", \"answer\": \"Artificial Intelligence\", \"source_model\": \"ModelA\"},\n  {\"question\": \"What is ML?\", \"answer\": \"Machine Learning\", \"source_model\": \"ModelA\"},\n  {\"question\": \"What is DL?\", \"answer\": \"Deep Learning\", \"source_model\": \"ModelB\"}\n]\n```\n\nThis ensures that each unique question-answer pair appears only once in the final result, and the \"source_model\" key is added to indicate which model produced each result."
      ],
      "code_start_line": 319,
      "code_end_line": 336,
      "params": [
        "self",
        "results"
      ],
      "have_return": true,
      "code_content": "    def aggregate_model_results(self, results: list) -> list:\n        \"\"\"聚合多个模型的结果并去重\"\"\"\n        seen_items = set()\n        unique_results = []\n        \n        for result in results:\n            for item in result[\"data\"]:\n                # 创建问题和答案的组合哈希值\n                item_hash = hashlib.md5(\n                    f\"{item['question'].strip().lower()}_{item['answer'].strip().lower()}\".encode()\n                ).hexdigest()\n                \n                if item_hash not in seen_items:\n                    seen_items.add(item_hash)\n                    item[\"source_model\"] = result[\"model\"]  # 添加来源模型信息\n                    unique_results.append(item)\n        \n        return unique_results\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section_with_models"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_window_content",
      "md_content": [
        "**process_window_content**: The function of process_window_content is to process the content of a single window, retrying if the result is empty.\n\n**parameters**: The parameters of this Function.\n· content: A string representing the content of the window to be processed.\n· max_retries: An integer specifying the maximum number of retry attempts if the result is empty. Default is 10.\n\n**Code Description**: The process_window_content function is designed to handle the processing of a specific content window by invoking the process_section method to extract relevant data. It operates within a retry mechanism, allowing for multiple attempts to process the content in case the initial attempts yield no results. \n\nUpon invocation, the function iterates up to max_retries times. In each iteration, it attempts to process the provided content by calling the process_section method. If process_section returns a non-empty result, the function then calls parse_tagged_data_to_table to convert the extracted data into a structured table format. If parse_tagged_data_to_table successfully returns parsed data, this data is returned as the output of process_window_content.\n\nIf the result from process_section is empty, the function logs a message indicating the attempt number and continues to retry until the maximum number of attempts is reached. If all attempts fail, the function returns an empty list to indicate that no valid data could be extracted from the content.\n\nThis function is called by generate_benchmark_item, which is responsible for generating benchmark items with caching support. Within generate_benchmark_item, process_window_content is called for each window of content that is prepared for extraction. The results from process_window_content are collected and returned as part of the final results, which may also be cached for future use.\n\n**Note**: The function relies on the successful execution of both process_section and parse_tagged_data_to_table. If either of these functions encounters an error or returns an empty result, process_window_content will continue to retry until the specified limit is reached. The retry mechanism is crucial for handling transient issues that may arise during data extraction.\n\n**Output Example**: A possible return value from process_window_content could be a list of parsed data entries, such as:\n```python\n[\n    {\"question\": \"What is the capital of France?\", \"format\": \"Geography\", \"answer\": \"\\\\boxed{Paris}\"},\n    {\"question\": \"What is the largest planet?\", \"format\": \"Astronomy\", \"answer\": \"\\\\boxed{Jupiter}\"}\n]\n```  \nIf no valid data is extracted after all retry attempts, the function will return an empty list:\n```python\n[]\n```",
        "### `process_window_content`\n\n#### **Function:**\nThe `process_window_content` function processes content extracted from a sliding window mechanism, designed to handle multiple models and implement a retry mechanism to ensure the successful extraction of relevant facts. It aims to enhance the extraction process by utilizing multiple models, retrying if an attempt fails, and returning extracted data when successful.\n\n#### **Parameters:**\n- **content** (`str`): A string containing the content that needs to be processed. This content typically comes from a section of text within a larger document.\n- **max_retries** (`int`, optional): The maximum number of retry attempts if the initial processing fails. The default value is 10.\n\n#### **Returns:**\n- **List**: A list of dictionaries containing the extracted facts from the processed content. If all retry attempts fail, an empty list is returned.\n\n#### **Detailed Analysis:**\n\n1. **Model Selection:**\n   The function first checks the configuration settings for a key named `extract_models`. If it exists in the settings, it uses the models listed there; otherwise, it defaults to using a single model, `gpt-4o`. This allows flexibility in the choice of models for content extraction, enabling the system to adapt to various configurations.\n\n2. **Retry Mechanism:**\n   The function uses a loop to attempt the processing of content multiple times, with a maximum number of retries specified by the `max_retries` parameter (default is 10). This retry approach ensures that temporary failures (such as network issues or model timeouts) do not result in a permanent failure. If any model processing fails within the retry attempts, the function continues trying with a new attempt until the maximum retry limit is reached.\n\n3. **Parallel Model Processing:**\n   To process the content efficiently, the function relies on the `process_section_with_models` method, which handles the interaction with the multiple models in parallel. This method is responsible for sending the content to the models and receiving their responses. If any model succeeds in processing the content and returns results, those results are returned immediately, ensuring that the system provides data as soon as possible.\n\n4. **Error Handling:**\n   If an exception occurs during the processing (such as a model failure or an issue with the content format), the function catches the exception and retries the operation. The retry mechanism continues until the maximum number of retries (`max_retries`) is exhausted. If all attempts fail, the function returns an empty list to indicate no data could be extracted.\n\n5. **Return Value:**\n   If any models successfully process the content, the extracted data is returned as a list. Each item in the list represents the output from one of the models. If none of the models provide valid results after all retry attempts, the function returns an empty list, signaling that no data was extracted.\n\n#### **Usage:**\nThis function is integral to the extraction process in scenarios where multiple models are used for content analysis, ensuring that the data extraction process can tolerate transient failures and still yield results when possible. It is commonly called in contexts where the content being processed may require the perspective of multiple models and where retrying failed attempts is crucial to obtaining accurate information.\n\n#### **Integration with Other Functions:**\nThe `process_window_content` function is typically used by functions that involve processing content derived from a sliding window approach, such as the `generate_benchmark_item` method. It plays a key role in facilitating the extraction of facts from content windows and ensuring that the extraction process is robust, even in the face of temporary failures.\n\n#### **Example Use Case:**\nThe `process_window_content` function is called during the creation of benchmark items where content windows are extracted from a larger document. Each content window is then processed to extract facts, and the function is tasked with handling retries if any model fails to provide valid results. It works together with caching and sliding window mechanisms to optimize performance.\n\n#### **Error Handling and Resilience:**\nThe retry logic provides fault tolerance for transient errors that might occur during model interaction. This makes the system more resilient and capable of recovering from temporary issues without manual intervention.\n\n#### **Conclusion:**\nIn summary, the `process_window_content` function provides a robust mechanism for processing content using multiple models, with built-in retries to handle intermittent failures. This ensures that the system can reliably extract relevant data even in challenging conditions, contributing to the overall efficiency and accuracy of the benchmarking and data extraction processes.",
        "**process_window_content**: The function of process_window_content is to process the content of a window using multiple models with a retry mechanism to ensure successful extraction of relevant information.\n\n**parameters**: The parameters of this Function.\n· content: A string representing the content of the window that needs to be processed.\n· max_retries: An integer specifying the maximum number of retry attempts to process the content if an error occurs. Default is 10.\n\n**Code Description**: The process_window_content function is a method within the ReportBenchmark class that modifies the existing processing method to utilize multiple models for content extraction. It begins by retrieving the list of models to be used from the settings, defaulting to a single model, \"gpt-4o\", if no models are specified. \n\nThe function then enters a loop that allows for a specified number of retry attempts (max_retries) to process the content. Within each attempt, it calls the process_section_with_models method, which handles the actual processing of the content using the specified models. This method is designed to run multiple models in parallel, capturing their results and handling any exceptions that may arise during processing.\n\nIf the processing is successful and results are obtained, the function returns these results. If all attempts fail, it returns an empty list, indicating that no results could be extracted from the content.\n\nThe process_window_content function is called by the generate_benchmark_item method, which is responsible for generating benchmark items from content windows. In this context, generate_benchmark_item prepares the content windows and invokes process_window_content to extract relevant facts from each window's content. This integration ensures that the extraction process is robust and can handle potential errors effectively.\n\n**Note**: It is important to ensure that the settings contain the appropriate model configurations for optimal performance. Additionally, the max_retries parameter can be adjusted based on the reliability of the models being used.\n\n**Output Example**: A possible return value from the process_window_content function could be a list of dictionaries representing the extracted facts, such as:\n\n```json\n[\n    {\"fact1\": \"value1\"},\n    {\"fact2\": \"value2\"}\n]\n```",
        "### Function: `process_window_content`\n\n#### Description:\nThe `process_window_content` function processes the content of a window, attempting to extract relevant facts using multiple models with a retry mechanism. The function is designed to handle potential failures by retrying the operation up to a specified number of attempts, returning results only if successful. If all retries fail, it returns an empty list.\n\n#### Parameters:\n- **content** (`str`): The content to be processed, typically a segment of text that needs fact extraction.\n- **max_retries** (`int`, optional): The maximum number of retries allowed if the processing fails. Default value is `10`.\n\n#### Returns:\n- **list**: A list of extracted results from the content after processing with the selected models. If all retries fail, an empty list is returned.\n\n#### Detailed Behavior:\n1. **Model Configuration**:\n   - The function retrieves the list of models to be used for processing from the `settings` configuration object. If the `settings` object contains an `extract_models` attribute, its value is used; otherwise, a default model, `\"gpt-4o\"`, is used.\n\n2. **Retry Mechanism**:\n   - The function attempts to process the content multiple times. The number of retries is controlled by the `max_retries` parameter. It will keep retrying until the process is successful or the maximum retry count is reached.\n\n3. **Processing with Multiple Models**:\n   - For each attempt, the function uses the `process_section_with_models` method to process the content with the specified models. This method processes the content in parallel using multiple models, increasing the likelihood of successful fact extraction.\n\n4. **Error Handling**:\n   - If an exception occurs during processing, the function catches the error and retries the operation, as long as the retry count has not been exhausted.\n\n5. **Return Value**:\n   - If processing is successful, the function returns the results extracted by the models. If all attempts fail, it returns an empty list.\n\n#### Example Workflow:\n1. The content to be processed and the maximum retry count are provided as input to the function.\n2. The function attempts to process the content up to `max_retries` times.\n3. If a successful extraction is performed, the results are returned.\n4. If all retry attempts fail, an empty list is returned.\n\n#### Usage:\nThis function is useful in scenarios where it is necessary to process content using multiple models, and where there may be temporary issues preventing successful processing. The retry mechanism ensures robustness, making it suitable for use in tasks such as automated data extraction, where occasional failures are expected and can be resolved through retries.\n\n"
      ],
      "code_start_line": 338,
      "code_end_line": 351,
      "params": [
        "self",
        "content",
        "max_retries"
      ],
      "have_return": true,
      "code_content": "    def process_window_content(self, content, max_retries=10):\n        # 从settings中直接获取extract_models配置\n        models = settings.extract_models if hasattr(settings, 'extract_models') else [\"gpt-4o\"]\n        \n        for attempt in range(max_retries):\n            try:\n                # 使用多个模型并行处理\n                results = self.process_section_with_models(content, models)\n                if results:  # 如果有结果则返回\n                    return results\n            except Exception:\n                continue\n        \n        return []  # 如果所有尝试都失败则返回空列表\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_benchmark_item"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section_with_models"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "generate_benchmark_item",
      "md_content": [
        "**generate_benchmark_item**: The function of generate_benchmark_item is to generate benchmark items with optional caching support.\n\n**parameters**: The parameters of this Function.\n· use_cache: A boolean indicating whether to use cached results. Default is True.\n· max_window_tokens: An integer specifying the maximum number of tokens allowed in each window. Default is 300.\n\n**Code Description**: The generate_benchmark_item function is designed to create benchmark items by processing content windows derived from a sliding window approach. It first checks if caching is enabled and attempts to load results from a cache using the _load_from_cache method. If valid cached results are found, they are returned immediately, which optimizes performance by avoiding redundant computations.\n\nIf no cached results are available or if caching is disabled, the function proceeds to generate new benchmark items. It utilizes the sliding_window_pairing method to create windows of content, ensuring that the content does not exceed the specified token limit defined by max_window_tokens. Each window contains merged section content and associated metadata, such as the path of the sections.\n\nFor each window, the function prepares the content for extraction by collecting the merged section content and its corresponding path. It then processes this content using the process_window_content method, which incorporates a retry mechanism to handle potential transient failures during data extraction. If the processing yields valid results, these are compiled into a final results list that includes the path, merged content, and extracted facts.\n\nOnce the new benchmark items are generated, if caching is enabled, the results are saved to the cache using the _save_to_cache method. This ensures that future calls to generate_benchmark_item can benefit from the cached data, improving efficiency.\n\nThe generate_benchmark_item function is called by the process_single_task function, which is responsible for initializing the benchmark generation process as part of a larger task management system. This integration allows for seamless generation and retrieval of benchmark items within the context of processing user-defined tasks.\n\n**Note**: It is essential to ensure that the cache directory is properly initialized and that the cache key generation method (_get_cache_key) functions correctly for caching to operate as intended. Additionally, the sliding_window_pairing method must be correctly implemented to ensure that content windows are generated accurately within the specified token limits.\n\n**Output Example**: A possible return value from the generate_benchmark_item function could be a list of dictionaries, each representing a benchmark item, such as:\n\n```json\n[\n    {\n        \"path\": \"/path/to/section\",\n        \"merged_section_window_content\": \"Example content\",\n        \"extracted_facts\": [{\"fact1\": \"value1\"}, {\"fact2\": \"value2\"}]\n    }\n]\n```",
        "**generate_benchmark_item**: The function of generate_benchmark_item is to generate benchmark items with optional caching support to optimize performance.\n\n**parameters**: The parameters of this Function.\n· use_cache: A boolean indicating whether to use cached results if available. Default is True.\n· max_window_tokens: An integer specifying the maximum number of tokens allowed in each content window. Default is 300.\n\n**Code Description**: The generate_benchmark_item function is a method within the ReportBenchmark class that facilitates the generation of benchmark items from content windows. It begins by checking if caching is enabled through the use_cache parameter. If caching is enabled and valid cached results are available, it retrieves these results using the _load_from_cache method, which reads from a cache file and returns the cached data. If cached results are found, a message is printed to the console indicating that the results are being loaded from the cache, and the function returns the cached results immediately.\n\nIf caching is not used or no valid cached results are found, the function proceeds to generate new benchmark items. It first calls the sliding_window_pairing method to create a list of content windows, each containing merged sections of text that adhere to the specified max_window_tokens limit. This method organizes the content into manageable segments for further processing.\n\nNext, the function prepares to extract relevant information from each content window. It initializes two lists: window_contents and window_paths, which will store the content and paths of each window, respectively. The function iterates over the generated windows, extracting the merged section content and the corresponding path for each window. \n\nAfter gathering the necessary information, the function processes each window's content using the process_window_content method. This method is responsible for extracting relevant facts from the content, potentially utilizing multiple models and implementing a retry mechanism to ensure successful extraction. The results from this processing are then compiled into a final_results list, which includes the path, merged section content, and extracted facts for each window.\n\nIf caching is enabled, the function saves the generated results to the cache using the _save_to_cache method, which writes the results to a JSON file for future retrieval. Finally, the function returns the final_results list, which contains the benchmark items generated from the content windows.\n\nThe generate_benchmark_item function is called by the process_single_task function, which manages the execution of individual tasks by coordinating interactions between various agents. This integration allows for the systematic generation of benchmark items as part of a larger task processing workflow.\n\n**Note**: It is important to ensure that the cache directory is properly set up and that the cache key generation method (_get_cache_key) functions correctly to facilitate effective caching. Additionally, the max_window_tokens parameter should be set appropriately to balance between content granularity and processing efficiency.\n\n**Output Example**: A possible return value from the generate_benchmark_item function could be a list of dictionaries representing the generated benchmark items, such as:\n\n```json\n[\n    {\n        \"path\": \"/path/to/section\",\n        \"merged_section_window_content\": \"This is the content of the merged section.\",\n        \"extracted_facts\": [{\"fact1\": \"value1\"}, {\"fact2\": \"value2\"}]\n    },\n    {\n        \"path\": \"/path/to/another/section\",\n        \"merged_section_window_content\": \"This is the content of another merged section.\",\n        \"extracted_facts\": [{\"fact3\": \"value3\"}, {\"fact4\": \"value4\"}]\n    }\n]\n```",
        "**generate_benchmark_item**: The function of generate_benchmark_item is to generate benchmark items for processing content windows, with optional caching support.\n\n**parameters**: The parameters of this Function.\n· use_cache: A boolean indicating whether to use cached results when available. Default is True.\n· max_window_tokens: An integer specifying the maximum number of tokens allowed per window. Default is 1.\n\n**Code Description**: The generate_benchmark_item function is designed to create benchmark items from content windows while providing support for caching to enhance performance. When invoked, the function first checks if caching is enabled through the use_cache parameter. If caching is enabled and valid cached results are found, it retrieves these results using the _load_from_cache method and returns them, thus avoiding unnecessary processing.\n\nIf no valid cached results are available, the function proceeds to generate new benchmark items. It does this by calling the sliding_window_pairing method, which organizes the content into manageable windows based on the specified max_window_tokens limit. Each window contains merged content from sections, ensuring that the total token count does not exceed the defined threshold.\n\nThe function then utilizes a ThreadPoolExecutor to process all the generated windows concurrently. For each window, it submits a task to the executor that calls the process_window_content method, which is responsible for extracting relevant facts from the window's content. The results from these processing tasks are collected as they complete, and the function constructs a list of dictionaries containing the path, merged content, and extracted facts for each processed window.\n\nOnce all windows have been processed, if caching is enabled, the function saves the results to the cache using the _save_to_cache method. This caching mechanism is crucial for optimizing performance, as it allows for quicker retrieval of results in future invocations.\n\nThe generate_benchmark_item function is called by various components in the project, including the process_single_task function, which manages the execution of user-defined tasks. This establishes a clear relationship where the benchmark items generated by this function are integral to the overall task processing workflow.\n\n**Note**: When using the generate_benchmark_item function, it is important to ensure that the caching mechanism is properly configured and that the max_window_tokens parameter is set according to the specific requirements of the content being processed. Additionally, developers should be aware of the potential for concurrent processing to impact performance based on the system's capabilities.\n\n**Output Example**: A possible appearance of the code's return value when executing the function could look like this:\n```json\n[\n    {\n        \"path\": \"Section 1\",\n        \"merged_section_window_content\": \"Content of section 1...\",\n        \"extracted_facts\": [{\"fact\": \"Fact 1\"}, {\"fact\": \"Fact 2\"}]\n    },\n    {\n        \"path\": \"Section 2\",\n        \"merged_section_window_content\": \"Content of section 2...\",\n        \"extracted_facts\": [{\"fact\": \"Fact 3\"}, {\"fact\": \"Fact 4\"}]\n    }\n]\n```"
      ],
      "code_start_line": 353,
      "code_end_line": 386,
      "params": [
        "self",
        "use_cache",
        "max_window_tokens"
      ],
      "have_return": true,
      "code_content": "    def generate_benchmark_item(self, use_cache=True, max_window_tokens=1):\n        \"\"\"添加缓存支持的基准测试项生成方法\"\"\"\n        if use_cache:\n            cached_results = self._load_from_cache()\n            if cached_results is not None:\n                printer.print(\"Loading results from cache...\")\n                return cached_results\n\n        windows = self.sliding_window_pairing(max_token_length=max_window_tokens)\n\n        # 并发处理所有窗口\n        results = []\n        with ThreadPoolExecutor(max_workers=50) as executor:\n            futures = {\n                executor.submit(self.process_window_content, w[\"merged_section_window_content\"]): w\n                for w in windows\n            }\n            for fut in tqdm(as_completed(futures),\n                            total=len(futures),\n                            desc=\"⏳ Processing windows\",\n                            unit=\"win\"):\n                w = futures[fut]\n                parsed = fut.result()\n                if parsed:\n                    results.append({\n                        \"path\": w[\"section_window_path_text\"],\n                        \"merged_section_window_content\": w[\"merged_section_window_content\"],\n                        \"extracted_facts\": parsed\n                    })\n\n        if use_cache:\n            self._save_to_cache(results)\n\n        return results\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_for_folder/_proc",
        "src/criticsearch/workflow.py/iterate_traj"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_load_from_cache",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_save_to_cache",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/sliding_window_pairing",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_window_content"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "verify_qa_format",
      "md_content": [
        "**verify_qa_format**: The function of verify_qa_format is to validate whether a question-answer pair conforms to specified format constraints.\n\n**parameters**: The parameters of this Function.\n· item: dict - A dictionary containing the question-answer pair to be validated, which includes keys for \"question\", \"format\", and \"answer\".\n\n**Code Description**: The verify_qa_format function is designed to assess the format of a question-answer pair by interacting with a conversational model. It constructs a data dictionary from the input item, extracting the \"question\", \"format\", and the \"answer\" (processed to extract boxed content using the utils.extract_boxed_content function). \n\nThe function then attempts to validate the format by calling the chat_with_template method from the BaseAgent class, passing it a template file named \"verify_qa_format.txt\" along with the constructed data. This method is responsible for rendering the template and sending the prompt to a conversational model (specifically \"gpt-4o\" in this case) to receive a response.\n\nUpon receiving the response, the function utilizes the extract_and_validate_json function to parse and validate the JSON content returned by the model. If the response indicates a successful validation (i.e., result[\"result\"] is True), it logs a confirmation message and returns True. Conversely, if the validation fails, it logs the failure reason and the original answer, returning False.\n\nIn the event of an exception during the validation process, the function calls the print_exception method from the RichPrinter class to log the error details, ensuring that any issues encountered are communicated effectively.\n\nThe verify_qa_format function is called within the process_section_with_models method of the ReportBenchmark class. In this context, it is used to verify each item in the results obtained from processing a section of text with multiple models. The function ensures that only those items that pass the format validation are included in the final results, thereby maintaining the integrity of the data being processed.\n\n**Note**: It is crucial to ensure that the item dictionary contains all necessary keys (\"question\", \"format\", and \"answer\") to avoid KeyError exceptions. Proper handling of the response from the chat_with_template method is also essential to ensure that valid JSON is returned for further processing.\n\n**Output Example**: A possible return value from the verify_qa_format function could be:\n```\nTrue\n```\nindicating that the question-answer pair is correctly formatted, or:\n```\nFalse\n```\nindicating a failure in validation, along with a logged reason for the failure.",
        "**verify_qa_format**: The function of verify_qa_format is to validate whether a question-answer pair conforms to specified format constraints.\n\n**parameters**: The parameters of this Function.\n· item: dict - A dictionary containing the question-answer pair and its associated format.\n\n**Code Description**: The verify_qa_format function is a method within the ReportBenchmark class that is responsible for validating the format of a question-answer pair. It takes a single parameter, `item`, which is expected to be a dictionary containing the keys \"question\", \"format\", and \"answer\". \n\nThe function begins by constructing a `data` dictionary that extracts the \"question\" and \"format\" directly from the `item`, while the \"answer\" is processed through the `extract_boxed_content` function to retrieve any content enclosed within LaTeX `\\boxed{}` syntax. This preprocessing ensures that the answer is formatted correctly before validation.\n\nNext, the function attempts to load a template file named \"verify_qa_format.txt\" using the `load_template` method from the BaseAgent class. This template is essential for generating a prompt that will be sent to a conversational model for validation. The loaded template is then rendered with the `data` dictionary using the `render_template` method, which replaces placeholders in the template with actual values.\n\nAfter preparing the prompt, the function invokes the `safe_chat_and_parse` method, passing the agent, the rendered prompt, and specifying the model \"gpt-4o\". This method handles the interaction with the conversational model and ensures that the response is valid JSON. If the model returns a successful result, the function logs the verification result using the `log` method from the RichPrinter class and returns a boolean indicating whether the validation was successful.\n\nIn the event of an exception during this process, such as issues with loading the template or parsing the response, the function catches the exception and logs an error message using the `print` method from the RichPrinter class. It then returns `False`, indicating that the validation failed.\n\nThe verify_qa_format function is called by the `process_section_with_models` function within the same ReportBenchmark class. This caller function processes multiple models in parallel and verifies the format of the responses received from each model. By using verify_qa_format, it ensures that only properly formatted question-answer pairs are included in the final results.\n\n**Note**: It is important to ensure that the `item` dictionary contains the required keys (\"question\", \"format\", and \"answer\") to avoid KeyError exceptions. Additionally, the template file \"verify_qa_format.txt\" must be present in the expected directory for the function to operate correctly.\n\n**Output Example**: A possible return value from the verify_qa_format function could be a boolean value indicating the success of the validation, such as:\n```python\nTrue  # Indicates that the question-answer pair is correctly formatted.\n```"
      ],
      "code_start_line": 388,
      "code_end_line": 404,
      "params": [
        "self",
        "item"
      ],
      "have_return": true,
      "code_content": "    def verify_qa_format(self, item: dict) -> bool:\n        \"\"\"验证问答对是否符合格式约束\"\"\"\n        data = {\n            \"question\": item[\"question\"],\n            \"format\": item[\"format\"],\n            \"answer\": extract_boxed_content(item[\"answer\"]),\n        }\n        try:\n            template_str = self.agent.load_template(\"verify_qa_format.txt\")\n            prompt = self.agent.render_template(template_str, data)\n            # 同样使用 safe_chat_and_parse 重试模型验证\n            result = safe_chat_and_parse(self.agent, prompt, model=\"gpt-4o\")\n            printer.log(f\"\\nVerification Result: \\n{result}\\n\")\n            return result.get(\"result\") is True\n        except Exception as e:\n            printer.print(f\"[ERROR] 验证失败 after {settings.max_retries} retries: {e}\")\n            return False\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section_with_models"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/utils.py/extract_boxed_content",
        "src/criticsearch/reportbench/report_benchmark.py/safe_chat_and_parse"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "parse_tagged_data_to_table",
      "md_content": [
        "**parse_tagged_data_to_table**: The function of parse_tagged_data_to_table is to process a list of entries containing tagged data, extract relevant information, and return a filtered collection of parsed data based on specific patterns.\n\n**parameters**:\n· entries: A list of strings representing entries, each containing tagged data.\n· csv_path: An optional parameter that specifies the path to a CSV file where the results could be stored. Default is None.\n\n**Code Description**:  \nThe `parse_tagged_data_to_table` function processes a list of entries where each entry contains tagged data in the form of XML-like tags. The function extracts specific pieces of information from each entry, such as the question, format description, and answer, by searching for the corresponding tags. For each entry, it performs the following steps:\n\n1. It uses regular expressions to extract the text between the `<question>` and `</question>` tags and assigns it to the `question` variable. If no match is found, it defaults to an empty string.\n2. Similarly, it extracts the content between the `<constrained_format>` and `</constrained_format>` tags, storing the result in the `format_desc` variable.\n3. The answer is extracted in the same manner, using the `<answer>` and `</answer>` tags.\n4. Additionally, the function checks if the answer contains a LaTeX-formatted boxed content (indicated by the pattern `\\boxed{...}`). If a match is found, the function adds the corresponding data (question, format description, and answer) to the `parsed_data` list.\n5. Finally, it returns the `parsed_data` list, which contains all the entries that have a boxed answer.\n\nThe `parse_tagged_data_to_table` function is used by the `process_window_content` method, which handles the processing of a single content window and retries the process in case of empty results. When `process_window_content` calls `parse_tagged_data_to_table`, it passes the extracted content (a list of entries) to the function. If the function successfully parses the entries and finds non-empty results, it returns the parsed data for further processing.\n\n**Note**:  \n- The function relies on regular expressions to extract specific patterns from the tagged data. If the expected tags are not properly formatted or missing, the function will return an empty string or skip processing for that entry.\n- The `csv_path` parameter is not currently used within the function, but it can be useful if the caller intends to save the parsed data to a CSV file in the future.\n- The boxed answer (`\\boxed{...}`) is a key criterion for including an entry in the final parsed data. If no boxed content is found, the entry will be excluded.\n\n**Output Example**:  \nFor a list of entries where one of them includes a boxed answer, the return value could look like the following:\n\n```python\n[\n    {\"question\": \"What is 2+2?\", \"format\": \"Basic Arithmetic\", \"answer\": \"4\"},\n    {\"question\": \"What is the square root of 16?\", \"format\": \"Mathematical Expression\", \"answer\": \"\\\\boxed{4}\"}\n]\n```  \n\nThis example shows the format of the returned data, where the second entry includes a boxed answer and thus is included in the parsed results.",
        "**parse_tagged_data_to_table**: The function of `parse_tagged_data_to_table` is to process a list of data entries, extracting specific tagged content and returning a structured list of parsed data.\n\n**parameters**:  \n· **entries** (`list`): A list of data entries where each entry is expected to contain text with specific tags (e.g., \"question\", \"constrained_format\", and \"answer\") to be extracted.  \n· **csv_path** (`str` or `None`): An optional parameter that can specify the path to a CSV file. This parameter is not used within the function but could potentially be implemented in future versions.\n\n**Code Description**:  \nThe `parse_tagged_data_to_table` function is designed to parse a list of entries, each containing tagged data, and extract the relevant information into a structured table format. \n\n1. **Processing Each Entry**: The function loops through each entry in the `entries` list. For each entry, it utilizes three helper functions to extract specific information:\n   - **Extracting the Question**: The function calls `extract_tag_content(entry, \"question\")` to extract the content within the `<question>` tag. This function returns the question string, or an empty string if the tag is not found.\n   - **Extracting the Format**: Similarly, the function calls `extract_tag_content(entry, \"constrained_format\")` to retrieve the content of the `<constrained_format>` tag, which represents the format of the response. Again, an empty string is returned if the tag is missing.\n   - **Extracting and Parsing the Answer**: The function uses `extract_answer_from_response(entry)` to extract the raw answer content from the entry. Then, it applies `extract_boxed_content(raw_ans)` to extract any content wrapped in a LaTeX `\\boxed{}` expression from the answer text.\n\n2. **Filtering Valid Data**: After extracting the content for \"question\", \"constrained_format\", and \"answer\", the function checks if all three values are present. If so, it appends a dictionary to the `parsed_data` list containing the extracted question, format, and answer. The dictionary also includes a `source_model` field set to `None`, which will be populated later by a separate process (e.g., in the `aggregate_model_results` function).\n\n3. **Return Value**: Once all entries have been processed, the function returns the `parsed_data` list, which contains the extracted and structured information from the provided entries.\n\nThe function is primarily used within the broader context of data processing tasks, particularly in the process of aggregating and analyzing model-generated responses, where structured data is required. \n\n**Reference Relationships**:\n- The function relies heavily on three other functions within the utility module: `extract_tag_content`, `extract_answer_from_response`, and `extract_boxed_content`.\n  - **`extract_tag_content`**: Used to extract the content of specific tags (e.g., \"question\" and \"constrained_format\") from the entry text.\n  - **`extract_answer_from_response`**: Used to retrieve the raw answer content from a given entry.\n  - **`extract_boxed_content`**: Used to extract any content inside LaTeX `\\boxed{}` expressions from the raw answer.\n\n**Calling Context**:  \nThe `parse_tagged_data_to_table` function is invoked within the `process_with_model` function. This function is responsible for interacting with an AI model to generate responses based on input data (such as a template and user query). After receiving the model's response, the function attempts to parse the response as JSON. If successful, it passes the parsed response (a list of entries) to `parse_tagged_data_to_table` for structured extraction. The output is then returned as part of the result, along with the model's identity.\n\n**Note**:  \n- The function expects the input data to be formatted with specific tags (`<question>`, `<constrained_format>`, `<answer>`). If these tags are absent or incorrectly formatted, the function may not return valid data.\n- The `csv_path` parameter is currently not utilized but could potentially be used for future enhancements, such as saving the parsed data to a CSV file.\n- The function does not handle errors explicitly. It assumes that the helper functions (`extract_tag_content`, `extract_answer_from_response`, `extract_boxed_content`) will handle any extraction issues. If any tag is missing or malformed, the function might fail to populate the corresponding fields in the output data.\n\nThis function is essential in transforming raw model responses into structured, tabular data for subsequent analysis or aggregation."
      ],
      "code_start_line": 406,
      "code_end_line": 423,
      "params": [
        "self",
        "entries",
        "csv_path"
      ],
      "have_return": true,
      "code_content": "    def parse_tagged_data_to_table(self, entries, csv_path=None):\n        \"\"\"解析模型返回的数据并添加来源模型字段\"\"\"\n        parsed_data = []\n        for entry in entries:\n            # 使用通用方法提取 question 和 format\n            q = extract_tag_content(entry, \"question\")\n            fmt = extract_tag_content(entry, \"constrained_format\")\n            # 提取 answer 并解析 boxed 内容\n            raw_ans = extract_answer_from_response(entry)\n            ans = extract_boxed_content(raw_ans)\n            if q and fmt and ans:\n                parsed_data.append({\n                    \"question\": q,\n                    \"format\": fmt,\n                    \"answer\": ans,\n                    \"source_model\": None,  # aggregate_model_results 会填充\n                })\n        return parsed_data\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section_with_models/process_with_model"
      ],
      "reference_who": [
        "src/criticsearch/utils.py/extract_answer_from_response",
        "src/criticsearch/utils.py/extract_boxed_content",
        "src/criticsearch/utils.py/extract_tag_content"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "verify_extraction_meaningful",
      "md_content": [
        "**verify_extraction_meaningful**: The function of verify_extraction_meaningful is to check if the fact extraction result is meaningful enough and correct.\n\n**parameters**:  \nThis function does not take any parameters.\n\n**Code Description**:  \nThe `verify_extraction_meaningful` function is designed to assess the quality and correctness of fact extraction results. However, the function body is currently empty (`pass` statement), indicating that the actual logic to perform the check has not yet been implemented. Based on the function name, it is intended to verify whether the extracted facts hold enough significance and accuracy, potentially ensuring that the results are useful and reliable for further processing or analysis.  \n\n**Note**:  \nAs it stands, this function serves as a placeholder for the functionality that will eventually assess the meaningfulness of extracted facts. Any future implementation would need to include a method for evaluating the extraction results, potentially comparing them to expected outcomes or verifying their relevance to the context in which they are being used."
      ],
      "code_start_line": 425,
      "code_end_line": 427,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def verify_extraction_meaningful(self):\n        # Check if the fact extraction result is meaningful enough and correct.\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "generate_for_folder",
      "md_content": [
        "**generate_for_folder**: The function of generate_for_folder is to generate benchmark items for all JSON files in a specified folder, utilizing caching and parallel execution.\n\n**parameters**: The parameters of this Function.\n· cls: The class type that the method is associated with, typically used to instantiate objects for processing JSON files.\n· folder_path: A string representing the path to the folder containing JSON files.\n· use_cache: A boolean indicating whether to use cached results for benchmark generation (default is True).\n· max_workers: An integer specifying the maximum number of threads to use for parallel processing (default is 50).\n· max_window_tokens: An integer that sets the maximum number of tokens for window processing during benchmark item generation (default is 1).\n\n**Code Description**: The generate_for_folder function is designed to process all JSON files within a specified directory, generating benchmark items for each file. It begins by verifying if the provided folder path is valid. If the path is not a directory, an error message is printed using the printer.print method, and the function returns an empty dictionary.\n\nNext, the function collects all JSON files in the folder, sorted in order. If no JSON files are found, a warning message is printed, and the function again returns an empty dictionary. The function then initializes an empty results dictionary and a list to hold files that need processing.\n\nFor each JSON file, an instance of the class (cls) is created. If caching is enabled, it attempts to load cached results using the _load_from_cache method. If valid cached data is found, it is added to the results, and the file is skipped for further processing. If no valid cache is found, the file is added to the to_process list.\n\nThe function defines an inner function, _proc, which is responsible for processing each JSON file. This function prints a processing rule message, creates a new instance of the class, and calls the generate_benchmark_item method to generate the benchmark item, returning the file name and the generated result.\n\nUsing a ThreadPoolExecutor, the function processes the files in parallel, submitting tasks for each file in the to_process list. As each task completes, the results are collected. If any errors occur during processing, an error message is printed.\n\nFinally, the function returns a dictionary containing the results of the benchmark item generation for each processed JSON file.\n\nThis method is closely related to the _load_from_cache function, which is called to retrieve cached results, enhancing efficiency by avoiding redundant computations. The generate_benchmark_item method is also invoked to create the benchmark items, ensuring that the generated results are based on the latest data available.\n\n**Note**: It is important to ensure that the folder path provided is valid and contains JSON files for the function to operate correctly. Additionally, the use of caching can significantly improve performance, especially when processing large numbers of files.\n\n**Output Example**: \n```json\n{\n    \"file1.json\": {\n        \"benchmark_item\": \"result1\",\n        \"metadata\": {...}\n    },\n    \"file2.json\": {\n        \"benchmark_item\": \"result2\",\n        \"metadata\": {...}\n    }\n}\n```"
      ],
      "code_start_line": 430,
      "code_end_line": 484,
      "params": [
        "cls",
        "folder_path",
        "use_cache",
        "max_workers",
        "max_window_tokens"
      ],
      "have_return": true,
      "code_content": "    def generate_for_folder(\n        cls,\n        folder_path: str,\n        use_cache: bool = True,\n        max_workers: int = 50,\n        max_window_tokens: int = 1\n    ) -> dict:\n        \"\"\"批量为文件夹中所有 JSON 生成基准测试项，使用缓存并行执行\"\"\"\n        import os\n        from pathlib import Path\n        from tqdm import tqdm\n        from concurrent.futures import ThreadPoolExecutor, as_completed\n\n        if not os.path.isdir(folder_path):\n            printer.print(f\"[ERROR] {folder_path} 不是有效的文件夹路径。\")\n            return {}\n        json_files = sorted(Path(folder_path).glob(\"*.json\"))\n        if not json_files:\n            printer.print(f\"[WARN] 文件夹 {folder_path} 不包含任何 json 文件。\")\n            return {}\n\n        results = {}\n        to_process = []\n        for fp in json_files:\n            bench = cls(str(fp))\n            if use_cache:\n                cached = bench._load_from_cache()\n                if cached is not None:\n                    printer.print(f\"[CACHE] 加载 {fp.name} 缓存\")\n                    results[fp.name] = cached\n                    continue\n            to_process.append(fp)\n\n        def _proc(fp: Path):\n            printer.rule(f\"Processing {fp.name}\")\n            bench = cls(str(fp))\n            return fp.name, bench.generate_benchmark_item(\n                use_cache=use_cache,\n                max_window_tokens=max_window_tokens\n            )\n\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = {executor.submit(_proc, fp): fp for fp in to_process}\n            for fut in tqdm(as_completed(futures),\n                            total=len(futures),\n                            desc=\"⏳ Processing files\",\n                            unit=\"file\"):\n                fp = futures[fut]\n                try:\n                    name, res = fut.result()\n                    results[name] = res\n                except Exception as e:\n                    printer.print(f\"[ERROR] 处理 {fp.name} 失败：{e}\")\n\n        return results\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_load_from_cache"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_proc",
      "md_content": [
        "**_proc**: The function of _proc is to process a file path, create a benchmark item using the contents of the file, and return the name of the file along with the generated benchmark data.\n\n**parameters**: The parameters of this Function.\n· fp: A Path object representing the file path to be processed.\n\n**Code Description**: The _proc function is designed to process a specific file, indicated by the parameter fp, and generate benchmarking data related to its contents. The function begins by printing a message indicating the start of the processing for the provided file path using the `printer.rule` method. The `rule` method helps to visually separate sections of output in the console by printing a formatted line that includes the file name. This message improves the clarity of the console output, making it easier for users to track the process.\n\nAfter printing the message, the function proceeds to instantiate the `ReportBenchmark` class by passing the string representation of the file path (`str(fp)`) to its constructor. This object (`bench`) is likely responsible for generating benchmark-related information based on the file's content.\n\nThe function then calls the `generate_benchmark_item` method on the `bench` object, passing two arguments: `use_cache` and `max_window_tokens`. These parameters control whether cached results are used in the benchmark generation and the maximum number of tokens that can be processed per window, respectively. The function then returns a tuple, consisting of the file name (`fp.name`) and the result of `generate_benchmark_item`.\n\nFrom a broader perspective, the `_proc` function is integral to processing files in a benchmark generation workflow, where each file's contents are analyzed and benchmarked based on configurable parameters like caching and token window size.\n\n**Note**: The `_proc` function relies on the `generate_benchmark_item` method, which itself depends on the correct configuration of caching and token window limits. The `printer.rule` method is also key in improving the clarity of the console output by marking the start of processing for each file. Therefore, ensure that the environment in which this function is used has the necessary configurations for both benchmark generation and output formatting.\n\n**Output Example**: A possible return value when executing the function might look like this:\n```json\n(\n    \"example_file.txt\",\n    [\n        {\n            \"path\": \"Section 1\",\n            \"merged_section_window_content\": \"Content of section 1...\",\n            \"extracted_facts\": [{\"fact\": \"Fact 1\"}, {\"fact\": \"Fact 2\"}]\n        },\n        {\n            \"path\": \"Section 2\",\n            \"merged_section_window_content\": \"Content of section 2...\",\n            \"extracted_facts\": [{\"fact\": \"Fact 3\"}, {\"fact\": \"Fact 4\"}]\n        }\n    ]\n)\n```"
      ],
      "code_start_line": 463,
      "code_end_line": 469,
      "params": [
        "fp"
      ],
      "have_return": true,
      "code_content": "        def _proc(fp: Path):\n            printer.rule(f\"Processing {fp.name}\")\n            bench = cls(str(fp))\n            return fp.name, bench.generate_benchmark_item(\n                use_cache=use_cache,\n                max_window_tokens=max_window_tokens\n            )\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_benchmark_item"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "src/criticsearch/reportbench/tree_comparison.py": [
    {
      "type": "FunctionDef",
      "name": "parse_tree",
      "md_content": [
        "**parse_tree**: The function of parse_tree is to recursively parse a tree-like data structure and organize its elements by their depth level.\n\n**parameters**: The parameters of this Function.\n- node: A dictionary representing a node in the tree, which may contain a 'title' and optionally 'children', a list of child nodes.\n- current_depth: An integer representing the current depth level in the tree being parsed. It defaults to 0.\n- levels: A defaultdict of lists used to store the titles of nodes grouped by their depth levels. It defaults to None.\n\n**Code Description**: The `parse_tree` function processes a tree-like structure represented by a nested dictionary. It starts at a given node (the root of the tree) and recursively explores all its descendants, accumulating the titles of nodes at each depth level.\n\n- If the `levels` parameter is not provided, it initializes it as a `defaultdict` to store lists, which will hold the titles of nodes at different depths.\n- If the node has a 'title', it adds that title to the list corresponding to its current depth (`current_depth`).\n- If the node has a 'children' key, the function recursively calls itself on each child node, increasing the depth by 1.\n- The function eventually returns the `levels` dictionary, which maps each depth level to a list of titles of nodes at that level.\n\nThis function is utilized by the `tree_similarity` function to process and compare two tree-like structures (`std_tree` and `student_tree`). It is called on both the standard tree and the student tree to build depth-based structures of node titles. These structures are then used to calculate the semantic similarity between nodes at corresponding depths, as well as to compute penalties based on the number of nodes at each level. \n\n**Note**: This function assumes that nodes are represented as dictionaries, each potentially containing a 'title' and 'children' key. It is important that the input tree structures follow this format for the function to operate correctly. \n\n**Output Example**: \nFor a tree like:\n\n```\n{\n  'title': 'Root',\n  'children': [\n    {'title': 'Child 1', 'children': []},\n    {'title': 'Child 2', 'children': [{'title': 'Grandchild 1', 'children': []}]}\n  ]\n}\n```\n\nThe output of the `parse_tree` function would be:\n\n```\n{\n  0: ['Root'],\n  1: ['Child 1', 'Child 2'],\n  2: ['Grandchild 1']\n}\n```"
      ],
      "code_start_line": 10,
      "code_end_line": 18,
      "params": [
        "node",
        "current_depth",
        "levels"
      ],
      "have_return": true,
      "code_content": "def parse_tree(node, current_depth=0, levels=None):\n    if levels is None:\n        levels = defaultdict(list)\n    if 'title' in node:\n        levels[current_depth].append(node['title'])\n    if 'children' in node:\n        for child in node['children']:\n            parse_tree(child, current_depth + 1, levels)\n    return levels\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/tree_comparison.py/tree_similarity"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "text_similarity",
      "md_content": [
        "**text_similarity**: The function of text_similarity is to compute the similarity between two text inputs using natural language processing (NLP) techniques.\n\n**parameters**:\n· text1: A string containing the first text to be compared.\n· text2: A string containing the second text to be compared.\n\n**Code Description**:  \nThe `text_similarity` function calculates the similarity between two given text inputs (`text1` and `text2`). This is achieved by processing each text input using a natural language processing model (`nlp`), which converts the text into a document object. The `similarity` method of the `spacy` document object is then used to compute a similarity score, which ranges from 0 to 1, where 1 indicates identical content and 0 indicates no similarity. This function returns the similarity score between the two documents.\n\nIn the broader context of the project, the `text_similarity` function is invoked by the `level_similarity` function. The `level_similarity` function uses `text_similarity` to compute a similarity matrix between two sets of nodes (`nodes_A` and `nodes_B`). These nodes are presumably elements in some data structure, and `level_similarity` calculates the overall similarity between two groups of nodes by comparing each pairwise combination of nodes. The `text_similarity` function is called within nested loops iterating over these nodes, and its output is used to populate a similarity matrix. Once the matrix is constructed, the `linear_sum_assignment` function is applied to optimize the assignment of nodes based on the similarity values, and the final similarity score is computed by summing the similarities of the assigned pairs.\n\nThus, `text_similarity` serves as a key component for comparing individual nodes at the text level within the `level_similarity` function, which ultimately computes a similarity score between entire groups of nodes.\n\n**Note**:  \n- The `text_similarity` function assumes that the `nlp` model (likely a `spaCy` model) has already been initialized before being called.\n- Both `text1` and `text2` must be valid text strings. If either of them is empty or malformed, the function may not behave as expected.\n- The returned similarity score is a floating-point number between 0 and 1, with higher values indicating greater textual similarity.\n\n**Output Example**:  \nFor example, if `text1` is \"Hello, how are you?\" and `text2` is \"Hi, how are you doing?\", the function might return a similarity score like `0.89`, indicating that the two texts are highly similar, but not identical."
      ],
      "code_start_line": 20,
      "code_end_line": 23,
      "params": [
        "text1",
        "text2"
      ],
      "have_return": true,
      "code_content": "def text_similarity(text1, text2):\n    doc1 = nlp(text1)\n    doc2 = nlp(text2)\n    return doc1.similarity(doc2)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/tree_comparison.py/level_similarity"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "level_similarity",
      "md_content": [
        "**level_similarity**: The function of level_similarity is to compute the similarity score between two sets of nodes based on their textual content.\n\n**parameters**: \n· nodes_A: A list of nodes representing the first group of elements to be compared. Each node is expected to contain textual content.  \n· nodes_B: A list of nodes representing the second group of elements to be compared. Each node is also expected to contain textual content.\n\n**Code Description**: The `level_similarity` function calculates a similarity score between two groups of nodes, `nodes_A` and `nodes_B`, by utilizing a similarity matrix derived from the textual content of each node. Initially, the function checks if both input lists are empty; if so, it returns a similarity score of 1.0, indicating a perfect match due to the absence of nodes. \n\nNext, a similarity matrix is created with dimensions corresponding to the lengths of `nodes_A` and `nodes_B`. The function then iterates through each node in `nodes_A` and `nodes_B`, calculating the textual similarity between each pair of nodes using the `text_similarity` function. This function employs natural language processing techniques to derive a similarity score for each pair, which is stored in the similarity matrix.\n\nAfter populating the matrix, the function applies the `linear_sum_assignment` method from the SciPy library to find the optimal assignment of nodes that maximizes the total similarity score. The total similarity score is computed by summing the values in the similarity matrix corresponding to the optimal assignments. Finally, the function normalizes this score by dividing it by the maximum possible value, which is determined by the larger of the two input lists' lengths. If both lists are non-empty, the function returns the computed similarity score; otherwise, it returns 0.\n\nThe `level_similarity` function is called by the `tree_similarity` function, which is responsible for comparing two hierarchical tree structures. Within `tree_similarity`, the function first parses the trees into levels and then computes the similarity for each level using `level_similarity`. This integration allows `tree_similarity` to assess the overall structural and semantic similarity between two trees by evaluating their respective levels.\n\n**Note**: \n- The `text_similarity` function, which is called within `level_similarity`, must be properly initialized with a natural language processing model before use.\n- Both `nodes_A` and `nodes_B` should contain valid text nodes; otherwise, the function may not yield meaningful results.\n- The returned similarity score is a floating-point number between 0 and 1, where higher values indicate greater similarity between the two sets of nodes.\n\n**Output Example**: For instance, if `nodes_A` contains [\"Node A1\", \"Node A2\"] and `nodes_B` contains [\"Node B1\", \"Node B2\"], the function might return a similarity score of `0.75`, indicating a moderate level of similarity between the two groups of nodes."
      ],
      "code_start_line": 25,
      "code_end_line": 35,
      "params": [
        "nodes_A",
        "nodes_B"
      ],
      "have_return": true,
      "code_content": "def level_similarity(nodes_A, nodes_B):\n    if not nodes_A and not nodes_B:\n        return 1.0  # 双方无节点视为完全匹配\n    sim_matrix = np.zeros((len(nodes_A), len(nodes_B)))\n    for i, a in enumerate(nodes_A):\n        for j, b in enumerate(nodes_B):\n            sim_matrix[i][j] = text_similarity(a, b)\n    row_ind, col_ind = linear_sum_assignment(-sim_matrix)\n    total_sim = sim_matrix[row_ind, col_ind].sum()\n    max_possible = max(len(nodes_A), len(nodes_B))\n    return total_sim / max_possible if max_possible > 0 else 0\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/tree_comparison.py/tree_similarity"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/tree_comparison.py/text_similarity"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "tree_similarity",
      "md_content": [
        "**tree_similarity**: The function of tree_similarity is to calculate the semantic and structural similarity score between two hierarchical tree structures.\n\n**parameters**: The parameters of this Function.\n- **std_tree**: A dictionary representing the standard tree structure, which is compared against the student's tree structure.\n- **student_tree**: A dictionary representing the student's tree structure, which is compared to the standard tree structure.\n- **depth_decay**: A float (default is 0.8) that determines the decay factor for the weight assigned to each tree depth. Deeper levels of the tree are given higher weights.\n- **alpha**: A float (default is 0.3) used in the penalty calculation for the difference in the number of nodes between corresponding levels of the trees.\n- **beta**: A float (default is 0.02) that penalizes missing layers in the student's tree compared to the standard tree.\n- **gamma**: A float (default is 0.02) that penalizes redundant layers in the student's tree compared to the standard tree.\n\n**Code Description**: \nThe `tree_similarity` function computes the overall similarity between two tree structures, `std_tree` and `student_tree`. It does so by first parsing both trees into levels using the `parse_tree` function. This process groups the nodes in each tree by their depth level. Then, the function compares these corresponding levels of the two trees by computing their semantic similarity using the `level_similarity` function. It also calculates penalties based on the differences in the number of nodes at each level and the presence of missing or redundant levels in the student's tree.\n\nThe function proceeds through the following steps:\n\n1. **Tree Parsing**: The two input trees (`std_tree` and `student_tree`) are parsed into levels using the `parse_tree` function. This gives each tree a dictionary mapping depth levels to lists of nodes at those depths.\n\n2. **Depth Calculation**: The function determines the maximum depth among the two trees, which will be the range of depths that the function iterates over. The comparison will be done from depth 0 up to the maximum depth in either tree.\n\n3. **Iterative Comparison**: The function iterates over each depth level from 0 to the maximum depth. For each depth:\n   - It checks if the current depth exists in both trees. If a depth exists in one tree but not the other, it counts as either a missing or redundant layer, depending on which tree lacks the depth.\n   - The function calculates a weight for the depth based on the `depth_decay` factor. Deeper levels of the tree receive a higher weight.\n   - For corresponding depths that exist in both trees, the function calculates the semantic similarity between the nodes at that level using the `level_similarity` function.\n   - A penalty is applied for discrepancies in the number of nodes at each depth. If the number of nodes differs between the two trees, a penalty is computed based on the `alpha` factor, which reduces the structure's similarity score.\n\n4. **Final Score Calculation**: After comparing all depths:\n   - The function computes a weighted average of the similarity scores at each depth, adjusting for missing or redundant layers in the student's tree using the `beta` and `gamma` penalties.\n   - The final score is computed by applying the structure penalty to the weighted average score.\n\nThe output is a final similarity score between the two trees, rounded to two decimal places. This score reflects how closely the student's tree structure matches the standard tree both semantically and structurally.\n\nThe `tree_similarity` function is used in the `evaluate_breadth` method of the `ReportEvaluation` class, which evaluates the similarity between the standard and student trees. The standard tree is fetched from `self.report_benchmark.breadth_gt`, and the student's tree is generated using `self.examinees_outline_generation()`. The resulting similarity score is returned as part of the evaluation process.\n\n**Note**:\n- The `parse_tree` function, which is called by `tree_similarity`, is responsible for parsing the tree structure and organizing it by depth levels.\n- The `level_similarity` function, which is used to calculate semantic similarity at each level, relies on comparing the nodes' textual content to compute a similarity score.\n- The parameters `depth_decay`, `alpha`, `beta`, and `gamma` influence how heavily the function weighs different aspects of the tree comparison, including the depth of nodes and the penalties for missing or redundant layers.\n- The returned score is a floating-point value between 0 and 1, with higher values indicating a closer match between the two trees."
      ],
      "code_start_line": 37,
      "code_end_line": 93,
      "params": [
        "std_tree",
        "student_tree",
        "depth_decay",
        "alpha",
        "beta",
        "gamma"
      ],
      "have_return": true,
      "code_content": "def tree_similarity(std_tree, student_tree, depth_decay=0.8, alpha=0.3, beta=0.02, gamma=0.02):\n    # 解析层级\n    std_levels = parse_tree(std_tree)\n    student_levels = parse_tree(student_tree)\n    \n    max_std_depth = max(std_levels.keys(), default=0)\n    max_student_depth = max(student_levels.keys(), default=0)\n    max_depth = max(max_std_depth, max_student_depth)\n    \n    total_score = 0.0\n    total_weight = 0.0\n    missing_layers = 0\n    redundant_layers = 0\n    \n    # 遍历每个可能的层级（从0到最大深度）\n    for depth in range(max_depth + 1):\n        in_std = depth in std_levels\n        in_student = depth in student_levels\n        weight = 1.0 / (depth_decay ** depth)  # 深层权重更高\n        \n        # 层级存在性检查\n        if in_std and not in_student:\n            missing_layers += 1\n            continue  # 学生缺失该层，跳过权重累加\n        elif not in_std and in_student:\n            redundant_layers += 1\n            continue  # 学生多出该层，跳过权重累加\n        \n        # 获取该层节点\n        nodes_std = std_levels.get(depth, [])\n        nodes_student = student_levels.get(depth, [])\n        m, n = len(nodes_std), len(nodes_student)\n        \n        # 计算语义相似度\n        semantic_score = level_similarity(nodes_std, nodes_student)\n        \n        # 节点数量差异惩罚\n        if m == 0:\n            num_penalty = 0  # 标准无节点时不惩罚\n        else:\n            num_diff = abs(m - n)\n            num_penalty = alpha * (num_diff / m)\n        structure_coeff = max(0, 1 - num_penalty)\n        \n        # 层级得分\n        layer_score = semantic_score * structure_coeff\n        total_score += layer_score * weight\n        total_weight += weight\n    \n    # 计算加权平均得分\n    weighted_avg = total_score / total_weight if total_weight > 0 else 0\n    \n    # 总结构惩罚\n    structure_penalty = beta * missing_layers + gamma * redundant_layers\n    final_score = weighted_avg * max(0, 1 - structure_penalty)\n    \n    return round(final_score, 2)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_evaluation.py",
        "src/criticsearch/reportbench/report_evaluation.py/ReportEvaluation/evaluate_breadth"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/tree_comparison.py/parse_tree",
        "src/criticsearch/reportbench/tree_comparison.py/level_similarity"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "src/criticsearch/reportbench/reward_calculation.py": [
    {
      "type": "ClassDef",
      "name": "RewardCalculator",
      "md_content": [
        "**RewardCalculator**: The function of RewardCalculator is to manage and store the details related to the current section's name, its ground truth, and the student's answer for evaluation.\n\n**attributes**: The attributes of this Class.\n- current_section_name: Holds the name of the current section being evaluated or processed.\n- current_section_ground_truth: Contains the ground truth data for the current section, which is used for comparison with the student’s answer.\n- current_section_student_answer: Stores the student's answer for the current section to be evaluated against the ground truth.\n\n**Code Description**:  \nThe `RewardCalculator` class is a simple container for tracking information related to a specific section within a larger evaluation framework. It holds three key attributes:\n1. `current_section_name`: This attribute is used to store the name of the section being worked on or evaluated. It could be a string representing the section's title or identifier, which the model uses to guide its operations.\n2. `current_section_ground_truth`: This attribute holds the correct or expected answer for the section, often referred to as the \"ground truth.\" It is likely to be used for comparison with the student’s answer to determine correctness or performance.\n3. `current_section_student_answer`: This stores the answer provided by the student for the current section. This answer is intended to be evaluated against the `current_section_ground_truth`.\n\nThe class does not have any methods or other functionality defined beyond these attributes. The primary role of this class appears to be to act as a data holder for the current section’s name, the correct answer, and the student's response, which can then be used in further processing, such as scoring or feedback generation.\n\n**Note**:  \n- The class is designed with simple attributes and currently lacks any methods for manipulating or processing the data stored within. It could be extended in the future to include methods for evaluating the student's answer or generating reports based on the ground truth.\n- The attributes are set to `None` by default, indicating that they are initially unassigned. It is expected that the attributes will be set to specific values during the execution of the larger program that utilizes this class."
      ],
      "code_start_line": 3,
      "code_end_line": 7,
      "params": [],
      "have_return": false,
      "code_content": "class RewardCalculator:\n    def __init__(self):\n        self.current_section_name = None  # Guide model to search/generate current section\n        self.current_section_ground_truth = None\n        self.current_section_student_answer = None\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the attributes of the object to their default values.\n\n**parameters**: The __init__ function does not accept any parameters apart from the default 'self' parameter, which refers to the instance of the object being created.\n\n**Code Description**:  \nThis function is the constructor for the class and is called automatically when a new instance of the class is created. It sets the initial values of three instance attributes:\n- `self.current_section_name`: Initialized to `None`. This attribute is intended to hold the name of the current section, which can be used by the guide model to search or generate the relevant section.\n- `self.current_section_ground_truth`: Also initialized to `None`. This attribute is meant to store the ground truth for the current section, possibly representing the correct or expected content for comparison or validation purposes.\n- `self.current_section_student_answer`: Set to `None` initially. This attribute is meant to store the student’s answer for the current section.\n\nThe default assignment of `None` allows these attributes to be dynamically updated later in the program's execution as the object interacts with data related to the current section being processed.\n\n**Note**: This constructor method is essential for initializing the object's state when it is instantiated. The attributes set to `None` allow for flexible handling of data later in the object’s lifecycle."
      ],
      "code_start_line": 4,
      "code_end_line": 7,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.current_section_name = None  # Guide model to search/generate current section\n        self.current_section_ground_truth = None\n        self.current_section_student_answer = None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/reportbench/report_evaluation.py": [
    {
      "type": "ClassDef",
      "name": "ReportEvaluation",
      "md_content": [
        "**ReportEvaluation**: The function of ReportEvaluation is to evaluate a student’s report against a predefined benchmark, using various assessment methods such as breadth, depth, and factual accuracy.\n\n**attributes**: The attributes of this Class.\n· report_benchmark: This attribute holds an instance of the `ReportBenchmark` class, which is responsible for providing the benchmark data used for comparison and evaluation of the student’s report.\n· student_report: This attribute stores the student's report in the form of a string, which will be evaluated based on the comparison with the `ReportBenchmark` data.\n\n**Code Description**: The `ReportEvaluation` class is designed to perform the evaluation of a student's report. It leverages the `ReportBenchmark` instance to obtain ground truth data and perform a series of assessment procedures.\n\n1. **`__init__(self, report_benchmark: ReportBenchmark, student_report: str)`**:\n   This is the constructor method for initializing the `ReportEvaluation` object. It accepts two parameters:\n   - `report_benchmark`: An instance of the `ReportBenchmark` class, which holds necessary information for comparison such as user queries, ground truth data, and the agent used for interaction.\n   - `student_report`: A string containing the student's report that will be evaluated.\n\n   The constructor method assigns these parameters to the corresponding attributes of the `ReportEvaluation` class.\n\n2. **`examinees_outline_generation(self)`**:\n   This method generates the student report outline using the `ReportBenchmark` agent. It loads a template file, \"outline_generation.txt\", and renders it using data from the `ReportBenchmark`, specifically the `user_query`. The method sends the rendered template as a prompt to the `ReportBenchmark` agent and returns the response.\n\n3. **`evaluate_breadth(self)`**:\n   This method evaluates the breadth of the student's report. It generates a student tree by calling the `examinees_outline_generation()` method. Then, it compares the generated tree with the benchmark's breadth ground truth (`breadth_gt`) using the `tree_similarity` function, which calculates a similarity score. The result is returned as the evaluation score for breadth.\n\n4. **`evaluate_factualqa(self)`**:\n   This method performs a factual QA evaluation using the student's report. It loads the \"factual_qa.txt\" template, which is used to assess the factual accuracy of the student's report. The method renders this template using both the benchmark's `user_query` and `breadth_gt`, along with the `student_report`. The rendered prompt is sent to the agent, and the response is returned as the evaluation of the factual accuracy.\n\n5. **`extract_student_tree_structure(self)`**:\n   This method extracts the logical structure of the student's report. It loads the \"student_tree_extraction.txt\" template, which is used to analyze the report's structure. After rendering the template with the `student_report`, the resulting response is parsed into a JSON format representing the student’s tree structure. This allows for further analysis of the report’s organization.\n\n6. **`evaluate_depth(self)`**:\n   This method is meant to evaluate the depth of the student's report. However, the method is not yet implemented. The description suggests that the depth evaluation would be based on factual QA results, using accuracy (ACC) of student report-based answers to compute a final score. The current implementation does not provide details on the exact logic.\n\n**Note**: \n- The methods in this class depend heavily on templates stored and rendered by the `ReportBenchmark` agent.\n- The class provides a flexible structure for evaluating different aspects of a student's report, including outline, breadth, and factual accuracy.\n- The `evaluate_depth` method has not been fully implemented, and thus depth evaluation cannot currently be performed.\n\n**Output Example**:\n- For `evaluate_breadth`, an output might look like:\n  ```json\n  0.85\n  ```\n  This score represents the similarity between the student’s tree structure and the benchmark’s breadth ground truth.\n\n- For `evaluate_factualqa`, an output could be:\n  ```json\n  {\n    \"accuracy\": 0.92,\n    \"feedback\": \"The factual accuracy of the student’s report is 92%.\"\n  }\n  ```\n  This result indicates the factual accuracy score of the student’s report, along with feedback.",
        "**ReportEvaluation**: The function of ReportEvaluation is to evaluate a student report against predefined benchmarks, specifically assessing the breadth, depth, and factual accuracy of the student's work using the provided report benchmarking tools.\n\n**attributes**: The attributes of this Class.\n· report_benchmark: An instance of the ReportBenchmark class, used to access the ground truths for evaluating the student report.  \n· student_report: A string representing the student's report that will be evaluated.\n\n**Code Description**: The `ReportEvaluation` class is designed to assess the quality of a student’s report by comparing it with a benchmark. It is initialized with two primary parameters: a `report_benchmark` object and the `student_report` string.\n\n- The `__init__` method initializes the class with a `report_benchmark` object, which holds various tools such as templates and ground truth data, and a `student_report` string that contains the student's submission. This method effectively prepares the class for later evaluation tasks.\n  \n- The `examinees_outline_generation` method uses the `ReportBenchmark` class's `BaseAgent` to generate an outline or tree structure for the student’s report. It loads the necessary template (`outline_generation.txt`) and uses it to create a prompt with the user query. This prompt is then sent to the agent, which returns a response representing the student’s report tree structure. This method is a key part of generating a student’s report structure that will later be used in evaluations.\n\n- The `evaluate_breadth` method evaluates the breadth of the student report by utilizing the `examinees_outline_generation` method to generate a student report tree. The generated tree is compared with the benchmark's breadth ground truth using a function called `tree_similarity`. This comparison results in a score that indicates how well the student’s report matches the expected breadth of coverage.\n\n- The `evaluate_factualqa` method evaluates the factual accuracy of the student report. It first loads the appropriate template (`factual_qa.txt`) and prepares a data dictionary that includes the user query, breadth ground truth, and the student report. The data is then used to create a prompt, which is sent to the agent. The response received is the factual QA evaluation result for the student report, indicating how factually accurate the report is in relation to the given benchmarks.\n\n- The `extract_student_tree_structure` method extracts the logical tree structure of the student report. By loading the `student_tree_extraction.txt` template and providing the student report, it generates a prompt that, when sent to the agent, returns a JSON representation of the student report’s structural elements. This allows for further processing or evaluation of how the student's report is organized.\n\n- The `evaluate_depth` method is a placeholder function intended for evaluating the depth of the student report. While not yet implemented, it is expected that this method will eventually use factual QA results to assess the accuracy of answers related to the depth of the student report and compute a final score based on the comparison.\n\n**Note**: \n- The `evaluate_depth` method is not yet implemented and may require future development to complete the depth evaluation process.\n- This class relies on external templates such as `outline_generation.txt`, `factual_qa.txt`, and `student_tree_extraction.txt` to generate prompts for the evaluation process, which means the templates should be available and properly configured in the environment.\n- The method `tree_similarity` is used in `evaluate_breadth` to compare the student report’s tree structure with the benchmark’s ground truth; however, the specifics of the `tree_similarity` function are not provided in the current class context.\n\n**Output Example**: \n- The `examinees_outline_generation` method might return a JSON structure like this:\n```json\n{\n  \"student_tree\": [\n    {\"section\": \"Introduction\", \"content\": \"Introduction text here...\"},\n    {\"section\": \"Methodology\", \"content\": \"Methodology text here...\"},\n    {\"section\": \"Conclusion\", \"content\": \"Conclusion text here...\"}\n  ]\n}\n```\n- The `evaluate_breadth` method might return a similarity score, such as:\n```json\n{\n  \"breadth_similarity_score\": 0.85\n}\n```\n- The `evaluate_factualqa` method might return a factual accuracy response like:\n```json\n{\n  \"factual_accuracy_score\": 0.92,\n  \"detailed_feedback\": \"The report accurately represents the core facts from the benchmark.\"\n}\n```\n- The `extract_student_tree_structure` method might return a structured representation like:\n```json\n{\n  \"student_report_structure\": {\n    \"Introduction\": {\"section_start\": 0, \"section_end\": 120},\n    \"Methodology\": {\"section_start\": 121, \"section_end\": 200},\n    \"Conclusion\": {\"section_start\": 201, \"section_end\": 240}\n  }\n}\n```",
        "**ReportEvaluation**: The function of ReportEvaluation is to evaluate student reports against a benchmark using various assessment methods.\n\n**attributes**: The attributes of this Class.\n· report_benchmark: An instance of the ReportBenchmark class that provides ground truths and evaluation metrics.\n· student_report: A string representing the student's report that is to be evaluated.\n\n**Code Description**: The ReportEvaluation class is designed to facilitate the evaluation of student reports by comparing them against established benchmarks. It utilizes an instance of the ReportBenchmark class to access necessary data and templates for generating assessments.\n\nThe constructor method `__init__` initializes the ReportEvaluation object with two parameters: `report_benchmark`, which is an instance of the ReportBenchmark class, and `student_report`, which is a string containing the student's report. This setup allows the class to leverage the functionalities provided by the ReportBenchmark instance throughout its methods.\n\nThe method `examinees_outline_generation` generates an outline of the student's report by utilizing a template for outline generation. It retrieves the template from the ReportBenchmark's agent, populates it with the user query, and then renders it to create a prompt. This prompt is sent to the agent's chat function to obtain a response, which represents the generated outline.\n\nThe `evaluate_breadth` method assesses the breadth of the student's report by first generating the student's tree structure using the `examinees_outline_generation` method. It then calculates the similarity score between the generated student tree and the ground truth breadth provided by the ReportBenchmark instance using the `tree_similarity` function.\n\nThe `evaluate_factualqa` method performs a FactualQA evaluation based on the student's report. It loads a specific template for factual QA, fills it with the user query, ground truth breadth, and the student's report, and then sends this prompt to the agent's chat function to receive a response.\n\nThe `extract_student_tree_structure` method is responsible for extracting the structure of the student's report. It uses a template for student tree extraction, populates it with the student's report, and retrieves the response from the agent's chat function, which is then parsed from JSON format.\n\nThe `evaluate_depth` method is currently not implemented but is intended to evaluate the depth of the student's report based on the results from the FactualQA evaluation. It is designed to calculate the accuracy of answers extracted from the student report.\n\n**Note**: Users should ensure that the ReportBenchmark instance is properly initialized with the necessary data before creating a ReportEvaluation object. Additionally, the evaluate_depth method is a placeholder and does not currently perform any evaluation.\n\n**Output Example**: \nFor the `examinees_outline_generation` method, a possible return value could be:\n{\n  \"Outline\": [\n    \"Introduction\",\n    \"Main Argument\",\n    \"Supporting Evidence\",\n    \"Conclusion\"\n  ]\n} \n\nFor the `evaluate_breadth` method, a possible score could be:\n{\n  \"BreadthScore\": 0.85\n} \n\nFor the `evaluate_factualqa` method, a possible response could be:\n{\n  \"FactualQAResult\": {\n    \"CorrectAnswers\": 5,\n    \"TotalQuestions\": 10,\n    \"Accuracy\": 0.5\n  }\n} \n\nFor the `extract_student_tree_structure` method, a possible return value could be:\n{\n  \"StudentTree\": {\n    \"Nodes\": [\n      {\"Title\": \"Introduction\", \"Content\": \"This is the introduction.\"},\n      {\"Title\": \"Main Argument\", \"Content\": \"This is the main argument.\"}\n    ]\n  }\n}"
      ],
      "code_start_line": 5,
      "code_end_line": 50,
      "params": [],
      "have_return": true,
      "code_content": "class ReportEvaluation:\n    def __init__(self, report_benchmark: ReportBenchmark, student_report: str):\n        # 使用 ReportBenchmark 实例获得 ground truths\n        self.report_benchmark = report_benchmark\n        self.student_report = student_report\n\n    def examinees_outline_generation(self):\n        # 使用 ReportBenchmark 的 BaseAgent 调用，生成学生树（此前在 ReportBenchmark 中的 run_outline_generation）\n        template_str = self.report_benchmark.agent.load_template(\"outline_generation.txt\")\n        data = {\n            \"Query\": self.report_benchmark.user_query,\n        }\n        prompt = self.report_benchmark.agent.render_template(template_str, data)\n        response = self.report_benchmark.agent.chat(usr_prompt=prompt)\n        return response\n\n    def evaluate_breadth(self):\n        # 直接使用 examinees_outline_generation 生成学生树\n        student_tree_str = self.examinees_outline_generation()\n        student_tree = json.loads(student_tree_str)\n        score = tree_similarity(self.report_benchmark.breadth_gt, student_tree)\n        return score\n\n    def evaluate_factualqa(self):\n        # 基于传入的 StudentReport 执行 FactualQA 评估\n        template_str = self.report_benchmark.agent.load_template(\"factual_qa.txt\")\n        data = {\n            \"Query\": self.report_benchmark.user_query,\n            \"BreadthGT\": json.dumps(self.report_benchmark.breadth_gt),\n            \"DepthGT\": self.student_report,\n        }\n        prompt = self.report_benchmark.agent.render_template(template_str, data)\n        response = self.report_benchmark.agent.chat(usr_prompt=prompt)\n        return response\n\n    def extract_student_tree_structure(self):\n        template_str = self.report_benchmark.agent.load_template(\"student_tree_extraction.txt\")\n        data = {\"StudentReport\": self.student_report}\n        prompt = self.report_benchmark.agent.render_template(template_str, data)\n        response = self.report_benchmark.agent.chat(usr_prompt=prompt)\n        return json.loads(response)\n    \n    def evaluate_depth(self):\n        # 深度评估逻辑暂不实现，基于factual QA结果评估ACC\n        # 这里实现抽取student report based answer 的acc来计算最后的分数\n        pass\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the ReportEvaluation class with a ReportBenchmark instance and a student report string.\n\n**parameters**: The parameters of this Function.\n· report_benchmark: An instance of the ReportBenchmark class, which is used to obtain ground truths for report evaluations.  \n· student_report: A string representing the student's report that is to be evaluated.\n\n**Code Description**: The __init__ function is the constructor for the ReportEvaluation class. It takes two parameters: report_benchmark and student_report. The report_benchmark parameter is expected to be an instance of the ReportBenchmark class, which is responsible for generating report evaluations by building ground truths and performing fact extraction. The student_report parameter is a string that contains the report of a student, which will be evaluated against the ground truths provided by the ReportBenchmark instance.\n\nUpon initialization, the constructor assigns the report_benchmark instance to the instance variable self.report_benchmark, allowing the ReportEvaluation class to access the methods and attributes of the ReportBenchmark class for further processing. Additionally, the student_report string is stored in the instance variable self.student_report, which will be used in the evaluation process.\n\nThe relationship between the ReportEvaluation class and the ReportBenchmark class is crucial, as the ReportEvaluation class relies on the functionalities provided by the ReportBenchmark instance to perform its evaluations. This constructor sets up the necessary context for the ReportEvaluation class to operate effectively, ensuring that it has access to the required ground truths and the specific report that needs to be evaluated.\n\n**Note**: When using the ReportEvaluation class, ensure that the report_benchmark instance is properly initialized with valid data, as it directly influences the evaluation process of the student report.",
        "**__init__**: The function of __init__ is to initialize an instance of the ReportEvaluation class with a report benchmark and a student report.\n\n**parameters**: The parameters of this Function.\n· report_benchmark: An instance of the ReportBenchmark class that provides the ground truths for evaluation.  \n· student_report: A string representing the report submitted by the student for evaluation.\n\n**Code Description**: The __init__ function is a constructor for the ReportEvaluation class. It takes two parameters: report_benchmark and student_report. The report_benchmark parameter is expected to be an instance of the ReportBenchmark class, which is utilized to obtain the ground truths necessary for evaluating the student's report. The student_report parameter is a string that contains the content of the report submitted by the student. Within the constructor, these parameters are assigned to instance variables self.report_benchmark and self.student_report, respectively. This setup allows the ReportEvaluation class to access the benchmark data and the student's report for further processing and evaluation.\n\n**Note**: It is important to ensure that the report_benchmark parameter is indeed an instance of the ReportBenchmark class to avoid type-related errors during evaluation. Additionally, the student_report should be a valid string representing the report content to ensure proper functionality of the class methods that may utilize these attributes."
      ],
      "code_start_line": 6,
      "code_end_line": 9,
      "params": [
        "self",
        "report_benchmark",
        "student_report"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, report_benchmark: ReportBenchmark, student_report: str):\n        # 使用 ReportBenchmark 实例获得 ground truths\n        self.report_benchmark = report_benchmark\n        self.student_report = student_report\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "examinees_outline_generation",
      "md_content": [
        "**examinees_outline_generation**: The function of examinees_outline_generation is to generate a student outline tree by utilizing a template rendering system and agent communication from the ReportBenchmark.\n\n**parameters**: The parameters of this Function.\n· There are no parameters passed to this function directly. It relies on the internal attributes of the `report_benchmark` object.\n\n**Code Description**: \nThe `examinees_outline_generation` function is responsible for generating a structured outline of the student data, referred to as a \"student tree,\" using the `ReportBenchmark`'s `BaseAgent`. The function performs the following steps:\n\n1. It loads a template file named `outline_generation.txt` using the `load_template` method of the `agent` object from `report_benchmark`. This template likely contains predefined instructions or a structure for generating the student outline.\n\n2. Next, it creates a data dictionary containing the user's query, retrieved from `report_benchmark.user_query`. This user query represents the input or context that will be used in the generation process.\n\n3. The template string loaded in step 1 is then rendered with the data dictionary using the `render_template` method of the `agent`. This step dynamically fills the template with the provided data (in this case, the user query) to create a customized prompt.\n\n4. The function sends the generated prompt to the agent for further processing by calling `common_chat` on the agent, passing the generated prompt as the `usr_prompt`. This step likely involves communicating with an external system or using an internal service to process the prompt and generate a response.\n\n5. The response, which is expected to be the student outline tree in string format, is returned as the output of the function.\n\nThe function does not take any parameters directly; instead, it relies on the `report_benchmark` attribute, which is assumed to be an instance of a class that holds the necessary data, such as the `user_query` and the `agent` responsible for communication and template handling.\n\nIn the context of the project, this function is called by the `evaluate_breadth` function. The `evaluate_breadth` function calls `examinees_outline_generation` to generate the student tree, then parses the returned string into a JSON object, which is compared to a ground truth value (`breadth_gt`) to calculate a similarity score using the `tree_similarity` function. This indicates that `examinees_outline_generation` is a part of the process that evaluates the breadth of student performance or outlines.\n\n**Note**: \n- The function relies heavily on the `report_benchmark` object, and any changes to this object may affect the function's behavior.\n- The response returned by `examinees_outline_generation` is expected to be in a specific format (likely a structured string representing a tree). Any discrepancies in the format may lead to errors in downstream processing.\n- This function is expected to interact with an external agent (through `common_chat`), so the performance or response times may depend on the efficiency and reliability of that external system.\n\n**Output Example**: \nThe output is expected to be a string representing the generated student tree. An example response could look like:\n\n```\n{\n    \"student_id\": \"12345\",\n    \"name\": \"John Doe\",\n    \"performance\": {\n        \"subject_1\": \"A\",\n        \"subject_2\": \"B\",\n        \"subject_3\": \"A\"\n    },\n    \"remarks\": \"Excellent performance in all subjects\"\n}\n```",
        "**examinees_outline_generation**: The function of examinees_outline_generation is to generate a structured outline of student data, referred to as the \"student tree,\" by utilizing a template rendering system and agent communication.\n\n**parameters**: The parameters of this Function.\n· There are no parameters passed directly to this function.\n\n**Code Description**: \nThe `examinees_outline_generation` function is a method within the `ReportEvaluation` class that plays a pivotal role in generating a \"student tree\" based on a user’s query. The function follows these steps:\n\n1. **Template Loading**: It begins by loading a template file named `outline_generation.txt` using the `load_template` method from the `report_benchmark.agent`. This template serves as a blueprint for the structure and content of the student tree that will be generated.\n\n2. **Data Preparation**: The function then prepares a data dictionary that contains the key `\"Query\"`, which is assigned the value of `self.report_benchmark.user_query`. This dictionary is essential as it will provide dynamic data to the template during the rendering process.\n\n3. **Template Rendering**: Using the `render_template` method from `report_benchmark.agent`, the function renders the template by passing the template string and the data dictionary. This process customizes the template with the user's query, resulting in a prompt that is tailored to the current context.\n\n4. **Chat Interaction**: The rendered prompt is then passed to the `chat` method of the `report_benchmark.agent`. This method simulates an interactive agent communication, where the agent processes the prompt and returns a response, which, in this case, is the generated student tree.\n\n5. **Return Value**: The function concludes by returning the response received from the agent, which is expected to be a string representing the student tree.\n\nThis function is fundamental in generating structured data that can be utilized in further evaluations, particularly in the context of the broader `ReportEvaluation` class.\n\nIn relation to its caller, the `evaluate_breadth` function, `examinees_outline_generation` is responsible for generating the student tree, which is then used to compare against a predefined standard tree. The generated student tree is essential for evaluating the breadth of the student's performance, as the similarity between the generated tree and the standard tree is computed in `evaluate_breadth`. The returned string from `examinees_outline_generation` is expected to be parsable into a JSON object, which is required for the similarity calculation.\n\n**Note**: It is crucial to ensure that the response returned by `examinees_outline_generation` adheres to the expected format (i.e., a structured string representation of a student tree) for seamless integration with other functions, particularly `evaluate_breadth`. Any discrepancies in the format may lead to issues in further processing, such as parsing errors or incorrect similarity scores.\n\n**Output Example**: The output is a string that represents the student tree. A mock-up example could be:\n\n```\n{\n  \"student_id\": \"12345\",\n  \"name\": \"John Doe\",\n  \"performance\": {\n    \"subject_1\": \"A\",\n    \"subject_2\": \"B+\",\n    \"subject_3\": \"A\"\n  }\n}\n```"
      ],
      "code_start_line": 11,
      "code_end_line": 19,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def examinees_outline_generation(self):\n        # 使用 ReportBenchmark 的 BaseAgent 调用，生成学生树（此前在 ReportBenchmark 中的 run_outline_generation）\n        template_str = self.report_benchmark.agent.load_template(\"outline_generation.txt\")\n        data = {\n            \"Query\": self.report_benchmark.user_query,\n        }\n        prompt = self.report_benchmark.agent.render_template(template_str, data)\n        response = self.report_benchmark.agent.chat(usr_prompt=prompt)\n        return response\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_evaluation.py/ReportEvaluation/evaluate_breadth"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "evaluate_breadth",
      "md_content": [
        "**evaluate_breadth**: The function of evaluate_breadth is to evaluate the similarity score between a generated student tree and a predefined standard tree based on breadth.\n\n**parameters**: The parameters of this Function.\n· There are no parameters passed directly to this function.\n\n**Code Description**: The `evaluate_breadth` function is a method within the `ReportEvaluation` class that is responsible for assessing the breadth of a student's performance by comparing a generated student tree against a ground truth tree structure. The function operates as follows:\n\n1. **Student Tree Generation**: The function first calls the `examinees_outline_generation` method, which generates a structured outline of the student data, referred to as a \"student tree.\" This method utilizes a template rendering system and agent communication to create the student tree based on the user's query.\n\n2. **Parsing the Student Tree**: The output from `examinees_outline_generation` is expected to be a string representation of the student tree. The function then parses this string into a JSON object using `json.loads`, allowing for structured manipulation and comparison.\n\n3. **Similarity Calculation**: The function then calculates the similarity score between the generated student tree and a predefined standard tree (`self.report_benchmark.breadth_gt`) by invoking the `tree_similarity` function. This function computes the semantic and structural similarity score between the two hierarchical tree structures, taking into account various parameters that influence the comparison.\n\n4. **Returning the Score**: Finally, the computed similarity score is returned as the output of the `evaluate_breadth` function. This score reflects how closely the student's tree structure matches the standard tree both semantically and structurally.\n\nThe `evaluate_breadth` function is integral to the evaluation process within the `ReportEvaluation` class, as it provides a quantitative measure of the student's performance in relation to established benchmarks.\n\n**Note**: It is important to ensure that the output from `examinees_outline_generation` is in the expected format for successful parsing. Any discrepancies in the format may lead to errors in the similarity calculation. The `tree_similarity` function, which is called within this method, relies on the proper structure of both the standard and student trees to produce an accurate similarity score.\n\n**Output Example**: The output of the `evaluate_breadth` function is expected to be a floating-point value representing the similarity score. An example return value could be:\n\n```\n0.85\n``` \n\nThis score indicates a high level of similarity between the generated student tree and the standard tree."
      ],
      "code_start_line": 21,
      "code_end_line": 26,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def evaluate_breadth(self):\n        # 直接使用 examinees_outline_generation 生成学生树\n        student_tree_str = self.examinees_outline_generation()\n        student_tree = json.loads(student_tree_str)\n        score = tree_similarity(self.report_benchmark.breadth_gt, student_tree)\n        return score\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/reportbench/tree_comparison.py/tree_similarity",
        "src/criticsearch/reportbench/report_evaluation.py/ReportEvaluation/examinees_outline_generation"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "evaluate_factualqa",
      "md_content": [
        "**evaluate_factualqa**: The function of evaluate_factualqa is to perform a FactualQA evaluation based on the provided StudentReport.\n\n**parameters**: The parameters of this Function.\n· self: An instance of the class that contains the report_benchmark and student_report attributes.\n\n**Code Description**: The evaluate_factualqa function is designed to execute a FactualQA evaluation using the information contained within the StudentReport. The function begins by loading a template string from a file named \"factual_qa.txt\" using the load_template method of the agent associated with report_benchmark. This template serves as a basis for constructing a prompt that will be sent for evaluation.\n\nNext, the function constructs a data dictionary that includes:\n- \"Query\": This key holds the user query from the report_benchmark.\n- \"BreadthGT\": This key contains the ground truth for breadth, which is serialized into a JSON format from the breadth_gt attribute of report_benchmark.\n- \"DepthGT\": This key directly references the student_report, which is expected to contain the depth ground truth for the evaluation.\n\nThe prompt is then generated by rendering the template string with the data dictionary using the render_template method of the agent. Following this, the function calls the common_chat method of the agent, passing the constructed prompt as the usr_prompt argument. This method is responsible for processing the prompt and generating a response based on the evaluation.\n\nFinally, the function returns the response obtained from the common_chat method, which is expected to contain the results of the FactualQA evaluation.\n\n**Note**: It is important to ensure that the template file \"factual_qa.txt\" exists and is correctly formatted, as the function relies on this template for generating the evaluation prompt. Additionally, the attributes report_benchmark and student_report must be properly initialized within the class instance for the function to operate correctly.\n\n**Output Example**: A possible return value from the evaluate_factualqa function could be a JSON object containing the evaluation results, such as:\n{\n    \"evaluation_score\": 0.85,\n    \"feedback\": \"The answer is mostly correct but lacks depth in certain areas.\"\n}",
        "**evaluate_factualqa**: The function of evaluate_factualqa is to perform a factual QA evaluation based on the provided StudentReport.\n\n**parameters**: \n- No parameters are explicitly passed into the function. However, the method relies on instance variables and attributes of the class to perform its operation.\n\n**Code Description**: \nThe `evaluate_factualqa` function is designed to evaluate a StudentReport using a factual question-answering (FactualQA) approach. Here's a step-by-step explanation of how it works:\n\n1. **Template Loading**: The function begins by loading a template string, \"factual_qa.txt\", through the `load_template` method of the `agent` attribute, which is part of the `report_benchmark` instance variable. This template is presumably used to format the evaluation process.\n\n2. **Data Preparation**: It then creates a dictionary called `data` that contains three key-value pairs:\n   - `\"Query\"`: This is populated with the `user_query` from the `report_benchmark`. This likely represents a question or query the model should evaluate.\n   - `\"BreadthGT\"`: This value is assigned the `breadth_gt` attribute, converted into a JSON string format. This attribute appears to contain ground truth data related to the breadth of the evaluation.\n   - `\"DepthGT\"`: This is populated with the `student_report` attribute, which presumably holds the student's report data for evaluation.\n\n3. **Template Rendering**: The `render_template` method is called on the `agent` attribute, passing the loaded template string (`template_str`) and the `data` dictionary. This method likely generates a formatted prompt by embedding the data into the template.\n\n4. **Chat Interaction**: The function then sends the generated prompt to the `chat` method of the `agent` attribute via the `usr_prompt` argument. The `chat` method presumably sends the prompt to a model or system that processes it and returns a response.\n\n5. **Return**: Finally, the response obtained from the `chat` method is returned, which likely contains the results of the factual QA evaluation.\n\n**Note**: \n- The method relies on several instance variables such as `report_benchmark`, `breadth_gt`, and `student_report`, all of which must be properly initialized before this function is called.\n- The `chat` method is expected to return a response in a format that can be used to evaluate the factual accuracy of the StudentReport.\n- The `load_template` and `render_template` methods are critical for generating the prompt that drives the factual evaluation process, and they depend on a well-structured template file (\"factual_qa.txt\").\n\n**Output Example**: \nAn example of the output might be a response from the `chat` method, such as:\n\n```json\n{\n  \"evaluation\": \"The report contains correct information regarding the query, but lacks detail in some areas. Further depth is required in the analysis of the topic.\"\n}\n```\n\nThis output would be generated based on the evaluation of the provided StudentReport in relation to the query."
      ],
      "code_start_line": 28,
      "code_end_line": 38,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def evaluate_factualqa(self):\n        # 基于传入的 StudentReport 执行 FactualQA 评估\n        template_str = self.report_benchmark.agent.load_template(\"factual_qa.txt\")\n        data = {\n            \"Query\": self.report_benchmark.user_query,\n            \"BreadthGT\": json.dumps(self.report_benchmark.breadth_gt),\n            \"DepthGT\": self.student_report,\n        }\n        prompt = self.report_benchmark.agent.render_template(template_str, data)\n        response = self.report_benchmark.agent.chat(usr_prompt=prompt)\n        return response\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_student_tree_structure",
      "md_content": [
        "**extract_student_tree_structure**: The function of extract_student_tree_structure is to extract the directory tree structure from a given student report.\n\n**parameters**: \n· None\n\n**Code Description**:  \nThe **extract_student_tree_structure** function is responsible for extracting the directory tree structure from the student report. The function operates as follows:\n\n1. **Load Template**: It starts by loading a template file, named `\"student_tree_extraction.txt\"`, using the `load_template` method from the `report_benchmark.agent`. This template is assumed to contain the necessary structure or instructions for processing the student report.\n\n2. **Prepare Data**: The function then prepares a dictionary with the key `\"StudentReport\"` which holds the `self.student_report` value. This suggests that the `self.student_report` contains the actual data of the student report to be processed.\n\n3. **Render Template**: Next, the function renders the loaded template using the `render_template` method of the `report_benchmark.agent`. It passes the template string (`template_str`) and the data dictionary (`data`) as parameters to generate a prompt. The prompt generated will likely contain placeholders or specific instructions based on the template, filled with the relevant data from the student report.\n\n4. **Chat with Agent**: The generated prompt is then sent to the `common_chat` method of the `report_benchmark.agent`. This method interacts with a system (likely a model or an agent) that processes the prompt and returns a response. The interaction here is presumably for processing or extracting the tree structure from the student report based on the prompt.\n\n5. **Parse Response**: Finally, the response from the `common_chat` method is expected to be in a JSON format. The `json.loads(response)` function is used to parse this JSON string into a Python dictionary or object, which is then returned as the output of the function.\n\n**Note**: \n- The function assumes that the student report (`self.student_report`) is already available and properly formatted.\n- The template `\"student_tree_extraction.txt\"` should exist in the specified location and be structured correctly to work with the `render_template` method.\n- The `common_chat` method's response is expected to be a JSON string that can be parsed directly with `json.loads()`.\n\n**Output Example**:\nAssuming the student report contains data structured with sections, the output might look like the following:\n\n```json\n{\n  \"root\": {\n    \"name\": \"Student Report\",\n    \"children\": [\n      {\n        \"name\": \"Personal Information\",\n        \"children\": [\n          {\n            \"name\": \"Name\",\n            \"value\": \"John Doe\"\n          },\n          {\n            \"name\": \"ID\",\n            \"value\": \"12345\"\n          }\n        ]\n      },\n      {\n        \"name\": \"Grades\",\n        \"children\": [\n          {\n            \"name\": \"Math\",\n            \"value\": \"A\"\n          },\n          {\n            \"name\": \"Science\",\n            \"value\": \"B\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\nIn this output example, the directory tree structure of the student report is represented as a hierarchical JSON object, where each node contains a `name` and can have `children` representing nested sections or data.",
        "**extract_student_tree_structure**: The function of extract_student_tree_structure is to extract a hierarchical tree structure from a student report.\n\n**parameters**: The function does not accept any parameters.\n\n**Code Description**:  \nThe `extract_student_tree_structure` function is designed to process a student report and generate a hierarchical tree structure based on its content. The function performs the following steps:\n\n1. **Loading Template**: It begins by loading a template file called `student_tree_extraction.txt` using the `load_template` method of the `agent` object. This template is presumably designed to define how the student report's structure should be interpreted.\n\n2. **Rendering Template**: After loading the template, the function prepares the necessary data to feed into the template. This data is a dictionary with the key `\"StudentReport\"` mapped to the `student_report` attribute of the current object. The template is then rendered by the `render_template` method of the `agent` object, which takes the loaded template and the data dictionary to generate a prompt for further processing.\n\n3. **Generating Response**: The generated prompt is passed to the `chat` method of the `agent` object. The chat function appears to simulate a conversation or query processing with the prompt, returning a response, which is expected to be a string containing structured information.\n\n4. **Parsing JSON Response**: Finally, the function parses the response from the `chat` method using Python's `json.loads()` to convert the string into a Python dictionary or list, which represents the hierarchical tree structure of the student report.\n\n**Note**: \n- The function depends on the availability and correctness of the `student_report` attribute and the `agent` object methods such as `load_template`, `render_template`, and `chat`.\n- The template file `student_tree_extraction.txt` must be properly formatted and located in the correct directory for it to be loaded successfully.\n- The response returned by the `chat` method must be a valid JSON string, as the function relies on `json.loads()` to convert it into a Python object.\n\n**Output Example**:  \nAssuming the student report contains a hierarchical structure, the output might look like the following after parsing the JSON response:\n\n```json\n{\n  \"root\": {\n    \"title\": \"Student Report\",\n    \"sections\": [\n      {\n        \"title\": \"Personal Information\",\n        \"content\": \"Details about the student.\"\n      },\n      {\n        \"title\": \"Academic Performance\",\n        \"content\": \"Summary of grades and achievements.\"\n      }\n    ]\n  }\n}\n```\n\nThis structure represents a tree with a root node titled \"Student Report\" and two child sections: \"Personal Information\" and \"Academic Performance\", each with their own content. The actual structure will depend on the content and formatting of the student report.",
        "**extract_student_tree_structure**: The function of extract_student_tree_structure is to extract and return the structured representation of a student's report by utilizing a template and a chat interface.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The extract_student_tree_structure function is designed to facilitate the extraction of a student's report structure. It begins by loading a template from a file named \"student_tree_extraction.txt\" using the load_template method of the report_benchmark.agent. This template serves as a blueprint for how the student's report data will be formatted. \n\nNext, the function prepares a data dictionary containing the key \"StudentReport\" which is assigned the value of the instance variable self.student_report. This dictionary is then passed to the render_template method of the report_benchmark.agent, which processes the template string with the provided data to create a prompt.\n\nThe generated prompt is subsequently sent to a chat interface through the chat method of the report_benchmark.agent, which is expected to return a response based on the prompt. Finally, the function parses the response using json.loads to convert the JSON formatted string into a Python object, which is then returned as the output of the function.\n\n**Note**: It is important to ensure that the \"student_tree_extraction.txt\" template file is correctly formatted and accessible. Additionally, the response from the chat method should be in valid JSON format to avoid errors during the parsing step.\n\n**Output Example**: A possible appearance of the code's return value could be:\n{\n    \"student_id\": \"12345\",\n    \"name\": \"John Doe\",\n    \"grades\": {\n        \"math\": \"A\",\n        \"science\": \"B+\",\n        \"literature\": \"A-\"\n    },\n    \"attendance\": {\n        \"total_classes\": 30,\n        \"present\": 28,\n        \"absent\": 2\n    }\n}"
      ],
      "code_start_line": 40,
      "code_end_line": 45,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def extract_student_tree_structure(self):\n        template_str = self.report_benchmark.agent.load_template(\"student_tree_extraction.txt\")\n        data = {\"StudentReport\": self.student_report}\n        prompt = self.report_benchmark.agent.render_template(template_str, data)\n        response = self.report_benchmark.agent.chat(usr_prompt=prompt)\n        return json.loads(response)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "evaluate_depth",
      "md_content": [
        "**evaluate_depth**: The function of evaluate_depth is to evaluate the depth of a report based on factual QA results and compute the accuracy of student-generated responses.\n\n**parameters**: \nThis function does not take any parameters.\n\n**Code Description**: \nThe **evaluate_depth** function is currently a placeholder and does not contain any functional implementation. As indicated in the comment within the code, it is intended to implement logic for depth evaluation based on factual QA results. However, the logic is not yet realized. The comment suggests that the function is designed to calculate the accuracy (ACC) of student report-based answers, which will ultimately contribute to determining a final score. The current implementation of this function simply contains a `pass` statement, implying that no action is taken when the function is called.\n\n**Note**: \n- The function is incomplete, and no depth evaluation logic has been implemented yet.\n- The function might be extended in the future to include actual calculations or data processing for evaluating the depth of responses in student reports."
      ],
      "code_start_line": 47,
      "code_end_line": 50,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def evaluate_depth(self):\n        # 深度评估逻辑暂不实现，基于factual QA结果评估ACC\n        # 这里实现抽取student report based answer 的acc来计算最后的分数\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/reportbench/test_llm.py": [
    {
      "type": "FunctionDef",
      "name": "test_llm",
      "md_content": [
        "**test_llm**: The function of test_llm is to test the functionality of the common_chat method in the BaseAgent class.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string that serves as the input prompt for the common_chat method.\n\n**Code Description**: The test_llm function initiates a test for the common_chat method of the BaseAgent class. It begins by defining a prompt in Chinese, which translates to \"Hello, is the common_chat call successful?\" This prompt is then passed to an instance of the BaseAgent class, which is created by invoking its constructor. The common_chat method is called on this instance with the defined prompt as an argument. The response from the common_chat method is captured in the variable 'answer'. Finally, the function prints the response to the console, prefixed by the string \"common_chat Response:\". This allows the developer to see the output generated by the common_chat method when provided with the specified prompt.\n\n**Note**: It is important to ensure that the BaseAgent class and its common_chat method are properly implemented and accessible in the context where test_llm is executed. Additionally, the prompt used in the test should be relevant to the expected functionality of the common_chat method to yield meaningful results.",
        "**test_llm**: The function of test_llm is to test the interaction with the BaseAgent's chat functionality using a predefined prompt.\n\n**parameters**: The parameters of this Function.\n· prompt: A string containing the user input that will be sent to the chat function of the BaseAgent.\n\n**Code Description**: The test_llm function serves as a simple test case to verify the functionality of the chat method within the BaseAgent class. It initializes a prompt in Chinese, which translates to \"Hello, is the common_chat call successful?\" This prompt is then passed to an instance of the BaseAgent class, which is responsible for managing conversations and interactions with a conversational model.\n\nThe function creates an instance of BaseAgent and calls its chat method with the user prompt. The chat method processes the input prompt and generates a response based on the internal logic of the BaseAgent, which may involve utilizing various tools and managing conversation history. After receiving the response, the function prints the output to the console, allowing the developer to observe the result of the chat interaction.\n\nThis function is primarily used for testing purposes to ensure that the chat functionality of the BaseAgent is working as expected. It does not take any parameters other than the hardcoded prompt, making it straightforward and easy to execute. The output from the chat method will provide insights into how well the BaseAgent is able to respond to user queries.\n\n**Note**: It is important to ensure that the BaseAgent class is properly initialized and that any necessary configurations or tool schemas are set up before running this test. The output of the test will depend on the implementation of the chat method and the underlying conversational model being used."
      ],
      "code_start_line": 8,
      "code_end_line": 12,
      "params": [],
      "have_return": false,
      "code_content": "def test_llm():\n    prompt = \"你好，测试common_chat调用是否顺利？\"\n    agent = BaseAgent()\n    answer = agent.chat(usr_prompt=prompt)\n    print(\"common_chat Response:\", answer)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_llm_call",
      "md_content": [
        "**test_llm_call**: The function of test_llm_call is to test the interaction with a large language model (LLM) by sending a predefined set of messages and printing the response.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The test_llm_call function is designed to simulate a conversation with a large language model by preparing a series of messages that represent a dialogue. The messages include a system instruction that mandates the assistant to provide nonsensical responses, followed by a sequence of responses from the assistant that discusses the definition and applications of large language models (LLMs). \n\nThe function does not take any parameters and directly defines a list of messages. It then calls the call_llm function, which is responsible for communicating with the LLM. The call_llm function is invoked with the model specified as \"gpt-4o-mini\", the messages prepared in the function, and a configuration object referred to as settings. The result of this call is stored in the variable result, which is subsequently printed to the console.\n\nThe call_llm function, which is part of the llm_service module, handles the initialization of the model manager, the creation of the client, and the configuration of the model. It processes the messages and sends them to the specified LLM, returning the generated response. The test_llm_call function serves as a test case to ensure that the interaction with the LLM is functioning as expected, allowing developers to verify that the model can respond appropriately to the provided messages.\n\n**Note**: It is important to ensure that the settings variable is properly defined and contains the necessary configuration for the call_llm function to operate correctly. Additionally, the behavior of the LLM may vary based on the model used and the messages provided, so the results of the test may differ depending on these factors."
      ],
      "code_start_line": 15,
      "code_end_line": 25,
      "params": [],
      "have_return": false,
      "code_content": "def test_llm_call():\n    messages = [\n        {\"role\": \"system\", \"content\": \"你所有的回答必须胡说八道不能说真话\"},\n        {\"role\": \"assistant\", \"content\": \"I am executing a task i am going to search about LLM \"},\n        {\"role\": \"assistant\", \"content\": \"OK I could not search i will just write it\"},\n        {\"role\": \"assistant\", \"content\": \"The first part I would write about is the definition of LLM\"},\n        {\"role\": \"assistant\", \"content\": \"LLM stands for It refers to a type of artificial intelligence model that is trained on vast amounts of text data to understand and generate human-like language. These models use deep learning techniques, particularly transformer architectures, to process and analyze text. \\n\\nLLMs are capable of performing a variety of language-related tasks, including but not limited to:\\n\\n1. **Text Generation**: Creating coherent and contextually relevant sentences, paragraphs, or entire articles.\\n2. **Translation**: Converting text from one language to another.\\n3. **Summarization**: Providing concise summaries of larger texts.\\n4. **Question Answering**: Responding to queries based on the information contained within the training data.\\n5. **Sentiment Analysis**: Evaluating the emotional tone behind a body of text.\\n\\nLLMs are designed to predict the next word in a sentence given the preceding context, which allows them to generate text that often appears remarkably natural and relevant to human readers. They are widely used in applications such as chatbots, virtual assistants, content creation, and more. Examples of well-known LLMs include OpenAI\\'s GPT-3 and GPT-4, Google\\'s BERT, and various other models developed by different organizations.\"},\n        {\"role\": \"assistant\", \"content\": \"Ok, after that I would like to write about the applications of LLM\"},\n    ]\n    result = call_llm(model=\"gpt-4o-mini\", messages=messages, config=settings)\n    print(result)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/llm_service.py/call_llm"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "src/criticsearch/reportbench/process_search_result.py": [
    {
      "type": "FunctionDef",
      "name": "filter_results",
      "md_content": [
        "**filter_results**: The function of filter_results is to filter out results that do not have raw content in the provided data.\n\n**parameters**: \n· data: A dictionary containing the key \"results\", which is expected to hold a list of result objects.\n\n**Code Description**: The function `filter_results` is designed to process a given dictionary called `data`. Specifically, it filters the list under the key \"results\" by removing any items (results) that do not contain the key \"raw_content\". If a result does not have \"raw_content\" or has it as a falsy value (e.g., None, empty string), it is excluded from the results list. The filtered data is then returned in the same dictionary format. If the \"results\" key does not exist in the input data, the function ensures that an empty list is used instead, effectively preventing errors during processing.\n\nHere is a step-by-step breakdown:\n1. The function first accesses the value associated with the \"results\" key in the input dictionary `data`.\n2. It then applies a list comprehension to iterate over each item (`r`) in this list.\n3. For each item, it checks whether the item contains the key \"raw_content\" and whether it holds a truthy value.\n4. If \"raw_content\" exists and has a truthy value, the item is kept in the list.\n5. Finally, the filtered list replaces the existing \"results\" key in the `data` dictionary.\n6. The updated `data` dictionary is returned.\n\n**Note**: \n- The function does not handle situations where the input `data` does not contain the \"results\" key at all. In this case, it will safely return the original data with an empty list under the \"results\" key.\n- The function assumes that the \"results\" key, when present, always holds a list, which is a standard format for handling multiple results.\n\n**Output Example**: \nGiven the following input data:\n```python\n{\n    \"results\": [\n        {\"raw_content\": \"valid content\"},\n        {\"raw_content\": None},\n        {\"raw_content\": \"another valid content\"}\n    ]\n}\n```\n\nThe output after calling `filter_results` would be:\n```python\n{\n    \"results\": [\n        {\"raw_content\": \"valid content\"},\n        {\"raw_content\": \"another valid content\"}\n    ]\n}\n```"
      ],
      "code_start_line": 7,
      "code_end_line": 9,
      "params": [
        "data"
      ],
      "have_return": true,
      "code_content": "def filter_results(data):\n    data[\"results\"] = [r for r in data.get(\"results\", []) if r.get(\"raw_content\")]\n    return data\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "generate_markdown",
      "md_content": [
        "**generate_markdown**: The function of generate_markdown is to generate a markdown formatted string based on input data containing images and search results.\n\n**parameters**:\n· parameter1: data (dict) - A dictionary containing the data with two possible keys: \"images\" and \"results\". The \"images\" key should contain a list of image objects, and the \"results\" key should contain a list of search result objects.\n\n**Code Description**: \nThe `generate_markdown` function is responsible for producing a markdown-formatted string based on the given input data. The function is structured to create two main sections: one for images and another for search results. It uses the input data, which is expected to be a dictionary, and processes the \"images\" and \"results\" lists within the dictionary to construct markdown content.\n\n1. **Images Section**: \n   The function first initializes an empty list, `lines`, to store the markdown content. It then checks if the input dictionary has an \"images\" key, and retrieves the associated list. For each image in this list, the function adds a description (or \"N/A\" if no description is found) and the URL (or \"N/A\" if no URL is found) to the `lines` list. The images are enumerated starting from 1, with each entry formatted as follows:\n   - `[idx] DESCRIPTION: {description}`\n   - `URL: {url}`\n   \n2. **Search Results Section**: \n   After handling the images, the function proceeds to process the \"results\" key, if present. It iterates over the list of search results, adding each result's title (or \"N/A\"), URL (or \"N/A\"), and raw content (or \"N/A\") to the markdown string. Each result is also enumerated starting from 1, and the entries are formatted as follows:\n   - `[idx]: TITLE: {title}`\n   - `URL: {url}`\n   - `CONTENT: {raw_content}`\n   \n3. **Return Value**: \n   Once both the images and search results sections have been processed, the function joins all lines in the `lines` list with newline characters and returns the final string. This string contains a well-structured markdown representation of the input data.\n\n**Note**:\n- The function assumes that the \"images\" and \"results\" keys in the input data are lists of dictionaries.\n- If the \"description\", \"url\", \"title\", \"raw_content\" are not provided for an item, the function defaults to \"N/A\".\n- The function does not handle any cases where the input data structure deviates from the expected format (i.e., the presence of \"images\" and \"results\" keys).\n- The markdown string generated can be directly used in markdown viewers or other systems that support markdown formatting.\n\n**Output Example**:\n\n```markdown\n# images \n\n[1] DESCRIPTION: Sunset over mountains\nURL: http://example.com/sunset.jpg\n\n[2] DESCRIPTION: N/A\nURL: http://example.com/placeholder.jpg\n\n# Search Result\n\n[1]: TITLE: Exploring the beauty of nature\nURL: http://example.com/nature-article\nCONTENT: This article dives deep into the beauty of nature and its impact on our lives.\n\n[2]: TITLE: N/A\nURL: http://example.com/empty-article\nCONTENT: N/A\n```"
      ],
      "code_start_line": 12,
      "code_end_line": 31,
      "params": [
        "data"
      ],
      "have_return": true,
      "code_content": "def generate_markdown(data):\n    lines = []\n    # Images section\n    lines.append(\"# images \\n\")\n    images = data.get(\"images\", [])\n    for idx, image in enumerate(images, start=1):\n        desc = image.get(\"description\") or \"N/A\"\n        url = image.get(\"url\") or \"N/A\"\n        lines.append(f\"[{idx}] DESCRIPTION: {desc}\")\n        lines.append(f\"URL: {url}\\n\")\n    # Search Results section\n    lines.append(\"# Search Result\\n\")\n    for idx, result in enumerate(data.get(\"results\", []), start=1):\n        title = result.get(\"title\") or \"N/A\"\n        url = result.get(\"url\") or \"N/A\"\n        raw = result.get(\"raw_content\") or \"N/A\"\n        lines.append(f\"[{idx}]:TITLE: {title}\")\n        lines.append(f\"URL: {url}\")\n        lines.append(f\"CONTENT: {raw}\\n\")\n    return \"\\n\".join(lines)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/reportbench/extract_ground_truth.py": [
    {
      "type": "FunctionDef",
      "name": "filter_node",
      "md_content": [
        "**filter_node**: The function of filter_node is to recursively filter a data structure, specifically a dictionary or list, by removing any entries that contain an 'id' with the letter \"s\" in it.\n\n**parameters**: The parameters of this Function.\n· node: The input data structure, which can be a dictionary, list, or any other data type.\n\n**Code Description**: The filter_node function is designed to traverse a given data structure, which may be a dictionary or a list, and filter out specific entries based on the presence of an 'id' key. If the 'id' key exists and contains the letter \"s\" (case insensitive), that entry is excluded from the output. \n\nWhen the input is a dictionary, the function checks each key-value pair. If a key-value pair meets the filtering criteria (i.e., contains an 'id' with \"s\"), it returns None for that entry. Otherwise, it recursively calls itself on the value to continue filtering deeper into the structure. The resulting filtered entries are collected into a new dictionary, which is returned at the end of the function.\n\nIf the input is a list, the function iterates through each item in the list, applying the same filtering logic. Filtered items that do not meet the exclusion criteria are appended to a new list, which is returned.\n\nIf the input is neither a dictionary nor a list, the function simply returns the input as is. This behavior ensures that the function can handle mixed data types gracefully.\n\nThe filter_node function is called within the extractDirectoryTree function, which reads a JSON file and constructs a tree structure from its contents. After loading the JSON data, extractDirectoryTree invokes filter_node to filter the data based on the specified criteria before proceeding to build a tree structure and validate it as valid JSON. This integration highlights the importance of filter_node in ensuring that only relevant data is processed and included in the final output.\n\n**Note**: It is important to ensure that the input to filter_node is either a dictionary or a list for the function to operate correctly. If the input does not conform to these types, the function will return the input unchanged.\n\n**Output Example**: Given an input like the following dictionary:\n{\n    \"id\": \"123\",\n    \"title\": \"Sample Title\",\n    \"children\": [\n        {\"id\": \"s456\", \"title\": \"Excluded Title\"},\n        {\"id\": \"789\", \"title\": \"Included Title\"}\n    ]\n}\nThe output of filter_node would be:\n{\n    \"id\": \"123\",\n    \"title\": \"Sample Title\",\n    \"children\": [\n        {\"id\": \"789\", \"title\": \"Included Title\"}\n    ]\n}"
      ],
      "code_start_line": 4,
      "code_end_line": 24,
      "params": [
        "node"
      ],
      "have_return": true,
      "code_content": "def filter_node(node):\n    # Recursively filter node: if a dict contains an 'id' with \"s\" in it, skip it.\n    if isinstance(node, dict):\n        if 'id' in node and ('s' in node['id'].lower()):\n            return None\n        new_dict = {}\n        for key, value in node.items():\n            filtered = filter_node(value)\n            # if filtering a list or dict returns a falsey (None), we still want to include keys like title/text\n            if filtered is not None:\n                new_dict[key] = filtered\n        return new_dict\n    elif isinstance(node, list):\n        new_list = []\n        for item in node:\n            filtered_item = filter_node(item)\n            if filtered_item is not None:\n                new_list.append(filtered_item)\n        return new_list\n    else:\n        return node\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/extract_ground_truth.py/extractDirectoryTree"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "build_tree",
      "md_content": [
        "**build_tree**: The function of build_tree is to recursively construct a tree structure from a given node, extracting only the \"title\" and optionally including \"children\" based on the content.\n\n**parameters**: \n- node: A dictionary or list representing the current node to be processed in the tree construction.\n\n**Code Description**:  \nThe `build_tree` function is designed to recursively transform a given node (which can be either a dictionary or a list) into a simplified tree structure. This tree only includes the \"title\" field and, when applicable, a list of \"children\" nodes, which are also processed recursively.\n\nThe function starts by checking the type of the `node`:\n1. **When the node is a dictionary**:  \n   - If the dictionary contains a \"title\" key, a new dictionary is created with just the \"title\" from the original node. \n   - It then checks for the presence of a \"content\" key, which, if it's a list, indicates that the node has child elements. The function then processes each child recursively using the same `build_tree` function.\n   - If child trees are found, they are added to the \"children\" key of the new node.\n   - If the node doesn’t contain a \"title\" but has \"content\", the function proceeds by building the children for the \"content\" and, if children are found, returns a dictionary containing just the \"children\".\n   \n2. **When the node is a list**:  \n   - The function iterates through each item in the list, calling `build_tree` recursively for each. It collects and returns all resulting trees in a list.\n   - If no valid trees are returned, it returns `None`.\n\n3. **When the node is neither a dictionary nor a list**, it returns `None`.\n\nThe function ensures that only the title and children (if any) are retained in the final tree structure, and this structure can be nested depending on the depth of the input data.\n\nFrom a functional perspective, this function is used by the `extractDirectoryTree` function to simplify and reformat a potentially complex JSON structure into a tree with only titles and hierarchical relationships. This tree structure can then be further validated or processed as needed.\n\n**Note**: \n- The input data passed into `build_tree` should be either a dictionary or a list of dictionaries that include \"title\" and optionally \"content\" keys.\n- If the \"content\" field contains a list, the function will process it recursively.\n- The function can return either a single node (with a title and children) or a list of nodes, depending on the structure of the input.\n\n**Output Example**:  \nIf the input data is as follows:\n```json\n{\n    \"title\": \"Root\",\n    \"content\": [\n        {\n            \"title\": \"Child 1\",\n            \"content\": []\n        },\n        {\n            \"title\": \"Child 2\",\n            \"content\": [\n                {\n                    \"title\": \"Grandchild 1\"\n                }\n            ]\n        }\n    ]\n}\n```\n\nThe output of the `build_tree` function would be:\n```json\n{\n    \"title\": \"Root\",\n    \"children\": [\n        {\n            \"title\": \"Child 1\"\n        },\n        {\n            \"title\": \"Child 2\",\n            \"children\": [\n                {\n                    \"title\": \"Grandchild 1\"\n                }\n            ]\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 26,
      "code_end_line": 66,
      "params": [
        "node"
      ],
      "have_return": true,
      "code_content": "def build_tree(node):\n    # Recursively build a tree with only \"title\" (and optional \"children\")\n    if isinstance(node, dict):\n        if \"title\" in node:\n            new_node = {\"title\": node[\"title\"]}\n            if \"content\" in node and isinstance(node[\"content\"], list):\n                children = []\n                for child in node[\"content\"]:\n                    child_tree = build_tree(child)\n                    if child_tree:\n                        if isinstance(child_tree, list):\n                            children.extend(child_tree)\n                        else:\n                            children.append(child_tree)\n                if children:\n                    new_node[\"children\"] = children\n            return new_node\n        else:\n            if \"content\" in node and isinstance(node[\"content\"], list):\n                children = []\n                for child in node[\"content\"]:\n                    child_tree = build_tree(child)\n                    if child_tree:\n                        if isinstance(child_tree, list):\n                            children.extend(child_tree)\n                        else:\n                            children.append(child_tree)\n                if children:\n                    return {\"children\": children}\n            return None\n    elif isinstance(node, list):\n        trees = []\n        for item in node:\n            tree = build_tree(item)\n            if tree:\n                if isinstance(tree, list):\n                    trees.extend(tree)\n                else:\n                    trees.append(tree)\n        return trees if trees else None\n    return None\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/extract_ground_truth.py/extractDirectoryTree"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "build_markdown",
      "md_content": [
        "**build_markdown**: The function of build_markdown is to recursively generate markdown text from a given JSON structure, extracting titles and sentences while ignoring references.\n\n**parameters**:\n· node: A dictionary or list representing the current level of the JSON content being processed.  \n· level: An integer representing the current level of recursion, defaulting to 1. \n\n**Code Description**:  \nThe `build_markdown` function is designed to recursively traverse a JSON structure, extracting specific content to generate markdown text. This function is primarily used to convert JSON data into a readable markdown format, which includes the titles and sentence text but excludes any reference data.\n\n1. **Input Structure**: The function accepts a `node`, which can be a dictionary or a list. If it is a dictionary, the function checks for the presence of certain keys, such as `title`, `sentences`, and `content`. The key `title` will be converted into a markdown header (using `#` characters based on the `level` argument), and `sentences` will be processed to extract the text to be included in the markdown output.\n\n2. **Markdown Construction**:\n   - If the node is a dictionary, the function checks if the node contains a `title`. If found, the title is prepended with `#` characters, based on the current level of recursion (`level`), followed by the title itself and a newline.\n   - Next, if the node has a `sentences` key containing a list, the function iterates through each sentence and appends the `text` of the sentence to the markdown.\n   - The function also recursively processes child nodes under the `content` key, if present, by calling `build_markdown` on each child node, increasing the `level` by 1 to indicate the deeper hierarchy in the markdown.\n\n3. **List Handling**: If the `node` is a list, the function processes each item in the list by calling `build_markdown` on each item. This ensures that the function can handle both list and dictionary-based JSON structures.\n\n4. **Return Value**: The function returns the accumulated markdown content as a string, which includes titles, sentences, and recursively processed content from nested structures.\n\nThis function plays a key role in processing structured data and converting it into a markdown format suitable for human-readable documentation. It is used by other parts of the project, such as in the `extractMarkdownContent` function, which loads a JSON file and passes the data to `build_markdown` to generate markdown content from the JSON structure.\n\n**Note**: This function only processes data associated with `title` and `sentences`, excluding references or other data that might be present in the JSON. Therefore, the output will consist only of text data that is relevant to the markdown generation process.\n\n**Output Example**:  \nFor a JSON structure like:\n```json\n{\n  \"title\": \"Main Title\",\n  \"sentences\": [\n    {\"text\": \"This is the first sentence.\"},\n    {\"text\": \"This is the second sentence.\"}\n  ],\n  \"content\": [\n    {\n      \"title\": \"Sub Title 1\",\n      \"sentences\": [\n        {\"text\": \"Subsentence 1.1\"}\n      ]\n    }\n  ]\n}\n```\n\nThe output markdown generated by `build_markdown` would be:\n```\n# Main Title\n\nThis is the first sentence.\n\nThis is the second sentence.\n\n## Sub Title 1\n\nSubsentence 1.1\n```"
      ],
      "code_start_line": 68,
      "code_end_line": 87,
      "params": [
        "node",
        "level"
      ],
      "have_return": true,
      "code_content": "def build_markdown(node, level=1):\n    \"\"\"\n    Recursively builds a markdown text from the JSON content.\n    Only includes 'title' and text from 'sentences', ignoring references.\n    \"\"\"\n    md = \"\"\n    if isinstance(node, dict):\n        if \"title\" in node:\n            md += (\"#\" * level) + \" \" + node[\"title\"] + \"\\n\\n\"\n        if \"sentences\" in node and isinstance(node[\"sentences\"], list):\n            for sentence in node[\"sentences\"]:\n                if \"text\" in sentence:\n                    md += sentence[\"text\"].strip() + \"\\n\\n\"\n        if \"content\" in node and isinstance(node[\"content\"], list):\n            for child in node[\"content\"]:\n                md += build_markdown(child, level+1)\n    elif isinstance(node, list):\n        for item in node:\n            md += build_markdown(item, level)\n    return md\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/extract_ground_truth.py/extractMarkdownContent"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "build_section_content_pairs",
      "md_content": [
        "**build_section_content_pairs**: The function of build_section_content_pairs is to recursively construct a tree-like structure of section-content pairs.\n\n**parameters**: The parameters of this Function.\n· parameter1: node - A dictionary representing a section of a document. This dictionary may contain a title, content, and other possible children sections.\n\n**Code Description**: The build_section_content_pairs function is designed to process a section of a document and construct a hierarchical structure of section-content pairs. This structure includes the section's title, the content within the section, references within the content, and any child sections that may exist.\n\n- The function begins by verifying that the input node is a dictionary. If the input is not a dictionary or does not contain a \"title\" key, it returns `None`, indicating that the node is not a valid section.\n- The function then initializes a `result` dictionary, which will hold the section's title.\n- A helper function, `process_sentences`, is defined and used to extract the text and references from the sentences contained within a section. It checks if the content has the \"sentences\" key and extracts the relevant information from each sentence. Text content is added to a list, and references are collected in a set to avoid duplicates.\n- The function checks if the node contains direct content that is not part of a child section. If such content exists, it processes these sentences and adds them to the result.\n- The function proceeds to gather any references associated with the text and adds them to the result. The references are sorted before being added.\n- The function then looks for child sections in the current node's content. If any child sections are found, the function recursively calls itself to process those children and add them to the \"children\" list of the result.\n- Finally, the function returns the `result` dictionary, which contains the title, content, references, and any children sections of the node, following the section-content pair structure.\n\nThis function is called by the `extractSectionContentPairs` function. The `extractSectionContentPairs` function reads a JSON file, loads the data, and calls `build_section_content_pairs` to construct the section-content pairs. After constructing the structure, it validates the structure by attempting to serialize it back into a JSON string and then deserializing it to ensure that the structure is valid. The validated section-content pairs are then returned.\n\n**Note**: \n- This function assumes that the input node is a valid section that may or may not contain child sections. \n- The text content in the section is extracted from sentences within the node and any items in the \"content\" list that are not child sections.\n- Only sections with a \"title\" are processed, while non-section content is treated as plain text and processed accordingly.\n- The function ensures that references are collected without duplication by using a set, which is later sorted before being returned.\n  \n**Output Example**: A possible return value could look like this:\n\n```json\n{\n    \"title\": \"Introduction\",\n    \"content\": \"This is the introductory section of the document.\",\n    \"references\": [\"ref1\", \"ref2\"],\n    \"children\": [\n        {\n            \"title\": \"Background\",\n            \"content\": \"This section provides the background information.\",\n            \"references\": [\"ref3\"],\n            \"children\": []\n        },\n        {\n            \"title\": \"Objectives\",\n            \"content\": \"This section outlines the objectives of the study.\",\n            \"references\": [\"ref4\", \"ref5\"],\n            \"children\": []\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 89,
      "code_end_line": 151,
      "params": [
        "node"
      ],
      "have_return": true,
      "code_content": "def build_section_content_pairs(node):\n    \"\"\"\n    递归构建section-content pairs格式的树状结构:\n    {\n        \"title\": str,            # section标题\n        \"content\": str,          # section直接包含的文本内容\n        \"references\": [str],     # 文本内容的引用\n        \"children\": [{...}]      # 子sections (可选)\n    }\n    \"\"\"\n    if not isinstance(node, dict):\n        return None\n\n    # 如果不是section节点(没有title)，返回None\n    if \"title\" not in node:\n        return None\n    \n    result = {\n        \"title\": node[\"title\"]\n    }\n    \n    # 收集当前节点的文本内容和引用\n    text_parts = []\n    references = set()\n\n    # 处理当前节点及其直接内容的sentences\n    def process_sentences(content):\n        if isinstance(content, dict) and \"sentences\" in content:\n            for sentence in content[\"sentences\"]:\n                if isinstance(sentence, dict):\n                    if \"text\" in sentence:\n                        text_parts.append(sentence[\"text\"].strip())\n                    if \"references\" in sentence and isinstance(sentence[\"references\"], list):\n                        references.update(sentence[\"references\"])\n\n    # 处理当前节点的直接sentences\n    process_sentences(node)\n\n    # 处理content列表中的直接sentences\n    if \"content\" in node and isinstance(node[\"content\"], list):\n        for item in node[\"content\"]:\n            if not isinstance(item, dict) or \"title\" not in item:  # 只处理非section的直接内容\n                process_sentences(item)\n\n    # 如果收集到了文本内容，添加到结果中\n    if text_parts:\n        result[\"content\"] = \" \".join(text_parts)\n        if references:\n            result[\"references\"] = sorted(list(references))\n\n    # 处理子sections\n    children = []\n    if \"content\" in node and isinstance(node[\"content\"], list):\n        for child in node[\"content\"]:\n            if isinstance(child, dict) and \"title\" in child:  # 只处理作为section的子节点\n                child_result = build_section_content_pairs(child)\n                if child_result:\n                    children.append(child_result)\n    \n    if children:\n        result[\"children\"] = children\n\n    return result\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/extract_ground_truth.py/extractSectionContentPairs"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_sentences",
      "md_content": [
        "**process_sentences**: The function of process_sentences is to extract and collect specific information from a structured content input, particularly focusing on sentence text and references.\n\n**parameters**: The parameters of this function.\n· content: A dictionary that may contain a list of sentences, each potentially with text and references.\n\n**Code Description**: The function `process_sentences` processes the input `content`, which is expected to be a dictionary. It first checks if the dictionary contains a key named \"sentences\". If this key exists, the function proceeds to iterate over each item in the \"sentences\" list. For each item in the list, the function performs further checks:\n1. It ensures that the item is a dictionary.\n2. If the dictionary contains a key \"text\", it appends the stripped text of the sentence to a global or previously defined list, `text_parts`.\n3. Additionally, if the dictionary contains a \"references\" key and the value associated with this key is a list, it updates a global or previously defined set, `references`, by adding all items from the references list.\n\nThe function is intended to process structured sentence data, extracting relevant text and references, which can later be used for further processing or analysis.\n\n**Note**: \n- The function does not return any value; it modifies external variables (`text_parts` and `references`).\n- The function assumes that the structure of `content` and each sentence is consistent with the described format. Any deviation (e.g., missing expected keys or non-list values where lists are expected) may lead to no processing or errors.\n- The variables `text_parts` and `references` should be initialized before the function is called to ensure that the function has a place to store the extracted data."
      ],
      "code_start_line": 115,
      "code_end_line": 122,
      "params": [
        "content"
      ],
      "have_return": false,
      "code_content": "    def process_sentences(content):\n        if isinstance(content, dict) and \"sentences\" in content:\n            for sentence in content[\"sentences\"]:\n                if isinstance(sentence, dict):\n                    if \"text\" in sentence:\n                        text_parts.append(sentence[\"text\"].strip())\n                    if \"references\" in sentence and isinstance(sentence[\"references\"], list):\n                        references.update(sentence[\"references\"])\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extractDirectoryTree",
      "md_content": [
        "**extractDirectoryTree**: The function of extractDirectoryTree is to read a JSON file, filter its contents based on specific criteria, build a hierarchical tree structure from the filtered data, and validate the resulting structure as valid JSON.\n\n**parameters**: \n· input_file_path: A string representing the path to the input JSON file that will be read and processed.\n\n**Code Description**: The extractDirectoryTree function begins by opening and reading a JSON file specified by the input_file_path parameter. It uses the json library to load the contents of the file into a Python data structure. The function then calls the filter_node function to filter the loaded data. This filtering process removes any entries that contain an 'id' with the letter \"s\" in it, ensuring that only relevant data is retained for further processing.\n\nAfter filtering, the function invokes the build_tree function, which constructs a simplified tree structure that includes only the \"title\" and its associated \"children\" nodes. This tree structure is essential for representing the hierarchical relationships within the data.\n\nTo ensure the integrity of the resulting tree structure, the function attempts to serialize the tree into a JSON string and then deserialize it back into a Python object. This step serves as a validation check to confirm that the tree structure is valid JSON. If the serialization or deserialization fails, a ValueError is raised, indicating that the structure is invalid.\n\nFinally, the function returns the validated tree structure, which can be utilized by other components of the application. The extractDirectoryTree function is called within the __init__ method of the ReportBenchmark class, where it is used to extract a breadth ground truth representation from the specified JSON input path. This integration highlights the function's role in preparing data for further analysis and reporting within the broader context of the application.\n\n**Note**: It is crucial that the input JSON file adheres to the expected structure, as the filtering and tree-building processes depend on the presence of specific keys such as \"id\" and \"title\". Any deviations from this structure may lead to unexpected results or errors during execution.\n\n**Output Example**: Given a valid input JSON file, the output of the extractDirectoryTree function might resemble the following structure:\n```json\n{\n    \"title\": \"Root Node\",\n    \"children\": [\n        {\n            \"title\": \"Child Node 1\"\n        },\n        {\n            \"title\": \"Child Node 2\",\n            \"children\": [\n                {\n                    \"title\": \"Grandchild Node\"\n                }\n            ]\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 153,
      "code_end_line": 172,
      "params": [
        "input_file_path"
      ],
      "have_return": true,
      "code_content": "def extractDirectoryTree(input_file_path):\n    # Read original JSON file\n    with open(input_file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    # Filter JSON tree based on node \"id\"\n    filtered_data = filter_node(data)\n\n    # Build tree containing only the layer structure and titles\n    tree_structure = build_tree(filtered_data)\n\n    # Validate that tree_structure is valid JSON by serializing and deserializing it\n    try:\n        s = json.dumps(tree_structure)\n        valid_tree = json.loads(s)\n    except Exception as e:\n        raise ValueError(\"Invalid JSON structure: \" + str(e))\n    \n    # Return the valid JSON structure\n    return valid_tree\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/__init__",
        "src/criticsearch/reportbench/reward_calculation.py"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/extract_ground_truth.py/filter_node",
        "src/criticsearch/reportbench/extract_ground_truth.py/build_tree"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "extractMarkdownContent",
      "md_content": [
        "**extractMarkdownContent**: The function of extractMarkdownContent is to read a JSON file and convert its content into a markdown format.\n\n**parameters**: The parameters of this Function.\n· input_file_path: A string representing the path to the input JSON file that contains the data to be processed.\n\n**Code Description**: The extractMarkdownContent function is responsible for reading a JSON file specified by the input_file_path parameter. It opens the file in read mode with UTF-8 encoding and loads its content into a Python dictionary using the json.load function. Once the JSON data is loaded, the function calls the build_markdown function, passing the loaded data as an argument. The build_markdown function processes the JSON structure recursively, extracting titles and sentences to generate a markdown representation of the content. After the markdown text is generated, extractMarkdownContent returns this text as a string instead of saving it to a file.\n\nThis function is utilized within the ReportBenchmark class's __init__ method, where it is called to extract markdown content from the JSON input path provided during the instantiation of the class. The markdown content generated by extractMarkdownContent is then used to create sections for the report, demonstrating its role in transforming structured JSON data into a human-readable markdown format.\n\n**Note**: It is important to ensure that the input JSON file is correctly formatted and contains the expected structure, as the function relies on the presence of specific keys to generate the markdown content accurately.\n\n**Output Example**: For a JSON structure like:\n```json\n{\n  \"title\": \"Sample Title\",\n  \"sentences\": [\n    {\"text\": \"This is a sample sentence.\"},\n    {\"text\": \"This is another sample sentence.\"}\n  ],\n  \"content\": [\n    {\n      \"title\": \"Subsection Title\",\n      \"sentences\": [\n        {\"text\": \"This is a sentence in the subsection.\"}\n      ]\n    }\n  ]\n}\n```\nThe output markdown generated by extractMarkdownContent would be:\n```\n# Sample Title\n\nThis is a sample sentence.\n\nThis is another sample sentence.\n\n## Subsection Title\n\nThis is a sentence in the subsection.\n```"
      ],
      "code_start_line": 174,
      "code_end_line": 183,
      "params": [
        "input_file_path"
      ],
      "have_return": true,
      "code_content": "def extractMarkdownContent(input_file_path):\n    # Read original JSON file\n    with open(input_file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    # Build markdown text from JSON structure\n    md_text = build_markdown(data)\n    \n    # Return markdown text instead of saving to a file\n    return md_text\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/__init__"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/extract_ground_truth.py/build_markdown"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "extractSectionContentPairs",
      "md_content": [
        "**extractSectionContentPairs**: The function of extractSectionContentPairs is to extract a structured set of section-content pairs from a JSON file and validate the structure.\n\n**parameters**:\n· parameter1: input_file_path (str) - The path to the JSON file that contains the data to be processed.\n\n**Code Description**: The function `extractSectionContentPairs` is designed to extract and return a structured set of section-content pairs from a given JSON file. It follows these main steps:\n\n1. **Reading the JSON File**: The function first opens and reads the JSON file specified by the `input_file_path` parameter. It uses Python's built-in `json.load()` method to parse the contents of the file into a Python dictionary.\n\n2. **Building Section-Content Pairs**: After loading the data, the function calls the `build_section_content_pairs` function, which is responsible for constructing the hierarchical structure of section-content pairs from the data. This function processes the JSON structure recursively to create a tree-like representation of the sections and their associated content.\n\n3. **Validating the Structure**: The function then attempts to validate the generated structure by serializing it into a JSON string using `json.dumps()` and deserializing it back into a Python object using `json.loads()`. This step ensures that the resulting data structure adheres to valid JSON formatting. If any error occurs during this process (e.g., invalid or malformed data), a `ValueError` is raised with a message indicating the specific issue.\n\n4. **Returning the Validated Pairs**: After validation, the function returns the validated section-content pairs structure, which is in the form of a dictionary containing titles, content, references, and child sections (if any).\n\nIn the broader context of the project, the `extractSectionContentPairs` function is invoked within the `ReportBenchmark` class's `__init__` method. Specifically, it is called to extract the section-content pairs from the input JSON file, which is then used as part of the initialization process for creating a report benchmark. The section-content pairs are essential for further processing and organizing the content into structured sections.\n\nThe function also relies on the `build_section_content_pairs` function to construct the section-content pairs. This helper function is responsible for processing each section recursively and ensuring that sections, their content, and any child sections are correctly represented in the final structure.\n\n**Note**: \n- The input file must be a valid JSON file containing a structure that can be processed into section-content pairs.\n- The validation step ensures that the resulting structure is compatible with standard JSON formatting, which prevents issues during further processing.\n- The `build_section_content_pairs` function should return a valid structure for each section in the input data to ensure proper functionality.\n\n**Output Example**: A possible return value from the `extractSectionContentPairs` function could look like this:\n\n```json\n{\n    \"title\": \"Introduction\",\n    \"content\": \"This is the introductory section of the document.\",\n    \"references\": [\"ref1\", \"ref2\"],\n    \"children\": [\n        {\n            \"title\": \"Background\",\n            \"content\": \"This section provides the background information.\",\n            \"references\": [\"ref3\"],\n            \"children\": []\n        },\n        {\n            \"title\": \"Objectives\",\n            \"content\": \"This section outlines the objectives of the study.\",\n            \"references\": [\"ref4\", \"ref5\"],\n            \"children\": []\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 185,
      "code_end_line": 203,
      "params": [
        "input_file_path"
      ],
      "have_return": true,
      "code_content": "def extractSectionContentPairs(input_file_path):\n    \"\"\"\n    从JSON文件中提取section-content pairs结构\n    \"\"\"\n    # 读取原始JSON文件\n    with open(input_file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    # 构建section-content pairs结构\n    pairs_structure = build_section_content_pairs(data)\n    \n    # 验证JSON结构\n    try:\n        s = json.dumps(pairs_structure)\n        valid_pairs = json.loads(s)\n    except Exception as e:\n        raise ValueError(\"Invalid JSON structure: \" + str(e))\n    \n    return valid_pairs\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/__init__"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/extract_ground_truth.py/build_section_content_pairs"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "extract_markdown_sections",
      "md_content": [
        "**extract_markdown_sections**: The function of extract_markdown_sections is to extract markdown sections based on header lines.\n\n**parameters**:\n· md_text: A string representing the markdown text from which sections will be extracted.\n\n**Code Description**: The `extract_markdown_sections` function processes a given markdown text (`md_text`) and divides it into sections based on headers, which are lines that start with the `#` symbol. It iterates through each line of the markdown text, identifying these header lines and treating them as boundaries between different sections. \n\n- The function initializes two variables: `sections` (an empty list to store the resulting sections) and `current_section` (an empty list that temporarily holds the lines of the current section being processed).\n- It then loops through each line of the markdown text:\n  - If the line starts with `#` (indicating a header), the function checks whether there is any content in `current_section`. If so, it joins the lines of `current_section` into a single string, strips any leading or trailing whitespace, and appends it to the `sections` list.\n  - After handling the previous section, `current_section` is reset to an empty list, and the new section starts accumulating lines.\n  - If the line does not start with a header, it is simply added to the current section being processed.\n- After the loop finishes, the function checks if there is any remaining content in `current_section` and appends it to the `sections` list if necessary.\n- The function returns the `sections` list, where each element is a string representing a separate section of the original markdown content.\n\nThis function is primarily called within the `ReportBenchmark` class constructor in `src/criticsearch/reportbench/report_benchmark.py`. In that context, it is used to process the markdown content of a report, splitting the content into distinct sections, each corresponding to a markdown header. These sections are then stored in the `sections` attribute, which can be used later in the class for generating reports or performing other operations related to the markdown content.\n\n**Note**: \n- The function assumes that the markdown content is formatted correctly with headers starting with `#`.\n- It does not handle cases where headers are malformed or have no content between them.\n- The function processes the markdown text line by line, so it may not handle large files efficiently in cases of extreme text sizes.\n\n**Output Example**:\nGiven an input markdown text:\n\n```\n# Section 1\nThis is the first section.\n\n# Section 2\nThis is the second section.\n```\n\nThe function would return the following list:\n\n```\n[\n    \"# Section 1\\nThis is the first section.\",\n    \"# Section 2\\nThis is the second section.\"\n]\n```"
      ],
      "code_start_line": 205,
      "code_end_line": 221,
      "params": [
        "md_text"
      ],
      "have_return": true,
      "code_content": "def extract_markdown_sections(md_text):\n    \"\"\"\n    Extract markdown sections based on header lines.\n    遇到新的标题（以#开头）则开始新的 section，\n    返回一个包含各 section 的列表，每个 section 为一个字符串。\n    \"\"\"\n    sections = []\n    current_section = []\n    for line in md_text.splitlines():\n        if line.strip().startswith(\"#\"):\n            if current_section:\n                sections.append(\"\\n\".join(current_section).strip())\n                current_section = []\n        current_section.append(line)\n    if current_section:\n        sections.append(\"\\n\".join(current_section).strip())\n    return sections\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/reportbench/instruction_generator.py": [
    {
      "type": "ClassDef",
      "name": "InstructionGenerator",
      "md_content": [
        "**InstructionGenerator**: The function of InstructionGenerator is to traverse all JSON files in the \"wiki_data\" directory, generate corresponding writing instructions, and maintain a file-to-instruction mapping, which can be loaded for querying at any time.\n\n**attributes**: The attributes of this Class.\n· data_dir: Path to the directory containing JSON files to be processed. If not provided, it defaults to the \"wiki_data\" directory located within the same directory as the script.\n· mapping_file: Path to the JSON file that stores the mapping between filenames and their corresponding instructions. If provided as a relative path, it is resolved relative to the \"reportbench\" directory; otherwise, the absolute path is used.\n· agent: An instance of the BaseAgent class used for generating instructions based on the content of the JSON files.\n· mapping: A dictionary holding the mapping of filenames to their generated instructions, loaded from the mapping file during initialization or generated on demand.\n\n**Code Description**: \n\nThe `InstructionGenerator` class is designed to automate the generation of writing instructions for JSON files stored in a designated directory, typically \"wiki_data\". The class works as follows:\n\n1. **Initialization**: \n   - The constructor (`__init__`) accepts two optional parameters: `data_dir` (the directory containing JSON files) and `mapping_file` (the file where the mapping between JSON filenames and their respective instructions is stored). If no `data_dir` is provided, it defaults to \"wiki_data\" located in the same directory as the script. Similarly, if no `mapping_file` is provided, it defaults to \"instruction_mapping.json\".\n   - The `agent` attribute is initialized as an instance of the `BaseAgent` class, which is responsible for generating instructions based on the content of each JSON file.\n   - The constructor also includes debug print statements to display the resolved paths for `data_dir` and `mapping_file`, which help track where the program is looking for the JSON files and where it saves the mapping.\n   - The constructor also attempts to load an existing mapping from the `mapping_file` into the `mapping` attribute.\n\n2. **generate_instructions**:\n   - This method is responsible for generating instructions for each JSON file in the `data_dir`. It first filters the files, selecting those that either don’t exist in the mapping yet or whose instructions should be regenerated if `overwrite` is set to `True`.\n   - The method then defines a helper function (`process_file`) that reads the content of each JSON file, selects a random instruction type (such as short, long, etc.), and constructs a prompt for the agent to generate an instruction based on the file's content. The instruction is then added to the `mapping` dictionary.\n   - To process multiple files concurrently, a thread pool executor (`ThreadPoolExecutor`) is used, allowing multiple files to be processed in parallel, thus speeding up the overall execution.\n   - Once all files have been processed, the method saves the updated `mapping` back to the `mapping_file`.\n\n3. **load_mapping**:\n   - This method attempts to load the existing mapping of filenames to instructions from the `mapping_file`. If the file exists and can be read, the mapping is returned as a dictionary. If the file is missing or the contents cannot be parsed as JSON, it returns an empty dictionary.\n\n4. **get_instruction_by_file**:\n   - This method retrieves the instruction corresponding to a given JSON file. It returns the instruction as a string if found, or `None` if the file is not present in the mapping.\n\n5. **get_file_by_instruction**:\n   - This method searches the `mapping` dictionary for a given instruction text. If the exact instruction is found, it returns the filename of the corresponding JSON file; otherwise, it returns `None`.\n\n**Note**:\n- The `generate_instructions` method uses concurrent processing with a thread pool to speed up the generation of instructions. The maximum number of workers (threads) can be controlled via the `max_workers` parameter, allowing for efficient parallel processing of files.\n- If the `overwrite` parameter is set to `True`, all instructions for existing files will be regenerated, even if they were previously processed.\n- The generated instructions are stored in the `mapping_file`, which can be loaded again for future use, reducing the need to regenerate instructions every time the program runs.\n- If the `mapping_file` cannot be loaded or parsed, the program defaults to an empty mapping, which means no instructions will be available until they are generated.\n\n**Output Example**:\nA sample output from the `generate_instructions` method might look like this:\n\n```json\n{\n  \"article1.json\": \"Please design a long and detailed writing task instruction to recreate this article based on the given content.\",\n  \"article2.json\": \"Create a one-sentence instruction to guide the model in recreating the article based on the provided data.\",\n  \"article3.json\": \"Write a short instruction that captures the essence of this article's content for a model to replicate.\"\n}\n```\n\nIn this example, each key represents a JSON filename, and the corresponding value is the generated instruction text. The instructions are saved into the `mapping_file` as a JSON object and can be queried later using the provided methods."
      ],
      "code_start_line": 9,
      "code_end_line": 105,
      "params": [],
      "have_return": true,
      "code_content": "class InstructionGenerator:\n    \"\"\"\n    遍历 wiki_data 目录下所有 json 文件，反向生成写作指令（instruction），\n    并保存 file<->instruction 映射，支持随时加载查询。\n    \"\"\"\n\n    def __init__(\n        self,\n        data_dir: str = None,\n        mapping_file: str = \"instruction_mapping.json\",\n    ):\n        # 确保 data_dir 指向本模块下的 wiki_data 目录\n        self.data_dir = Path(data_dir) if data_dir else Path(__file__).parent / \"wiki_data\"\n\n        # 如果 mapping_file 是相对路径，强制放到父目录 reportbench 下；否则保持绝对路径\n        base_dir = Path(__file__).parent      # reportbench 目录\n        mf = Path(mapping_file)\n        self.mapping_file = (base_dir / mf.name) if not mf.is_absolute() else mf\n\n        self.agent = BaseAgent()\n\n        # debug：打印一下实际扫描目录和映射文件路径\n        print(f\"[DEBUG] InstructionGenerator.data_dir    = {self.data_dir.resolve()}\")\n        print(f\"[DEBUG] InstructionGenerator.mapping_file = {self.mapping_file.resolve()}\")\n\n        # 如果已有映射，则直接加载\n        self.mapping = self.load_mapping()\n\n    def generate_instructions(self, overwrite: bool = False, max_workers: int = 20) -> dict:\n        \"\"\"\n        并发为每个 json 文件生成一条 instruction，保存到 mapping_file。\n        :param overwrite: 如果为 True，则重新生成所有文件的指令，否则跳过已存在的条目。\n        :param max_workers: 线程池的最大工作线程数。\n        :return: mapping: { \"filename.json\": \"生成的指令文本\", ... }\n        \"\"\"\n        # 筛选需要处理的文件\n        files = [\n            jp for jp in self.data_dir.glob(\"*.json\")\n            if overwrite or jp.name not in self.mapping\n        ]\n        if not files:\n            print(\"[INFO] No new JSON files to process.\")\n            return self.mapping\n\n        def process_file(jp: Path):\n            content = jp.read_text(encoding=\"utf-8\")\n            instruction_type = random.choice([\"short\", \"long\", \"long and detailed\", \"super short\", \"one-sentence\"])\n            prompt = (\n                \"Below is the complete JSON content of an article:\\n\"\n                f\"{content}\\n\\n\"\n                f\"Please design an appropriate {instruction_type} writing task instruction that would allow \"\n                \"a model to recreate this article based on the instruction.\\n\"\n                \"Output only the instruction text, without any other explanations.\"\n                \"Make sure you don't include any specific detailed content about the article in the instruction.\\n\"\n            )\n            instr = self.agent.chat(usr_prompt=prompt)\n            return jp.name, instr.strip()\n\n        # 使用线程池并发调用\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            future_to_name = {executor.submit(process_file, jp): jp.name for jp in files}\n            for future in as_completed(future_to_name):\n                fname, instruction = future.result()\n                self.mapping[fname] = instruction\n                print(f\"[INFO] Generated instruction for {fname}\")\n\n        # 保存映射到文件\n        with self.mapping_file.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(self.mapping, f, ensure_ascii=False, indent=2)\n\n        return self.mapping\n\n    def load_mapping(self) -> dict:\n        \"\"\"\n        加载已存在的 file<->instruction 映射，不存在则返回空 dict。\n        \"\"\"\n        if self.mapping_file.exists():\n            try:\n                return json.loads(self.mapping_file.read_text(encoding=\"utf-8\"))\n            except json.JSONDecodeError:\n                return {}\n        return {}\n\n    def get_instruction_by_file(self, filename: str) -> str | None:\n        \"\"\"\n        根据 JSON 文件名获取对应的 instruction。\n        \"\"\"\n        return self.mapping.get(filename)\n\n    def get_file_by_instruction(self, instruction: str) -> str | None:\n        \"\"\"\n        根据 instruction 文本查找对应的 JSON 文件名（精确匹配）。\n        \"\"\"\n        for fname, instr in self.mapping.items():\n            if instr == instruction:\n                return fname\n        return None\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "## Class: `InstructionGenerator`\n### Method: `__init__`\n\n#### Description:\nThe `__init__` method is the constructor for the `InstructionGenerator` class. It initializes the core attributes of the class, including the directory paths for data and mapping files, and sets up the necessary components to function within the system. The constructor ensures that the class is properly configured to handle instructions and map them effectively using the provided data.\n\n#### Parameters:\n- `data_dir` (str, optional): This parameter specifies the directory path where the data required by the `InstructionGenerator` is stored. If no value is provided, the default value is set to a relative path pointing to the `wiki_data` directory located in the same folder as the script. \n- `mapping_file` (str, optional): This parameter defines the mapping file to be used for instructions. The default value is set to `\"instruction_mapping.json\"`. If the provided path is relative, it will be resolved to the parent directory (`reportbench`). If the provided path is absolute, it will remain unchanged.\n\n#### Attributes:\n- `data_dir` (Path): A `Path` object representing the directory path for storing data files. It is either provided by the user through the `data_dir` parameter or defaults to the `wiki_data` directory located in the same folder as the script.\n- `mapping_file` (Path): A `Path` object pointing to the file used for mapping instructions. It is resolved based on the provided `mapping_file` argument, ensuring that relative paths are mapped to the `reportbench` directory.\n- `agent` (BaseAgent): An instance of the `BaseAgent` class, which provides core functionality for managing search queries, executing searches, and handling interactions with various tools.\n- `mapping` (dict): A dictionary object representing the mapping of instructions, which is loaded during initialization.\n\n#### Functionality:\n1. **Directory Setup:**\n   The `data_dir` is set either from the provided `data_dir` argument or defaults to a relative path pointing to the `wiki_data` directory within the same location as the script file. If no `data_dir` is specified, the constructor will automatically determine this path.\n\n2. **Mapping File Resolution:**\n   The `mapping_file` parameter is processed such that if it is a relative path, it is resolved to the `reportbench` parent directory. If an absolute path is provided, the constructor retains the absolute path as-is.\n\n3. **Agent Initialization:**\n   The constructor initializes an instance of the `BaseAgent` class and assigns it to the `agent` attribute. The `BaseAgent` provides essential functionalities for handling queries and interacting with tools, supporting the agent's operations within the `InstructionGenerator` class.\n\n4. **Debug Logging:**\n   The method includes debug print statements that log the resolved `data_dir` and `mapping_file` paths to provide transparency about the actual locations used by the class.\n\n5. **Mapping Loading:**\n   The constructor attempts to load the instruction mapping using the `load_mapping()` method, which is executed immediately upon initialization. This ensures that any existing mappings are available for use in the instruction generation process.\n\n#### Example Usage:\n```python\n# Initialize the InstructionGenerator with default paths\ninstruction_generator = InstructionGenerator()\n\n# Initialize with a custom data directory and mapping file\ninstruction_generator = InstructionGenerator(data_dir=\"path/to/data\", mapping_file=\"custom_mapping.json\")\n```\n\n#### Notes:\n- The constructor will always ensure that the data directory is set to the correct path, either from the argument or by defaulting to the `wiki_data` directory.\n- The mapping file path will be resolved to an absolute path if it is relative, ensuring consistency in file resolution across different environments."
      ],
      "code_start_line": 15,
      "code_end_line": 35,
      "params": [
        "self",
        "data_dir",
        "mapping_file"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        data_dir: str = None,\n        mapping_file: str = \"instruction_mapping.json\",\n    ):\n        # 确保 data_dir 指向本模块下的 wiki_data 目录\n        self.data_dir = Path(data_dir) if data_dir else Path(__file__).parent / \"wiki_data\"\n\n        # 如果 mapping_file 是相对路径，强制放到父目录 reportbench 下；否则保持绝对路径\n        base_dir = Path(__file__).parent      # reportbench 目录\n        mf = Path(mapping_file)\n        self.mapping_file = (base_dir / mf.name) if not mf.is_absolute() else mf\n\n        self.agent = BaseAgent()\n\n        # debug：打印一下实际扫描目录和映射文件路径\n        print(f\"[DEBUG] InstructionGenerator.data_dir    = {self.data_dir.resolve()}\")\n        print(f\"[DEBUG] InstructionGenerator.mapping_file = {self.mapping_file.resolve()}\")\n\n        # 如果已有映射，则直接加载\n        self.mapping = self.load_mapping()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/reportbench/instruction_generator.py/InstructionGenerator/load_mapping"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "generate_instructions",
      "md_content": [
        "**generate_instructions**: The function of generate_instructions is to concurrently generate instructions for each JSON file and save them to a mapping file.\n\n**parameters**: The parameters of this Function.\n· overwrite: A boolean flag indicating whether to regenerate instructions for all files or skip existing entries. Default is False.\n· max_workers: An integer specifying the maximum number of worker threads in the thread pool. Default is 20.\n\n**Code Description**: The generate_instructions function is designed to process JSON files located in a specified directory, generating writing task instructions based on the content of each file. It begins by filtering the JSON files that need to be processed, either by checking if the overwrite parameter is set to True or if the file is not already present in the mapping dictionary. If no new files are found, it logs an informational message and returns the existing mapping.\n\nThe function defines a nested process_file function that reads the content of a JSON file, randomly selects an instruction type, and constructs a prompt for generating an instruction. This prompt is sent to an agent (presumably an AI model) that generates the instruction text. The results are collected using a ThreadPoolExecutor, which allows for concurrent processing of multiple files, improving efficiency.\n\nOnce all instructions are generated, the mapping of filenames to their respective instructions is saved to a specified mapping file in JSON format. Finally, the function returns the updated mapping.\n\n**Note**: It is important to ensure that the data directory contains the appropriate JSON files and that the mapping file is accessible for writing. The overwrite parameter should be used carefully to avoid unintentional data loss.\n\n**Output Example**: A possible return value of the function could look like this:\n{\n  \"article1.json\": \"Create a short summary of the article.\",\n  \"article2.json\": \"Draft a detailed analysis based on the provided content.\",\n  \"article3.json\": \"Write a one-sentence description of the main topic.\"\n}"
      ],
      "code_start_line": 37,
      "code_end_line": 79,
      "params": [
        "self",
        "overwrite",
        "max_workers"
      ],
      "have_return": true,
      "code_content": "    def generate_instructions(self, overwrite: bool = False, max_workers: int = 20) -> dict:\n        \"\"\"\n        并发为每个 json 文件生成一条 instruction，保存到 mapping_file。\n        :param overwrite: 如果为 True，则重新生成所有文件的指令，否则跳过已存在的条目。\n        :param max_workers: 线程池的最大工作线程数。\n        :return: mapping: { \"filename.json\": \"生成的指令文本\", ... }\n        \"\"\"\n        # 筛选需要处理的文件\n        files = [\n            jp for jp in self.data_dir.glob(\"*.json\")\n            if overwrite or jp.name not in self.mapping\n        ]\n        if not files:\n            print(\"[INFO] No new JSON files to process.\")\n            return self.mapping\n\n        def process_file(jp: Path):\n            content = jp.read_text(encoding=\"utf-8\")\n            instruction_type = random.choice([\"short\", \"long\", \"long and detailed\", \"super short\", \"one-sentence\"])\n            prompt = (\n                \"Below is the complete JSON content of an article:\\n\"\n                f\"{content}\\n\\n\"\n                f\"Please design an appropriate {instruction_type} writing task instruction that would allow \"\n                \"a model to recreate this article based on the instruction.\\n\"\n                \"Output only the instruction text, without any other explanations.\"\n                \"Make sure you don't include any specific detailed content about the article in the instruction.\\n\"\n            )\n            instr = self.agent.chat(usr_prompt=prompt)\n            return jp.name, instr.strip()\n\n        # 使用线程池并发调用\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            future_to_name = {executor.submit(process_file, jp): jp.name for jp in files}\n            for future in as_completed(future_to_name):\n                fname, instruction = future.result()\n                self.mapping[fname] = instruction\n                print(f\"[INFO] Generated instruction for {fname}\")\n\n        # 保存映射到文件\n        with self.mapping_file.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(self.mapping, f, ensure_ascii=False, indent=2)\n\n        return self.mapping\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_file",
      "md_content": [
        "**process_file**: The function of process_file is to read the content of a specified file and generate a writing task instruction based on that content.\n\n**parameters**: The parameters of this Function.\n· jp: A Path object representing the file path from which the content will be read.\n\n**Code Description**: The process_file function is designed to facilitate the generation of writing task instructions based on the content of a file. It begins by reading the text content of the file specified by the Path object `jp` using UTF-8 encoding. Once the content is successfully retrieved, the function randomly selects an instruction type from a predefined list, which includes options such as \"short\", \"long\", \"long and detailed\", \"super short\", and \"one-sentence\".\n\nThe function then constructs a prompt that incorporates the file content and the randomly chosen instruction type. This prompt is formatted to instruct a conversational model to create a writing task instruction that allows the model to recreate the article based on the provided instruction. Importantly, the prompt specifies that the instruction should not include any specific details about the article itself, ensuring that the generated instruction remains general and applicable.\n\nTo generate the instruction, the function calls the `chat` method from the BaseAgent class, passing the constructed prompt as the `usr_prompt` parameter. This method is responsible for interacting with the conversational model and obtaining a response based on the provided input. The response, which is expected to be the writing task instruction, is then stripped of any leading or trailing whitespace before being returned along with the name of the file.\n\nThe process_file function plays a crucial role in the overall workflow of the project by enabling the dynamic generation of writing tasks based on varying content. It leverages the capabilities of the chat method to ensure that the instructions generated are contextually relevant and aligned with the content of the file being processed.\n\n**Note**: It is essential to ensure that the file specified by the `jp` parameter exists and is accessible. Additionally, the content of the file should be appropriate for generating meaningful writing task instructions.\n\n**Output Example**: A possible return value from the process_file function could be:\n(\"example_article.txt\", \"Create a long and detailed writing task instruction based on the provided content.\")"
      ],
      "code_start_line": 53,
      "code_end_line": 65,
      "params": [
        "jp"
      ],
      "have_return": true,
      "code_content": "        def process_file(jp: Path):\n            content = jp.read_text(encoding=\"utf-8\")\n            instruction_type = random.choice([\"short\", \"long\", \"long and detailed\", \"super short\", \"one-sentence\"])\n            prompt = (\n                \"Below is the complete JSON content of an article:\\n\"\n                f\"{content}\\n\\n\"\n                f\"Please design an appropriate {instruction_type} writing task instruction that would allow \"\n                \"a model to recreate this article based on the instruction.\\n\"\n                \"Output only the instruction text, without any other explanations.\"\n                \"Make sure you don't include any specific detailed content about the article in the instruction.\\n\"\n            )\n            instr = self.agent.chat(usr_prompt=prompt)\n            return jp.name, instr.strip()\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat",
        "src/criticsearch/base_agent.py/BaseAgent/chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "load_mapping",
      "md_content": [
        "**load_mapping**: The function of load_mapping is to load an existing file-to-instruction mapping from a file, or return an empty dictionary if the mapping file does not exist or cannot be read.\n\n**parameters**: The function does not take any parameters.\n\n**Code Description**:  \nThe `load_mapping` function is a method of the `InstructionGenerator` class, designed to load a previously saved mapping of files to instructions from a JSON file. Specifically, it checks if the `mapping_file` exists on the file system and attempts to read its contents. \n\n- If the file exists, it tries to parse the contents as JSON using the `json.loads()` method. If the parsing is successful, it returns the resulting dictionary. \n- If the file exists but the contents cannot be parsed as valid JSON (e.g., due to file corruption or incorrect formatting), it catches the `JSONDecodeError` and returns an empty dictionary.\n- If the file does not exist, the function also returns an empty dictionary.\n\nThis method is used by the `InstructionGenerator` class, which is responsible for managing the mapping between files and their corresponding instructions. The `mapping_file` attribute, which points to the location of the mapping file, is set when the `InstructionGenerator` is initialized. During initialization, if an existing mapping is available, it is loaded into the `mapping` attribute by calling this `load_mapping` method.\n\nThe `load_mapping` function is crucial for ensuring that the `InstructionGenerator` has access to any previously stored file-to-instruction mappings, allowing it to efficiently retrieve instructions associated with files. If no valid mapping is found (either due to a missing or corrupt file), the function ensures that the system can proceed without errors by returning an empty dictionary, allowing for the handling of such cases without disrupting the flow of the program.\n\n**Note**:  \n- The function ensures that the application can continue even if the mapping file is missing or corrupt. However, this could lead to a situation where no file-to-instruction mappings are available, which may affect the functionality depending on the context in which the `InstructionGenerator` is used.\n- The function uses UTF-8 encoding for reading the file, ensuring compatibility with common text file formats.\n- The method is safe to use in environments where the mapping file may or may not exist, as it gracefully handles both cases.\n\n**Output Example**:  \nIn the case where the `mapping_file` exists and contains valid JSON, the return value might look like the following:\n\n```json\n{\n    \"file1.txt\": \"instruction1\",\n    \"file2.txt\": \"instruction2\"\n}\n```\n\nIf the file does not exist, or if it contains invalid JSON, the return value would be:\n\n```json\n{}\n```"
      ],
      "code_start_line": 81,
      "code_end_line": 90,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def load_mapping(self) -> dict:\n        \"\"\"\n        加载已存在的 file<->instruction 映射，不存在则返回空 dict。\n        \"\"\"\n        if self.mapping_file.exists():\n            try:\n                return json.loads(self.mapping_file.read_text(encoding=\"utf-8\"))\n            except json.JSONDecodeError:\n                return {}\n        return {}\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/instruction_generator.py/InstructionGenerator/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_instruction_by_file",
      "md_content": [
        "**get_instruction_by_file**: The function of get_instruction_by_file is to retrieve the corresponding instruction based on the provided JSON filename.\n\n**parameters**: The parameters of this Function.\n· filename: A string representing the name of the JSON file for which the instruction is to be retrieved.\n\n**Code Description**: The get_instruction_by_file function is designed to access a mapping of instructions associated with various JSON filenames. It takes a single parameter, filename, which is expected to be a string. The function attempts to retrieve the instruction corresponding to the given filename from a mapping attribute (presumably a dictionary) of the class instance. If the filename exists in the mapping, the function returns the associated instruction as a string. If the filename does not exist in the mapping, the function returns None, indicating that no instruction could be found for the provided filename.\n\n**Note**: It is important to ensure that the filename provided as an argument is valid and corresponds to an entry in the mapping. If the filename is not found, the function will return None, which should be handled appropriately in the calling code to avoid unexpected behavior.\n\n**Output Example**: If the mapping contains an entry for \"example.json\" with the instruction \"This is an example instruction.\", calling get_instruction_by_file(\"example.json\") would return \"This is an example instruction.\". Conversely, if the filename \"nonexistent.json\" is provided and does not exist in the mapping, the function would return None."
      ],
      "code_start_line": 92,
      "code_end_line": 96,
      "params": [
        "self",
        "filename"
      ],
      "have_return": true,
      "code_content": "    def get_instruction_by_file(self, filename: str) -> str | None:\n        \"\"\"\n        根据 JSON 文件名获取对应的 instruction。\n        \"\"\"\n        return self.mapping.get(filename)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_file_by_instruction",
      "md_content": [
        "**get_file_by_instruction**: The function of get_file_by_instruction is to find the corresponding JSON file name based on an exact match of the provided instruction text.\n\n**parameters**: The parameters of this Function.\n· instruction: A string representing the instruction text to be matched against the mapping.\n\n**Code Description**: The get_file_by_instruction function iterates through a mapping of file names to instruction texts stored in the object's attribute `self.mapping`. For each entry in this mapping, it checks if the instruction text provided as an argument matches the instruction text associated with the file name. If a match is found, the function returns the corresponding file name. If no match is found after checking all entries, the function returns None, indicating that there is no file associated with the provided instruction.\n\nThis function is useful in scenarios where you need to retrieve a specific JSON file based on a known instruction, ensuring that the retrieval process is efficient and straightforward. The function relies on exact matching, meaning that the instruction must match exactly with one of the entries in the mapping for a file name to be returned.\n\n**Note**: It is important to ensure that the instruction provided is accurate and matches the expected format in the mapping. If the instruction does not exist in the mapping, the function will return None, which should be handled appropriately in the calling code to avoid potential errors.\n\n**Output Example**: If the mapping contains an entry where the instruction \"Load Data\" corresponds to the file name \"data_load.json\", calling get_file_by_instruction(\"Load Data\") would return \"data_load.json\". If the instruction \"Save Data\" does not exist in the mapping, the function would return None."
      ],
      "code_start_line": 98,
      "code_end_line": 105,
      "params": [
        "self",
        "instruction"
      ],
      "have_return": true,
      "code_content": "    def get_file_by_instruction(self, instruction: str) -> str | None:\n        \"\"\"\n        根据 instruction 文本查找对应的 JSON 文件名（精确匹配）。\n        \"\"\"\n        for fname, instr in self.mapping.items():\n            if instr == instruction:\n                return fname\n        return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/reportbench/verifier.py": [
    {
      "type": "ClassDef",
      "name": "ReportVerifier",
      "md_content": [
        "# ReportVerifier Class Documentation\n\n## Overview\nThe `ReportVerifier` class is designed to perform factual verification of extracted data based on a provided context. It verifies whether the answers extracted from a report or document match the expected answers to specific questions. The class utilizes the Rouge-L scoring metric for partial matches and supports multi-threaded verification of multiple questions for efficiency.\n\n### Key Features:\n- **Factual Question Verification**: Verifies the correctness of answers to factual questions.\n- **Answer Comparison**: Compares model-generated answers with ground truth using exact matches and Rouge-L score for partial matches.\n- **Parallel Processing**: Uses a thread pool to process multiple questions concurrently, improving efficiency.\n- **Answer Normalization**: Standardizes answers by removing non-alphanumeric characters and converting text to lowercase.\n\n## Constructor: `__init__(self, agent)`\n### Parameters:\n- `agent`: The agent used to interact with the external source (e.g., a model or API) for verifying answers.\n\n### Description:\nThe constructor initializes the `ReportVerifier` with an agent for interacting with external systems and a `RougeScorer` to calculate Rouge-L scores for partial answer matches. \n\n---\n\n## Methods\n\n### 1. `verify_section(self, context: str, extracted_facts: List[Dict]) -> float`\n#### Parameters:\n- `context`: A string representing the context (e.g., document or passage) that is used to verify the extracted facts.\n- `extracted_facts`: A list of dictionaries, where each dictionary contains a question (`\"question\"`), the expected answer (`\"answer\"`), and the format in which the answer should be.\n\n#### Returns:\n- A float representing the final accuracy score for the verification process.\n\n#### Description:\nThis method verifies a list of factual questions using the provided context. Each question is matched against the extracted answers. For each question, it sends the question to an agent for verification, compares the model's answer with the ground truth, and computes the accuracy. The verification process is executed in parallel using a thread pool for efficiency.\n\n- For each extracted fact, the `verify_single_question` helper function is used to verify the question by interacting with the agent.\n- After all questions are verified, the method calculates the accuracy by considering exact matches and Rouge-L scores for partial matches.\n- The final accuracy is a weighted score (70% exact match and 30% Rouge-L score).\n\n---\n\n### 2. `_normalize_text(self, text: str) -> str`\n#### Parameters:\n- `text`: The string to be normalized.\n\n#### Returns:\n- A normalized string containing only alphanumeric characters, converted to lowercase, with spaces removed.\n\n#### Description:\nThis method standardizes the input text by:\n- Removing all non-alphanumeric characters.\n- Converting the text to lowercase.\n- This is particularly useful for comparing answers while ignoring irrelevant formatting differences.\n\n---\n\n### 3. `_check_answer(self, model_answer: str, ground_truth: str) -> tuple\n#### Parameters:\n- `model_answer`: The answer generated by the model or system.\n- `ground_truth`: The expected correct answer.\n\n#### Returns:\n- A tuple `(is_correct, rouge_score)`, where:\n  - `is_correct` is a boolean indicating whether the answer is an exact match.\n  - `rouge_score` is the Rouge-L score for the answer, if the match is partial.\n\n#### Description:\nThis method compares the model’s answer to the ground truth:\n- It extracts the boxed answers from both the model and the ground truth, if present.\n- The answers are normalized (non-alphanumeric characters are removed, and text is converted to lowercase) for comparison.\n- The method checks if the answers are an exact match. If not, it computes the Rouge-L score for partial matches.\n- The method also prints detailed information about the comparison, showing both the original and normalized answers, and indicating whether the match was exact or partial.\n\n---\n\n### 4. `_calculate_score(self, results: List, total: int) -> float`\n#### Parameters:\n- `results`: A list of tuples containing the verification results, where each tuple consists of a boolean indicating if the answer was correct and the Rouge-L score for partial matches.\n- `total`: The total number of questions verified.\n\n#### Returns:\n- A float representing the final weighted accuracy score.\n\n#### Description:\nThis method calculates the final accuracy score:\n- It counts the exact matches and computes the average Rouge-L score for partial matches.\n- The final accuracy is a weighted sum of the exact matches (70%) and the average Rouge-L score (30%).\n- The method prints a summary of the verification results, including the number of exact matches, average Rouge-L score for partial matches, and the final weighted score.\n\n---\n\n## Example Usage\n\n```python\n# Example of initializing the ReportVerifier class and verifying a section\ncommon_agent = BaseAgent()  # Assuming BaseAgent is defined elsewhere\nverifier = ReportVerifier(common_agent)\n\ncontext = \"The context of the document or report.\"\nextracted_facts = [\n    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\", \"format\": \"text\"},\n    {\"question\": \"Who wrote Hamlet?\", \"answer\": \"Shakespeare\", \"format\": \"text\"}\n]\n\naccuracy = verifier.verify_section(context, extracted_facts)\nprint(f\"Final Verification Accuracy: {accuracy:.2%}\")\n```\n\n---\n\n## Notes:\n- The `verify_section` method performs concurrent verification of multiple facts using a thread pool with a maximum of 20 workers.\n- The Rouge-L score is used to assess partial matches between the model's answer and the ground truth. This score measures the overlap of the longest common subsequences between the answers.\n- The method prints detailed logs during the verification process, including the comparison results for each question.",
        "**ReportVerifier**: The function of ReportVerifier is to verify the accuracy of extracted facts against generated answers in a factual question-answering context.\n\n**attributes**: The attributes of this Class.\n· agent: An instance of an agent responsible for interacting with templates and generating responses.\n· scorer: An instance of RougeScorer used to evaluate the quality of generated answers based on ROUGE-L metrics.\n\n**Code Description**: The ReportVerifier class is designed to facilitate the verification of factual accuracy in responses generated by an agent. It achieves this by comparing the answers produced by the agent against a set of extracted facts. The class is initialized with an agent, which is utilized to interact with templates for generating responses based on user questions.\n\nThe primary method, verify_section, takes a context string and a list of extracted facts as input. It initiates the verification process by printing the total number of questions to be verified. The method defines a nested function, verify_single_question, which processes each fact individually. This function constructs a data dictionary containing the context, user question, and expected format, and then calls the agent to generate a response using a predefined template. The generated response is subsequently checked against the expected answer using the _check_answer method.\n\nTo enhance performance, the verification of multiple questions is executed concurrently using a ThreadPoolExecutor, allowing for efficient processing of up to 20 questions simultaneously. The results of the verification are collected, and the overall accuracy is calculated using the _calculate_score method, which combines exact matches and ROUGE-L scores to produce a final accuracy metric.\n\nThe class also includes helper methods such as _normalize_text, which standardizes text by removing non-alphanumeric characters and converting it to lowercase, and _check_answer, which evaluates the correctness of the model's answer by checking for exact matches and calculating the ROUGE-L score if necessary. The _calculate_score method aggregates the results of the verification process, providing a summary of exact matches and average ROUGE-L scores.\n\nThe ReportVerifier class is called within the process_single_task function of the main module. It plays a crucial role in ensuring the accuracy of the report generated by the agent by verifying the factual correctness of the content produced in response to user queries. This integration allows for a systematic approach to validating the information presented in the final report, enhancing the reliability of the output.\n\n**Note**: It is essential to ensure that the agent is properly configured and that the extracted facts are accurately provided to the ReportVerifier for effective verification. The performance of the verification process may vary based on the complexity of the questions and the quality of the agent's responses.\n\n**Output Example**: A possible return value from the verify_section method could be a float representing the accuracy score, such as:\n```\n0.85  # indicating an 85% accuracy in the verification process.\n```",
        "**ReportVerifier**: The function of ReportVerifier is to verify the factual accuracy of extracted information against a given context and assess the correctness of answers provided by an agent.\n\n**attributes**: The attributes of this Class.\n· agent: An instance of the agent responsible for generating responses and interacting with templates.\n· scorer: An instance of RougeScorer used to compute ROUGE-L scores for evaluating answer quality.\n\n**Code Description**: The ReportVerifier class is designed to facilitate the verification of factual accuracy in responses generated by an agent. It takes an agent as a parameter during initialization, which allows it to utilize the agent's capabilities to interact with templates and gather necessary data for verification.\n\nThe primary method of this class is `verify_section`, which accepts a context string and a list of extracted facts. This method initiates the verification process by printing the total number of questions to be verified. It defines a nested function, `verify_single_question`, which constructs a data dictionary containing the context, user question, and expected format. This function then calls the agent to verify each question using a predefined template, and it checks the agent's response against the expected answer using the `_check_answer` method.\n\nTo optimize performance, the verification of multiple questions is handled concurrently using a ThreadPoolExecutor, allowing for efficient processing of up to 20 questions simultaneously. The results of the verification are collected, and the accuracy is calculated using the `_calculate_score` method, which computes the proportion of exact matches and the average ROUGE-L score for partial matches.\n\nThe class also includes helper methods such as `_normalize_text`, which standardizes text by removing non-alphanumeric characters and converting it to lowercase, and `_check_answer`, which evaluates the correctness of the model's answer against the ground truth. The `_calculate_score` method summarizes the verification results, providing a final weighted score based on exact matches and ROUGE-L scores.\n\nThe ReportVerifier class is utilized in various parts of the project, specifically within the `process_single_task` function and the `Session` class. In `process_single_task`, an instance of ReportVerifier is created to ensure the factual accuracy of the content generated by the agent during report creation. The accuracy of each section is assessed, and this information is logged for further analysis. Similarly, in the `Session` class, the ReportVerifier is instantiated to evaluate the agent's responses based on the user's prompt, ensuring that the generated content meets the required factual standards.\n\n**Note**: It is essential to ensure that the agent is properly configured and that the input data is relevant to the verification process. The performance of the ReportVerifier may vary based on the complexity of the questions and the quality of the agent's responses.\n\n**Output Example**: A possible output of the verification process could be:\n```\nVerification Results Summary\nTotal Questions: 5\nExact Matches: 3/5 (60.00%)\nAverage ROUGE-L for Partial Matches: 75.00%\nFinal Weighted Score: 68.00%\n```",
        "# ReportVerifier Class Documentation\n\n## Overview\n\nThe `ReportVerifier` class is designed for performing factual quality assurance (QA) verification on extracted facts. It utilizes a specified agent to interact with a context and verifies the factual accuracy of the extracted data through a series of questions and answers. The class integrates the use of a scoring mechanism, specifically the Rouge metric, to evaluate the factual correctness of responses.\n\n## Methods\n\n### `__init__(self, agent)`\n\nThe constructor initializes the `ReportVerifier` class. It requires an agent to interact with during the verification process and sets up a Rouge scorer for evaluating the factual correctness of responses.\n\n**Parameters:**\n- `agent`: An object that facilitates communication with the QA system, which is used to verify the correctness of the extracted facts.\n\n### `verify_section(self, context: str, extracted_facts: List[Dict]) -> float`\n\nThis method performs the factual QA verification process for a given section of the report. It processes each fact, verifies it by generating responses, and calculates a score to indicate the overall correctness of the section.\n\n**Parameters:**\n- `context`: A string containing the context or background information required to verify the facts.\n- `extracted_facts`: A list of dictionaries, each containing a fact to verify, including the question, format, and expected answer.\n\n**Returns:**\n- A floating-point value representing the overall score of the verification process, indicating the correctness of the section.\n\n**Process:**\n1. **Initialization**: It starts by printing the total number of questions to verify.\n2. **Verification**: For each extracted fact, a helper function (`verify_single_question`) is used to:\n   - Prepare the QA data, including the context, user question, and the fact format.\n   - Communicate with the agent using the `chat_with_template` method, passing the QA data and the template file `factQA_verifier.txt`.\n   - Check the agent’s response against the expected answer.\n3. **Parallel Execution**: The verification of each question is handled concurrently using a thread pool with a maximum of 20 workers. This improves performance when verifying large numbers of facts.\n4. **Result**: The method returns a floating-point score, reflecting the overall verification results, after comparing each response against the correct answers.\n\n### `verify_single_question(self, fact) -> float`\n\nThis method is a helper function used within `verify_section` to verify a single extracted fact. It prepares the QA data, queries the agent, and compares the response to the expected answer.\n\n**Parameters:**\n- `fact`: A dictionary containing a single extracted fact, which includes the question, format, and expected answer.\n\n**Returns:**\n- A floating-point score reflecting the accuracy of the response for the given fact.\n\n### `_check_answer(self, response: str, correct_answer: str) -> float`\n\nThis private method compares the response from the agent with the correct answer and calculates a score based on the accuracy of the match.\n\n**Parameters:**\n- `response`: The response from the agent after processing the fact.\n- `correct_answer`: The expected correct answer for the fact.\n\n**Returns:**\n- A floating-point score representing the accuracy of the response, calculated using the Rouge metric.\n\n## Dependencies\n\n- `rouge_scorer.RougeScorer`: A Rouge scorer is used to evaluate the factual correctness of the responses based on the Rouge-L score.\n- `ThreadPoolExecutor`: Used to run multiple fact verification tasks concurrently for improved performance.\n\n## Usage Example\n\n```python\n# Instantiate the agent (not shown in this example)\nagent = SomeAgent()\n\n# Instantiate the ReportVerifier with the agent\nreport_verifier = ReportVerifier(agent)\n\n# Example context and extracted facts\ncontext = \"The capital of France is Paris.\"\nextracted_facts = [\n    {\"question\": \"What is the capital of France?\", \"format\": \"text\", \"answer\": \"Paris\"},\n    {\"question\": \"Is Paris the capital of France?\", \"format\": \"text\", \"answer\": \"Yes\"}\n]\n\n# Verify the section of extracted facts\nscore = report_verifier.verify_section(context, extracted_facts)\n\n# Print the score\nprint(f\"Verification Score: {score}\")\n```\n\nIn this example, the `verify_section` method processes each question in the `extracted_facts` list and compares the agent's responses to the expected answers. It then calculates a final score for the section's factual accuracy.\n\n## Conclusion\n\nThe `ReportVerifier` class provides a structured approach to verifying the factual correctness of extracted data. It integrates a powerful scoring mechanism, multi-threaded execution, and interaction with a provided agent to efficiently perform QA verification on a section of facts."
      ],
      "code_start_line": 9,
      "code_end_line": 116,
      "params": [],
      "have_return": true,
      "code_content": "class ReportVerifier:\n    def __init__(self, agent):\n        self.agent = agent\n        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n        \n    def verify_section(self, context: str, extracted_facts: List[Dict]) -> float:\n        print(\"\\n=== Starting Factual QA Verification ===\")\n        print(f\"Total questions to verify: {len(extracted_facts)}\\n\")\n\n        def verify_single_question(fact):\n            qa_data = {\n                \"context\": context,\n                \"user_question\": fact[\"question\"],\n                \"constrained_format\": fact[\"format\"]\n            }\n            \n            print(f\"\\nVerifying Question: {fact['question']}\")\n            print(f\"Format: {fact['format']}\")\n            \n            response = self.agent.chat_with_template(\n                \"factQA_verifier.txt\",\n                qa_data,\n                save_history=False,\n            )\n            \n            return self._check_answer(response, fact[\"answer\"])\n            \n        with ThreadPoolExecutor(max_workers=20) as executor:\n            futures = {executor.submit(verify_single_question, fact): fact \n                      for fact in extracted_facts}\n            results = []\n            \n            for future in tqdm(\n                concurrent.futures.as_completed(futures),\n                total=len(extracted_facts),\n                desc=\"Verifying questions\"\n            ):\n                results.append(future.result())\n                \n        accuracy = self._calculate_score(results, len(extracted_facts))\n        return accuracy\n\n    def _normalize_text(self, text: str) -> str:\n        \"\"\"标准化文本,只保留字母数字,转小写并去除空格\n        \n        Args:\n            text: 输入文本\n            \n        Returns:\n            标准化后的文本\n        \"\"\"\n        # 只保留字母和数字\n        text = re.sub(r'[^a-zA-Z0-9]', '', text)\n        # 转换为小写\n        text = text.lower()\n        return text\n\n    def _check_answer(self, model_answer: str, ground_truth: str) -> tuple:\n        if model_answer is None:\n            return False, 0.0\n        \n        pattern = r'\\\\boxed{(.*?)}'\n        model_boxed = re.findall(pattern, model_answer)\n        ground_truth_boxed = re.findall(pattern, ground_truth)\n        \n        is_correct = False\n        rouge_score = 0.0\n        \n        if model_boxed and ground_truth_boxed:\n            # 对答案进行标准化处理\n            model_ans = self._normalize_text(model_boxed[0])\n            ground_truth = self._normalize_text(ground_truth_boxed[0])\n            \n            # 完全匹配检查\n            is_correct = model_ans == ground_truth\n            \n            if not is_correct:\n                scores = self.scorer.score(ground_truth, model_ans)\n                rouge_score = scores['rougeL'].fmeasure\n\n            # 输出时显示原始答案和标准化后的答案\n            print(\"-\" * 50)\n            if is_correct:\n                print(\"✓ Exact Match\")\n            else:\n                print(f\"✗ Partial Match (ROUGE-L: {rouge_score:.2%})\")\n            print(f\"Expected (original): {ground_truth_boxed[0]}\")\n            print(f\"Got (original): {model_boxed[0]}\")\n            print(f\"Expected (normalized): {ground_truth}\")\n            print(f\"Got (normalized): {model_ans}\")\n            print(\"-\" * 50)\n                \n        return is_correct, rouge_score\n\n    def _calculate_score(self, results: List, total: int) -> float:\n        exact_matches = sum(1 for correct, _ in results if correct)\n        rouge_scores = [score for correct, score in results if not correct]\n        avg_rouge = sum(rouge_scores) / total if rouge_scores else 0\n        \n        final_accuracy = 0.7 * (exact_matches / total) + 0.3 * avg_rouge\n        \n        printer.rule(\"Verification Results Summary\")\n        printer.log(f\"Total Questions: {total}\")\n        printer.log(f\"Exact Matches: {exact_matches}/{total} ({exact_matches/total:.2%})\")\n        printer.log(f\"Average ROUGE-L for Partial Matches: {avg_rouge:.2%}\")\n        printer.log(f\"Final Weighted Score: {final_accuracy:.2%}\")\n        \n        return final_accuracy\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/session.py",
        "src/criticsearch/session.py/Session/__init__",
        "src/criticsearch/workflow.py",
        "src/criticsearch/workflow.py/iterate_traj"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the ReportVerifier object with a given agent and configure a RougeScorer instance for evaluating text similarity.\n\n**parameters**: The parameters of this Function.\n- agent: An object that is assigned to the `agent` attribute of the ReportVerifier class.\n\n**Code Description**: \nThe `__init__` function is the constructor method for the `ReportVerifier` class. When an instance of `ReportVerifier` is created, the method takes in a parameter `agent` and assigns it to the instance's `agent` attribute. This allows the `ReportVerifier` object to interact with or use the agent throughout its lifecycle.\n\nIn addition to initializing the `agent`, the method also initializes a `rouge_scorer.RougeScorer` object. The `RougeScorer` is configured to evaluate the Rouge-L metric, which is used to assess the quality of text summaries by comparing them with reference texts. The `use_stemmer=True` argument is passed to the `RougeScorer` to ensure that stemming is applied during the comparison process. Stemming reduces words to their base or root form, enhancing the robustness of the text comparison by ignoring minor variations in word forms.\n\n**Note**: The `agent` parameter must be an object that is compatible with the functionality intended in the `ReportVerifier` class. The `RougeScorer` instance will always be configured with the Rouge-L metric and stemming enabled, which is crucial for ensuring consistent evaluation results when performing text similarity comparisons."
      ],
      "code_start_line": 10,
      "code_end_line": 12,
      "params": [
        "self",
        "agent"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, agent):\n        self.agent = agent\n        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "verify_section",
      "md_content": [
        "## Function Documentation: `verify_section`\n\n### Description:\nThe `verify_section` function is responsible for verifying a series of factual questions based on a provided context and extracted facts. It evaluates the accuracy of answers for each question by comparing the response generated by an agent with the expected answer. The function utilizes parallel processing to verify multiple questions simultaneously and calculates an overall accuracy score based on the results.\n\n### Parameters:\n- **context** (str): A string representing the context in which the questions are being asked. This context is used by the agent to generate responses relevant to each question.\n  \n- **extracted_facts** (List[Dict]): A list of dictionaries, where each dictionary represents a question and its associated metadata. Each dictionary must contain the following keys:\n  - `\"question\"` (str): The question to be verified.\n  - `\"format\"` (str): The expected format of the answer.\n  - `\"answer\"` (str): The expected answer to the question.\n\n### Return Value:\n- **float**: The function returns a floating-point number representing the overall accuracy score of the verification process. This score is calculated by evaluating exact matches and partial matches using the ROUGE-L metric.\n\n### Functionality:\n1. **Initialization**: The function begins by printing a summary of the number of questions to be verified.\n  \n2. **Verification Process**:\n   - A nested function `verify_single_question` is defined to verify each question individually. This function creates a `qa_data` dictionary containing the context, user question, and expected answer format.\n   - For each fact in `extracted_facts`, the function invokes the agent's `chat_with_template` method, passing the `qa_data` to generate a response based on the context.\n   - The agent's response is then compared with the expected answer using the `_check_answer` method to determine if the answer is correct.\n   \n3. **Parallel Processing**:\n   - The function utilizes `ThreadPoolExecutor` to concurrently verify multiple questions, optimizing the process and reducing the overall verification time.\n   - A list of future tasks is created, each representing a call to `verify_single_question`. These tasks are executed in parallel, and their results are collected as they complete.\n\n4. **Results Compilation**:\n   - After all questions have been verified, the results (whether each answer was correct or not, and the corresponding ROUGE-L score) are collected.\n\n5. **Accuracy Calculation**:\n   - The function calls the `_calculate_score` method to compute a final accuracy score based on the results of the verification. The score is calculated as a weighted average of exact matches and ROUGE-L scores.\n\n6. **Final Output**:\n   - The accuracy score is returned to indicate the overall correctness of the factual question-answer verification process.\n\n### Example:\n```python\ncontext = \"The context of the document or passage to verify.\"\nextracted_facts = [\n    {\"question\": \"What is the capital of France?\", \"format\": \"text\", \"answer\": \"Paris\"},\n    {\"question\": \"Who wrote '1984'?\", \"format\": \"text\", \"answer\": \"George Orwell\"}\n]\naccuracy = verifier.verify_section(context, extracted_facts)\nprint(f\"Verification Accuracy: {accuracy}\")\n```\n\n### Notes:\n- The function is designed to handle large numbers of questions efficiently by utilizing multithreading.\n- The accuracy score returned provides a combined metric of both exact matches and partial matches, ensuring a comprehensive evaluation of the verification process.",
        "**verify_section**: The function of verify_section is to verify the factual accuracy of a series of questions based on a given context and extracted facts.\n\n**parameters**: The parameters of this Function.\n· context: str - A string representing the context in which the questions are to be verified.  \n· extracted_facts: List[Dict] - A list of dictionaries, where each dictionary contains a question, its expected answer, and the format in which the answer should be provided.\n\n**Code Description**: The verify_section function initiates the verification process for a set of factual questions. It begins by printing a message indicating the start of the verification process and the total number of questions to be verified. The function defines an inner function, verify_single_question, which takes a single fact as input. This inner function constructs a data dictionary containing the context, user question, and the expected format. It then prints the question and format being verified.\n\nThe function utilizes an agent to interact with a predefined template (\"factQA_verifier.txt\") to obtain a response regarding the accuracy of the answer provided for each question. The response is then checked against the expected answer using the _check_answer method, which is not detailed in the provided code but is assumed to return a boolean indicating correctness.\n\nTo handle multiple questions concurrently, the verify_section function employs a ThreadPoolExecutor with a maximum of 20 workers. It submits each question to the executor, which allows for parallel processing of the verification tasks. The results of these tasks are collected as they complete, utilizing the tqdm library to provide a progress bar indicating the status of the verification process.\n\nOnce all questions have been processed, the results are passed to the _calculate_score method, which computes an overall accuracy score based on the verification outcomes. This score is then returned by the verify_section function, indicating the effectiveness of the verification process.\n\nThe verify_section function is called by the process_single_task function, which orchestrates the execution of a single task. Within process_single_task, the ReportVerifier instance is created, and the verify_section method is invoked to ensure the factual accuracy of the content generated by the agent against the extracted facts. Additionally, the start_writing method in the Session class also calls verify_section to validate the content generated for a specific section, although it passes an empty list for extracted facts in its current implementation.\n\n**Note**: It is crucial to ensure that the extracted_facts parameter is well-structured and contains accurate information, as the verification process relies heavily on this data. The context provided should also be relevant to the questions being verified to ensure accurate assessments.\n\n**Output Example**: \nAssuming the extracted_facts list contains the following data:\n```python\n[\n    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\", \"format\": \"text\"},\n    {\"question\": \"Is the sky blue?\", \"answer\": \"Yes\", \"format\": \"text\"}\n]\n```\nThe output might indicate an accuracy score of 0.95 if the majority of the answers were verified as correct.",
        "**verify_section**: The function of verify_section is to perform factual verification of a series of questions based on a given context and extracted facts, returning an accuracy score.\n\n**parameters**: The parameters of this Function.\n· context: str - A string representing the context in which the questions are to be verified.\n· extracted_facts: List[Dict] - A list of dictionaries, where each dictionary contains a question, its expected answer, and the format of the question.\n\n**Code Description**: The verify_section method is designed to initiate the verification process for a set of factual questions. It begins by printing a message indicating the start of the verification process and the total number of questions to be verified. The method defines an inner function, verify_single_question, which takes a single fact as input. This inner function constructs a dictionary containing the context, user question, and its format, and then prints the details of the question being verified.\n\nThe method utilizes an agent's chat_with_template function to interact with a predefined template for factual verification. The response from this interaction is then checked against the expected answer using the _check_answer method. The results of these checks are collected in a list.\n\nTo optimize the verification process, the method employs a ThreadPoolExecutor to handle multiple questions concurrently, allowing for up to 20 workers. Each question is submitted to the executor, and the results are gathered as they are completed. The tqdm library is used to provide a progress bar, enhancing user experience by visually indicating the verification progress.\n\nOnce all questions have been verified, the method calls the _calculate_score function, passing the results and the total number of questions to compute an overall accuracy score. This score is then returned as the output of the verify_section method.\n\nThe verify_section method is called by the process_single_task function, which manages the execution of a single task. Within this function, after generating content for a section, the verify_section method is invoked to assess the factual accuracy of that content against the extracted facts. The accuracy score obtained from verify_section is then appended to the agent's training data, providing feedback on the quality of the generated content.\n\n**Note**: It is crucial to ensure that the extracted_facts parameter is populated with accurate and relevant data to facilitate effective verification. The performance of the verification process may vary based on the complexity and clarity of the questions being assessed.\n\n**Output Example**: Assuming the extracted_facts contain the following data:\n```json\n[\n    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\", \"format\": \"simple\"},\n    {\"question\": \"What is the largest planet?\", \"answer\": \"Jupiter\", \"format\": \"simple\"}\n]\n```\nThe output might look like:\n```\nTotal questions to verify: 2\nVerifying Question: What is the capital of France?\nFormat: simple\nVerifying Question: What is the largest planet?\nFormat: simple\nAccuracy Score: 95.0%\n```"
      ],
      "code_start_line": 14,
      "code_end_line": 49,
      "params": [
        "self",
        "context",
        "extracted_facts"
      ],
      "have_return": true,
      "code_content": "    def verify_section(self, context: str, extracted_facts: List[Dict]) -> float:\n        print(\"\\n=== Starting Factual QA Verification ===\")\n        print(f\"Total questions to verify: {len(extracted_facts)}\\n\")\n\n        def verify_single_question(fact):\n            qa_data = {\n                \"context\": context,\n                \"user_question\": fact[\"question\"],\n                \"constrained_format\": fact[\"format\"]\n            }\n            \n            print(f\"\\nVerifying Question: {fact['question']}\")\n            print(f\"Format: {fact['format']}\")\n            \n            response = self.agent.chat_with_template(\n                \"factQA_verifier.txt\",\n                qa_data,\n                save_history=False,\n            )\n            \n            return self._check_answer(response, fact[\"answer\"])\n            \n        with ThreadPoolExecutor(max_workers=20) as executor:\n            futures = {executor.submit(verify_single_question, fact): fact \n                      for fact in extracted_facts}\n            results = []\n            \n            for future in tqdm(\n                concurrent.futures.as_completed(futures),\n                total=len(extracted_facts),\n                desc=\"Verifying questions\"\n            ):\n                results.append(future.result())\n                \n        accuracy = self._calculate_score(results, len(extracted_facts))\n        return accuracy\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/session.py/Session/start_writing",
        "src/criticsearch/workflow.py/iterate_traj"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/_calculate_score"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "verify_single_question",
      "md_content": [
        "**verify_single_question**: The function of verify_single_question is to verify a single question by interacting with a model, comparing its answer to the expected answer, and checking the accuracy.\n\n**parameters**: The parameters of this Function.\n· fact: A dictionary containing information about the question to be verified, including the question itself, the format of the question, and the expected answer.\n\n**Code Description**: The verify_single_question function is responsible for verifying a specific question and its associated answer by interacting with a conversational model. It accepts a single parameter, fact, which is a dictionary containing key information about the question, its format, and the expected answer.\n\nThe function begins by constructing a dictionary, `qa_data`, which stores the relevant details of the question and its format. The dictionary contains the following keys:\n- \"context\": context (assumed to be defined elsewhere in the class or script)\n- \"user_question\": the question to be verified, which is retrieved from the fact dictionary under the key \"question\"\n- \"constrained_format\": the format of the question, which is retrieved from the fact dictionary under the key \"format\"\n\nNext, the function prints the question and its format to the console for logging or tracking purposes.\n\nThe function then interacts with a model by calling the `chat_with_template` method from the `self.agent` object. This method is used to communicate with the model, utilizing a specific template file, \"factQA_verifier.txt\". The `qa_data` dictionary is passed as input to the template, allowing the model to generate a response that is contextually relevant to the provided question and format.\n\nAfter receiving the model's response, the function calls another method, `_check_answer`, to evaluate the model's answer. The response from the model is compared to the expected answer stored in the fact dictionary under the key \"answer\". The `_check_answer` method checks if the model's answer matches the expected answer and provides a score (typically ROUGE-L) if there is a partial match. This result is returned by the `verify_single_question` function.\n\nThe function relies on the `chat_with_template` method from the `BaseAgent` class to facilitate communication with the model, and the `_check_answer` method to determine the accuracy of the response.\n\n**Note**: The `verify_single_question` function assumes that the necessary context is available when it is called, as the \"context\" field is part of the `qa_data` but is not explicitly provided in the function itself. Additionally, the correct formatting of the \"fact\" dictionary is essential for the function to perform its task successfully.\n\n**Output Example**: The output of the function is the result of the `_check_answer` method, which typically returns a tuple indicating whether the model's answer is correct and the ROUGE-L score (if applicable). A possible return value could be:\n```\n(is_correct=True, rouge_score=0.0)\n```",
        "**verify_single_question**: The function of verify_single_question is to verify a single question's answer by interacting with a model and comparing the model's response to the expected answer.\n\n**parameters**: The parameters of this Function.\n· fact: A dictionary containing the question, expected answer, and format of the question.\n\n**Code Description**:  \nThe `verify_single_question` function is responsible for verifying a single question by interacting with a model, providing it with a question, its expected format, and checking the model’s response against the expected answer.\n\nThe function begins by creating a dictionary called `qa_data`, which contains the following keys:\n- `\"context\"`: A context variable, though it is not provided within this specific function, likely defined elsewhere in the class.\n- `\"user_question\"`: The question extracted from the `fact` dictionary passed into the function.\n- `\"constrained_format\"`: The expected format of the answer, also taken from the `fact` dictionary.\n\nThen, the function prints out the question and its format to the console for tracking and debugging purposes.\n\nFollowing this, the function calls the `chat_with_template` method of the `agent` object. This method is responsible for generating a response from the model based on the provided template (`\"factQA_verifier.txt\"`) and the data contained in the `qa_data` dictionary. The method does not save conversation history, as indicated by `save_history=False`. The `chat_with_template` method processes the input and returns a model-generated response, which is then passed to the `_check_answer` method along with the expected answer from the `fact` dictionary.\n\nThe `_check_answer` function is designed to evaluate the model's answer against the expected answer, checking for an exact match or calculating a ROUGE-L score for partial matches. It returns a tuple containing:\n- A boolean indicating whether the model’s answer is correct.\n- A floating-point value representing the ROUGE-L score if there is a partial match.\n\nFinally, `verify_single_question` returns the result from `_check_answer`, providing the verification outcome of the model’s response.\n\nThis function relies heavily on the `chat_with_template` and `_check_answer` methods, with the former generating the response and the latter evaluating it. \n\n**Note**:  \n- The `verify_single_question` function assumes that the `fact` dictionary passed to it will contain the keys `\"question\"`, `\"format\"`, and `\"answer\"`. \n- It is essential that the template (`\"factQA_verifier.txt\"`) exists in the system and is correctly formatted for the model to generate an appropriate response. \n- This function does not save history by default (`save_history=False`), but this behavior can be changed if required by modifying the relevant parameter.\n\n**Output Example**:  \nAn example of the return value could look like this:\n```\n(True, 0.0)\n```\nOr for a partial match:\n```\n(False, 75.56)\n```"
      ],
      "code_start_line": 18,
      "code_end_line": 34,
      "params": [
        "fact"
      ],
      "have_return": true,
      "code_content": "        def verify_single_question(fact):\n            qa_data = {\n                \"context\": context,\n                \"user_question\": fact[\"question\"],\n                \"constrained_format\": fact[\"format\"]\n            }\n            \n            print(f\"\\nVerifying Question: {fact['question']}\")\n            print(f\"Format: {fact['format']}\")\n            \n            response = self.agent.chat_with_template(\n                \"factQA_verifier.txt\",\n                qa_data,\n                save_history=False,\n            )\n            \n            return self._check_answer(response, fact[\"answer\"])\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/_check_answer"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_normalize_text",
      "md_content": [
        "**_normalize_text**: The function of _normalize_text is to standardize the input text by retaining only alphanumeric characters, converting the text to lowercase, and removing any spaces.\n\n**parameters**: The parameters of this Function.\n· text: The input string that needs to be standardized.\n\n**Code Description**: \nThe _normalize_text function is responsible for normalizing an input string by performing the following operations:\n1. It removes any non-alphanumeric characters using a regular expression (`re.sub(r'[^a-zA-Z0-9]', '', text)`), which ensures that only letters (both uppercase and lowercase) and numbers remain in the string.\n2. It converts the resulting string to lowercase using the `lower()` method, ensuring case uniformity.\n3. It then returns the transformed string.\n\nThis function is primarily used within the context of answer verification. For example, the function is called by `_check_answer`, where it plays a key role in standardizing both the model's predicted answer and the ground truth answer. After extracting specific parts of the answers (using a regular expression to find content inside `\\boxed{}`), `_normalize_text` is applied to both the model's and the ground truth answers before performing any further comparisons. This standardization ensures that the comparison is made in a consistent format, eliminating discrepancies due to case differences, spacing, or punctuation.\n\n**Note**: \n- The function only retains alphanumeric characters, so punctuation and spaces are completely discarded. Ensure that the input text is appropriate for this kind of transformation.\n- This function may be useful in scenarios where exact matches are required and formatting"
      ],
      "code_start_line": 51,
      "code_end_line": 64,
      "params": [
        "self",
        "text"
      ],
      "have_return": true,
      "code_content": "    def _normalize_text(self, text: str) -> str:\n        \"\"\"标准化文本,只保留字母数字,转小写并去除空格\n        \n        Args:\n            text: 输入文本\n            \n        Returns:\n            标准化后的文本\n        \"\"\"\n        # 只保留字母和数字\n        text = re.sub(r'[^a-zA-Z0-9]', '', text)\n        # 转换为小写\n        text = text.lower()\n        return text\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/_check_answer"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_answer",
      "md_content": [
        "**_check_answer**: The function of _check_answer is to evaluate the correctness of a model's answer against a ground truth answer by performing normalization and comparison, and to calculate a ROUGE-L score if the answers do not match exactly.\n\n**parameters**: The parameters of this Function.\n· model_answer: A string representing the answer generated by the model that needs to be evaluated.\n· ground_truth: A string representing the correct answer against which the model's answer is compared.\n\n**Code Description**: The _check_answer function is designed to assess the accuracy of a model's answer by comparing it with the ground truth answer. It begins by defining a regular expression pattern to extract content enclosed within `\\boxed{}` from both the model's answer and the ground truth. The function utilizes the `re.findall` method to retrieve these boxed answers.\n\nOnce the boxed answers are extracted, the function initializes two variables: `is_correct`, which is set to False, and `rouge_score`, initialized to 0.0. If both the model's boxed answer and the ground truth boxed answer are present, the function proceeds to normalize these answers using the _normalize_text method. This normalization process standardizes the text by removing non-alphanumeric characters, converting it to lowercase, and eliminating spaces.\n\nAfter normalization, the function checks for an exact match between the normalized model answer and the normalized ground truth. If they match, `is_correct` is set to True. If they do not match, the function calculates the ROUGE-L score using the scorer's score method, which provides a measure of similarity between the two answers based on their longest common subsequence.\n\nThe function then prints a detailed output, indicating whether the answers matched exactly or partially, along with the original and normalized versions of both answers. Finally, it returns a tuple containing the correctness of the model's answer (as a boolean) and the calculated ROUGE-L score.\n\nThis function is called by the verify_single_question function, which is responsible for verifying individual questions by preparing the necessary data and invoking the _check_answer function to perform the evaluation. The verify_single_question function collects the question and its expected answer, interacts with an agent to obtain the model's response, and then utilizes _check_answer to determine the accuracy of that response.\n\n**Note**: It is important to ensure that the model_answer and ground_truth inputs are formatted correctly to contain boxed answers for the function to operate effectively. The function's output will provide insights into the accuracy of the model's response, which is crucial for evaluating the performance of the model in generating answers.\n\n**Output Example**: \n(is_correct=True, rouge_score=0.0)",
        "**_check_answer**: The function of _check_answer is to evaluate the model's answer against the expected answer, providing an exact match result or calculating a ROUGE-L score for partial matches.\n\n**parameters**: The parameters of this Function.\n· model_answer: A string representing the answer generated by the model.\n· ground_truth: A string representing the correct or expected answer.\n\n**Code Description**: \nThe `_check_answer` function is designed to compare a model's answer (`model_answer`) with the expected correct answer (`ground_truth`). It begins by checking if the `model_answer` is `None`, in which case it immediately returns `False` (indicating no match) and a score of `0.0`.\n\nTo perform the comparison, the function first uses a regular expression to extract content enclosed in `\\boxed{}` from both the `model_answer` and `ground_truth`. These are stored in the lists `model_boxed` and `ground_truth_boxed`, respectively. If both lists are non-empty, the function proceeds to normalize the answers.\n\nThe normalization step involves the `_normalize_text` method, which standardizes both the model’s and ground truth answers by removing non-alphanumeric characters, converting them to lowercase, and eliminating any spaces. This ensures that the comparison is made on a consistent, normalized version of the answers, free from case or formatting discrepancies.\n\nAfter the answers are normalized, the function performs a comparison to check for an exact match. If the model's answer exactly matches the ground truth, it sets `is_correct` to `True` and the `rouge_score` remains at `0.0`. If the answers do not match, the function calculates a ROUGE-L score, which measures the similarity between the two answers, providing a score that indicates the level of partial match.\n\nThe function then prints detailed output, including whether the match was exact or partial, along with the original and normalized versions of both the expected answer and the model's answer. The print statements are useful for debugging or analyzing the performance of the model.\n\nFinally, the function returns a tuple: `is_correct` (a boolean indicating whether the model's answer matches the expected answer) and `rouge_score` (a floating-point value representing the ROUGE-L score for partial matches).\n\nThe `_check_answer` function is called by the `verify_single_question` function, which is responsible for verifying individual questions by interacting with a model. The `verify_single_question` function gathers the question and its expected answer from a dictionary (`fact`) and passes them to the model. After the model generates an answer, `verify_single_question` calls `_check_answer` to compare the model's answer to the expected answer and return the results.\n\n**Note**: \n- The function relies on the `_normalize_text` method for standardizing the answers. Ensure that the text passed to `_check_answer` contains content enclosed in `\\boxed{}` to match the expected structure.\n- If either the model's answer or the ground truth answer does not contain content within `\\boxed{}`, the function will not proceed with the comparison and will return a score of `0.0`.\n- This function is useful in scenarios where exact answer matching is required, and it also supports partial matches through the ROUGE-L score.\n\n**Output Example**: \nAn example of the return value could look like this:\n```\n(is_correct=True, rouge_score=0.0)\n```\nOr for a partial match:\n```\n(is_correct=False, rouge_score=75.56)\n```"
      ],
      "code_start_line": 66,
      "code_end_line": 101,
      "params": [
        "self",
        "model_answer",
        "ground_truth"
      ],
      "have_return": true,
      "code_content": "    def _check_answer(self, model_answer: str, ground_truth: str) -> tuple:\n        if model_answer is None:\n            return False, 0.0\n        \n        pattern = r'\\\\boxed{(.*?)}'\n        model_boxed = re.findall(pattern, model_answer)\n        ground_truth_boxed = re.findall(pattern, ground_truth)\n        \n        is_correct = False\n        rouge_score = 0.0\n        \n        if model_boxed and ground_truth_boxed:\n            # 对答案进行标准化处理\n            model_ans = self._normalize_text(model_boxed[0])\n            ground_truth = self._normalize_text(ground_truth_boxed[0])\n            \n            # 完全匹配检查\n            is_correct = model_ans == ground_truth\n            \n            if not is_correct:\n                scores = self.scorer.score(ground_truth, model_ans)\n                rouge_score = scores['rougeL'].fmeasure\n\n            # 输出时显示原始答案和标准化后的答案\n            print(\"-\" * 50)\n            if is_correct:\n                print(\"✓ Exact Match\")\n            else:\n                print(f\"✗ Partial Match (ROUGE-L: {rouge_score:.2%})\")\n            print(f\"Expected (original): {ground_truth_boxed[0]}\")\n            print(f\"Got (original): {model_boxed[0]}\")\n            print(f\"Expected (normalized): {ground_truth}\")\n            print(f\"Got (normalized): {model_ans}\")\n            print(\"-\" * 50)\n                \n        return is_correct, rouge_score\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/verify_section/verify_single_question"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/_normalize_text"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_calculate_score",
      "md_content": [
        "**_calculate_score**: The function of _calculate_score is to compute a final accuracy score based on exact matches and ROUGE-L scores.\n\n**parameters**:\n· results: A list of tuples where each tuple consists of a boolean indicating whether the answer was correct and the ROUGE-L score for incorrect answers.  \n· total: An integer representing the total number of questions being evaluated.\n\n**Code Description**:  \nThe _calculate_score function calculates a weighted accuracy score for a factual question-answer verification task. The function takes in two arguments: `results` and `total`.\n\n1. **exact_matches**: This is calculated using a generator expression that iterates through the `results` list. For each tuple, it checks if the first element (a boolean) is `True` and counts the number of such occurrences. This represents the number of exact matches between the expected answer and the actual answer.\n\n2. **rouge_scores**: A list comprehension is used to extract the second element (ROUGE-L score) from the tuples in the `results` list where the first element is `False`. This is done to track the ROUGE-L scores for answers that were not an exact match. ROUGE-L is a metric used to evaluate the quality of partial matches in text.\n\n3. **avg_rouge**: The average ROUGE-L score is computed by summing up the values in the `rouge_scores` list and dividing by the `total`. If there are no partial matches (i.e., `rouge_scores` is empty), it defaults to 0.\n\n4. **final_accuracy**: This is the weighted average of the exact match rate and the average ROUGE-L score. The final score is calculated as 70% of the exact match ratio and 30% of the average ROUGE-L score. This weighted score provides a combined metric of both exact and partial matches.\n\n5. **Print Statements**: The function outputs a summary of the verification process, including the total number of questions, the number of exact matches, the average ROUGE-L score for partial matches, and the final weighted accuracy score.\n\n6. **Return Value**: The function returns the final accuracy score as a floating-point value.\n\nThe _calculate_score function is called by the `verify_section` function, which is responsible for verifying a set of factual questions. The results from the verification process (a list of tuples containing exact match statuses and ROUGE-L scores) are passed into _calculate_score to determine the final accuracy of the verification process.\n\n**Note**:  \n- The accuracy score returned by _calculate_score is crucial for evaluating the performance of the factual question-answer verification task.  \n- The exact match percentage has a higher weight (70%) compared to the partial match score (30%).  \n- Ensure that the `results` list contains the appropriate structure, i.e., a tuple with a boolean and a score, for this function to work correctly.\n\n**Output Example**:  \nThe following is an example of the output printed by the function:\n```\n=== Verification Results Summary ===\nTotal Questions: 100\nExact Matches: 85/100 (85.00%)\nAverage ROUGE-L for Partial Matches: 25.00%\nFinal Weighted Score: 66.50%\n========================================\n```\nThis indicates that out of 100 questions, 85 exact matches were found, and the average ROUGE-L score for partial matches is 25%. The final weighted score is 66.50%.",
        "**_calculate_score**: The function of _calculate_score is to compute a final accuracy score based on the results of a verification process.\n\n**parameters**: The parameters of this Function.\n· results: List - A list of tuples where each tuple contains a boolean indicating whether the answer was correct and the corresponding ROUGE-L score for partial matches.  \n· total: int - An integer representing the total number of questions that were verified.\n\n**Code Description**: The _calculate_score function is responsible for calculating a weighted accuracy score based on the verification results of a series of questions. It first counts the number of exact matches by summing up the boolean values from the results list, where a value of `True` indicates a correct answer. Next, it extracts the ROUGE-L scores for the answers that were not exact matches. The average ROUGE-L score is computed by dividing the total of these scores by the total number of questions, ensuring that if there are no ROUGE scores, it defaults to zero.\n\nThe final accuracy score is calculated as a weighted average, where 70% of the score is derived from the proportion of exact matches to the total number of questions, and 30% is derived from the average ROUGE-L score. This approach allows for a balanced evaluation of both exact correctness and partial correctness.\n\nThe function also includes logging statements that utilize the RichPrinter class to provide a structured output of the verification results. It prints a summary that includes the total number of questions, the number of exact matches, the average ROUGE-L score for partial matches, and the final weighted score. This output is formatted to enhance readability and clarity for users reviewing the verification results.\n\nThe _calculate_score function is called by the verify_section method, which is responsible for verifying a series of factual questions. After the verification process is completed, the results are passed to _calculate_score to obtain the overall accuracy score. This score is then returned to indicate the effectiveness of the verification process.\n\n**Note**: It is important to ensure that the results list accurately reflects the verification outcomes, as the accuracy score is directly dependent on the correctness of the data provided. Proper handling of the total parameter is also crucial to avoid division errors.\n\n**Output Example**: \nAssuming the results list contains the following data: `[(True, 0.9), (False, 0.7), (True, 0.85)]` and the total is 3, the output might look like:\n```\nTotal Questions: 3\nExact Matches: 2/3 (66.67%)\nAverage ROUGE-L for Partial Matches: 70.00%\nFinal Weighted Score: 76.67%\n```"
      ],
      "code_start_line": 103,
      "code_end_line": 116,
      "params": [
        "self",
        "results",
        "total"
      ],
      "have_return": true,
      "code_content": "    def _calculate_score(self, results: List, total: int) -> float:\n        exact_matches = sum(1 for correct, _ in results if correct)\n        rouge_scores = [score for correct, score in results if not correct]\n        avg_rouge = sum(rouge_scores) / total if rouge_scores else 0\n        \n        final_accuracy = 0.7 * (exact_matches / total) + 0.3 * avg_rouge\n        \n        printer.rule(\"Verification Results Summary\")\n        printer.log(f\"Total Questions: {total}\")\n        printer.log(f\"Exact Matches: {exact_matches}/{total} ({exact_matches/total:.2%})\")\n        printer.log(f\"Average ROUGE-L for Partial Matches: {avg_rouge:.2%}\")\n        printer.log(f\"Final Weighted Score: {final_accuracy:.2%}\")\n        \n        return final_accuracy\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/verify_section"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/log"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ]
}