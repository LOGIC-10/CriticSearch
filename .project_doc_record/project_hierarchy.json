{
  "critic_search/critic_agent.py": [
    {
      "type": "ClassDef",
      "name": "CriticAgent",
      "md_content": [],
      "code_start_line": 6,
      "code_end_line": 35,
      "params": [],
      "have_return": true,
      "code_content": "class CriticAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.original_task = \"\"\n        self.critic_prompt = self.load_template(\"critic_agent.txt\")\n\n    def critic(self):\n        \"\"\"\n        生成评论。\n        \"\"\"\n        data = self.get_data_for_critic()\n\n        rendered_prompt = self.render_template(self.critic_prompt, data)\n        model_response = self.common_chat(usr_prompt=rendered_prompt)\n\n        BaseAgent.conversation_manager.add_history(role=\"user\", content=model_response)\n\n        try:\n            formatted_yaml = self.extract_and_validate_yaml(model_response)\n            return formatted_yaml\n\n        except yaml.YAMLError as exc:\n            print(f\"Invalid YAML content: {exc}\")\n            return None\n\n    def receive_agent_answer(self, agent_answer):\n        self.agent_answer = agent_answer\n\n    def get_data_for_critic(self):\n        return {\"user_question\": self.original_task, \"agent_answer\": self.agent_answer}\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/main.py",
        "critic_search/main.py/main"
      ],
      "reference_who": [
        "critic_search/base_agent.py/BaseAgent"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 7,
      "code_end_line": 10,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super().__init__()\n        self.original_task = \"\"\n        self.critic_prompt = self.load_template(\"critic_agent.txt\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [
        "critic_search/base_agent.py/BaseAgent/load_template"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "critic",
      "md_content": [],
      "code_start_line": 12,
      "code_end_line": 29,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def critic(self):\n        \"\"\"\n        生成评论。\n        \"\"\"\n        data = self.get_data_for_critic()\n\n        rendered_prompt = self.render_template(self.critic_prompt, data)\n        model_response = self.common_chat(usr_prompt=rendered_prompt)\n\n        BaseAgent.conversation_manager.add_history(role=\"user\", content=model_response)\n\n        try:\n            formatted_yaml = self.extract_and_validate_yaml(model_response)\n            return formatted_yaml\n\n        except yaml.YAMLError as exc:\n            print(f\"Invalid YAML content: {exc}\")\n            return None\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/main.py/main"
      ],
      "reference_who": [
        "critic_search/critic_agent.py/CriticAgent/get_data_for_critic",
        "critic_search/base_agent.py/BaseAgent",
        "critic_search/base_agent.py/BaseAgent/render_template",
        "critic_search/base_agent.py/BaseAgent/common_chat",
        "critic_search/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "critic_search/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "critic_search/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)",
        "critic_search/base_agent.py/BaseAgent/extract_and_validate_yaml",
        "critic_search/models.py/ConversationManager/add_history"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "receive_agent_answer",
      "md_content": [],
      "code_start_line": 31,
      "code_end_line": 32,
      "params": [
        "self",
        "agent_answer"
      ],
      "have_return": false,
      "code_content": "    def receive_agent_answer(self, agent_answer):\n        self.agent_answer = agent_answer\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/main.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_data_for_critic",
      "md_content": [],
      "code_start_line": 34,
      "code_end_line": 35,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_data_for_critic(self):\n        return {\"user_question\": self.original_task, \"agent_answer\": self.agent_answer}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/critic_agent.py/CriticAgent/critic"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "critic_search/main.py": [
    {
      "type": "FunctionDef",
      "name": "truncate_to_character_limit",
      "md_content": [],
      "code_start_line": 10,
      "code_end_line": 17,
      "params": [
        "text",
        "max_chars"
      ],
      "have_return": true,
      "code_content": "def truncate_to_character_limit(text, max_chars=100000):\n    if hasattr(text, \"content\"):\n        text = text.content  # Extract the content attribute\n\n    # Ensure `text` is a string before truncating\n    if isinstance(text, str) and len(text) > max_chars:\n        return text[:max_chars]\n    return text\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/main.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [],
      "code_start_line": 20,
      "code_end_line": 216,
      "params": [
        "TASK",
        "MAX_ITERATION"
      ],
      "have_return": true,
      "code_content": "def main(TASK, MAX_ITERATION=1):\n    # Initialize agents\n    common_agent = BaseAgent()\n\n    # initialize the task\n    common_agent.user_question = TASK\n\n    for iteration in range(MAX_ITERATION):\n        logger.info(\n            colorize_message(\n                message_title=f\"ITERATION {iteration + 1}\", color=\"cyan\", style=\"bold\"\n            )\n        )\n\n        if iteration == 0:\n            # Initialize search_results as None\n            search_results = None\n\n            # Model confidence check - yellow\n            agent_confident = common_agent.model_confident(TASK)\n            agent_confident_yaml = common_agent.extract_and_validate_yaml(\n                agent_confident\n            )\n\n            if agent_confident_yaml is None:\n                logger.warning(\n                    \"Failed to extract valid YAML content. Defaulting to 'false'.\"\n                )\n                agent_confident = False\n            else:\n                agent_confident_dict = yaml.safe_load(agent_confident_yaml)\n                agent_confident = (\n                    agent_confident_dict.get(\"confidence\", \"true\").lower() == \"true\"\n                )\n\n            if agent_confident:\n                # When confident, only get the answer\n                common_agent_answer = common_agent.common_chat(usr_prompt=TASK)\n            else:\n                # When not confident, get both answer and search results\n                data = {\n                    \"user_question\": TASK,\n                }\n                initial_search_prompt = common_agent.load_template(\n                    \"planner_agent_initial_search_plan.txt\"\n                )\n                initial_search_rendered_prompt = common_agent.render_template(\n                    initial_search_prompt, data\n                )\n\n                initial_web_result_markdown_text = common_agent.search_and_browse(\n                    initial_search_rendered_prompt\n                )\n\n                rag_based_answer_prompt = common_agent.render_template(\n                    common_agent.load_template(\"rag_based_answer.txt\"),\n                    {\n                        \"user_question\": common_agent.user_question,\n                        \"web_result_markdown_text\": initial_web_result_markdown_text,\n                    },\n                )\n\n                common_agent_answer = common_agent.common_chat(\n                    usr_prompt=rag_based_answer_prompt,\n                )\n\n        else:\n            # 前面根据critc的返回得到了新的网页搜索结果web_result_markdown_text\n            common_agent_answer = common_agent.update_answer(\n                query=TASK,\n                previous_answer=common_agent_answer,\n                search_results=truncate_to_character_limit(web_result_markdown_text),\n                critic_feedback=critic_agent_response,\n            )\n            time.sleep(0.5)  # hitting rate limits for gpt mini\n\n        logger.info(\n            colorize_message(\n                message_title=\"COMMON AGENT ANSWER\",\n                color=\"magenta\",\n                message_content=common_agent_answer,\n            )\n        )\n\n        # Critic evaluation - blue\n        critic_agent = CriticAgent()\n        critic_agent.receive_task(TASK)\n        critic_agent.receive_agent_answer(common_agent_answer)\n        critic_agent_response = critic_agent.critic()\n\n        logger.info(\n            colorize_message(\n                message_title=\"CRITIC_AGENT_RESPONSE\",\n                color=\"blue\",\n                message_content=common_agent_answer,\n            )\n        )\n\n        if yaml.safe_load(critic_agent_response).get(\"Stop\", {}).lower() == \"true\":\n            logger.info(\n                colorize_message(\n                    message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n                )\n            )\n\n            logger.info(\n                colorize_message(\n                    message_title=\"ALL SEARCH QUERIES\",\n                    color=\"black\",\n                    message_content=\", \".join(map(str, common_agent.queryDB)),\n                )\n            )\n            logger.info(\n                colorize_message(\n                    message_title=\"FINAL ANSWER\",\n                    color=\"red\",\n                    message_content=common_agent_answer,\n                )\n            )\n\n            return f\"\\n{common_agent_answer}\\n\"\n\n        # 根据critic的建议再执行一次搜索和爬虫操作\n        # 先构建rendered_prompt\n        reflection_data = {\n            \"user_question\": TASK,\n            \"previous_answer\": common_agent_answer,\n            \"user_feedback\": critic_agent_response,\n            \"search_history\": common_agent.queryDB,\n        }\n        search_again_prompt = common_agent.render_template(\n            common_agent.load_template(\"planner_agent_with_reflection.txt\"),\n            reflection_data,\n        )\n        try:\n            web_result_markdown_text = common_agent.search_and_browse(\n                search_again_prompt\n            )\n        except:\n            logger.info(\n                colorize_message(\n                    message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n                )\n            )\n\n            logger.info(\n                colorize_message(\n                    message_title=\"ALL SEARCH QUERIES\",\n                    color=\"black\",\n                    message_content=\", \".join(map(str, common_agent.queryDB)),\n                )\n            )\n\n            logger.info(\n                colorize_message(\n                    message_title=\"FINAL ANSWER\",\n                    color=\"red\",\n                    message_content=common_agent_answer,\n                )\n            )\n\n            # we run out of searches for now, so we force the agent to give a final answer:\n            return f\"\\n{common_agent_answer}\\n\"\n\n        logger.info(\n            colorize_message(\n                message_title=\"WEB RESULT MARKDOWN TEXT\",\n                color=\"blue\",\n                message_content=web_result_markdown_text,\n            )\n        )\n\n        # Check if reached max iterations\n        if iteration == MAX_ITERATION - 1:\n            logger.info(\n                colorize_message(\n                    message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n                )\n            )\n\n            logger.info(\n                colorize_message(\n                    message_title=\"ALL SEARCH QUERIES\",\n                    color=\"black\",\n                    message_content=\", \".join(map(str, common_agent.queryDB)),\n                )\n            )\n\n            logger.info(\n                colorize_message(\n                    message_title=\"FINAL ANSWER\",\n                    color=\"red\",\n                    message_content=common_agent_answer,\n                )\n            )\n\n            return f\"\\n{common_agent_answer}\\n\"\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/__main__.py"
      ],
      "reference_who": [
        "critic_search/critic_agent.py/CriticAgent",
        "critic_search/critic_agent.py/CriticAgent/critic",
        "critic_search/critic_agent.py/CriticAgent/receive_agent_answer",
        "critic_search/main.py/truncate_to_character_limit",
        "critic_search/base_agent.py/BaseAgent",
        "critic_search/base_agent.py/BaseAgent/load_template",
        "critic_search/base_agent.py/BaseAgent/render_template",
        "critic_search/base_agent.py/BaseAgent/common_chat",
        "critic_search/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "critic_search/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "critic_search/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)",
        "critic_search/base_agent.py/BaseAgent/update_answer",
        "critic_search/base_agent.py/BaseAgent/model_confident",
        "critic_search/base_agent.py/BaseAgent/search_and_browse",
        "critic_search/base_agent.py/BaseAgent/receive_task",
        "critic_search/base_agent.py/BaseAgent/extract_and_validate_yaml",
        "critic_search/log.py/colorize_message"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "critic_search/base_agent.py": [
    {
      "type": "ClassDef",
      "name": "BaseAgent",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 315,
      "params": [],
      "have_return": true,
      "code_content": "class BaseAgent:\n    # Class-level attributes, shared across all instances\n    queryDB = set()  # A set to store queries\n    tool_registry = ToolRegistry()  # Registry for tools\n    user_question = \"\"\n    conversation_manager = ConversationManager()\n\n    def __init__(self):\n        base_dir = os.path.dirname(\n            os.path.abspath(__file__)\n        )  # Directory of the current script\n        self.prompts_dir = os.path.join(base_dir, \"prompts\")\n        # self.env = Environment(loader=FileSystemLoader(self.prompts_dir))\n\n        # 对于citationDB,应该是一个字典，key是query，value是内容和来源\n        # 这个列表中的每个元素都是一个字典，代表一个搜索的问题以及对应的搜索结果\n        self.citationDB = [\n            {  # citationDB中只会把受到critic表扬的搜索结果加入\n                \"why do we say google was facing challenges in 2019?\": {\n                    \"document_id\": {  # 这个document_id是一个唯一的标识符，用于标识这个文档\n                        \"url\": \"\",\n                        \"title\": \"\",\n                        \"content\": \"\",\n                    }\n                }\n            }\n        ]\n        self.search_aggregator = SearchAggregator()\n        self.web_scraper = AsyncWebScraper()\n\n        self.repeat_turns = 10\n\n    def load_template(self, filename):\n        \"\"\"\n        Loads a template file from the prompts directory.\n\n        :param filename: The name of the template file to load.\n        :return: The content of the file as a string.\n        \"\"\"\n        filepath = os.path.join(self.prompts_dir, filename)\n\n        # Ensure the file exists\n        if not os.path.exists(filepath):\n            raise FileNotFoundError(\n                f\"Template file '{filename}' not found in {self.prompts_dir}\"\n            )\n\n        # Read and return the content of the file\n        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n            return file.read()\n\n    def render_template(self, template_str, data):\n        \"\"\"\n        Render a template using string formatting.\n\n        :param template_str: Template content as a string.\n        :param data: Dictionary of variables to replace in the template.\n        :return: Rendered string.\n        \"\"\"\n        template = Template(template_str)\n        return template.render(**data)\n\n    @overload\n    def common_chat(\n        self, usr_prompt: List, tools: None = None\n    ) -> ChatCompletionMessage: ...\n\n    @overload\n    def common_chat(self, usr_prompt: str, tools: List) -> ChatCompletionMessage: ...\n\n    @overload\n    def common_chat(self, usr_prompt: str, tools: None = None) -> str: ...\n\n    def common_chat(\n        self, usr_prompt: str | List, tools: Optional[List] = None\n    ) -> ChatCompletionMessage | str | None:\n        llm_response = call_llm(\n            model=settings.default_model,\n            usr_prompt=usr_prompt,\n            config=settings,\n            tools=tools,\n        )\n        # TODO: Do we need to add this message to history?\n        # self.history.append({\"role\": \"user\", \"content\": usr_prompt})\n        # self.history.append({\"role\": \"assistant\", \"content\": llm_response})\n\n        if tools is not None:\n            # logger.debug(f\"usr_prompt:\\n {usr_prompt}\")\n            # logger.debug(f\"llm_response:\\n {llm_response}\")\n            return llm_response\n        return llm_response.content\n\n    def update_answer(self, query, previous_answer, search_results, critic_feedback):\n        data = {\n            \"query\": query,\n            \"previous_answer\": previous_answer,\n            \"search_results\": search_results,\n            \"critic_feedback\": critic_feedback,\n        }\n\n        agent_update_answer_prompt = self.load_template(\"agent_update_answer.txt\")\n        rendered_prompt = self.render_template(agent_update_answer_prompt, data)\n\n        agent_update_answer_response = self.common_chat(usr_prompt=rendered_prompt)\n\n        return agent_update_answer_response\n\n    def model_confident(self, query):\n        \"\"\"\n        检查模型是否对当前问题有信心。\n        \"\"\"\n        data = {\"user_question\": query}\n        agent_confidence_prompt = self.load_template(\"agent_confidence.txt\")\n\n        rendered_prompt = self.render_template(agent_confidence_prompt, data)\n        agent_confidence_response = self.common_chat(usr_prompt=rendered_prompt)\n\n        return agent_confidence_response\n\n    def search_and_browse(self, rendered_prompt) -> str | None:\n        search_with_tool_response = self.common_chat(\n            usr_prompt=rendered_prompt,\n            tools=BaseAgent.tool_registry.get_or_create_tool_schema(\n                self.search_aggregator.search\n            ),\n        )\n\n        # If no tool calls, return the response immediately\n        if search_with_tool_response.tool_calls is None:\n            return search_with_tool_response.content\n\n        BaseAgent.conversation_manager.add_tool_call_to_history(\n            tool_calls=search_with_tool_response.tool_calls,\n            content=search_with_tool_response.content,\n        )\n\n        # Extract tool call IDs and their corresponding queries\n        tool_call_id_to_queries = {\n            tool_call.id: json.loads(tool_call.function.arguments).get(\"query\", [])\n            for tool_call in search_with_tool_response.tool_calls\n        }\n\n        # Collect all queries in a single list for batch search\n        all_queries = [\n            query for queries in tool_call_id_to_queries.values() for query in queries\n        ]\n\n        logger.info(f\"All queries extracted: {all_queries}\")\n\n        # Execute the batch search and retrieve results\n        search_results = asyncio.run(\n            self.search_aggregator.search(query=all_queries)\n        )  # Returns a dictionary\n\n        # Build the response map (tool_call_id -> query -> search_result)\n        tool_call_id_to_response = {\n            tool_call_id: {\n                query: search_results.get(\n                    query\n                )  # Match queries with their search results\n                for query in queries\n            }\n            for tool_call_id, queries in tool_call_id_to_queries.items()\n        }\n\n        # Initialize a list to store the function call result messages\n        function_call_result_messages = []\n\n        # Iterate through the tool_call_id_to_response dictionary and build messages\n        for tool_call_id, queries_to_responses in tool_call_id_to_response.items():\n            # Concatenate all search results\n            search_result = \"\\n\".join(\n                value for value in queries_to_responses.values() if value is not None\n            )\n\n            # Build the response message for each tool call\n            message = {\n                \"role\": \"tool\",\n                \"content\": json.dumps(\n                    {\n                        \"query\": list(queries_to_responses.keys()),  # List of queries\n                        \"search_result\": search_result,  # Concatenated search results\n                    }\n                ),\n                \"tool_call_id\": tool_call_id,\n            }\n\n            BaseAgent.conversation_manager.add_tool_call_result_to_history(\n                message={**message, \"name\": \"search\"}\n            )\n\n            function_call_result_messages.append(message)\n\n        # 根据初步的搜索结果进行筛选然后网页爬取\n        # Extract search results from tool messages\n        search_results = [\n            json.loads(msg[\"content\"])[\"search_result\"]\n            for msg in function_call_result_messages\n        ]\n\n        web_scraper_prompt = self.load_template(\"web_scraper.txt\")\n        web_scraper_rendered_prompt = self.render_template(\n            web_scraper_prompt,\n            {\n                \"user_question\": self.user_question,\n                \"initial_search_results\": search_results,\n            },\n        )\n\n        # Interact with the model for web scraping\n        web_scraper_response = self.common_chat(\n            usr_prompt=web_scraper_rendered_prompt,\n            tools=BaseAgent.tool_registry.get_or_create_tool_schema(\n                self.web_scraper.scrape\n            ),\n        )\n\n        # If no tool calls, return the response immediately\n        if web_scraper_response.tool_calls is None:\n            return web_scraper_response.content\n\n        BaseAgent.conversation_manager.add_tool_call_to_history(\n            tool_calls=web_scraper_response.tool_calls,\n            content=search_with_tool_response.content,\n        )\n\n        # Extract tool call IDs and their corresponding queries\n        tool_call_id_to_urls = {\n            tool_call.id: json.loads(tool_call.function.arguments).get(\"urls\", [])\n            for tool_call in web_scraper_response.tool_calls\n        }\n\n        # Collect all URLs in a single list for batch scraping\n        all_urls = [url for urls in tool_call_id_to_urls.values() for url in urls]\n        logger.info(f\"All URLs extracted: {all_urls}\")\n\n        # TODO: Add scrape tool result to history which involes changing scrape function return format equal to search function\n        # Execute the batch scraping and retrieve results\n        web_scraper_results = asyncio.run(\n            self.web_scraper.scrape(urls=all_urls)\n        )  # Returns a dictionary\n\n        web_result_markdown_text = self._format_web_scraper_results(web_scraper_results)\n\n        # Update the query DB with the new queries\n        BaseAgent.queryDB.update(set(all_queries))  # type: ignore\n\n        return web_result_markdown_text\n\n    def _format_web_scraper_results(self, web_scraper_results) -> str:\n        \"\"\"\n        Format web scraper results into markdown text.\n\n        Args:\n            web_scraper_results: List of scraping results with title, url and content\n\n        Returns:\n            str: Formatted markdown text with results\n        \"\"\"\n        result_blocks = []\n        for item in web_scraper_results:\n            title = item.title or \"No Title\"\n            content_lines = item.content\n            if content_lines is None:\n                content_lines = []\n            elif isinstance(content_lines, str):\n                content_lines = [content_lines]\n\n            block = f\"## [{title}]({item.url})\\n\\n\" + \"\\n\".join(content_lines)\n            result_blocks.append(block)\n\n        return \"\\n\\n---\\n\\n\".join(result_blocks)\n\n    def receive_task(self, task):\n        \"\"\"\n        接收原始任务。\n        \"\"\"\n        self.original_task = task\n\n    def extract_and_validate_yaml(self, model_response):\n        # 正则表达式匹配包裹在```yaml```之间的内容\n        import re\n\n        match = re.search(r\"```yaml\\n([\\s\\S]*?)\\n```\", model_response, re.DOTALL)\n\n        if not match:\n            return None  # 如果没有找到匹配的内容，返回None\n\n        model_response = match.group(1).strip()\n\n        try:\n            # 尝试解析YAML内容\n            parsed_yaml = yaml.safe_load(model_response)\n            return yaml.dump(parsed_yaml, default_flow_style=False)\n\n        except yaml.YAMLError as exc:\n            print(f\"Invalid YAML content: {exc}\")\n            return None\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/critic_agent.py",
        "critic_search/critic_agent.py/CriticAgent",
        "critic_search/critic_agent.py/CriticAgent/critic",
        "critic_search/main.py",
        "critic_search/main.py/main"
      ],
      "reference_who": [
        "critic_search/models.py/ConversationManager",
        "critic_search/tools/tool_registry.py/ToolRegistry"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 25,
      "code_end_line": 48,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        base_dir = os.path.dirname(\n            os.path.abspath(__file__)\n        )  # Directory of the current script\n        self.prompts_dir = os.path.join(base_dir, \"prompts\")\n        # self.env = Environment(loader=FileSystemLoader(self.prompts_dir))\n\n        # 对于citationDB,应该是一个字典，key是query，value是内容和来源\n        # 这个列表中的每个元素都是一个字典，代表一个搜索的问题以及对应的搜索结果\n        self.citationDB = [\n            {  # citationDB中只会把受到critic表扬的搜索结果加入\n                \"why do we say google was facing challenges in 2019?\": {\n                    \"document_id\": {  # 这个document_id是一个唯一的标识符，用于标识这个文档\n                        \"url\": \"\",\n                        \"title\": \"\",\n                        \"content\": \"\",\n                    }\n                }\n            }\n        ]\n        self.search_aggregator = SearchAggregator()\n        self.web_scraper = AsyncWebScraper()\n\n        self.repeat_turns = 10\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [
        "critic_search/tools/web_scraper.py/AsyncWebScraper",
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "load_template",
      "md_content": [],
      "code_start_line": 50,
      "code_end_line": 67,
      "params": [
        "self",
        "filename"
      ],
      "have_return": true,
      "code_content": "    def load_template(self, filename):\n        \"\"\"\n        Loads a template file from the prompts directory.\n\n        :param filename: The name of the template file to load.\n        :return: The content of the file as a string.\n        \"\"\"\n        filepath = os.path.join(self.prompts_dir, filename)\n\n        # Ensure the file exists\n        if not os.path.exists(filepath):\n            raise FileNotFoundError(\n                f\"Template file '{filename}' not found in {self.prompts_dir}\"\n            )\n\n        # Read and return the content of the file\n        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n            return file.read()\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py/BaseAgent/update_answer",
        "critic_search/base_agent.py/BaseAgent/model_confident",
        "critic_search/base_agent.py/BaseAgent/search_and_browse",
        "critic_search/critic_agent.py/CriticAgent/__init__",
        "critic_search/main.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "render_template",
      "md_content": [],
      "code_start_line": 69,
      "code_end_line": 78,
      "params": [
        "self",
        "template_str",
        "data"
      ],
      "have_return": true,
      "code_content": "    def render_template(self, template_str, data):\n        \"\"\"\n        Render a template using string formatting.\n\n        :param template_str: Template content as a string.\n        :param data: Dictionary of variables to replace in the template.\n        :return: Rendered string.\n        \"\"\"\n        template = Template(template_str)\n        return template.render(**data)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py/BaseAgent/update_answer",
        "critic_search/base_agent.py/BaseAgent/model_confident",
        "critic_search/base_agent.py/BaseAgent/search_and_browse",
        "critic_search/critic_agent.py/CriticAgent/critic",
        "critic_search/main.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "common_chat",
      "md_content": [],
      "code_start_line": 81,
      "code_end_line": 83,
      "params": [
        "self",
        "usr_prompt",
        "tools"
      ],
      "have_return": false,
      "code_content": "    def common_chat(\n        self, usr_prompt: List, tools: None = None\n    ) -> ChatCompletionMessage: ...\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py/BaseAgent/update_answer",
        "critic_search/base_agent.py/BaseAgent/model_confident",
        "critic_search/base_agent.py/BaseAgent/search_and_browse",
        "critic_search/critic_agent.py/CriticAgent/critic",
        "critic_search/main.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "common_chat",
      "md_content": [],
      "code_start_line": 86,
      "code_end_line": 86,
      "params": [
        "self",
        "usr_prompt",
        "tools"
      ],
      "have_return": false,
      "code_content": "    def common_chat(self, usr_prompt: str, tools: List) -> ChatCompletionMessage: ...\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py/BaseAgent/update_answer",
        "critic_search/base_agent.py/BaseAgent/model_confident",
        "critic_search/base_agent.py/BaseAgent/search_and_browse",
        "critic_search/critic_agent.py/CriticAgent/critic",
        "critic_search/main.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "common_chat",
      "md_content": [],
      "code_start_line": 89,
      "code_end_line": 89,
      "params": [
        "self",
        "usr_prompt",
        "tools"
      ],
      "have_return": false,
      "code_content": "    def common_chat(self, usr_prompt: str, tools: None = None) -> str: ...\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py/BaseAgent/update_answer",
        "critic_search/base_agent.py/BaseAgent/model_confident",
        "critic_search/base_agent.py/BaseAgent/search_and_browse",
        "critic_search/critic_agent.py/CriticAgent/critic",
        "critic_search/main.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "common_chat",
      "md_content": [],
      "code_start_line": 91,
      "code_end_line": 108,
      "params": [
        "self",
        "usr_prompt",
        "tools"
      ],
      "have_return": true,
      "code_content": "    def common_chat(\n        self, usr_prompt: str | List, tools: Optional[List] = None\n    ) -> ChatCompletionMessage | str | None:\n        llm_response = call_llm(\n            model=settings.default_model,\n            usr_prompt=usr_prompt,\n            config=settings,\n            tools=tools,\n        )\n        # TODO: Do we need to add this message to history?\n        # self.history.append({\"role\": \"user\", \"content\": usr_prompt})\n        # self.history.append({\"role\": \"assistant\", \"content\": llm_response})\n\n        if tools is not None:\n            # logger.debug(f\"usr_prompt:\\n {usr_prompt}\")\n            # logger.debug(f\"llm_response:\\n {llm_response}\")\n            return llm_response\n        return llm_response.content\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py/BaseAgent/update_answer",
        "critic_search/base_agent.py/BaseAgent/model_confident",
        "critic_search/base_agent.py/BaseAgent/search_and_browse",
        "critic_search/critic_agent.py/CriticAgent/critic",
        "critic_search/main.py/main"
      ],
      "reference_who": [
        "critic_search/llm_service.py/call_llm"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "update_answer",
      "md_content": [],
      "code_start_line": 110,
      "code_end_line": 123,
      "params": [
        "self",
        "query",
        "previous_answer",
        "search_results",
        "critic_feedback"
      ],
      "have_return": true,
      "code_content": "    def update_answer(self, query, previous_answer, search_results, critic_feedback):\n        data = {\n            \"query\": query,\n            \"previous_answer\": previous_answer,\n            \"search_results\": search_results,\n            \"critic_feedback\": critic_feedback,\n        }\n\n        agent_update_answer_prompt = self.load_template(\"agent_update_answer.txt\")\n        rendered_prompt = self.render_template(agent_update_answer_prompt, data)\n\n        agent_update_answer_response = self.common_chat(usr_prompt=rendered_prompt)\n\n        return agent_update_answer_response\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/main.py/main"
      ],
      "reference_who": [
        "critic_search/base_agent.py/BaseAgent/load_template",
        "critic_search/base_agent.py/BaseAgent/render_template",
        "critic_search/base_agent.py/BaseAgent/common_chat",
        "critic_search/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "critic_search/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "critic_search/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "model_confident",
      "md_content": [],
      "code_start_line": 125,
      "code_end_line": 135,
      "params": [
        "self",
        "query"
      ],
      "have_return": true,
      "code_content": "    def model_confident(self, query):\n        \"\"\"\n        检查模型是否对当前问题有信心。\n        \"\"\"\n        data = {\"user_question\": query}\n        agent_confidence_prompt = self.load_template(\"agent_confidence.txt\")\n\n        rendered_prompt = self.render_template(agent_confidence_prompt, data)\n        agent_confidence_response = self.common_chat(usr_prompt=rendered_prompt)\n\n        return agent_confidence_response\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/main.py/main"
      ],
      "reference_who": [
        "critic_search/base_agent.py/BaseAgent/load_template",
        "critic_search/base_agent.py/BaseAgent/render_template",
        "critic_search/base_agent.py/BaseAgent/common_chat",
        "critic_search/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "critic_search/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "critic_search/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "search_and_browse",
      "md_content": [],
      "code_start_line": 137,
      "code_end_line": 265,
      "params": [
        "self",
        "rendered_prompt"
      ],
      "have_return": true,
      "code_content": "    def search_and_browse(self, rendered_prompt) -> str | None:\n        search_with_tool_response = self.common_chat(\n            usr_prompt=rendered_prompt,\n            tools=BaseAgent.tool_registry.get_or_create_tool_schema(\n                self.search_aggregator.search\n            ),\n        )\n\n        # If no tool calls, return the response immediately\n        if search_with_tool_response.tool_calls is None:\n            return search_with_tool_response.content\n\n        BaseAgent.conversation_manager.add_tool_call_to_history(\n            tool_calls=search_with_tool_response.tool_calls,\n            content=search_with_tool_response.content,\n        )\n\n        # Extract tool call IDs and their corresponding queries\n        tool_call_id_to_queries = {\n            tool_call.id: json.loads(tool_call.function.arguments).get(\"query\", [])\n            for tool_call in search_with_tool_response.tool_calls\n        }\n\n        # Collect all queries in a single list for batch search\n        all_queries = [\n            query for queries in tool_call_id_to_queries.values() for query in queries\n        ]\n\n        logger.info(f\"All queries extracted: {all_queries}\")\n\n        # Execute the batch search and retrieve results\n        search_results = asyncio.run(\n            self.search_aggregator.search(query=all_queries)\n        )  # Returns a dictionary\n\n        # Build the response map (tool_call_id -> query -> search_result)\n        tool_call_id_to_response = {\n            tool_call_id: {\n                query: search_results.get(\n                    query\n                )  # Match queries with their search results\n                for query in queries\n            }\n            for tool_call_id, queries in tool_call_id_to_queries.items()\n        }\n\n        # Initialize a list to store the function call result messages\n        function_call_result_messages = []\n\n        # Iterate through the tool_call_id_to_response dictionary and build messages\n        for tool_call_id, queries_to_responses in tool_call_id_to_response.items():\n            # Concatenate all search results\n            search_result = \"\\n\".join(\n                value for value in queries_to_responses.values() if value is not None\n            )\n\n            # Build the response message for each tool call\n            message = {\n                \"role\": \"tool\",\n                \"content\": json.dumps(\n                    {\n                        \"query\": list(queries_to_responses.keys()),  # List of queries\n                        \"search_result\": search_result,  # Concatenated search results\n                    }\n                ),\n                \"tool_call_id\": tool_call_id,\n            }\n\n            BaseAgent.conversation_manager.add_tool_call_result_to_history(\n                message={**message, \"name\": \"search\"}\n            )\n\n            function_call_result_messages.append(message)\n\n        # 根据初步的搜索结果进行筛选然后网页爬取\n        # Extract search results from tool messages\n        search_results = [\n            json.loads(msg[\"content\"])[\"search_result\"]\n            for msg in function_call_result_messages\n        ]\n\n        web_scraper_prompt = self.load_template(\"web_scraper.txt\")\n        web_scraper_rendered_prompt = self.render_template(\n            web_scraper_prompt,\n            {\n                \"user_question\": self.user_question,\n                \"initial_search_results\": search_results,\n            },\n        )\n\n        # Interact with the model for web scraping\n        web_scraper_response = self.common_chat(\n            usr_prompt=web_scraper_rendered_prompt,\n            tools=BaseAgent.tool_registry.get_or_create_tool_schema(\n                self.web_scraper.scrape\n            ),\n        )\n\n        # If no tool calls, return the response immediately\n        if web_scraper_response.tool_calls is None:\n            return web_scraper_response.content\n\n        BaseAgent.conversation_manager.add_tool_call_to_history(\n            tool_calls=web_scraper_response.tool_calls,\n            content=search_with_tool_response.content,\n        )\n\n        # Extract tool call IDs and their corresponding queries\n        tool_call_id_to_urls = {\n            tool_call.id: json.loads(tool_call.function.arguments).get(\"urls\", [])\n            for tool_call in web_scraper_response.tool_calls\n        }\n\n        # Collect all URLs in a single list for batch scraping\n        all_urls = [url for urls in tool_call_id_to_urls.values() for url in urls]\n        logger.info(f\"All URLs extracted: {all_urls}\")\n\n        # TODO: Add scrape tool result to history which involes changing scrape function return format equal to search function\n        # Execute the batch scraping and retrieve results\n        web_scraper_results = asyncio.run(\n            self.web_scraper.scrape(urls=all_urls)\n        )  # Returns a dictionary\n\n        web_result_markdown_text = self._format_web_scraper_results(web_scraper_results)\n\n        # Update the query DB with the new queries\n        BaseAgent.queryDB.update(set(all_queries))  # type: ignore\n\n        return web_result_markdown_text\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/main.py/main"
      ],
      "reference_who": [
        "critic_search/base_agent.py/BaseAgent/load_template",
        "critic_search/base_agent.py/BaseAgent/render_template",
        "critic_search/base_agent.py/BaseAgent/common_chat",
        "critic_search/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "critic_search/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "critic_search/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)",
        "critic_search/base_agent.py/BaseAgent/_format_web_scraper_results",
        "critic_search/models.py/ConversationManager/add_tool_call_to_history",
        "critic_search/models.py/ConversationManager/add_tool_call_result_to_history",
        "critic_search/tools/tool_registry.py/ToolRegistry/get_or_create_tool_schema",
        "critic_search/tools/web_scraper.py/AsyncWebScraper/scrape",
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_format_web_scraper_results",
      "md_content": [],
      "code_start_line": 267,
      "code_end_line": 289,
      "params": [
        "self",
        "web_scraper_results"
      ],
      "have_return": true,
      "code_content": "    def _format_web_scraper_results(self, web_scraper_results) -> str:\n        \"\"\"\n        Format web scraper results into markdown text.\n\n        Args:\n            web_scraper_results: List of scraping results with title, url and content\n\n        Returns:\n            str: Formatted markdown text with results\n        \"\"\"\n        result_blocks = []\n        for item in web_scraper_results:\n            title = item.title or \"No Title\"\n            content_lines = item.content\n            if content_lines is None:\n                content_lines = []\n            elif isinstance(content_lines, str):\n                content_lines = [content_lines]\n\n            block = f\"## [{title}]({item.url})\\n\\n\" + \"\\n\".join(content_lines)\n            result_blocks.append(block)\n\n        return \"\\n\\n---\\n\\n\".join(result_blocks)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py/BaseAgent/search_and_browse"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "receive_task",
      "md_content": [],
      "code_start_line": 291,
      "code_end_line": 295,
      "params": [
        "self",
        "task"
      ],
      "have_return": false,
      "code_content": "    def receive_task(self, task):\n        \"\"\"\n        接收原始任务。\n        \"\"\"\n        self.original_task = task\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/main.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_and_validate_yaml",
      "md_content": [],
      "code_start_line": 297,
      "code_end_line": 315,
      "params": [
        "self",
        "model_response"
      ],
      "have_return": true,
      "code_content": "    def extract_and_validate_yaml(self, model_response):\n        # 正则表达式匹配包裹在```yaml```之间的内容\n        import re\n\n        match = re.search(r\"```yaml\\n([\\s\\S]*?)\\n```\", model_response, re.DOTALL)\n\n        if not match:\n            return None  # 如果没有找到匹配的内容，返回None\n\n        model_response = match.group(1).strip()\n\n        try:\n            # 尝试解析YAML内容\n            parsed_yaml = yaml.safe_load(model_response)\n            return yaml.dump(parsed_yaml, default_flow_style=False)\n\n        except yaml.YAMLError as exc:\n            print(f\"Invalid YAML content: {exc}\")\n            return None\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/critic_agent.py/CriticAgent/critic",
        "critic_search/main.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "critic_search/__init__.py": [],
  "critic_search/log.py": [
    {
      "type": "ClassDef",
      "name": "InterceptHandler",
      "md_content": [],
      "code_start_line": 13,
      "code_end_line": 30,
      "params": [],
      "have_return": false,
      "code_content": "class InterceptHandler(logging.Handler):\n    def emit(self, record: logging.LogRecord) -> None:\n        # Get corresponding Loguru level if it exists.\n        level: str | int\n        try:\n            level = logger.level(record.levelname).name\n        except ValueError:\n            level = record.levelno\n\n        # Find caller from where originated the logged message.\n        frame, depth = inspect.currentframe(), 0\n        while frame and (depth == 0 or frame.f_code.co_filename == logging.__file__):\n            frame = frame.f_back\n            depth += 1\n\n        logger.opt(depth=depth, exception=record.exc_info).log(\n            level, record.getMessage()\n        )\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/log.py/set_logger_level_from_config"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "emit",
      "md_content": [],
      "code_start_line": 14,
      "code_end_line": 30,
      "params": [
        "self",
        "record"
      ],
      "have_return": false,
      "code_content": "    def emit(self, record: logging.LogRecord) -> None:\n        # Get corresponding Loguru level if it exists.\n        level: str | int\n        try:\n            level = logger.level(record.levelname).name\n        except ValueError:\n            level = record.levelno\n\n        # Find caller from where originated the logged message.\n        frame, depth = inspect.currentframe(), 0\n        while frame and (depth == 0 or frame.f_code.co_filename == logging.__file__):\n            frame = frame.f_back\n            depth += 1\n\n        logger.opt(depth=depth, exception=record.exc_info).log(\n            level, record.getMessage()\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "set_logger_level_from_config",
      "md_content": [],
      "code_start_line": 33,
      "code_end_line": 57,
      "params": [
        "log_level"
      ],
      "have_return": false,
      "code_content": "def set_logger_level_from_config(log_level):\n    \"\"\"\n    Configures the loguru logger with specified log level and integrates it with the standard logging module.\n\n    Args:\n        log_level (str): The log level to set for loguru (e.g., \"DEBUG\", \"INFO\", \"WARNING\").\n\n    This function:\n    - Removes any existing loguru handlers to ensure a clean slate.\n    - Adds a new handler to loguru, directing output to stderr with the specified level.\n      - `enqueue=True` ensures thread-safe logging by using a queue, helpful in multi-threaded contexts.\n      - `backtrace=False` minimizes detailed traceback to prevent overly verbose output.\n      - `diagnose=False` suppresses additional loguru diagnostic information for more concise logs.\n    - Redirects the standard logging output to loguru using the InterceptHandler, allowing loguru to handle\n      all logs consistently across the application.\n    \"\"\"\n    logger.remove()\n    logger.add(\n        sys.stderr, level=log_level, enqueue=True, backtrace=False, diagnose=False\n    )\n\n    # Intercept standard logging\n    logging.basicConfig(handlers=[InterceptHandler()], level=0, force=True)\n\n    logger.success(f\"Log level set to {log_level}!\")\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [
        "critic_search/log.py/InterceptHandler"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "colorize_message",
      "md_content": [],
      "code_start_line": 72,
      "code_end_line": 107,
      "params": [
        "message_title",
        "color",
        "style",
        "message_content"
      ],
      "have_return": true,
      "code_content": "def colorize_message(\n    message_title: Optional[str] = \"\",\n    color: Literal[\n        \"black\", \"blue\", \"cyan\", \"green\", \"magenta\", \"red\", \"white\", \"yellow\"\n    ] = \"black\",\n    style: Literal[\"bold\", \"dim\", \"normal\", \"italic\", \"underline\"] = \"normal\",\n    message_content: Optional[str] = \"\",\n) -> str:\n    # Determine the theme-specific color\n    theme_color = COLOR_MAP[color][settings.theme]\n\n    # Open and close tags for color and style\n    color_tag = f\"<{theme_color}>\" if theme_color else \"\"\n\n    style_tag = f\"<{style}>\" if style != \"normal\" else \"\"\n\n    if color_tag and style_tag:\n        close_tag = \"</></>\"\n    elif color_tag or style_tag:\n        close_tag = \"</>\"\n    else:\n        close_tag = \"\"\n\n    # Fixed separator and styled title\n    styled_title = (\n        f\"{style_tag}{color_tag}{'=' * 20} {message_title} {'=' * 20}{close_tag}\"\n        if message_title\n        else \"\"\n    )\n\n    # Combine title and content\n    return (\n        f\"\\n{styled_title}\\n{message_content}\\n\"\n        if message_content\n        else f\"\\n{styled_title}\\n\"\n    )\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/main.py",
        "critic_search/main.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "critic_search/models.py": [
    {
      "type": "ClassDef",
      "name": "HistoryItem",
      "md_content": [],
      "code_start_line": 10,
      "code_end_line": 29,
      "params": [],
      "have_return": true,
      "code_content": "class HistoryItem(BaseModel):\n    role: Literal[\"user\", \"assistant\", \"tools\"]\n    content: Optional[str] = None\n    tool_calls: Optional[List[ChatCompletionMessageToolCall]] = None\n    tool_call_id: Optional[str] = None\n    name: Optional[str] = None\n\n    @model_serializer\n    def ser_model(self) -> Dict:\n        \"\"\"序列化单条历史记录为 ShareGPT 格式\"\"\"\n        if self.role == \"tools\":\n            return {\n                \"from\": \"function_call\",\n                \"value\": self.content,\n            }\n        else:\n            return {\n                \"from\": self.role,\n                \"value\": self.content or \"\",\n            }\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/models.py/ConversationManager",
        "critic_search/models.py/ConversationManager/add_history",
        "critic_search/models.py/ConversationManager/add_tool_call_result_to_history"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "ser_model",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 29,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def ser_model(self) -> Dict:\n        \"\"\"序列化单条历史记录为 ShareGPT 格式\"\"\"\n        if self.role == \"tools\":\n            return {\n                \"from\": \"function_call\",\n                \"value\": self.content,\n            }\n        else:\n            return {\n                \"from\": self.role,\n                \"value\": self.content or \"\",\n            }\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "ConversationManager",
      "md_content": [],
      "code_start_line": 33,
      "code_end_line": 54,
      "params": [],
      "have_return": true,
      "code_content": "class ConversationManager(BaseModel):\n    history: List[HistoryItem] = []\n\n    def add_history(self, role: Literal[\"user\", \"assistant\"], content, **kwargs):\n        self.history.append(HistoryItem(role=role, content=content, **kwargs))\n\n    def add_tool_call_to_history(self, tool_calls, content):\n        self.add_history(role=\"assistant\", content=content, tool_calls=tool_calls)\n\n    def add_tool_call_result_to_history(self, message: Dict):\n        self.history.append(HistoryItem.model_validate((message)))\n\n    def clear_history(self):\n        self.history.clear()\n\n    # TODO:\n    # 1. 使得 system 在最开始\n    # 2. 使得 human 和 observation 必须出现在奇数位置，gpt 和 function 必须出现在偶数位置\n    @model_serializer\n    def ser_model(self) -> Dict:\n        \"\"\"序列化完整历史记录为 ShareGPT 格式\"\"\"\n        return {\"conversations\": [item.model_dump() for item in self.history]}\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py",
        "critic_search/base_agent.py/BaseAgent"
      ],
      "reference_who": [
        "critic_search/models.py/HistoryItem"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "add_history",
      "md_content": [],
      "code_start_line": 36,
      "code_end_line": 37,
      "params": [
        "self",
        "role",
        "content"
      ],
      "have_return": false,
      "code_content": "    def add_history(self, role: Literal[\"user\", \"assistant\"], content, **kwargs):\n        self.history.append(HistoryItem(role=role, content=content, **kwargs))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/critic_agent.py/CriticAgent/critic",
        "critic_search/models.py/ConversationManager/add_tool_call_to_history"
      ],
      "reference_who": [
        "critic_search/models.py/HistoryItem"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "add_tool_call_to_history",
      "md_content": [],
      "code_start_line": 39,
      "code_end_line": 40,
      "params": [
        "self",
        "tool_calls",
        "content"
      ],
      "have_return": false,
      "code_content": "    def add_tool_call_to_history(self, tool_calls, content):\n        self.add_history(role=\"assistant\", content=content, tool_calls=tool_calls)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py/BaseAgent/search_and_browse"
      ],
      "reference_who": [
        "critic_search/models.py/ConversationManager/add_history"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "add_tool_call_result_to_history",
      "md_content": [],
      "code_start_line": 42,
      "code_end_line": 43,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def add_tool_call_result_to_history(self, message: Dict):\n        self.history.append(HistoryItem.model_validate((message)))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py/BaseAgent/search_and_browse"
      ],
      "reference_who": [
        "critic_search/models.py/HistoryItem"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "clear_history",
      "md_content": [],
      "code_start_line": 45,
      "code_end_line": 46,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def clear_history(self):\n        self.history.clear()\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "ser_model",
      "md_content": [],
      "code_start_line": 52,
      "code_end_line": 54,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def ser_model(self) -> Dict:\n        \"\"\"序列化完整历史记录为 ShareGPT 格式\"\"\"\n        return {\"conversations\": [item.model_dump() for item in self.history]}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "critic_search/__main__.py": [],
  "critic_search/llm_service.py": [
    {
      "type": "ClassDef",
      "name": "ModelManager",
      "md_content": [],
      "code_start_line": 11,
      "code_end_line": 48,
      "params": [],
      "have_return": true,
      "code_content": "class ModelManager:\n    def __init__(self, config):\n        self.config = config\n        self.clients = {}\n\n    def get_model_config(self, model_name=None):\n        models = self.config.get(\"models\", {})\n        if not models:\n            raise ValueError(\"No models found in configuration.\")\n\n        if model_name is None:\n            model_name = next(iter(models.keys()))\n\n        if model_name not in models:\n            raise ValueError(\n                f\"Model '{model_name}' not found in configuration. Available models: {list(models.keys())}\"\n            )\n\n        model_config = models.get(model_name, {})\n        return model_config\n\n    def create_client(self, model_name=None):\n        if model_name is None:\n            model_name = next(iter(self.config.models.keys()))\n\n        if model_name in self.clients:\n            return self.clients[model_name]\n\n        model_config = self.get_model_config(model_name)\n        client = OpenAI(\n            api_key=model_config.get(\"api_key\"),\n            base_url=model_config.get(\"base_url\", \"https://api.openai.com/v1\"),\n            timeout=self.config.get(\"timeout\", 60),\n            max_retries=self.config.get(\"max_retries\"),\n        )\n\n        self.clients[model_name] = client\n        return client\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/llm_service.py/call_llm"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 12,
      "code_end_line": 14,
      "params": [
        "self",
        "config"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, config):\n        self.config = config\n        self.clients = {}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_model_config",
      "md_content": [],
      "code_start_line": 16,
      "code_end_line": 30,
      "params": [
        "self",
        "model_name"
      ],
      "have_return": true,
      "code_content": "    def get_model_config(self, model_name=None):\n        models = self.config.get(\"models\", {})\n        if not models:\n            raise ValueError(\"No models found in configuration.\")\n\n        if model_name is None:\n            model_name = next(iter(models.keys()))\n\n        if model_name not in models:\n            raise ValueError(\n                f\"Model '{model_name}' not found in configuration. Available models: {list(models.keys())}\"\n            )\n\n        model_config = models.get(model_name, {})\n        return model_config\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/llm_service.py/ModelManager/create_client",
        "critic_search/llm_service.py/call_llm"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "create_client",
      "md_content": [],
      "code_start_line": 32,
      "code_end_line": 48,
      "params": [
        "self",
        "model_name"
      ],
      "have_return": true,
      "code_content": "    def create_client(self, model_name=None):\n        if model_name is None:\n            model_name = next(iter(self.config.models.keys()))\n\n        if model_name in self.clients:\n            return self.clients[model_name]\n\n        model_config = self.get_model_config(model_name)\n        client = OpenAI(\n            api_key=model_config.get(\"api_key\"),\n            base_url=model_config.get(\"base_url\", \"https://api.openai.com/v1\"),\n            timeout=self.config.get(\"timeout\", 60),\n            max_retries=self.config.get(\"max_retries\"),\n        )\n\n        self.clients[model_name] = client\n        return client\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/llm_service.py/call_llm"
      ],
      "reference_who": [
        "critic_search/llm_service.py/ModelManager/get_model_config"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "call_llm",
      "md_content": [],
      "code_start_line": 51,
      "code_end_line": 82,
      "params": [
        "model",
        "usr_prompt",
        "config",
        "tools"
      ],
      "have_return": true,
      "code_content": "def call_llm(\n    model,\n    usr_prompt: str | Iterable[ChatCompletionMessageParam],\n    config,\n    tools: List | None,\n) -> ChatCompletionMessage:\n    try:\n        model_manager = ModelManager(config)\n        client = model_manager.create_client(model)\n\n        # 从 ModelManager 获取配置\n        model_config = model_manager.get_model_config(model)\n        if isinstance(usr_prompt, str):\n            messages = [ChatCompletionUserMessageParam(content=usr_prompt, role=\"user\")]\n        else:\n            messages = usr_prompt\n\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            temperature=model_config.get(\"temperature\", 0.7),\n            max_tokens=model_config.get(\"max_tokens\", 8192),\n            tools=tools,  # type: ignore\n        )\n\n        response_message = response.choices[0].message\n        return response_message\n\n    except APIConnectionError as e:\n        raise RuntimeError(f\"Failed to connect to OpenAI API: {e}\")\n    except ValueError as e:\n        raise ValueError(f\"Error in configuration or model: {e}\")\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py",
        "critic_search/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)"
      ],
      "reference_who": [
        "critic_search/llm_service.py/ModelManager",
        "critic_search/llm_service.py/ModelManager/get_model_config",
        "critic_search/llm_service.py/ModelManager/create_client"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    }
  ],
  "critic_search/config.py": [],
  "critic_search/tools/tool_registry.py": [
    {
      "type": "ClassDef",
      "name": "ToolRegistry",
      "md_content": [],
      "code_start_line": 8,
      "code_end_line": 55,
      "params": [],
      "have_return": true,
      "code_content": "class ToolRegistry:\n    \"\"\"\n    A registry for managing tools using their function names as keys.\n\n    This class provides functionality to retrieve or create schemas for tools\n    based on provided functions, storing them for reuse and easy access.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize an empty registry for storing tool schemas.\n\n        Attributes:\n            _tools (Dict[str, dict]): A dictionary mapping function names\n                                      to their respective schemas.\n        \"\"\"\n        self._tools: Dict[str, dict] = {}\n\n    def get_or_create_tool_schema(self, *target_functions: Callable) -> List[Dict]:\n        \"\"\"\n        Retrieve or create tool schemas for the given functions.\n\n        If a function's schema is not already registered, it will be created\n        using `Tool.create_schema_from_function` and added to the registry.\n\n        Args:\n            *target_functions (Callable): One or more functions for which\n                                          schemas are to be retrieved or created.\n\n        Returns:\n            List[Dict]: A list of schemas corresponding to the provided functions.\n        \"\"\"\n        schemas = []\n        for target_function in target_functions:\n            func_name = target_function.__name__\n\n            # Create schema if not already registered\n            if func_name not in self._tools:\n                self._tools[func_name] = Tool.create_schema_from_function(\n                    target_function\n                )\n                logger.debug(\n                    f\"Created tool schema for: {func_name}, schema: {self._tools[func_name]}\"\n                )\n\n            schemas.append(self._tools[func_name])\n\n        return schemas\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py",
        "critic_search/base_agent.py/BaseAgent",
        "critic_search/tools/__init__.py"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 16,
      "code_end_line": 24,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        \"\"\"\n        Initialize an empty registry for storing tool schemas.\n\n        Attributes:\n            _tools (Dict[str, dict]): A dictionary mapping function names\n                                      to their respective schemas.\n        \"\"\"\n        self._tools: Dict[str, dict] = {}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_or_create_tool_schema",
      "md_content": [],
      "code_start_line": 26,
      "code_end_line": 55,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_or_create_tool_schema(self, *target_functions: Callable) -> List[Dict]:\n        \"\"\"\n        Retrieve or create tool schemas for the given functions.\n\n        If a function's schema is not already registered, it will be created\n        using `Tool.create_schema_from_function` and added to the registry.\n\n        Args:\n            *target_functions (Callable): One or more functions for which\n                                          schemas are to be retrieved or created.\n\n        Returns:\n            List[Dict]: A list of schemas corresponding to the provided functions.\n        \"\"\"\n        schemas = []\n        for target_function in target_functions:\n            func_name = target_function.__name__\n\n            # Create schema if not already registered\n            if func_name not in self._tools:\n                self._tools[func_name] = Tool.create_schema_from_function(\n                    target_function\n                )\n                logger.debug(\n                    f\"Created tool schema for: {func_name}, schema: {self._tools[func_name]}\"\n                )\n\n            schemas.append(self._tools[func_name])\n\n        return schemas\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py/BaseAgent/search_and_browse"
      ],
      "reference_who": [
        "critic_search/tools/models.py/Tool",
        "critic_search/tools/models.py/Tool/create_schema_from_function"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "critic_search/tools/web_scraper.py": [
    {
      "type": "ClassDef",
      "name": "ScrapedData",
      "md_content": [],
      "code_start_line": 10,
      "code_end_line": 15,
      "params": [],
      "have_return": false,
      "code_content": "class ScrapedData(BaseModel):\n    url: str\n    title: Optional[str] = None\n    content: Optional[List[str]] = None\n    metadata: Optional[dict] = None\n    error: Optional[str] = None\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/web_scraper.py/AsyncWebScraper/scrape",
        "critic_search/tools/web_scraper.py/AsyncWebScraper/scrape/fetch_url"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "AsyncWebScraper",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 77,
      "params": [],
      "have_return": true,
      "code_content": "class AsyncWebScraper:\n    def __init__(self):\n        self.headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n        }\n\n    async def scrape(self, urls: List[str]) -> List[ScrapedData]:\n        \"\"\"\n        Scrapes content from a list of webpages asynchronously.\n\n        Args:\n            urls (List[str]): A list of URLs to scrape.\n        \"\"\"\n\n        async def fetch_url(url: str) -> ScrapedData:\n            try:\n                async with httpx.AsyncClient() as client:\n                    response = await client.get(url, headers=self.headers, timeout=10)\n\n                    if response.status_code != 200:\n                        return ScrapedData(\n                            url=url,\n                            error=f\"HTTP {response.status_code}: Unable to fetch page.\",\n                        )\n\n                    html = response.text\n                    soup = BeautifulSoup(html, \"html.parser\")\n\n                    # Remove script and style elements\n                    for script in soup([\"script\", \"style\", \"meta\", \"noscript\"]):\n                        script.decompose()\n\n                    # Extract content based on specified elements or automatic content detection\n                    main_content = (\n                        soup.find(\"main\")\n                        or soup.find(\"article\")\n                        or soup.find(\"div\", class_=re.compile(r\"content|main|article\"))\n                    )\n                    if main_content:\n                        content = main_content.get_text(strip=True).splitlines()\n                    else:\n                        content = [p.get_text(strip=True) for p in soup.find_all(\"p\")]\n\n                    # Clean up the content and return\n                    content = [re.sub(r\"\\s+\", \" \", c).strip() for c in content]\n\n                    return ScrapedData(\n                        url=url,\n                        title=soup.title.string if soup.title else None,\n                        content=content[:5000],  # Limit content length\n                        metadata={\n                            \"length\": sum(len(c.split()) for c in content),\n                            \"elements_found\": len(content),\n                        },\n                    )\n            except Exception as e:\n                return ScrapedData(url=url, error=f\"Error: {str(e)}\")\n\n        # Asynchronously scrape all URLs\n        return await asyncio.gather(*(fetch_url(url) for url in urls))\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py",
        "critic_search/base_agent.py/BaseAgent/__init__",
        "critic_search/tools/__init__.py"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 19,
      "code_end_line": 22,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n        }\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "scrape",
      "md_content": [],
      "code_start_line": 24,
      "code_end_line": 77,
      "params": [
        "self",
        "urls"
      ],
      "have_return": true,
      "code_content": "    async def scrape(self, urls: List[str]) -> List[ScrapedData]:\n        \"\"\"\n        Scrapes content from a list of webpages asynchronously.\n\n        Args:\n            urls (List[str]): A list of URLs to scrape.\n        \"\"\"\n\n        async def fetch_url(url: str) -> ScrapedData:\n            try:\n                async with httpx.AsyncClient() as client:\n                    response = await client.get(url, headers=self.headers, timeout=10)\n\n                    if response.status_code != 200:\n                        return ScrapedData(\n                            url=url,\n                            error=f\"HTTP {response.status_code}: Unable to fetch page.\",\n                        )\n\n                    html = response.text\n                    soup = BeautifulSoup(html, \"html.parser\")\n\n                    # Remove script and style elements\n                    for script in soup([\"script\", \"style\", \"meta\", \"noscript\"]):\n                        script.decompose()\n\n                    # Extract content based on specified elements or automatic content detection\n                    main_content = (\n                        soup.find(\"main\")\n                        or soup.find(\"article\")\n                        or soup.find(\"div\", class_=re.compile(r\"content|main|article\"))\n                    )\n                    if main_content:\n                        content = main_content.get_text(strip=True).splitlines()\n                    else:\n                        content = [p.get_text(strip=True) for p in soup.find_all(\"p\")]\n\n                    # Clean up the content and return\n                    content = [re.sub(r\"\\s+\", \" \", c).strip() for c in content]\n\n                    return ScrapedData(\n                        url=url,\n                        title=soup.title.string if soup.title else None,\n                        content=content[:5000],  # Limit content length\n                        metadata={\n                            \"length\": sum(len(c.split()) for c in content),\n                            \"elements_found\": len(content),\n                        },\n                    )\n            except Exception as e:\n                return ScrapedData(url=url, error=f\"Error: {str(e)}\")\n\n        # Asynchronously scrape all URLs\n        return await asyncio.gather(*(fetch_url(url) for url in urls))\n",
      "name_column": 14,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py/BaseAgent/search_and_browse"
      ],
      "reference_who": [
        "critic_search/tools/web_scraper.py/ScrapedData"
      ],
      "special_reference_type": [
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "fetch_url",
      "md_content": [],
      "code_start_line": 32,
      "code_end_line": 74,
      "params": [
        "url"
      ],
      "have_return": true,
      "code_content": "        async def fetch_url(url: str) -> ScrapedData:\n            try:\n                async with httpx.AsyncClient() as client:\n                    response = await client.get(url, headers=self.headers, timeout=10)\n\n                    if response.status_code != 200:\n                        return ScrapedData(\n                            url=url,\n                            error=f\"HTTP {response.status_code}: Unable to fetch page.\",\n                        )\n\n                    html = response.text\n                    soup = BeautifulSoup(html, \"html.parser\")\n\n                    # Remove script and style elements\n                    for script in soup([\"script\", \"style\", \"meta\", \"noscript\"]):\n                        script.decompose()\n\n                    # Extract content based on specified elements or automatic content detection\n                    main_content = (\n                        soup.find(\"main\")\n                        or soup.find(\"article\")\n                        or soup.find(\"div\", class_=re.compile(r\"content|main|article\"))\n                    )\n                    if main_content:\n                        content = main_content.get_text(strip=True).splitlines()\n                    else:\n                        content = [p.get_text(strip=True) for p in soup.find_all(\"p\")]\n\n                    # Clean up the content and return\n                    content = [re.sub(r\"\\s+\", \" \", c).strip() for c in content]\n\n                    return ScrapedData(\n                        url=url,\n                        title=soup.title.string if soup.title else None,\n                        content=content[:5000],  # Limit content length\n                        metadata={\n                            \"length\": sum(len(c.split()) for c in content),\n                            \"elements_found\": len(content),\n                        },\n                    )\n            except Exception as e:\n                return ScrapedData(url=url, error=f\"Error: {str(e)}\")\n",
      "name_column": 18,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [
        "critic_search/tools/web_scraper.py/ScrapedData"
      ],
      "special_reference_type": [
        true
      ]
    }
  ],
  "critic_search/tools/__init__.py": [],
  "critic_search/tools/image_analyzer.py": [
    {
      "type": "ClassDef",
      "name": "ImageAnalyzer",
      "md_content": [],
      "code_start_line": 9,
      "code_end_line": 37,
      "params": [],
      "have_return": true,
      "code_content": "class ImageAnalyzer:\n    def __init__(self, model=\"gpt-4o-mini\"):\n        self.model = model\n\n    def analyze_image(self, image_data: str) -> Dict:\n        \"\"\"Analyze an image using the specified vision model.\"\"\"\n        try:\n            # Convert image data to base64 if it's a URL or file path\n            if isinstance(image_data, str):\n                if image_data.startswith(\"http\"):\n                    response = requests.get(image_data)\n                    image = Image.open(BytesIO(response.content))\n                else:\n                    image = Image.open(image_data)\n\n                buffered = BytesIO()\n                image.save(buffered, format=\"PNG\")\n                image_base64 = base64.b64encode(buffered.getvalue()).decode()\n            else:\n                image_base64 = image_data\n\n            # TODO this is a stand in for later use\n            return {\n                \"description\": \"Image analysis completed\",\n                \"model_used\": self.model,\n                \"image_format\": \"base64\",\n            }\n        except Exception as e:\n            return {\"error\": str(e)}\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 10,
      "code_end_line": 11,
      "params": [
        "self",
        "model"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, model=\"gpt-4o-mini\"):\n        self.model = model\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "analyze_image",
      "md_content": [],
      "code_start_line": 13,
      "code_end_line": 37,
      "params": [
        "self",
        "image_data"
      ],
      "have_return": true,
      "code_content": "    def analyze_image(self, image_data: str) -> Dict:\n        \"\"\"Analyze an image using the specified vision model.\"\"\"\n        try:\n            # Convert image data to base64 if it's a URL or file path\n            if isinstance(image_data, str):\n                if image_data.startswith(\"http\"):\n                    response = requests.get(image_data)\n                    image = Image.open(BytesIO(response.content))\n                else:\n                    image = Image.open(image_data)\n\n                buffered = BytesIO()\n                image.save(buffered, format=\"PNG\")\n                image_base64 = base64.b64encode(buffered.getvalue()).decode()\n            else:\n                image_base64 = image_data\n\n            # TODO this is a stand in for later use\n            return {\n                \"description\": \"Image analysis completed\",\n                \"model_used\": self.model,\n                \"image_format\": \"base64\",\n            }\n        except Exception as e:\n            return {\"error\": str(e)}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "critic_search/tools/models.py": [
    {
      "type": "FunctionDef",
      "name": "get_list_type_annotation",
      "md_content": [],
      "code_start_line": 8,
      "code_end_line": 20,
      "params": [
        "param_type"
      ],
      "have_return": true,
      "code_content": "def get_list_type_annotation(param_type):\n    \"\"\"\n    获取列表中元素的类型，用于构造 JSON Schema 的 items 字段。\n    支持 List[str]、List[int] 等类型。\n    \"\"\"\n    # 检查是否为泛型 List 类型\n    if get_origin(param_type) is list or get_origin(param_type) is list:\n        # 获取列表的元素类型\n        args = get_args(param_type)\n        if args and isinstance(args[0], type):\n            return {\"type\": args[0].__name__}\n    # 默认返回字符串类型（未明确指定类型时）\n    return {\"type\": \"string\"}\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/models.py/Tool/create_schema_from_function"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "serialize_type",
      "md_content": [],
      "code_start_line": 23,
      "code_end_line": 33,
      "params": [
        "value"
      ],
      "have_return": true,
      "code_content": "def serialize_type(value: str) -> str:\n    type_mapping = {\n        \"str\": \"string\",\n        \"int\": \"integer\",\n        \"float\": \"number\",\n        \"bool\": \"boolean\",\n        \"list\": \"array\",\n        \"dict\": \"object\",\n        \"None\": \"null\",\n    }\n    return type_mapping.get(value.lower(), \"null\")\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "Item",
      "md_content": [],
      "code_start_line": 36,
      "code_end_line": 41,
      "params": [],
      "have_return": true,
      "code_content": "class Item(BaseModel):\n    type: str = Field(..., description=\"The type of the list item\")\n\n    @field_serializer(\"type\")\n    def serialize_type(self, value: str, _info) -> str:\n        return serialize_type(value)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/models.py/ParameterProperty"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "serialize_type",
      "md_content": [],
      "code_start_line": 40,
      "code_end_line": 41,
      "params": [
        "self",
        "value",
        "_info"
      ],
      "have_return": true,
      "code_content": "    def serialize_type(self, value: str, _info) -> str:\n        return serialize_type(value)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "ParameterProperty",
      "md_content": [],
      "code_start_line": 44,
      "code_end_line": 53,
      "params": [],
      "have_return": true,
      "code_content": "class ParameterProperty(BaseModel):\n    type: str = Field(..., description=\"The data type of the parameter.\")\n    description: Optional[str] = Field(\n        None, description=\"A description of the parameter.\"\n    )\n    items: Optional[Item] = None\n\n    @field_serializer(\"type\")\n    def serialize_type(self, value: str, _info) -> str:\n        return serialize_type(value)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/models.py/Parameters"
      ],
      "reference_who": [
        "critic_search/tools/models.py/Item"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "serialize_type",
      "md_content": [],
      "code_start_line": 52,
      "code_end_line": 53,
      "params": [
        "self",
        "value",
        "_info"
      ],
      "have_return": true,
      "code_content": "    def serialize_type(self, value: str, _info) -> str:\n        return serialize_type(value)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "Parameters",
      "md_content": [],
      "code_start_line": 56,
      "code_end_line": 65,
      "params": [],
      "have_return": false,
      "code_content": "class Parameters(BaseModel):\n    type: str = Field(\"object\", description=\"The type of the parameter object.\")\n    properties: Dict[str, ParameterProperty] = Field(\n        ...,\n        description=\"A dictionary where keys are parameter names and values are their properties.\",\n    )\n    required: List[str] = Field(..., description=\"A list of required parameter names.\")\n    additionalProperties: bool = Field(\n        False, description=\"Whether additional properties are allowed.\"\n    )\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/models.py/Function",
        "critic_search/tools/models.py/Tool/create_schema_from_function"
      ],
      "reference_who": [
        "critic_search/tools/models.py/ParameterProperty"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "Function",
      "md_content": [],
      "code_start_line": 68,
      "code_end_line": 75,
      "params": [],
      "have_return": false,
      "code_content": "class Function(BaseModel):\n    name: str = Field(..., description=\"The name of the function.\")\n    description: str = Field(\n        ..., description=\"A description of what the function does.\"\n    )\n    parameters: Parameters = Field(\n        ..., description=\"The parameters schema for the function.\"\n    )\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/models.py/Tool",
        "critic_search/tools/models.py/Tool/create_schema_from_function"
      ],
      "reference_who": [
        "critic_search/tools/models.py/Parameters"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "Tool",
      "md_content": [],
      "code_start_line": 78,
      "code_end_line": 147,
      "params": [],
      "have_return": true,
      "code_content": "class Tool(BaseModel):\n    type: str = Field(\n        \"function\", description=\"The type of the tool, typically 'function'.\"\n    )\n    function: Function = Field(..., description=\"The function definition for the tool.\")\n\n    @classmethod\n    def create_schema_from_function(cls, target_function):\n        \"\"\"Create a Tool schema from a target function.\"\"\"\n\n        # 提取函数名称和文档字符串\n        func_name = target_function.__name__\n        func_doc = inspect.getdoc(target_function) or \"No description provided.\"\n\n        # 解析文档字符串，生成 sections\n        docstring = Docstring(func_doc)\n        # NOTE: Only support  Google-style right now.\n        sections = docstring.parse(\"google\")\n\n        # 提取描述信息\n        description = \"\"\n        parameters = []\n\n        for section in sections:\n            if section.kind == DocstringSectionKind.text:\n                description = section.value.strip()\n            elif section.kind == DocstringSectionKind.parameters:\n                parameters = section.value\n\n        # 提取参数信息\n        signature = inspect.signature(target_function)\n        required = []\n        properties = {}\n\n        for param_name, param in signature.parameters.items():\n            param_type = param.annotation if param.annotation != inspect._empty else Any\n            param_default = param.default if param.default != inspect._empty else ...\n\n            # 从解析的参数部分提取描述\n            param_description = None\n            for param_info in parameters:\n                if param_info.name == param_name:  # 使用属性访问\n                    param_description = param_info.description\n                    break\n\n            if param_default is ...:\n                required.append(param_name)\n\n            properties[param_name] = {\n                \"type\": param_type.__name__,\n                \"description\": param_description or f\"The {param_name} parameter.\",\n            }\n\n            if get_origin(param_type) is list:\n                properties[param_name][\"items\"] = get_list_type_annotation(param_type)\n\n        # Build the final Function and Tool schema\n        function_schema = Function(\n            name=func_name,\n            description=description,\n            parameters=Parameters(\n                type=\"object\",\n                properties=properties,\n                required=required,\n                additionalProperties=False,\n            ),\n        )\n        return cls(type=\"function\", function=function_schema).model_dump(\n            exclude_none=True\n        )\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/tool_registry.py",
        "critic_search/tools/tool_registry.py/ToolRegistry/get_or_create_tool_schema"
      ],
      "reference_who": [
        "critic_search/tools/models.py/Function"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "create_schema_from_function",
      "md_content": [],
      "code_start_line": 85,
      "code_end_line": 147,
      "params": [
        "cls",
        "target_function"
      ],
      "have_return": true,
      "code_content": "    def create_schema_from_function(cls, target_function):\n        \"\"\"Create a Tool schema from a target function.\"\"\"\n\n        # 提取函数名称和文档字符串\n        func_name = target_function.__name__\n        func_doc = inspect.getdoc(target_function) or \"No description provided.\"\n\n        # 解析文档字符串，生成 sections\n        docstring = Docstring(func_doc)\n        # NOTE: Only support  Google-style right now.\n        sections = docstring.parse(\"google\")\n\n        # 提取描述信息\n        description = \"\"\n        parameters = []\n\n        for section in sections:\n            if section.kind == DocstringSectionKind.text:\n                description = section.value.strip()\n            elif section.kind == DocstringSectionKind.parameters:\n                parameters = section.value\n\n        # 提取参数信息\n        signature = inspect.signature(target_function)\n        required = []\n        properties = {}\n\n        for param_name, param in signature.parameters.items():\n            param_type = param.annotation if param.annotation != inspect._empty else Any\n            param_default = param.default if param.default != inspect._empty else ...\n\n            # 从解析的参数部分提取描述\n            param_description = None\n            for param_info in parameters:\n                if param_info.name == param_name:  # 使用属性访问\n                    param_description = param_info.description\n                    break\n\n            if param_default is ...:\n                required.append(param_name)\n\n            properties[param_name] = {\n                \"type\": param_type.__name__,\n                \"description\": param_description or f\"The {param_name} parameter.\",\n            }\n\n            if get_origin(param_type) is list:\n                properties[param_name][\"items\"] = get_list_type_annotation(param_type)\n\n        # Build the final Function and Tool schema\n        function_schema = Function(\n            name=func_name,\n            description=description,\n            parameters=Parameters(\n                type=\"object\",\n                properties=properties,\n                required=required,\n                additionalProperties=False,\n            ),\n        )\n        return cls(type=\"function\", function=function_schema).model_dump(\n            exclude_none=True\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/tool_registry.py/ToolRegistry/get_or_create_tool_schema"
      ],
      "reference_who": [
        "critic_search/tools/models.py/get_list_type_annotation",
        "critic_search/tools/models.py/Parameters",
        "critic_search/tools/models.py/Function"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_delivery_date",
      "md_content": [],
      "code_start_line": 152,
      "code_end_line": 160,
      "params": [
        "order_id",
        "delivery_type"
      ],
      "have_return": false,
      "code_content": "    def get_delivery_date(order_id: str, delivery_type: str = \"standard\"):\n        \"\"\"\n        Get the delivery date for a customer's order.\n\n        Parameters:\n            order_id (str): The unique ID of the order.\n            delivery_type (str): The type of delivery (e.g., standard or express).\n        \"\"\"\n        pass\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "critic_search/tools/search_adapter/bing_client.py": [
    {
      "type": "ClassDef",
      "name": "BingClient",
      "md_content": [],
      "code_start_line": 19,
      "code_end_line": 93,
      "params": [],
      "have_return": true,
      "code_content": "class BingClient(BaseSearchClient):\n    \"\"\"\n    Bing Search API client.\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        self.base_url = \"https://api.bing.microsoft.com/v7.0/search\"\n        self._api_key = api_key\n\n    @retry(\n        stop=stop_after_attempt(5),  # 重试最多5次\n        wait=wait_exponential(multiplier=1, min=4, max=30)\n        + wait_random(min=1, max=5),  # 指数退避 + 随机抖动\n        retry=retry_if_exception_type(RatelimitException),\n    )\n    async def search(\n        self,\n        query: str,\n        max_results: int = 10,\n    ) -> SearchResponse:\n        \"\"\"\n        Perform an asynchronous search on Bing.\n\n        Args:\n            query (str): The search query.\n            days (int, optional): Time limit in days. Bing doesn't support direct day filtering similarly,\n                                  so you can ignore or implement custom logic.\n            max_results (int, optional): Maximum number of results to return.\n            region (Literal[\"us-en\", \"cn-zh\"], optional): Region or language code for the search.\n\n        Returns:\n            SearchResponse: Pydantic model containing the search results.\n        \"\"\"\n        headers = {\"Ocp-Apim-Subscription-Key\": self._api_key}\n\n        # You could add a 'mkt' parameter to target a specific market.\n        # For example, \"mkt\": \"en-US\".\n        params = {\n            \"q\": query,\n            \"count\": max_results,\n            # You can add additional Bing parameters here if needed.\n        }\n\n        logger.debug(f\"Using 'bing' for query '{query}'.\")\n\n        async with httpx.AsyncClient(timeout=10) as client:\n            response = await client.get(self.base_url, headers=headers, params=params)\n\n            # 处理 HTTP 状态码\n            if response.status_code == 429:\n                raise RatelimitException(\"Rate limit exceeded.\")\n            elif response.status_code == 401:\n                raise InvalidAPIKeyError()\n\n            # 如果响应为其他错误状态码，抛出异常\n            response.raise_for_status()\n\n            # 如果响应成功，解析 JSON 数据\n            json_response = response.json()\n\n        # 解析 Bing 的响应\n        web_pages = json_response.get(\"webPages\", {})\n        items = web_pages.get(\"value\", [])\n\n        results = []\n        for item in items:\n            results.append(\n                SearchResult(\n                    title=item.get(\"name\", \"\"),\n                    url=item.get(\"url\", \"\"),\n                    content=item.get(\"snippet\", \"\"),\n                )\n            )\n\n        return SearchResponse(query=query, results=results)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/search_aggregator.py",
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/__init__"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/exceptions.py/RatelimitException",
        "critic_search/tools/search_adapter/base_search_client.py/BaseSearchClient"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 24,
      "code_end_line": 26,
      "params": [
        "self",
        "api_key"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, api_key: str):\n        self.base_url = \"https://api.bing.microsoft.com/v7.0/search\"\n        self._api_key = api_key\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [],
      "code_start_line": 34,
      "code_end_line": 93,
      "params": [
        "self",
        "query",
        "max_results"
      ],
      "have_return": true,
      "code_content": "    async def search(\n        self,\n        query: str,\n        max_results: int = 10,\n    ) -> SearchResponse:\n        \"\"\"\n        Perform an asynchronous search on Bing.\n\n        Args:\n            query (str): The search query.\n            days (int, optional): Time limit in days. Bing doesn't support direct day filtering similarly,\n                                  so you can ignore or implement custom logic.\n            max_results (int, optional): Maximum number of results to return.\n            region (Literal[\"us-en\", \"cn-zh\"], optional): Region or language code for the search.\n\n        Returns:\n            SearchResponse: Pydantic model containing the search results.\n        \"\"\"\n        headers = {\"Ocp-Apim-Subscription-Key\": self._api_key}\n\n        # You could add a 'mkt' parameter to target a specific market.\n        # For example, \"mkt\": \"en-US\".\n        params = {\n            \"q\": query,\n            \"count\": max_results,\n            # You can add additional Bing parameters here if needed.\n        }\n\n        logger.debug(f\"Using 'bing' for query '{query}'.\")\n\n        async with httpx.AsyncClient(timeout=10) as client:\n            response = await client.get(self.base_url, headers=headers, params=params)\n\n            # 处理 HTTP 状态码\n            if response.status_code == 429:\n                raise RatelimitException(\"Rate limit exceeded.\")\n            elif response.status_code == 401:\n                raise InvalidAPIKeyError()\n\n            # 如果响应为其他错误状态码，抛出异常\n            response.raise_for_status()\n\n            # 如果响应成功，解析 JSON 数据\n            json_response = response.json()\n\n        # 解析 Bing 的响应\n        web_pages = json_response.get(\"webPages\", {})\n        items = web_pages.get(\"value\", [])\n\n        results = []\n        for item in items:\n            results.append(\n                SearchResult(\n                    title=item.get(\"name\", \"\"),\n                    url=item.get(\"url\", \"\"),\n                    content=item.get(\"snippet\", \"\"),\n                )\n            )\n\n        return SearchResponse(query=query, results=results)\n",
      "name_column": 14,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/exceptions.py/InvalidAPIKeyError",
        "critic_search/tools/search_adapter/exceptions.py/RatelimitException",
        "critic_search/tools/search_adapter/models.py/SearchResult",
        "critic_search/tools/search_adapter/models.py/SearchResponse"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    }
  ],
  "critic_search/tools/search_adapter/tavily_client.py": [
    {
      "type": "ClassDef",
      "name": "TavilyClient",
      "md_content": [],
      "code_start_line": 25,
      "code_end_line": 131,
      "params": [],
      "have_return": true,
      "code_content": "class TavilyClient(BaseSearchClient):\n    \"\"\"\n    Tavily API client class.\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        self.base_url = \"https://api.tavily.com\"\n        self._api_key = api_key\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n        }\n\n    def _increment_usage_count(self):\n        \"\"\"\n        增加使用次数，如果达到上限则抛出异常\n        \"\"\"\n        with Session(engine) as session:\n            usage_record = session.exec(\n                select(SearchClientUsage).where(\n                    SearchClientUsage.client_name == \"TavilyClient\"\n                )\n            ).first()\n\n            if not usage_record:\n                # 如果记录不存在，则创建新记录\n                usage_record = SearchClientUsage(\n                    client_name=\"TavilyClient\",\n                    usage_count=0,\n                    reset_time=get_second_day_naive(),\n                )\n                session.add(usage_record)\n\n            # 检查是否达到使用上限\n            if usage_record.usage_count >= usage_record.max_usage:\n                if get_current_time_of_new_york_naive() >= usage_record.reset_time:\n                    logger.debug(\"Reset time reached. Resetting usage count.\")\n                    # 重置计数器\n                    usage_record.usage_count = 0\n                    usage_record.reset_time = get_second_day_naive()\n                else:\n                    raise UsageLimitExceededError(\"Monthly usage limit reached.\")\n\n            # 增加使用次数\n            usage_record.usage_count += 1\n            session.commit()\n\n    @retry(\n        stop=stop_after_attempt(5),  # 重试最多5次\n        wait=wait_exponential(multiplier=1, min=4, max=30)\n        + wait_random(min=1, max=5),  # 指数退避 + 随机抖动\n        retry=retry_if_exception_type(RatelimitException),\n    )\n    async def search(\n        self,\n        query: str,\n        search_depth: Literal[\"basic\", \"advanced\"] = \"basic\",\n        topic: Literal[\"general\", \"news\"] = \"general\",\n        days: int = 7,\n        max_results: int = 10,\n    ) -> SearchResponse:\n        \"\"\"\n        异步搜索方法\n        \"\"\"\n\n        logger.debug(f\"Attempting to use engine 'Tavily' for query '{query}'. \")\n\n        # 发起异步请求\n        data = {\n            \"query\": query,\n            \"search_depth\": search_depth,\n            \"topic\": topic,\n            \"days\": days,\n            \"max_results\": max_results,\n            \"api_key\": self._api_key,\n        }\n\n        async with httpx.AsyncClient(timeout=30) as client:\n            # 检查和更新使用次数\n            self._increment_usage_count()\n\n            response = await client.post(\n                self.base_url + \"/search\", json=data, headers=self.headers\n            )\n\n        if response.status_code == 200:\n            return SearchResponse.model_validate(response.json())\n        elif response.status_code == 429:\n            try:\n                detail = response.json().get(\"detail\", {}).get(\"error\")\n                if detail:\n                    raise UsageLimitExceededError(detail)  # 抛出后直接传播，不被捕获\n            except UsageLimitExceededError:\n                raise  # 直接传播 UsageLimitExceededError，避免被后续捕获\n            except Exception as e:\n                # 捕获其他异常并记录日志\n                logger.error(f\"Failed to process 429 response: {e}\")\n                logger.error(f\"Response content: {response.text}\")\n                raise RatelimitException()  # 抛出通用限流异常\n\n            raise RatelimitException()\n        elif response.status_code == 401:\n            raise InvalidAPIKeyError()\n        else:\n            return SearchResponse(\n                query=query,\n                error_message=f\"Unexpected status code: {response.status_code}. Response: {response.text}\",\n            )\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/search_aggregator.py",
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/__init__"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/exceptions.py/RatelimitException",
        "critic_search/tools/search_adapter/base_search_client.py/BaseSearchClient"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 30,
      "code_end_line": 35,
      "params": [
        "self",
        "api_key"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, api_key: str):\n        self.base_url = \"https://api.tavily.com\"\n        self._api_key = api_key\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n        }\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_increment_usage_count",
      "md_content": [],
      "code_start_line": 37,
      "code_end_line": 69,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def _increment_usage_count(self):\n        \"\"\"\n        增加使用次数，如果达到上限则抛出异常\n        \"\"\"\n        with Session(engine) as session:\n            usage_record = session.exec(\n                select(SearchClientUsage).where(\n                    SearchClientUsage.client_name == \"TavilyClient\"\n                )\n            ).first()\n\n            if not usage_record:\n                # 如果记录不存在，则创建新记录\n                usage_record = SearchClientUsage(\n                    client_name=\"TavilyClient\",\n                    usage_count=0,\n                    reset_time=get_second_day_naive(),\n                )\n                session.add(usage_record)\n\n            # 检查是否达到使用上限\n            if usage_record.usage_count >= usage_record.max_usage:\n                if get_current_time_of_new_york_naive() >= usage_record.reset_time:\n                    logger.debug(\"Reset time reached. Resetting usage count.\")\n                    # 重置计数器\n                    usage_record.usage_count = 0\n                    usage_record.reset_time = get_second_day_naive()\n                else:\n                    raise UsageLimitExceededError(\"Monthly usage limit reached.\")\n\n            # 增加使用次数\n            usage_record.usage_count += 1\n            session.commit()\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/exceptions.py/UsageLimitExceededError",
        "critic_search/tools/search_adapter/models.py/SearchClientUsage",
        "critic_search/tools/search_adapter/search_client_usage_db.py/get_second_day_naive",
        "critic_search/tools/search_adapter/search_client_usage_db.py/get_current_time_of_new_york_naive"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [],
      "code_start_line": 77,
      "code_end_line": 131,
      "params": [
        "self",
        "query",
        "search_depth",
        "topic",
        "days",
        "max_results"
      ],
      "have_return": true,
      "code_content": "    async def search(\n        self,\n        query: str,\n        search_depth: Literal[\"basic\", \"advanced\"] = \"basic\",\n        topic: Literal[\"general\", \"news\"] = \"general\",\n        days: int = 7,\n        max_results: int = 10,\n    ) -> SearchResponse:\n        \"\"\"\n        异步搜索方法\n        \"\"\"\n\n        logger.debug(f\"Attempting to use engine 'Tavily' for query '{query}'. \")\n\n        # 发起异步请求\n        data = {\n            \"query\": query,\n            \"search_depth\": search_depth,\n            \"topic\": topic,\n            \"days\": days,\n            \"max_results\": max_results,\n            \"api_key\": self._api_key,\n        }\n\n        async with httpx.AsyncClient(timeout=30) as client:\n            # 检查和更新使用次数\n            self._increment_usage_count()\n\n            response = await client.post(\n                self.base_url + \"/search\", json=data, headers=self.headers\n            )\n\n        if response.status_code == 200:\n            return SearchResponse.model_validate(response.json())\n        elif response.status_code == 429:\n            try:\n                detail = response.json().get(\"detail\", {}).get(\"error\")\n                if detail:\n                    raise UsageLimitExceededError(detail)  # 抛出后直接传播，不被捕获\n            except UsageLimitExceededError:\n                raise  # 直接传播 UsageLimitExceededError，避免被后续捕获\n            except Exception as e:\n                # 捕获其他异常并记录日志\n                logger.error(f\"Failed to process 429 response: {e}\")\n                logger.error(f\"Response content: {response.text}\")\n                raise RatelimitException()  # 抛出通用限流异常\n\n            raise RatelimitException()\n        elif response.status_code == 401:\n            raise InvalidAPIKeyError()\n        else:\n            return SearchResponse(\n                query=query,\n                error_message=f\"Unexpected status code: {response.status_code}. Response: {response.text}\",\n            )\n",
      "name_column": 14,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient/_increment_usage_count",
        "critic_search/tools/search_adapter/exceptions.py/UsageLimitExceededError",
        "critic_search/tools/search_adapter/exceptions.py/InvalidAPIKeyError",
        "critic_search/tools/search_adapter/exceptions.py/RatelimitException",
        "critic_search/tools/search_adapter/models.py/SearchResponse"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "critic_search/tools/search_adapter/exceptions.py": [
    {
      "type": "ClassDef",
      "name": "SearchClientError",
      "md_content": [],
      "code_start_line": 4,
      "code_end_line": 6,
      "params": [],
      "have_return": false,
      "code_content": "class SearchClientError(Exception):\n    def __init__(self, message: str = \"An error occurred in the search client.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/exceptions.py/UsageLimitExceededError",
        "critic_search/tools/search_adapter/exceptions.py/BadRequestError",
        "critic_search/tools/search_adapter/exceptions.py/InvalidAPIKeyError",
        "critic_search/tools/search_adapter/exceptions.py/RatelimitException",
        "critic_search/tools/search_adapter/exceptions.py/TimeoutException"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 5,
      "code_end_line": 6,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"An error occurred in the search client.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "UsageLimitExceededError",
      "md_content": [],
      "code_start_line": 9,
      "code_end_line": 11,
      "params": [],
      "have_return": false,
      "code_content": "class UsageLimitExceededError(SearchClientError):\n    def __init__(self, message: str = \"Usage limit exceeded.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/bing_client.py",
        "critic_search/tools/search_adapter/search_aggregator.py",
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "critic_search/tools/search_adapter/tavily_client.py",
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient/_increment_usage_count",
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 10,
      "code_end_line": 11,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Usage limit exceeded.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "BadRequestError",
      "md_content": [],
      "code_start_line": 14,
      "code_end_line": 16,
      "params": [],
      "have_return": false,
      "code_content": "class BadRequestError(SearchClientError):\n    def __init__(self, message: str = \"Bad request.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [
        "critic_search/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 15,
      "code_end_line": 16,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Bad request.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "InvalidAPIKeyError",
      "md_content": [],
      "code_start_line": 19,
      "code_end_line": 21,
      "params": [],
      "have_return": false,
      "code_content": "class InvalidAPIKeyError(SearchClientError):\n    def __init__(self, message: str = \"Invalid API key.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/bing_client.py",
        "critic_search/tools/search_adapter/bing_client.py/BingClient/search",
        "critic_search/tools/search_adapter/search_aggregator.py",
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "critic_search/tools/search_adapter/tavily_client.py",
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 20,
      "code_end_line": 21,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Invalid API key.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "RatelimitException",
      "md_content": [],
      "code_start_line": 24,
      "code_end_line": 26,
      "params": [],
      "have_return": false,
      "code_content": "class RatelimitException(SearchClientError):\n    def __init__(self, message: str = \"Rate limit exceeded.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/bing_client.py",
        "critic_search/tools/search_adapter/bing_client.py/BingClient",
        "critic_search/tools/search_adapter/bing_client.py/BingClient/search",
        "critic_search/tools/search_adapter/tavily_client.py",
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient",
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 25,
      "code_end_line": 26,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Rate limit exceeded.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "TimeoutException",
      "md_content": [],
      "code_start_line": 29,
      "code_end_line": 31,
      "params": [],
      "have_return": false,
      "code_content": "class TimeoutException(SearchClientError):\n    def __init__(self, message: str = \"Timeout occurred.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [
        "critic_search/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 30,
      "code_end_line": 31,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Timeout occurred.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "critic_search/tools/search_adapter/__init__.py": [],
  "critic_search/tools/search_adapter/base_search_client.py": [
    {
      "type": "ClassDef",
      "name": "BaseSearchClient",
      "md_content": [],
      "code_start_line": 5,
      "code_end_line": 8,
      "params": [],
      "have_return": false,
      "code_content": "class BaseSearchClient(ABC):\n    @abstractmethod\n    async def search(self, query: str, **kwargs: Any) -> Any:\n        pass\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/bing_client.py",
        "critic_search/tools/search_adapter/bing_client.py/BingClient",
        "critic_search/tools/search_adapter/duckduckgo_client.py",
        "critic_search/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient",
        "critic_search/tools/search_adapter/tavily_client.py",
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [],
      "code_start_line": 7,
      "code_end_line": 8,
      "params": [
        "self",
        "query"
      ],
      "have_return": false,
      "code_content": "    async def search(self, query: str, **kwargs: Any) -> Any:\n        pass\n",
      "name_column": 14,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "critic_search/tools/search_adapter/search_aggregator.py": [
    {
      "type": "ClassDef",
      "name": "SearchAggregator",
      "md_content": [],
      "code_start_line": 17,
      "code_end_line": 136,
      "params": [],
      "have_return": true,
      "code_content": "class SearchAggregator:\n    def __init__(self):\n        self.clients: Dict[str, DuckDuckGoClient | TavilyClient | BingClient] = {\n            \"duckduckgo\": DuckDuckGoClient()\n        }\n\n        # 如果 Tavily 的 API key 存在，初始化客户端\n        tavily_api_key = settings.search_engine.tavily.api_key\n        if tavily_api_key:\n            self.clients[\"tavily\"] = TavilyClient(tavily_api_key)\n\n        # 如果 Bing 的 API key 存在，初始化客户端\n        bing_api_key = settings.search_engine.bing.api_key\n        if bing_api_key:\n            self.clients[\"bing\"] = BingClient(bing_api_key)\n\n        self.available_clients = set(self.clients.keys())\n\n        # Define a regex pattern for identifying search operators\n        self.search_operators_pattern = re.compile(\n            r'(\".*?\"|\\bsite:|filetype:|intitle:|inurl:|\\b[-+~]\\b)'\n        )\n\n        initialize_db()\n\n    def mark_engine_unavailable(self, engine: str):\n        \"\"\"\n        Mark a specific search engine as unavailable.\n\n        Args:\n            engine (str): The name of the engine to mark as unavailable.\n        \"\"\"\n        if engine in self.available_clients:\n            self.available_clients.remove(engine)\n\n    def contains_search_operators(self, query: str) -> bool:\n        \"\"\"\n        Check if a query contains special search operators.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            bool: True if the query contains special operators, False otherwise.\n        \"\"\"\n        return bool(self.search_operators_pattern.search(query))\n\n    async def _search_single_query(\n        self, query: str, engines: List[str]\n    ) -> SearchResponse:\n        for engine in engines:\n            if engine in self.available_clients:\n                try:\n                    # Call the asynchronous search method for the specified engine\n                    result = await self.clients[engine].search(query)\n                    logger.info(f\"{result.model_dump()}\")\n                    return result\n                except RetryError:\n                    logger.warning(\n                        f\"Engine '{engine}' for query: {query} failed after multiple retries. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except InvalidAPIKeyError:\n                    logger.error(\n                        f\"Engine '{engine}' for query: {query} failed because of wrong api key. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except UsageLimitExceededError:\n                    logger.warning(\n                        f\"Engine '{engine}' for query: {query} failed because of usage limit exceeded. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except Exception:\n                    logger.exception(\n                        f\"Engine '{engine}' encountered error. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n\n        logger.error(\"All specified search engines are unavailable.\")\n        return SearchResponse(\n            query=query,\n            error_message=\"Search failed: No available search engines for this query.\",\n        )\n\n    async def search(self, query: List[str]) -> Dict[str, str]:\n        \"\"\"\n        Performs a search using the provided query.\n        Supports various search techniques using special syntax.\n\n        Args:\n            query (List[str]): A list of search queries.\n        \"\"\"\n        # If query is a single string, convert it into a list with one element\n        if isinstance(query, str):\n            query = [query]\n\n        # Choose the engine dynamically based on whether the query contains search operators\n        async def dynamic_engine_task(q: str):\n            \"\"\"\n            动态选择引擎并执行任务。\n            Args:\n                q (str): 当前查询。\n                delay (float): 延迟时间。\n            \"\"\"\n            # 动态选择有效的引擎\n            engines = (\n                [\"duckduckgo\"]\n                if self.contains_search_operators(q)\n                else list(self.available_clients)\n            )\n\n            if not engines:\n                raise ValueError(f\"No available engines for query: {q}\")\n            return await self._search_single_query(q, engines)\n\n        # Perform concurrent searches for multiple queries\n        # 为每个任务动态添加延迟，但保持并发\n        tasks = [dynamic_engine_task(q) for i, q in enumerate(query)]\n        responses = await gather(*tasks)\n        return SearchResponseList(responses=responses).model_dump()\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py",
        "critic_search/base_agent.py/BaseAgent/__init__",
        "critic_search/tools/__init__.py",
        "critic_search/tools/search_adapter/__init__.py",
        "critic_search/tools/search_adapter/__main__.py",
        "critic_search/tools/search_adapter/__main__.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 40,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.clients: Dict[str, DuckDuckGoClient | TavilyClient | BingClient] = {\n            \"duckduckgo\": DuckDuckGoClient()\n        }\n\n        # 如果 Tavily 的 API key 存在，初始化客户端\n        tavily_api_key = settings.search_engine.tavily.api_key\n        if tavily_api_key:\n            self.clients[\"tavily\"] = TavilyClient(tavily_api_key)\n\n        # 如果 Bing 的 API key 存在，初始化客户端\n        bing_api_key = settings.search_engine.bing.api_key\n        if bing_api_key:\n            self.clients[\"bing\"] = BingClient(bing_api_key)\n\n        self.available_clients = set(self.clients.keys())\n\n        # Define a regex pattern for identifying search operators\n        self.search_operators_pattern = re.compile(\n            r'(\".*?\"|\\bsite:|filetype:|intitle:|inurl:|\\b[-+~]\\b)'\n        )\n\n        initialize_db()\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [
        "critic_search/tools/search_adapter/bing_client.py/BingClient",
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient",
        "critic_search/tools/search_adapter/search_client_usage_db.py/initialize_db",
        "critic_search/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "mark_engine_unavailable",
      "md_content": [],
      "code_start_line": 42,
      "code_end_line": 50,
      "params": [
        "self",
        "engine"
      ],
      "have_return": false,
      "code_content": "    def mark_engine_unavailable(self, engine: str):\n        \"\"\"\n        Mark a specific search engine as unavailable.\n\n        Args:\n            engine (str): The name of the engine to mark as unavailable.\n        \"\"\"\n        if engine in self.available_clients:\n            self.available_clients.remove(engine)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "contains_search_operators",
      "md_content": [],
      "code_start_line": 52,
      "code_end_line": 62,
      "params": [
        "self",
        "query"
      ],
      "have_return": true,
      "code_content": "    def contains_search_operators(self, query: str) -> bool:\n        \"\"\"\n        Check if a query contains special search operators.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            bool: True if the query contains special operators, False otherwise.\n        \"\"\"\n        return bool(self.search_operators_pattern.search(query))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/search/dynamic_engine_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_search_single_query",
      "md_content": [],
      "code_start_line": 64,
      "code_end_line": 99,
      "params": [
        "self",
        "query",
        "engines"
      ],
      "have_return": true,
      "code_content": "    async def _search_single_query(\n        self, query: str, engines: List[str]\n    ) -> SearchResponse:\n        for engine in engines:\n            if engine in self.available_clients:\n                try:\n                    # Call the asynchronous search method for the specified engine\n                    result = await self.clients[engine].search(query)\n                    logger.info(f\"{result.model_dump()}\")\n                    return result\n                except RetryError:\n                    logger.warning(\n                        f\"Engine '{engine}' for query: {query} failed after multiple retries. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except InvalidAPIKeyError:\n                    logger.error(\n                        f\"Engine '{engine}' for query: {query} failed because of wrong api key. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except UsageLimitExceededError:\n                    logger.warning(\n                        f\"Engine '{engine}' for query: {query} failed because of usage limit exceeded. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except Exception:\n                    logger.exception(\n                        f\"Engine '{engine}' encountered error. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n\n        logger.error(\"All specified search engines are unavailable.\")\n        return SearchResponse(\n            query=query,\n            error_message=\"Search failed: No available search engines for this query.\",\n        )\n",
      "name_column": 14,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/search/dynamic_engine_task"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/bing_client.py/BingClient/search",
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient/search",
        "critic_search/tools/search_adapter/exceptions.py/UsageLimitExceededError",
        "critic_search/tools/search_adapter/exceptions.py/InvalidAPIKeyError",
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/mark_engine_unavailable",
        "critic_search/tools/search_adapter/models.py/SearchResponse",
        "critic_search/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient/search"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [],
      "code_start_line": 101,
      "code_end_line": 136,
      "params": [
        "self",
        "query"
      ],
      "have_return": true,
      "code_content": "    async def search(self, query: List[str]) -> Dict[str, str]:\n        \"\"\"\n        Performs a search using the provided query.\n        Supports various search techniques using special syntax.\n\n        Args:\n            query (List[str]): A list of search queries.\n        \"\"\"\n        # If query is a single string, convert it into a list with one element\n        if isinstance(query, str):\n            query = [query]\n\n        # Choose the engine dynamically based on whether the query contains search operators\n        async def dynamic_engine_task(q: str):\n            \"\"\"\n            动态选择引擎并执行任务。\n            Args:\n                q (str): 当前查询。\n                delay (float): 延迟时间。\n            \"\"\"\n            # 动态选择有效的引擎\n            engines = (\n                [\"duckduckgo\"]\n                if self.contains_search_operators(q)\n                else list(self.available_clients)\n            )\n\n            if not engines:\n                raise ValueError(f\"No available engines for query: {q}\")\n            return await self._search_single_query(q, engines)\n\n        # Perform concurrent searches for multiple queries\n        # 为每个任务动态添加延迟，但保持并发\n        tasks = [dynamic_engine_task(q) for i, q in enumerate(query)]\n        responses = await gather(*tasks)\n        return SearchResponseList(responses=responses).model_dump()\n",
      "name_column": 14,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/base_agent.py/BaseAgent/search_and_browse",
        "critic_search/tools/search_adapter/__main__.py/main"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/models.py/SearchResponseList"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "dynamic_engine_task",
      "md_content": [],
      "code_start_line": 114,
      "code_end_line": 130,
      "params": [
        "q"
      ],
      "have_return": true,
      "code_content": "        async def dynamic_engine_task(q: str):\n            \"\"\"\n            动态选择引擎并执行任务。\n            Args:\n                q (str): 当前查询。\n                delay (float): 延迟时间。\n            \"\"\"\n            # 动态选择有效的引擎\n            engines = (\n                [\"duckduckgo\"]\n                if self.contains_search_operators(q)\n                else list(self.available_clients)\n            )\n\n            if not engines:\n                raise ValueError(f\"No available engines for query: {q}\")\n            return await self._search_single_query(q, engines)\n",
      "name_column": 18,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/contains_search_operators",
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "critic_search/tools/search_adapter/models.py": [
    {
      "type": "ClassDef",
      "name": "SearchResult",
      "md_content": [],
      "code_start_line": 11,
      "code_end_line": 16,
      "params": [],
      "have_return": false,
      "code_content": "class SearchResult(BaseModel):\n    title: str\n    url: str\n    content: str\n    score: Optional[float] = None\n    published_date: Optional[str] = None\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/bing_client.py",
        "critic_search/tools/search_adapter/bing_client.py/BingClient/search",
        "critic_search/tools/search_adapter/duckduckgo_client.py",
        "critic_search/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient/search",
        "critic_search/tools/search_adapter/models.py/SearchResponse"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "SearchResponse",
      "md_content": [],
      "code_start_line": 19,
      "code_end_line": 38,
      "params": [],
      "have_return": true,
      "code_content": "class SearchResponse(BaseModel):\n    query: str\n    results: List[SearchResult] = Field(default_factory=list)\n    response_time: Optional[float] = None\n    error_message: Optional[str] = None\n\n    @model_serializer\n    def ser_model(self) -> str:\n        if self.error_message:\n            formatted_response = (\n                f\"\\nQuery: {self.query}\\nError: {self.error_message}\\n\" + \"-\" * 50\n            )\n        else:\n            formatted_response = f\"\\nQuery: {self.query}\\nSearch Results:\\n\" + \"-\" * 50\n            for i, res in enumerate(self.results, 1):\n                formatted_response += (\n                    f\"\\n[{i}]:\\nTITLE: {res.title}\\nURL: {res.url}\\nCONTENT: {res.content}\\n\"\n                    + \"-\" * 50\n                )\n        return formatted_response\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/bing_client.py",
        "critic_search/tools/search_adapter/bing_client.py/BingClient/search",
        "critic_search/tools/search_adapter/duckduckgo_client.py",
        "critic_search/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient/search",
        "critic_search/tools/search_adapter/models.py/SearchResponseList",
        "critic_search/tools/search_adapter/search_aggregator.py",
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "critic_search/tools/search_adapter/tavily_client.py",
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/models.py/SearchResult"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "ser_model",
      "md_content": [],
      "code_start_line": 26,
      "code_end_line": 38,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def ser_model(self) -> str:\n        if self.error_message:\n            formatted_response = (\n                f\"\\nQuery: {self.query}\\nError: {self.error_message}\\n\" + \"-\" * 50\n            )\n        else:\n            formatted_response = f\"\\nQuery: {self.query}\\nSearch Results:\\n\" + \"-\" * 50\n            for i, res in enumerate(self.results, 1):\n                formatted_response += (\n                    f\"\\n[{i}]:\\nTITLE: {res.title}\\nURL: {res.url}\\nCONTENT: {res.content}\\n\"\n                    + \"-\" * 50\n                )\n        return formatted_response\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "SearchResponseList",
      "md_content": [],
      "code_start_line": 41,
      "code_end_line": 86,
      "params": [],
      "have_return": true,
      "code_content": "class SearchResponseList(BaseModel):\n    responses: List[SearchResponse] = Field(default_factory=list)\n\n    @model_serializer\n    def ser_model(self) -> Dict[str, str]:\n        \"\"\"\n        Serialize the list of SearchResponse objects into a dictionary,\n        ensuring unique content across queries.\n\n        Returns:\n            Dict[str, str]: A dictionary where the key is the query,\n                            and the value is a formatted string representation of the search response.\n        \"\"\"\n        result = {}\n        global_seen_contents = set()  # 全局去重逻辑\n        total_results = 0\n        unique_results_count = 0\n\n        for response in self.responses:\n            if response.error_message:\n                logger.debug(\n                    f\"Skipping serialize query '{response.query}' due to error: {response.error_message}\"\n                )\n                continue  # 跳过有 error_message 的响应\n\n            unique_results = []\n            for res in response.results:\n                total_results += 1\n                if res.content not in global_seen_contents:\n                    global_seen_contents.add(res.content)\n                    unique_results.append(res)\n\n            # 将去重后的结果更新到当前 response\n            response.results = unique_results\n            unique_results_count += len(unique_results)\n            result[response.query] = response.model_dump()\n\n        # 打印提示信息\n        duplicates_removed = total_results - unique_results_count\n        logger.success(\n            f\"Serialization completed. Total results: {total_results}, \"\n            f\"Unique results: {unique_results_count}, \"\n            f\"Duplicates removed: {duplicates_removed}.\"\n        )\n\n        return result\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/search_aggregator.py",
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/models.py/SearchResponse"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "ser_model",
      "md_content": [],
      "code_start_line": 45,
      "code_end_line": 86,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def ser_model(self) -> Dict[str, str]:\n        \"\"\"\n        Serialize the list of SearchResponse objects into a dictionary,\n        ensuring unique content across queries.\n\n        Returns:\n            Dict[str, str]: A dictionary where the key is the query,\n                            and the value is a formatted string representation of the search response.\n        \"\"\"\n        result = {}\n        global_seen_contents = set()  # 全局去重逻辑\n        total_results = 0\n        unique_results_count = 0\n\n        for response in self.responses:\n            if response.error_message:\n                logger.debug(\n                    f\"Skipping serialize query '{response.query}' due to error: {response.error_message}\"\n                )\n                continue  # 跳过有 error_message 的响应\n\n            unique_results = []\n            for res in response.results:\n                total_results += 1\n                if res.content not in global_seen_contents:\n                    global_seen_contents.add(res.content)\n                    unique_results.append(res)\n\n            # 将去重后的结果更新到当前 response\n            response.results = unique_results\n            unique_results_count += len(unique_results)\n            result[response.query] = response.model_dump()\n\n        # 打印提示信息\n        duplicates_removed = total_results - unique_results_count\n        logger.success(\n            f\"Serialization completed. Total results: {total_results}, \"\n            f\"Unique results: {unique_results_count}, \"\n            f\"Duplicates removed: {duplicates_removed}.\"\n        )\n\n        return result\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "SearchClientUsage",
      "md_content": [],
      "code_start_line": 89,
      "code_end_line": 94,
      "params": [],
      "have_return": false,
      "code_content": "class SearchClientUsage(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    client_name: str = Field(default=None, index=True)\n    usage_count: int = Field(default=0)\n    max_usage: int = Field(default=1000)\n    reset_time: datetime = Field(default=None)  # 初始值为 None\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/tavily_client.py",
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient/_increment_usage_count"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "critic_search/tools/search_adapter/search_client_usage_db.py": [
    {
      "type": "FunctionDef",
      "name": "initialize_db",
      "md_content": [],
      "code_start_line": 21,
      "code_end_line": 29,
      "params": [],
      "have_return": false,
      "code_content": "def initialize_db():\n    \"\"\"\n    Initialize the database and tables (only executed once).\n    \"\"\"\n    global _db_initialized\n    if not _db_initialized:\n        SQLModel.metadata.create_all(engine)  # Create all tables\n        _db_initialized = True\n        logger.info(f\"Database initialized at {DATABASE_PATH}\")\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/search_aggregator.py",
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_second_day_naive",
      "md_content": [],
      "code_start_line": 32,
      "code_end_line": 41,
      "params": [],
      "have_return": true,
      "code_content": "def get_second_day_naive() -> datetime:\n    \"\"\"\n    Get the second day of the current month (naive datetime without timezone).\n\n    Returns:\n        naive datetime (without timezone).\n    \"\"\"\n    now = datetime.now(ZoneInfo(\"America/New_York\"))\n    # Create a naive datetime object\n    return datetime(now.year, now.month, 2, 0, 0, 0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/tavily_client.py",
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient/_increment_usage_count"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_current_time_of_new_york_naive",
      "md_content": [],
      "code_start_line": 44,
      "code_end_line": 55,
      "params": [],
      "have_return": true,
      "code_content": "def get_current_time_of_new_york_naive() -> datetime:\n    \"\"\"\n    Get the current time in New York and remove the timezone information.\n\n    Returns:\n        naive datetime (without timezone).\n    \"\"\"\n    # Get the current time with timezone information\n    aware_time = datetime.now(ZoneInfo(\"America/New_York\"))\n    # Remove the timezone information\n    naive_time = aware_time.replace(tzinfo=None)\n    return naive_time\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/tavily_client.py",
        "critic_search/tools/search_adapter/tavily_client.py/TavilyClient/_increment_usage_count"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "critic_search/tools/search_adapter/__main__.py": [
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [],
      "code_start_line": 6,
      "code_end_line": 12,
      "params": [],
      "have_return": false,
      "code_content": "    async def main():\n        search_aggregator = SearchAggregator()\n\n        # 调用异步搜索方法\n        response = await search_aggregator.search(query=[\"Who is Leo Messi?\"])\n\n        print(response)\n",
      "name_column": 14,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": [
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator",
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "critic_search/tools/search_adapter/duckduckgo_client.py": [
    {
      "type": "ClassDef",
      "name": "DuckDuckGoClient",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 74,
      "params": [],
      "have_return": true,
      "code_content": "class DuckDuckGoClient(BaseSearchClient):\n    def _convert_days_to_timelimit(self, days: int) -> str:\n        \"\"\"\n        Convert days to DuckDuckGo's timelimit format.\n\n        Args:\n            days (int): Number of days to filter results.\n\n        Returns:\n            str: A string representing DuckDuckGo's timelimit format ('d', 'w', 'm', 'y').\n        \"\"\"\n        if days <= 1:\n            return \"d\"  # Last 24 hours\n        elif days <= 7:\n            return \"w\"  # Last week\n        elif days <= 30:\n            return \"m\"  # Last month\n        else:\n            return \"y\"  # Last year\n\n    @retry(\n        stop=stop_after_attempt(5),  # 重试最多5次\n        wait=wait_exponential(multiplier=1, min=4, max=30)\n        + wait_random(min=1, max=5),  # 指数退避 + 随机抖动\n        retry=retry_if_exception_type(RatelimitException),\n    )\n    async def search(\n        self,\n        query: str,\n        days: int = 7,\n        max_results: int = 10,\n        region: Literal[\"us-en\", \"cn-zh\"] = \"us-en\",\n    ) -> SearchResponse:\n        timelimit = self._convert_days_to_timelimit(days)\n\n        logger.debug(f\"Using 'duckduckgo-search-client' for query '{query}'.\")\n\n        raw_results = await AsyncDDGS(timeout=10).atext(\n            query,\n            region=region,\n            safesearch=\"on\",\n            timelimit=timelimit,\n            max_results=max_results,\n        )\n\n        results = [\n            SearchResult(\n                title=result[\"title\"],\n                url=result[\"href\"],\n                content=result[\"body\"],\n            )\n            for result in raw_results\n        ]\n        return SearchResponse(\n            query=query,\n            results=results[:max_results],\n        )\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/search_aggregator.py",
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/__init__"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/base_search_client.py/BaseSearchClient"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_convert_days_to_timelimit",
      "md_content": [],
      "code_start_line": 19,
      "code_end_line": 36,
      "params": [
        "self",
        "days"
      ],
      "have_return": true,
      "code_content": "    def _convert_days_to_timelimit(self, days: int) -> str:\n        \"\"\"\n        Convert days to DuckDuckGo's timelimit format.\n\n        Args:\n            days (int): Number of days to filter results.\n\n        Returns:\n            str: A string representing DuckDuckGo's timelimit format ('d', 'w', 'm', 'y').\n        \"\"\"\n        if days <= 1:\n            return \"d\"  # Last 24 hours\n        elif days <= 7:\n            return \"w\"  # Last week\n        elif days <= 30:\n            return \"m\"  # Last month\n        else:\n            return \"y\"  # Last year\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient/search"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [],
      "code_start_line": 44,
      "code_end_line": 74,
      "params": [
        "self",
        "query",
        "days",
        "max_results",
        "region"
      ],
      "have_return": true,
      "code_content": "    async def search(\n        self,\n        query: str,\n        days: int = 7,\n        max_results: int = 10,\n        region: Literal[\"us-en\", \"cn-zh\"] = \"us-en\",\n    ) -> SearchResponse:\n        timelimit = self._convert_days_to_timelimit(days)\n\n        logger.debug(f\"Using 'duckduckgo-search-client' for query '{query}'.\")\n\n        raw_results = await AsyncDDGS(timeout=10).atext(\n            query,\n            region=region,\n            safesearch=\"on\",\n            timelimit=timelimit,\n            max_results=max_results,\n        )\n\n        results = [\n            SearchResult(\n                title=result[\"title\"],\n                url=result[\"href\"],\n                content=result[\"body\"],\n            )\n            for result in raw_results\n        ]\n        return SearchResponse(\n            query=query,\n            results=results[:max_results],\n        )\n",
      "name_column": 14,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [
        "critic_search/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query"
      ],
      "reference_who": [
        "critic_search/tools/search_adapter/models.py/SearchResult",
        "critic_search/tools/search_adapter/models.py/SearchResponse",
        "critic_search/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient/_convert_days_to_timelimit"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    }
  ]
}