{
  "openai_deepresearch_pipeline.py": [
    {
      "type": "FunctionDef",
      "name": "tavily_search",
      "md_content": [
        "**tavily_search**: The function of tavily_search is to perform a search query using the Tavily API.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the search query to be executed.\n· api_key: A string representing the API key for authenticating with the Tavily service. It defaults to \"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\".\n\n**Code Description**: The tavily_search function initiates a search request to the Tavily API based on the provided query. It first prints the search query to the console for logging purposes. Then, it creates an instance of the TavilyClient using the provided API key. The function calls the search method of the TavilyClient, passing the query and specifying that raw content should be included in the response. After the search request is made, the function introduces a brief pause of 0.1 seconds to manage the request rate. Finally, it returns the search results extracted from the response, specifically targeting the \"results\" key. If no results are found, it returns an empty list.\n\nThis function is called within the process_single_activity function, which processes individual activities. When an activity contains an action of type \"search\", the tavily_search function is invoked with the content of that action as the query. The results from the tavily_search function are then stored in the action dictionary under the \"result\" key. This integration allows for seamless handling of search actions within the broader activity processing workflow.\n\n**Note**: It is important to ensure that the API key used is valid and has the necessary permissions to perform searches. Additionally, the function includes a slight delay after the search request to avoid overwhelming the API with rapid successive calls.\n\n**Output Example**: A possible return value from the tavily_search function could look like this:\n```json\n{\n    \"results\": [\n        {\n            \"title\": \"Who is Leo Messi?\",\n            \"snippet\": \"Lionel Messi is an Argentine professional footballer...\"\n        },\n        {\n            \"title\": \"Lionel Messi - Wikipedia\",\n            \"snippet\": \"Lionel Messi is widely regarded as one of the greatest football players...\"\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 9,
      "code_end_line": 18,
      "params": [
        "query",
        "api_key"
      ],
      "have_return": true,
      "code_content": "def tavily_search(query, api_key=\"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\"):\n    \"\"\"\n    Perform a Tavily search query\n    # Example usage: results = tavily_search(\"Who is Leo Messi?\")\n    \"\"\"\n    print(f\"Searching: {query}\")\n    tavily_client = TavilyClient(api_key=api_key)\n    response = tavily_client.search(query, include_raw_content=True)\n    time.sleep(0.1)  # 添加0.5秒延时\n    return response.get(\"results\", [])\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "openai_deepresearch_pipeline.py/process_single_activity"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "tavily_extract",
      "md_content": [
        "**tavily_extract**: The function of tavily_extract is to extract content from a specified URL using the Tavily API.\n\n**parameters**: The parameters of this Function.\n· url: A string representing the URL from which content is to be extracted.  \n· api_key: An optional string that serves as the API key for authentication with the Tavily service. The default value is \"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\".\n\n**Code Description**: The tavily_extract function is designed to retrieve content from a given URL by utilizing the Tavily API. Upon invocation, it first prints a message indicating the URL being processed. It then creates an instance of the TavilyClient, passing the provided API key for authentication. The function calls the extract method of the TavilyClient with the specified URL, which returns a response containing the extracted content.\n\nAfter a brief pause of 0.1 seconds (to potentially manage rate limits or server load), the function checks the response for results. If results are present, it retrieves the \"raw_content\" from the first result; if no results are found, it returns an empty string. This function is called within the process_single_activity function, which processes individual activities and determines the type of action to perform. Specifically, when the action type is \"browse\", the tavily_extract function is invoked with the content of the activity, allowing for the extraction of relevant information from the specified URL.\n\n**Note**: It is important to ensure that the provided URL is valid and that the API key has the necessary permissions to access the Tavily service. Additionally, users should be aware of any rate limits imposed by the Tavily API to avoid potential errors during extraction.\n\n**Output Example**: An example of the return value from the tavily_extract function could be a string containing the raw content extracted from the specified URL, such as: \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\" If no content is found, the return value would simply be an empty string: \"\"."
      ],
      "code_start_line": 21,
      "code_end_line": 34,
      "params": [
        "url",
        "api_key"
      ],
      "have_return": true,
      "code_content": "def tavily_extract(url, api_key=\"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\"):\n    \"\"\"\n    Extract content from a URL using Tavily\n    # Example usage: content = tavily_extract(\"https://en.wikipedia.org/wiki/Artificial_intelligence\")\n    \"\"\"\n    print(f\"Extracting: {url}\")\n    tavily_client = TavilyClient(api_key=api_key)\n    response = tavily_client.extract(url)\n    time.sleep(0.1)  # 添加延时\n    return (\n        response.get(\"results\", [])[0].get(\"raw_content\", \"\")\n        if response.get(\"results\")\n        else \"\"\n    )\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "openai_deepresearch_pipeline.py/process_single_activity"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "read_json_file",
      "md_content": [
        "**read_json_file**: The function of read_json_file is to read a JSON file from a given file path and return its content as a Python object.\n\n**parameters**: \n· file_path: The path to the JSON file to be read. Default is \"/Users/logic/Documents/CodeSpace/CriticSearch/Deep Research detection_0214.json\".\n\n**Code Description**:  \nThe `read_json_file` function is designed to read a JSON file from a specified location and load its content into a Python object. The function takes a single parameter `file_path`, which defines the location of the JSON file. If no path is provided, it defaults to a specific file path.\n\nThe function operates as follows:\n1. It first attempts to open the file at the provided `file_path` in read mode with UTF-8 encoding using the `open()` function.\n2. If the file is successfully opened, the function proceeds to parse the JSON content using the `json.load()` method. This method converts the JSON data into a Python dictionary or list, depending on the structure of the JSON file.\n3. If no errors occur during this process, the parsed data is returned to the caller.\n\nThe function handles errors gracefully using multiple `except` blocks:\n- If the specified file cannot be found at the provided path, a `FileNotFoundError` is caught, and a message is printed to the console indicating the missing file.\n- If the file is found but the content is not in a valid JSON format, a `json.JSONDecodeError` is raised, and an error message is displayed.\n- Any other unexpected exceptions during the process are captured by a general `Exception` handler, and an error message is printed.\n\nIn each error case, the function returns `None`, signaling that the file could not be successfully read or parsed.\n\n**Note**: \n- Ensure the file path provided is correct and the file exists at the specified location to avoid a `FileNotFoundError`.\n- The function expects the content of the file to be in valid JSON format. If the file contains malformed JSON, a `JSONDecodeError` will be raised.\n- The function does not handle cases where the file is empty or contains non-JSON data other than the expected format.\n  \n**Output Example**:\nIf the file located at `file_path` contains a valid JSON object such as:\n```json\n{\n  \"name\": \"John Doe\",\n  \"age\": 30,\n  \"city\": \"New York\"\n}\n```\nThe function will return the following Python dictionary:\n```python\n{\n  \"name\": \"John Doe\",\n  \"age\": 30,\n  \"city\": \"New York\"\n}\n```\nIf an error occurs, such as the file not being found or containing invalid JSON, the function will return `None`."
      ],
      "code_start_line": 37,
      "code_end_line": 52,
      "params": [
        "file_path"
      ],
      "have_return": true,
      "code_content": "def read_json_file(\n    file_path=\"/Users/logic/Documents/CodeSpace/CriticSearch/Deep Research detection_0214.json\",\n):\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            data = json.load(file)\n            return data\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}\")\n        return None\n    except json.JSONDecodeError:\n        print(f\"Error: Invalid JSON format in {file_path}\")\n        return None\n    except Exception as e:\n        print(f\"Error reading file: {str(e)}\")\n        return None\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_activity",
      "md_content": [
        "**process_activity**: The function of process_activity is to analyze a given text for specific actions related to searching and browsing, extracting relevant information accordingly.\n\n**parameters**: The parameters of this Function.\n· text: A string input that contains the activity description, which may include search and browse actions.\n\n**Code Description**: The process_activity function utilizes regular expressions to identify and extract specific actions from the input text. It defines two patterns: one for detecting search actions that follow the phrase \"Searched for\" and another for detecting browse actions that follow \"Read\" or \"Read more from\". \n\nThe function initializes an empty list called actions to store the identified actions and a variable named thinking to hold the remaining text after processing. It then iterates through the input text to find matches for the search pattern. For each match found, it appends a dictionary containing the action type (\"search\") and the associated content to the actions list, while updating the thinking variable to exclude the processed portion of the text.\n\nSimilarly, the function searches for browse actions using the defined browse pattern. If a valid URL is found (ensuring it does not contain certain characters like brackets), it appends a corresponding dictionary to the actions list and updates the thinking variable accordingly.\n\nFinally, the function returns a dictionary containing the remaining text (thinking) and the list of actions. If no actions were found, it returns None for the action key.\n\nThis function is called by the process_single_activity function, which serves as a helper to process a single activity. The process_single_activity function takes the output of process_activity and further processes each action by performing searches or extracting information based on the action type. This relationship indicates that process_activity is a foundational component that prepares the data for more specific operations in the context of processing activities.\n\n**Note**: It is important to ensure that the input text is formatted correctly to maximize the effectiveness of the regular expressions used in this function. The function assumes that the text follows a specific structure to identify actions accurately.\n\n**Output Example**: A possible return value of the function could look like this:\n{\n    \"thinking\": \"The user was interested in the following topics.\",\n    \"action\": [\n        {\"type\": \"search\", \"content\": \"machine learning\"},\n        {\"type\": \"browse\", \"content\": \"https://example.com/resource\"}\n    ]\n}"
      ],
      "code_start_line": 56,
      "code_end_line": 76,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def process_activity(text):\n    # 定义正则：匹配换行后跟\"Searched for\"和\"Read\"或\"Read more from\"\n    pattern_search = re.compile(r\"\\nSearched for\\s*(.+)\")\n    pattern_browse = re.compile(r\"\\nRead(?: more from)?\\s*(https?://[^\\s\\[\\]()]+)\")\n\n    actions = []\n    thinking = text\n\n    # 查找所有search匹配\n    for match in pattern_search.finditer(text):\n        actions.append({\"type\": \"search\", \"content\": match.group(1).strip()})\n        thinking = text[: match.start()].strip()\n\n    # 查找所有browse匹配\n    for match in pattern_browse.finditer(text):\n        url = match.group(1).strip()\n        if \"[\" not in url and \"]\" not in url and \"(\" not in url and \")\" not in url:\n            actions.append({\"type\": \"browse\", \"content\": url})\n            thinking = text[: match.start()].strip()\n\n    return {\"thinking\": thinking.strip(), \"action\": actions if actions else None}\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "openai_deepresearch_pipeline.py/process_single_activity"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_single_activity",
      "md_content": [
        "**process_single_activity**: The function of process_single_activity is to process a single activity by analyzing its actions and executing corresponding search or browse operations.\n\n**parameters**: The parameters of this Function.\n· activity: A dictionary representing a single activity that may contain actions to be processed.\n\n**Code Description**: The process_single_activity function serves as a helper function designed to handle individual activities by processing their associated actions. It begins by calling the process_activity function, which analyzes the input activity and extracts any actions related to searching or browsing. The output of process_activity is a dictionary that includes both the remaining text (referred to as \"thinking\") and a list of actions.\n\nOnce the actions are identified, the function iterates through each action in the list. For actions of type \"search\", it invokes the tavily_search function, passing the action's content as the search query. This function performs a search using the Tavily API and returns the results, which are then stored in the action dictionary under the \"result\" key. Similarly, for actions of type \"browse\", the tavily_extract function is called with the action's content as the URL. This function extracts content from the specified URL and also stores the result in the action dictionary.\n\nThe processed activity, now enriched with the results of the actions, is returned at the end of the function. This integration allows for a seamless workflow where individual activities can be processed to yield actionable insights based on the specified actions.\n\nThe process_single_activity function is called within the process_activities function, which handles a list of activities. It utilizes a thread pool to process each activity concurrently, thereby improving efficiency. The processed results are collected and returned as part of the overall output.\n\n**Note**: It is essential to ensure that the input activity is correctly formatted and contains valid action types. Additionally, the API key used in tavily_search and tavily_extract must be valid and have the necessary permissions to perform the respective operations.\n\n**Output Example**: A possible return value from the process_single_activity function could look like this:\n```json\n{\n    \"thinking\": \"The user was interested in the following topics.\",\n    \"action\": [\n        {\"type\": \"search\", \"content\": \"machine learning\", \"result\": [{\"title\": \"Machine Learning Overview\", \"snippet\": \"Machine learning is a subset of artificial intelligence...\"}]},\n        {\"type\": \"browse\", \"content\": \"https://example.com/resource\", \"result\": \"Content extracted from the specified URL.\"}\n    ]\n}\n```"
      ],
      "code_start_line": 79,
      "code_end_line": 88,
      "params": [
        "activity"
      ],
      "have_return": true,
      "code_content": "def process_single_activity(activity):\n    \"\"\"Helper function to process a single activity\"\"\"\n    processed = process_activity(activity)\n    if processed.get(\"action\"):\n        for action in processed[\"action\"]:\n            if action[\"type\"] == \"search\":\n                action[\"result\"] = tavily_search(action[\"content\"])\n            elif action[\"type\"] == \"browse\":\n                action[\"result\"] = tavily_extract(action[\"content\"])\n    return processed\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "openai_deepresearch_pipeline.py/process_activities"
      ],
      "reference_who": [
        "openai_deepresearch_pipeline.py/tavily_search",
        "openai_deepresearch_pipeline.py/tavily_extract",
        "openai_deepresearch_pipeline.py/process_activity"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "process_activities",
      "md_content": [
        "**process_activities**: The function of process_activities is to process a list of activities, handling both regular activities and a special \"Deep Research\" item, while utilizing concurrent processing for efficiency.\n\n**parameters**: The parameters of this Function.\n· activities: A list of activities, where each activity can be a dictionary containing either \"Activity\" or \"Deep Research\" keys.\n\n**Code Description**: The process_activities function begins by checking if the input, activities, is a list. If it is not, the function returns the input as is. This ensures that the function can handle unexpected input types gracefully.\n\nThe function initializes an empty list called final_result to store the processed activities and a variable deep_research set to None to temporarily hold any \"Deep Research\" item found during processing.\n\nThe function then iterates over each item in the activities list. If an item is a dictionary and contains the key \"Deep Research,\" it is stored in the deep_research variable, and the iteration continues to the next item without further processing of this item. This allows the function to prioritize the handling of \"Deep Research\" items.\n\nFor items that are dictionaries containing the key \"Activity,\" the function prepares to process the activities listed under this key. It creates an empty list called processed_activities to hold the results of processing each individual activity. The function employs a ThreadPoolExecutor with a maximum of 20 worker threads to process the activities concurrently. This parallel processing is achieved by mapping the process_single_activity function to each activity in the item[\"Activity\"] list. The results are collected into the processed_activities list.\n\nAfter processing, the function updates the original item by replacing its \"Activity\" key with the processed_activities list. The modified item is then appended to the final_result list.\n\nOnce all items have been processed, if a deep_research item was found, it is appended to the final_result list at the end. This ensures that the \"Deep Research\" item is included in the output while maintaining the order of the other activities.\n\nFinally, the function returns the final_result list, which contains all processed activities along with any \"Deep Research\" item.\n\nThe process_single_activity function, which is called within process_activities, is responsible for handling individual activities and their associated actions. It processes each activity to yield actionable insights based on the specified actions, thereby enhancing the overall functionality of process_activities.\n\n**Note**: It is important to ensure that the input activities are formatted correctly and contain valid keys. The function assumes that the \"Activity\" key will always contain a list of activities to be processed.\n\n**Output Example**: A possible return value from the process_activities function could look like this:\n```json\n[\n    {\n        \"Activity\": [\n            {\n                \"thinking\": \"The user was interested in machine learning.\",\n                \"action\": [\n                    {\"type\": \"search\", \"content\": \"machine learning\", \"result\": [{\"title\": \"Machine Learning Overview\", \"snippet\": \"Machine learning is a subset of artificial intelligence...\"}]}\n                ]\n            }\n        ]\n    },\n    {\n        \"Deep Research\": {\n            \"topic\": \"Advanced AI Techniques\",\n            \"details\": \"In-depth analysis of the latest AI methodologies.\"\n        }\n    }\n]\n```"
      ],
      "code_start_line": 92,
      "code_end_line": 122,
      "params": [
        "activities"
      ],
      "have_return": true,
      "code_content": "def process_activities(activities):\n    if not isinstance(activities, list):\n        return activities\n\n    final_result = []\n    deep_research = None\n\n    for item in activities:\n        # 如果是Deep Research项,先保存起来\n        if isinstance(item, dict) and \"Deep Research\" in item:\n            deep_research = item\n            continue\n\n        # 处理其他项(包括Activity)\n        if isinstance(item, dict) and \"Activity\" in item:\n            processed_activities = []\n\n            # 使用线程池并行处理activities\n            with ThreadPoolExecutor(max_workers=20) as executor:\n                processed_activities = list(\n                    executor.map(process_single_activity, item[\"Activity\"])\n                )\n\n            item[\"Activity\"] = processed_activities\n        final_result.append(item)\n\n    # 最后添加Deep Research\n    if deep_research:\n        final_result.append(deep_research)\n\n    return final_result\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "openai_deepresearch_pipeline.py/process_single_activity"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "eval.py": [],
  "tests/test_scrape_empty_result.py": [],
  "tests/test_function_call_between_models.py": [
    {
      "type": "FunctionDef",
      "name": "test_function_call",
      "md_content": [
        "**test_function_call**: The function of `test_function_call` is to test the behavior of the `call_llm` function when called with different models, ensuring the correct handling of tool calls and function arguments based on model type.\n\n**parameters**: The parameters of this function are as follows:\n- model: A string representing the model name to be tested.\n\n**Code Description**: \nThe `test_function_call` function is designed to verify the behavior of the `call_llm` function when different models are used. It takes a single parameter, `model`, which represents the name of the model to be tested. \n\nThe function performs the following steps:\n1. It first calls the `call_llm` function with the `model`, `usr_prompt`, `config`, and `tools` parameters, where `usr_prompt` is assumed to be defined elsewhere in the code (likely as a list or string) and contains the input message for the model, `config` is a settings dictionary for the model, and `tools` is an optional list of tools available for the model's interaction.\n\n2. The function then proceeds to check the behavior based on the model name. If the model belongs to the \"deepseek_family\" (i.e., \"r1\" or \"reasoner\"), it asserts that no tool calls are returned by the `call_llm` function, raising an `AssertionError` if tool calls are present. This is done using the `pytest.raises(AssertionError)` construct, which ensures that the model should not generate any tool calls.\n\n3. For models other than those in the \"deepseek_family\", such as GPT-4o-mini or similar, the function checks that valid tool calls are returned. If no tool calls are returned, the function will raise an assertion failure with a specific message. Additionally, for each tool call returned, the function verifies that the tool call's function name is `\"get_weather\"` and that the function arguments contain `\"location\"`. These checks ensure that the correct function and arguments are used, validating the model's response and interaction with tools.\n\nThe test ensures the correctness of the `call_llm` function’s integration with various models, verifying both the presence of tool calls and the correctness of the function name and arguments.\n\n**Note**: \n- The test assumes that `messages`, `settings`, and `tools` are predefined elsewhere in the code.\n- The behavior of the test is dependent on the `model` parameter passed to the function, and it distinguishes between models that should not return tool calls (e.g., \"r1\", \"reasoner\") and models that should return valid tool calls.\n- The specific function name checked is `\"get_weather\"`, and it is assumed that all models generating tool calls should utilize this function for weather-related queries.\n- The use of `pytest` for exception handling ensures that expected errors are correctly raised in the event of invalid tool calls.\n\n**Output Example**:\nFor a model such as \"r1\" that belongs to the deepseek family, the expected result would be an AssertionError if any tool calls are present:\n\n```\nAssertionError: Model r1 should not return tool_calls.\n```\n\nFor a model like \"GPT-4o-mini\" that is expected to return tool calls, a correct output would look like:\n\n```\nModel GPT-4o-mini returned empty tool_calls.\nModel GPT-4o-mini returned incorrect function name.\nModel GPT-4o-mini returned incorrect function arguments.\n```\n\nIn these cases, the test would fail, highlighting the specific mismatch in the tool calls, function name, or function arguments."
      ],
      "code_start_line": 38,
      "code_end_line": 67,
      "params": [
        "model"
      ],
      "have_return": true,
      "code_content": "def test_function_call(model):\n    # 调用 call_llm 函数\n    response = call_llm(\n        model=model,\n        usr_prompt=messages,\n        config=settings,\n        tools=tools,\n    )\n\n    # Case 1: DeepSeek model should not return a function call\n    deepseek_family = [\"r1\", \"reasoner\"]\n\n    if any(sub in model.lower() for sub in deepseek_family):\n        with pytest.raises(AssertionError):\n            assert response.tool_calls, f\"Model {model} should not return tool_calls.\"\n\n    # Case 2: Other models (like GPT-4o-mini) should return valid tool calls\n    else:\n        assert response.tool_calls, f\"Model {model} returned empty tool_calls.\"\n\n        # 检查 function 的名称是否正确\n        for tool_call in response.tool_calls:\n            assert tool_call.function.name == \"get_weather\", (\n                f\"Model {model} returned incorrect function name.\"\n            )\n\n            # 检查 function 的参数是否正确\n            assert \"location\" in tool_call.function.arguments, (\n                f\"Model {model} returned incorrect function arguments.\"\n            )\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/llm_service.py/call_llm"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "tests/test_search_empty_result.py": [],
  "tests/test_colorize_message.py": [],
  "tests/test_tool_result_overwrite.py": [],
  "tests/test_multiple_same_role.py": [],
  "src/criticsearch/main.py": [
    {
      "type": "FunctionDef",
      "name": "flatten_outline",
      "md_content": [
        "**flatten_outline**: The function of flatten_outline is to flatten a hierarchical outline structure into a list of sections, each annotated with its depth and path.\n\n**parameters**: The parameters of this Function.\n· section: A dictionary representing a section of the outline, which may contain a title and potentially a list of child sections.\n· depth: An integer indicating the current depth level in the hierarchy, defaulting to 1.\n· path: A list that tracks the path of titles leading to the current section, defaulting to None.\n\n**Code Description**: The flatten_outline function takes a section of an outline and recursively flattens it into a list of dictionaries. Each dictionary in the resulting list contains the following keys: \"path\", \"section\", and \"depth\". The \"path\" key holds a list of titles leading to the current section, the \"section\" key contains the original section data, and the \"depth\" key indicates how deep the section is in the hierarchy.\n\nInitially, if the path is not provided, it is set to an empty list. The function constructs a dictionary for the current section, appending its title to the path and setting its depth. This dictionary is added to a list called flat. If the current section has children, the function iterates over each child, calling itself recursively with the child section, incrementing the depth by one, and updating the path to include the current section's title. The results from these recursive calls are then extended into the flat list.\n\nThis function is called within the process_single_task function, where it is used to flatten an outline generated by a benchmark report. The flattened outline is essential for generating content for each section in parallel, allowing for efficient processing of multiple sections at once. The flattened structure enables the system to maintain the context of each section while generating content, ensuring that the generated report is coherent and well-structured.\n\n**Note**: It is important to ensure that the input section is correctly formatted as a dictionary with the expected keys (\"title\" and \"children\") to avoid errors during execution. The function assumes that the outline follows a specific structure, and deviations from this structure may lead to unexpected results.\n\n**Output Example**: An example output of the flatten_outline function might look like this for a given outline:\n\n```json\n[\n    {\n        \"path\": [\"Introduction\"],\n        \"section\": {\"title\": \"Introduction\", \"children\": [...]},\n        \"depth\": 1\n    },\n    {\n        \"path\": [\"Introduction\", \"Background\"],\n        \"section\": {\"title\": \"Background\", \"children\": [...]},\n        \"depth\": 2\n    },\n    {\n        \"path\": [\"Introduction\", \"Background\", \"Details\"],\n        \"section\": {\"title\": \"Details\", \"children\": []},\n        \"depth\": 3\n    }\n]\n``` \n\nThis output illustrates how the function captures the hierarchical structure of the outline while providing a flat representation that can be easily processed for content generation."
      ],
      "code_start_line": 22,
      "code_end_line": 37,
      "params": [
        "section",
        "depth",
        "path"
      ],
      "have_return": true,
      "code_content": "def flatten_outline(section, depth=1, path=None):\n    # 将 outline_json 展平，记录每个节点及其层级和路径\n    if path is None:\n        path = []\n    current = {\n        \"path\": path + [section.get(\"title\")],\n        \"section\": section,\n        \"depth\": depth,\n    }\n    flat = [current]\n    if \"children\" in section:\n        for child in section[\"children\"]:\n            flat.extend(\n                flatten_outline(child, depth + 1, path + [section.get(\"title\")])\n            )\n    return flat\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "generate_content_for_section",
      "md_content": [
        "**generate_content_for_section**: The function of generate_content_for_section is to generate detailed content for a specific section based on a given task and title.\n\n**parameters**: The parameters of this Function.\n· common_agent: An instance of a class that facilitates interactions with various agents, including searching and chatting functionalities.\n· section: A dictionary containing information about the section, specifically its title.\n· task: A string that represents the overarching topic or task under which the section content is to be generated.\n\n**Code Description**: The generate_content_for_section function is designed to create content for a specific section of a report or document. It begins by extracting the title from the provided section dictionary. Using this title, it formulates a search query that instructs the common_agent to generate relevant search queries related to the title within the context of the specified task. The search query is structured to prompt the agent to gather information that will be used to construct the content.\n\nThe function then calls the common_agent's search_and_browse method with the formulated search query, which retrieves search results. These results are then incorporated into a prompt that instructs the agent to write one or several detailed paragraphs about the section title, ensuring that the content is logically structured and includes citations for any data sourced from the web. The prompt explicitly requests that the output be formatted in plain text without summary sentences, and it emphasizes the importance of including citations in a specified format.\n\nAfter constructing the prompt, the function invokes the common_agent's common_chat method to generate the actual content based on the prompt. The generated paragraph is then printed to the console using the printer's print method, preceded by a horizontal rule that indicates the section for which content has been generated. Finally, the function returns the generated paragraph.\n\nThis function is called within the process_single_task function, which orchestrates the overall task processing. In process_single_task, the generate_content_for_section function is executed in parallel for multiple sections of a report, utilizing a ThreadPoolExecutor to manage concurrent execution. This allows for efficient generation of content across different sections, enhancing the overall performance of the task.\n\n**Note**: When using the generate_content_for_section function, it is crucial to ensure that the common_agent is properly initialized and capable of performing search and chat operations. Additionally, the section parameter must contain a valid title to generate meaningful content.\n\n**Output Example**: A possible return value of the function could be a detailed paragraph such as: \"The Syrian opposition has undergone significant transformations since the onset of the civil war. Various factions have emerged, each with distinct ideologies and objectives. According to <cite>https://example.com/source</cite>, the fragmentation of the opposition has complicated efforts for a unified front against the regime. Furthermore, the international community's involvement has influenced the dynamics on the ground, as noted by <cite>https://example.com/another-source</cite>.\""
      ],
      "code_start_line": 40,
      "code_end_line": 53,
      "params": [
        "common_agent",
        "section",
        "task"
      ],
      "have_return": true,
      "code_content": "def generate_content_for_section(common_agent, section, task):\n    # 保持 prompt 不变，仅生成单层章节内容，不递归\n    title = section.get(\"title\")\n    search_query = f\"generate some search queries about '{title}' under the background of this TOPIC/TASK: '{task}'.\"\n    search_results = common_agent.search_and_browse(search_query)\n    prompt = (\n        f\"Using the following search results:\\n\\n{search_results}\\n\\n\"\n        f\"Write one or several detailed paragraphs with data and facts in a logical way about '{title}' under the background of this TOPIC/TASK: '{task}', formatted in pure text, without summary sentences.\"\n        f\"Please make sure you are always obeying and using '<\\cite>The url link that you used for supporting the previous statement<\\cite>' format in every sentence that you are using data from the web.\"\n    )\n    paragraph = common_agent.common_chat(usr_prompt=prompt)\n    printer.rule(f\"Generated content for '{title}'\")\n    printer.print(paragraph)\n    return paragraph\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/print"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "reconstruct_markdown",
      "md_content": [
        "### Function: `reconstruct_markdown`\n\n#### Overview:\nThe `reconstruct_markdown` function generates a Markdown formatted text from an outline structure and a set of flattened content. It uses the provided outline's structure, iterating through each section, and then associates the content from the flat list based on the full path of each section to ensure the correct order and content placement. The result is a well-structured Markdown document that preserves the hierarchical organization of the sections.\n\n#### Parameters:\n- **outline (dict)**: A dictionary representing the hierarchical structure of the document. This outline contains keys such as \"title\" for section titles, and \"children\" for sub-sections. The structure represents the sections and subsections in a nested manner.\n  \n- **flat_contents (list of tuples)**: A list of tuples where each tuple contains an item (a dictionary with a key \"path\" representing the unique path to the section) and its associated content. The content for each section is mapped to its full path, ensuring that the correct content is placed in the respective section.\n\n#### Return:\n- **str**: A string containing the reconstructed Markdown content. The structure follows the outline's hierarchy, with the correct content inserted under each section title. The content is formatted using Markdown syntax, with headings marked by `#` corresponding to the section depth.\n\n#### Description:\n1. **Mapping Content to Paths**:  \n   The function starts by creating a dictionary (`content_map`) that maps a unique path (represented as a tuple of section titles) to its corresponding content. This ensures that each section's content is correctly aligned with the section’s path in the outline.\n\n2. **Recursive Helper Function**:  \n   A recursive helper function (`helper`) is defined to traverse the outline. For each section, it:\n   - Constructs the path by appending the current section's title to the path of its parent.\n   - Generates the appropriate heading for the section based on its depth in the hierarchy (using the number of titles in the path to determine the heading level).\n   - Checks if the section has corresponding content in the `content_map` and adds it to the Markdown output if available.\n   - Recursively processes child sections if they exist.\n\n3. **Result Construction**:  \n   The function starts by adding the title of the root section (if available) to the Markdown output. Then, it iterates through each section in the outline’s children and invokes the helper function to build the full document.\n\n#### Example Usage:\n\n```python\noutline = {\n    \"title\": \"Main Title\",\n    \"children\": [\n        {\n            \"title\": \"Section 1\",\n            \"children\": []\n        },\n        {\n            \"title\": \"Section 2\",\n            \"children\": [\n                {\"title\": \"Subsection 2.1\", \"children\": []}\n            ]\n        }\n    ]\n}\n\nflat_contents = [\n    ({\"path\": [\"Main Title\", \"Section 1\"]}, \"Content for Section 1\"),\n    ({\"path\": [\"Main Title\", \"Section 2\"]}, \"Content for Section 2\"),\n    ({\"path\": [\"Main Title\", \"Section 2\", \"Subsection 2.1\"]}, \"Content for Subsection 2.1\")\n]\n\nmarkdown_text = reconstruct_markdown(outline, flat_contents)\nprint(markdown_text)\n```\n\n#### Output:\n\n```markdown\n# Main Title\n\n## Section 1\n\nContent for Section 1\n\n## Section 2\n\n### Subsection 2.1\n\nContent for Subsection 2.1\n\nContent for Section 2\n```\n\n#### Notes:\n- The function ensures that the Markdown content is structured hierarchically, with section titles prefixed by the appropriate number of `#` characters based on their depth in the outline.\n- The path-based content mapping ensures that even deeply nested sections receive their correct content, avoiding misalignment in the final document.\n"
      ],
      "code_start_line": 56,
      "code_end_line": 92,
      "params": [
        "outline",
        "flat_contents"
      ],
      "have_return": true,
      "code_content": "def reconstruct_markdown(outline, flat_contents):\n    \"\"\"\n    根据展平后的内容与原 outline 结构，拼接生成最终 Markdown 文本\n    使用完整路径为key确保唯一性\n    \"\"\"\n    # 构建以完整路径为key的映射字典\n    content_map = {}\n    for item, content in flat_contents:\n        path_key = tuple(item[\"path\"])  # 使用完整路径作为key\n        content_map[path_key] = content\n\n    def helper(section, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n\n        # 生成标题（深度就是路径的长度）\n        depth = len(current_path)\n        md = f\"{'#' * depth} {section.get('title')}\\n\\n\"\n\n        # 添加该节的内容（如果有）\n        if path_key in content_map:\n            md += f\"{content_map[path_key]}\\n\\n\"\n\n        # 处理子节点\n        if \"children\" in section:\n            for child in section[\"children\"]:\n                md += helper(child, current_path)\n        return md\n\n    result = \"\"\n    if \"title\" in outline:\n        result += f\"# {outline['title']}\\n\\n\"\n\n    for section in outline.get(\"children\", []):\n        result += helper(section, [outline.get(\"title\")] if \"title\" in outline else [])\n\n    return result\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "helper",
      "md_content": [
        "**helper**: The function of helper is to recursively generate a Markdown representation of a structured document based on the provided section and its children.\n\n**parameters**: The parameters of this Function.\n· parameter1: section - A dictionary representing a section of the document, which includes a title and potentially child sections.\n· parameter2: path - A list that keeps track of the current path of titles leading to the current section, defaulting to an empty list.\n\n**Code Description**: The helper function constructs a Markdown string for a given section of a document. It begins by creating a new path that includes the title of the current section. This path is converted into a tuple, which serves as a key to look up content in a predefined `content_map`. The depth of the current section is determined by the length of the path, which is used to format the section title with the appropriate number of hash symbols for Markdown headers.\n\nIf the current section's path key exists in the `content_map`, the corresponding content is appended to the Markdown string. The function then checks if the section contains any child sections. If so, it iterates through each child and recursively calls itself, appending the resulting Markdown from each child to the current Markdown string. Finally, the complete Markdown string for the section and its children is returned.\n\n**Note**: It is important to ensure that the `content_map` is defined and contains the necessary content for the sections being processed. The function assumes that the structure of the input section is consistent and that each section may have a \"title\" and optionally \"children\".\n\n**Output Example**: An example output for a section with the title \"Introduction\" and a child section titled \"Background\" might look like this:\n\n```\n# Introduction\n\nThis is the content for the Introduction section.\n\n## Background\n\nThis is the content for the Background section.\n```"
      ],
      "code_start_line": 67,
      "code_end_line": 83,
      "params": [
        "section",
        "path"
      ],
      "have_return": true,
      "code_content": "    def helper(section, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n\n        # 生成标题（深度就是路径的长度）\n        depth = len(current_path)\n        md = f\"{'#' * depth} {section.get('title')}\\n\\n\"\n\n        # 添加该节的内容（如果有）\n        if path_key in content_map:\n            md += f\"{content_map[path_key]}\\n\\n\"\n\n        # 处理子节点\n        if \"children\" in section:\n            for child in section[\"children\"]:\n                md += helper(child, current_path)\n        return md\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_citations",
      "md_content": [
        "**extract_citations**: The function of extract_citations is to extract URLs enclosed within <cite> tags from a given text.\n\n**parameters**: The parameters of this function are as follows:\n· text: A string containing the text from which citations are to be extracted.\n\n**Code Description**: The extract_citations function utilizes a regular expression to identify and extract all occurrences of text that are enclosed within <cite> tags in the provided input text. It initializes an empty list called citations to store the matched URLs. The regular expression pattern defined as r\"<cite>(.*?)<\\/cite>\" is used to find all matches in the text. The re.findall function is then employed to search for all occurrences of this pattern, returning a list of matches. This list is subsequently returned as the output of the function.\n\nThe extract_citations function is called within two other functions in the project: process_section and parse_markdown_to_structure. In process_section, it is used to extract citations from paragraphs of text that are processed from a section of a document. Each paragraph is checked for citations, and any found are added to the corresponding section data structure. Similarly, in parse_markdown_to_structure, the function is called to extract citations from paragraphs collected while parsing markdown text into a structured format. This indicates that extract_citations plays a crucial role in ensuring that any citations present in the text are captured and organized appropriately within the document structure.\n\n**Note**: It is important to ensure that the input text contains properly formatted <cite> tags for the function to work effectively. If the tags are malformed or absent, the function will return an empty list.\n\n**Output Example**: An example of the function's return value could be: ['http://example.com/citation1', 'http://example.com/citation2'] if the input text contained the following: \"Here is a reference <cite>http://example.com/citation1</cite> and another one <cite>http://example.com/citation2</cite>.\""
      ],
      "code_start_line": 95,
      "code_end_line": 100,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def extract_citations(text):\n    \"\"\"从文本中提取引用的URLs\"\"\"\n    citations = []\n    pattern = r\"<cite>(.*?)<\\/cite>\"\n    matches = re.findall(pattern, text)\n    return matches\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/create_document_structure/process_section",
        "src/criticsearch/main.py/parse_markdown_to_structure",
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "create_document_structure",
      "md_content": [
        "**create_document_structure**: The function of create_document_structure is to create a structured document based on an outline and associated content.\n\n**parameters**: The parameters of this Function.\n· outline_json: A JSON object representing the outline structure of the document, including titles and hierarchical relationships.\n· flat_contents: A list of tuples, where each tuple contains an item (with a path) and its corresponding content as a string.\n\n**Code Description**: The create_document_structure function constructs a document structure by utilizing the provided outline and content. It initializes a document dictionary with a title and a level, setting up a place for subsections. The function then creates a mapping of content to paths derived from the flat_contents parameter, allowing for easy retrieval of content based on the section paths.\n\nThe core of the function is the nested process_section function, which recursively processes each section of the outline. It constructs a section_data dictionary that includes the title, level, and paragraphs for each section. If content exists for a given path, it splits the content into paragraphs based on double newlines, ignoring any empty paragraphs. Each paragraph is processed to extract citations, which are stored alongside the paragraph text.\n\nThe function also handles child sections by recursively calling process_section for each child, appending the resulting data to the current section's subsections. Finally, the function iterates through the root sections of the outline, processing each one and appending it to the main document structure before returning the complete document.\n\n**Note**: It is important to ensure that the outline_json is well-structured and that the flat_contents accurately corresponds to the paths defined in the outline. The function assumes that paragraphs are separated by double newlines and that citations can be extracted from the paragraphs.\n\n**Output Example**: A possible appearance of the code's return value could be as follows:\n{\n    \"document\": {\n        \"title\": \"Sample Document Title\",\n        \"level\": 1,\n        \"subsections\": [\n            {\n                \"title\": \"Introduction\",\n                \"level\": 2,\n                \"paragraphs\": [\n                    {\n                        \"text\": \"This is the first paragraph of the introduction.\",\n                        \"citations\": [\"Citation1\", \"Citation2\"]\n                    },\n                    {\n                        \"text\": \"This is the second paragraph of the introduction.\",\n                        \"citations\": []\n                    }\n                ],\n                \"subsections\": []\n            },\n            {\n                \"title\": \"Methodology\",\n                \"level\": 2,\n                \"paragraphs\": [],\n                \"subsections\": []\n            }\n        ]\n    }\n}"
      ],
      "code_start_line": 103,
      "code_end_line": 156,
      "params": [
        "outline_json",
        "flat_contents"
      ],
      "have_return": true,
      "code_content": "def create_document_structure(outline_json, flat_contents):\n    \"\"\"\n    基于outline结构和生成的内容创建文档结构\n    \"\"\"\n    document = {\n        \"document\": {\n            \"title\": outline_json.get(\"title\", \"\"),\n            \"level\": 1,\n            \"subsections\": [],\n        }\n    }\n\n    # 构建路径到内容的映射\n    content_map = {}\n    for item, content in flat_contents:\n        path_key = tuple(item[\"path\"])\n        content_map[path_key] = content\n\n    def process_section(section, depth=1, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n\n        section_data = {\"title\": section.get(\"title\"), \"level\": depth, \"paragraphs\": []}\n\n        # 如果有内容，处理段落\n        if path_key in content_map:\n            content = content_map[path_key]\n            paragraphs = content.split(\"\\n\\n\")  # 假设段落用空行分隔\n            for para in paragraphs:\n                if para.strip():  # 忽略空段落\n                    citations = extract_citations(para)\n                    paragraph_data = {\n                        \"text\": para.strip(),  # 保留原始文本，包括cite标记\n                        \"citations\": citations,\n                    }\n                    section_data[\"paragraphs\"].append(paragraph_data)\n\n        # 处理子节点\n        if \"children\" in section:\n            section_data[\"subsections\"] = []\n            for child in section[\"children\"]:\n                child_data = process_section(child, depth + 1, current_path)\n                section_data[\"subsections\"].append(child_data)\n\n        return section_data\n\n    # 处理根节点下的所有节点\n    for section in outline_json.get(\"children\", []):\n        doc_section = process_section(\n            section, 2, [outline_json.get(\"title\")] if \"title\" in outline_json else []\n        )\n        document[\"document\"][\"subsections\"].append(doc_section)\n\n    return document\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_section",
      "md_content": [
        "**process_section**: The function of process_section is to recursively process a section of a document, extracting its title, paragraphs, and any nested subsections.\n\n**parameters**: The parameters of this Function are as follows:\n· section: A dictionary representing a section of the document, which includes a title and potentially child sections.\n· depth: An integer indicating the current depth of the section in the document hierarchy, defaulting to 1.\n· path: A list that tracks the path of titles leading to the current section, defaulting to an empty list.\n\n**Code Description**: The process_section function begins by constructing the current path of titles by appending the title of the current section to the existing path. This path is then converted into a tuple, which serves as a key to access content from a predefined content_map. The function initializes a dictionary, section_data, to store the title, depth level, and an empty list for paragraphs.\n\nIf the current path key exists in the content_map, the function retrieves the associated content and splits it into paragraphs based on double newlines, assuming that paragraphs are separated by empty lines. Each paragraph is processed to extract citations using the extract_citations function, which identifies URLs enclosed within <cite> tags. Non-empty paragraphs are added to the section_data dictionary, along with their corresponding citations.\n\nThe function then checks if the current section has any child sections. If so, it initializes a list for subsections and recursively calls process_section for each child, incrementing the depth by one and passing the updated path. Each child's processed data is appended to the subsections list within section_data.\n\nFinally, the function returns the constructed section_data dictionary, which encapsulates the processed information of the section, including its title, depth, paragraphs, and any nested subsections.\n\nThis function is integral to the overall structure of the document processing system, as it organizes sections and their content into a structured format. It relies on the extract_citations function to ensure that citations are accurately captured and associated with their respective paragraphs.\n\n**Note**: It is essential that the content_map contains properly formatted entries corresponding to the section titles for the function to retrieve and process content effectively. If the section does not have associated content or if the structure of the input data is incorrect, the function may return incomplete or empty section data.\n\n**Output Example**: An example of the function's return value could be:\n{\n    \"title\": \"Introduction\",\n    \"level\": 1,\n    \"paragraphs\": [\n        {\n            \"text\": \"This is the first paragraph of the introduction.<cite>http://example.com/citation1</cite>\",\n            \"citations\": [\"http://example.com/citation1\"]\n        },\n        {\n            \"text\": \"This is the second paragraph of the introduction.\",\n            \"citations\": []\n        }\n    ],\n    \"subsections\": [\n        {\n            \"title\": \"Background\",\n            \"level\": 2,\n            \"paragraphs\": [],\n            \"subsections\": []\n        }\n    ]\n}"
      ],
      "code_start_line": 121,
      "code_end_line": 147,
      "params": [
        "section",
        "depth",
        "path"
      ],
      "have_return": true,
      "code_content": "    def process_section(section, depth=1, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n\n        section_data = {\"title\": section.get(\"title\"), \"level\": depth, \"paragraphs\": []}\n\n        # 如果有内容，处理段落\n        if path_key in content_map:\n            content = content_map[path_key]\n            paragraphs = content.split(\"\\n\\n\")  # 假设段落用空行分隔\n            for para in paragraphs:\n                if para.strip():  # 忽略空段落\n                    citations = extract_citations(para)\n                    paragraph_data = {\n                        \"text\": para.strip(),  # 保留原始文本，包括cite标记\n                        \"citations\": citations,\n                    }\n                    section_data[\"paragraphs\"].append(paragraph_data)\n\n        # 处理子节点\n        if \"children\" in section:\n            section_data[\"subsections\"] = []\n            for child in section[\"children\"]:\n                child_data = process_section(child, depth + 1, current_path)\n                section_data[\"subsections\"].append(child_data)\n\n        return section_data\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/main.py/extract_citations"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "parse_markdown_to_structure",
      "md_content": [
        "## Function Documentation: `parse_markdown_to_structure`\n\n### Overview\n\nThe `parse_markdown_to_structure` function is responsible for parsing a given markdown text and converting it into a structured document format. It organizes the markdown content into a hierarchical document structure, which includes titles, subsections, and paragraphs. Citations within paragraphs are also extracted and included in the structure.\n\n### Parameters\n\n- **markdown_text** (str): The input markdown text to be parsed. This should be a string containing the raw markdown content.\n\n### Returns\n\n- **document** (dict): A dictionary representing the parsed document structure. It contains the following keys:\n  - **document**: The root level of the document, which includes:\n    - **title**: The title of the document (empty string initially).\n    - **level**: The level of the document (1 initially).\n    - **subsections**: A list of subsections at this level.\n  - **subsections**: A list of subsections for each heading found in the markdown text, with each subsection containing:\n    - **title**: The title of the subsection.\n    - **level**: The level of the subsection, corresponding to the number of `#` symbols in the markdown heading.\n    - **paragraphs**: A list of paragraphs under the subsection.\n    - **subsections**: Subsections under the current section.\n\n### Description\n\nThe function performs the following tasks:\n\n1. **Initial Setup**: \n   - It splits the input `markdown_text` into lines and initializes a dictionary (`document`) with a structure containing a title, level, and subsections.\n   - It sets up a `section_stack` to keep track of the current section as the parsing progresses.\n\n2. **Processing Lines**: \n   - The function iterates through each line of the markdown text:\n     - **Handling Headings**: \n       - When a line starts with `#`, the function processes it as a heading. The number of `#` symbols in the heading determines the level of the subsection.\n       - If there is any unprocessed paragraph text before encountering a heading, it is added as a paragraph under the current section.\n       - The function adjusts the `section_stack` based on the heading level to ensure that subsections are nested correctly.\n     - **Handling Paragraphs**: \n       - For lines that are not headings, the function accumulates them as paragraph text until a blank line is encountered or a new heading is found. When a paragraph is fully processed, the function extracts any citations within it and adds the paragraph to the current section.\n   \n3. **Finalizing Paragraphs**: \n   - After processing all lines, any remaining paragraph text is finalized and added to the document structure.\n\n4. **Citation Extraction**:\n   - Throughout the parsing process, the `extract_citations` function is called to extract any citations (URLs within `<cite>` tags) from the paragraph text.\n\n### Example\n\n#### Input:\n```markdown\n# Main Title\n\nThis is an introduction paragraph with a citation <cite>http://example.com/citation1</cite>.\n\n## Subsection 1\n\nThis is a paragraph under subsection 1 with another citation <cite>http://example.com/citation2</cite>.\n\n### Subsubsection 1.1\n\nText in the subsubsection.\n```\n\n#### Output:\n```python\n{\n  \"document\": {\n    \"title\": \"\",\n    \"level\": 1,\n    \"subsections\": [\n      {\n        \"title\": \"Main Title\",\n        \"level\": 1,\n        \"subsections\": [\n          {\n            \"title\": \"Subsection 1\",\n            \"level\": 2,\n            \"subsections\": [\n              {\n                \"title\": \"Subsubsection 1.1\",\n                \"level\": 3,\n                \"subsections\": [],\n                \"paragraphs\": [\n                  {\"text\": \"Text in the subsubsection.\", \"citations\": []}\n                ]\n              }\n            ],\n            \"paragraphs\": [\n              {\"text\": \"This is a paragraph under subsection 1 with another citation http://example.com/citation2.\", \"citations\": [\"http://example.com/citation2\"]}\n            ]\n          }\n        ],\n        \"paragraphs\": [\n          {\"text\": \"This is an introduction paragraph with a citation http://example.com/citation1.\", \"citations\": [\"http://example.com/citation1\"]}\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Notes\n\n- The function relies on the proper formatting of markdown headings (e.g., using `#` for titles) and paragraphs (text separated by blank lines).\n- Citations are identified by the `extract_citations` function, which looks for URLs enclosed within `<cite>` tags in the text.\n- The document structure is hierarchical, allowing for nested subsections and paragraphs."
      ],
      "code_start_line": 159,
      "code_end_line": 225,
      "params": [
        "markdown_text"
      ],
      "have_return": true,
      "code_content": "def parse_markdown_to_structure(markdown_text):\n    \"\"\"从markdown文本解析出文档结构\"\"\"\n    lines = markdown_text.split(\"\\n\")\n    document = {\"document\": {\"title\": \"\", \"level\": 1, \"subsections\": []}}\n\n    current_section = document[\"document\"]\n    section_stack = [current_section]\n    current_level = 1\n    current_text = []\n\n    for line in lines:\n        if line.strip():\n            # 处理标题\n            if line.startswith(\"#\"):\n                # 如果有待处理的段落文本，先处理完\n                if current_text:\n                    paragraph_text = \" \".join(current_text)\n                    if paragraph_text.strip():\n                        citations = extract_citations(paragraph_text)\n                        current_section.setdefault(\"paragraphs\", []).append(\n                            {\"text\": paragraph_text.strip(), \"citations\": citations}\n                        )\n                    current_text = []\n\n                # 处理新标题\n                level = len(line.split()[0])  # 计算#的数量\n                title = \" \".join(line.split()[1:])\n\n                # 根据层级调整当前section\n                while len(section_stack) > 1 and level <= section_stack[-1][\"level\"]:\n                    section_stack.pop()\n\n                new_section = {\n                    \"title\": title,\n                    \"level\": level,\n                    \"subsections\": [],\n                    \"paragraphs\": [],\n                }\n\n                section_stack[-1].setdefault(\"subsections\", []).append(new_section)\n                section_stack.append(new_section)\n                current_section = new_section\n\n            else:\n                # 收集段落文本\n                current_text.append(line)\n        else:\n            # 空行，处理当前段落\n            if current_text:\n                paragraph_text = \" \".join(current_text)\n                if paragraph_text.strip():\n                    citations = extract_citations(paragraph_text)\n                    current_section.setdefault(\"paragraphs\", []).append(\n                        {\"text\": paragraph_text.strip(), \"citations\": citations}\n                    )\n                current_text = []\n\n    # 处理最后一个段落\n    if current_text:\n        paragraph_text = \" \".join(current_text)\n        if paragraph_text.strip():\n            citations = extract_citations(paragraph_text)\n            current_section.setdefault(\"paragraphs\", []).append(\n                {\"text\": paragraph_text.strip(), \"citations\": citations}\n            )\n\n    return document\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [
        "src/criticsearch/main.py/extract_citations"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "process_single_task",
      "md_content": [
        "**process_single_task**: The function of process_single_task is to execute a single task by interacting with various agents to generate a comprehensive report based on the provided task and specified maximum iterations.\n\n**parameters**: The parameters of this Function.\n· task: A string representing the task or question that the agent needs to process.\n· max_iterations: An integer indicating the maximum number of iterations to perform while processing the task.\n\n**Code Description**: The process_single_task function is designed to manage the execution of a specific task by coordinating interactions between multiple agents, including a common agent, a search aggregator, and a verifier. The function begins by initializing necessary components such as the common agent, search aggregator, and report verifier. It then loads a JSON file containing benchmark data, which is essential for generating the report.\n\nThe function processes the task iteratively, up to the specified maximum number of iterations. In each iteration, it assesses the confidence of the agent in its response and determines whether to generate a direct answer or to conduct a guided search based on the outline of the report. If the agent is confident, it provides a direct response; otherwise, it engages in a guided search to gather relevant information.\n\nDuring the guided search, the function extracts thought processes and queries from the agent's responses, which are then used to perform web searches and scrape results. The generated content for each section of the report is constructed in parallel using a thread pool, allowing for efficient processing of multiple sections simultaneously. The function also incorporates a verification step, where the generated answers are checked against extracted facts to ensure accuracy.\n\nThe process_single_task function is called by the execute_multiple_tasks function, which handles the execution of multiple tasks in a loop. This integration allows for the systematic processing of each task, logging the conversation history for later review.\n\n**Note**: It is important to ensure that the input task is well-defined and that the maximum iterations parameter is set appropriately to balance between thoroughness and efficiency. The function relies on the proper functioning of the agents and the availability of the JSON benchmark file to operate effectively.\n\n**Output Example**: A possible return value from the process_single_task function could be a string summarizing the final report generated, such as:\n```\n\"Based on the gathered information, the report concludes that the Syrian opposition has faced significant challenges in recent offensives, with various factions emerging and evolving over time.\"\n```"
      ],
      "code_start_line": 228,
      "code_end_line": 556,
      "params": [
        "task",
        "max_iterations"
      ],
      "have_return": true,
      "code_content": "def process_single_task(task, max_iterations):\n    # Initialize agents\n    common_agent = BaseAgent()\n    search_agg = SearchAggregator()\n    verifier = ReportVerifier(common_agent)\n\n    package_name = \"criticsearch.reportbench.wiki_data\"\n    file_name = \"2024_Syrian_opposition_offensives.json\"\n\n    with importlib.resources.files(package_name).joinpath(file_name) as json_file_path:\n        json_file = str(json_file_path)\n\n    benchmark = ReportBenchmark(json_file)\n    outline = benchmark.generate_benchmark_item(max_window_tokens=200)\n\n    conversation_data = [{\"from\": \"huamn\", \"value\": task}]\n\n    # initialize the task\n    common_agent.user_question = task\n\n    printer.log(f\"Starting the conversation with task: \\n{task}\")\n\n    BaseAgent.conversation_manager.append_to_history(role=\"user\", content=task)\n\n    for iteration in range(max_iterations):\n        printer.rule(f\"ITERATION {iteration + 1}\")\n\n        if iteration == 0:\n            # Initialize search_results as None\n            search_results = None\n\n            # Model confidence check - yellow\n            agent_confident = common_agent.chat_with_template(\n                \"agent_confidence.txt\", {\"user_question\": task}\n            )\n            agent_confident_yaml = common_agent.extract_and_validate_yaml(\n                agent_confident\n            )\n\n            if agent_confident_yaml is None:\n                printer.log(\n                    \"Failed to extract valid YAML content. Defaulting to 'false'.\"\n                )\n                agent_confident = False\n            else:\n                agent_confident_dict = yaml.safe_load(agent_confident_yaml)\n                agent_confident = (\n                    agent_confident_dict.get(\"confidence\", \"true\").lower() == \"true\"\n                )\n\n            if agent_confident:\n                # 使用chat_with_template替换direct chat\n                common_agent_answer = common_agent.chat_with_template(\n                    \"direct_response.txt\", {\"task\": task}\n                )\n            else:\n                agent_report_sections = []\n\n                for item in outline:\n                    section_path = item[\"path\"]\n                    merged_section_content = item[\"merged_section_window_content\"]\n                    extracted_facts = item[\"extracted_facts\"]\n\n                    angent_report = \"\\n\".join(agent_report_sections)\n\n                    search_thought_and_queries = common_agent.chat_with_template(\n                        \"guided_search_thought.txt\",\n                        {\n                            \"task\": task,\n                            \"context\": \"There is no previous context since this is the first section at the beginning of the article, only the user's question exists\"\n                            if iteration == 0\n                            else f\"Previous text written by the Agent is:\\n\\n{angent_report}\",\n                            \"GroundTruth\": merged_section_content,\n                        },\n                    )\n\n                    # 提取thought内容\n                    thought_content = extract_thought_from_response(\n                        search_thought_and_queries\n                    )\n                    if thought_content:\n                        printer.print(f\"Extracted thought process: {thought_content}\")\n                    else:\n                        printer.print(\n                            \"No thought process found in the response\", style=\"yellow\"\n                        )\n\n                    # 提取查询列表,最多尝试5次\n                    max_retries = 5\n                    retry_count = 0\n                    queries_list = None\n\n                    while retry_count < max_retries:\n                        queries_list = extract_queries_from_response(\n                            search_thought_and_queries\n                        )\n                        if (\n                            queries_list\n                            and isinstance(queries_list, list)\n                            and len(queries_list) > 0\n                        ):\n                            printer.print(f\"Extracted queries: {queries_list}\")\n                            conversation_data.append(\n                                {\n                                    \"from\": \"agent\",\n                                    \"thougth\": thought_content,\n                                    \"search\": queries_list,\n                                }\n                            )\n                            break\n\n                        printer.log(\n                            f\"Attempt {retry_count + 1}: Invalid or empty queries, retrying...\",\n                            style=\"yellow\",\n                        )\n                        printer.print(search_thought_and_queries)\n                        # 重新生成��索思维和查询\n                        search_thought_and_queries = common_agent.chat_with_template(\n                            \"guided_search_thought.txt\",\n                            {\n                                \"task\": task,\n                                \"context\": \"There is no previous context since this is the first section at the beginning of the article, only the user's question exists\"\n                                if iteration == 0\n                                else f\"Previous text written by the Agent is:\\n\\n{angent_report}\",\n                                \"GroundTruth\": merged_section_content,\n                            },\n                        )\n                        retry_count += 1\n\n                    if retry_count == max_retries:\n                        printer.print(\n                            \"Failed to extract valid queries after maximum retries\",\n                            style=\"red, bold\",\n                        )\n                        continue  # 跳过当���section,处理下一个\n\n                    printer.print(search_thought_and_queries)\n                    search_results = asyncio.run(search_agg.search(queries_list))\n                    detailed_web_results = common_agent.web_scrape_results(\n                        search_results\n                    )\n\n                    # 生成section内容\n                    section_content = common_agent.chat_with_template(\n                        \"guided_generation_thought.txt\",\n                        {\n                            \"task\": task,\n                            \"context\": \"There is no previous context since this is the first section at the beginning of the article, only the user's question exists\"\n                            if iteration == 0\n                            else f\"Previous report content written by you is:\\n\\n{angent_report}\",\n                            \"guidline\": section_path,\n                            \"search_result\": detailed_web_results,\n                        },\n                    )\n\n                    printer.rule(\"Section Content\")\n                    printer.print(section_content)\n\n                    # 分别提取thought和answer，citation\n                    thought_content = extract_thought_from_response(section_content)\n                    answer_content = extract_answer_from_response(section_content)\n                    citations = extract_citations(answer_content)\n\n                    # 将生成的内容添加到agent_report_sections\n                    agent_report_sections.append(answer_content)\n\n                    conversation_data.append(\n                        {\n                            \"from\": \"agent\",\n                            \"thougth\": thought_content,\n                            \"answer\": answer_content,\n                            \"citations\": citations,\n                        }\n                    )\n\n                    # 使用verifier进行factual QA验证\n                    accuracy = verifier.verify_section(answer_content, extracted_facts)\n                    conversation_data.append(\n                        {\n                            \"from\": \"verifier\",\n                            \"accuracy\": accuracy,\n                            \"section\": section_path,\n                        }\n                    )\n\n                    # 保存到一个json文件\n                    with open(\"conversation_data.json\", \"w\", encoding=\"utf-8\") as f:\n                        json.dump(conversation_data, f, indent=4, ensure_ascii=False)\n\n                exit(1)\n\n                # verify the outline is a json string\n                outline_json = common_agent.extract_and_validate_json(outline)\n\n                # Flatten the outline to get all sections at all levels\n                flat_sections = flatten_outline(outline_json)\n\n                # Generate content for all sections in parallel\n                with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n                    future_to_section = {\n                        executor.submit(\n                            generate_content_for_section,\n                            common_agent,\n                            item[\"section\"],\n                            task,\n                        ): item\n                        for item in flat_sections\n                    }\n\n                    # Collect results as they complete\n                    flat_contents = []\n                    for future in concurrent.futures.as_completed(future_to_section):\n                        item = future_to_section[future]\n                        try:\n                            content = future.result()\n                            flat_contents.append((item, content))\n                        except Exception:\n                            printer.print_exception(\n                                f\"Error generating content for section {item['section'].get('title')}\"\n                            )\n\n                # Reconstruct the markdown with proper hierarchy\n                common_agent_answer = reconstruct_markdown(outline_json, flat_contents)\n                printer.log(\n                    common_agent_answer\n                )  # 这里是第一次生成的report,还没有polish\n\n                # 用deepseek润色一下得到正式的version1 report\n                polish_prompt = common_agent.load_template(\"polish_first_version.txt\")\n                polish_rendered_prompt = common_agent.render_template(\n                    polish_prompt,\n                    {\n                        \"task\": task,\n                        \"report\": common_agent_answer,\n                    },\n                )\n                common_agent_answer = common_agent.chat_with_template(\n                    \"polish_first_version.txt\",\n                    {\n                        \"task\": task,\n                        \"report\": common_agent_answer,\n                    },\n                    model=\"gpt-4o\",\n                )\n                printer.log(\n                    common_agent_answer\n                )  # 这里是第一次生成的正式的polished report\n                # 保存到一个md\n                with open(\"first_version_report.md\", \"w\") as f:\n                    f.write(common_agent_answer)\n\n                # Polish后从markdown解析出文档结构\n                document_structure = parse_markdown_to_structure(common_agent_answer)\n                common_agent.document_structure = document_structure\n\n                # 保存到一个json文件\n                with open(\"document_structure.json\", \"w\") as f:\n                    json.dump(document_structure, f, indent=4, ensure_ascii=False)\n\n        else:\n            # 替换update answer调用\n            common_agent_answer = common_agent.chat_with_template(\n                \"agent_update_answer.txt\",\n                {\n                    \"query\": task,\n                    \"previous_answer\": common_agent_answer,\n                    \"search_results\": web_result_markdown_text,\n                    \"critic_feedback\": critic_agent_response,\n                },\n            )\n\n        # ========================== #\n        ## 这里在if-else结构之外 ##\n        printer.rule(f\"COMMON AGENT ANSWER\")\n        printer.log(common_agent_answer)\n\n        critic_agent = CriticAgent()\n        critic_agent.receive_task(task)\n        critic_agent.receive_agent_answer(common_agent_answer)\n        critic_agent_response = critic_agent.chat_with_template(\n            \"critic_evaluation.txt\", {\"task\": task, \"answer\": common_agent_answer}\n        )\n        printer.rule(f\"CRITIC_AGENT_RESPONSE\")\n        printer.log(critic_agent_response)\n\n        if yaml.safe_load(critic_agent_response).get(\"Stop\", {}).lower() == \"true\":\n            printer.rule(f\"[red]TOTAL ITERATIONS: {iteration + 1}\")\n            printer.rule(f\"ALL SEARCH QUERIES\")\n            printer.log(common_agent.queryDB)\n            printer.rule(f\"[red]FINAL ANSWER\")\n            printer.log(common_agent_answer)\n\n            return f\"\\n{common_agent_answer}\\n\"\n\n        # 根据critic的建议再执行一次搜索和爬虫操作\n        # 先构建rendered_prompt\n        reflection_data = {\n            \"user_question\": task,\n            \"previous_answer\": common_agent_answer,\n            \"user_feedback\": critic_agent_response,\n            \"search_history\": common_agent.queryDB,\n        }\n        search_again_prompt = common_agent.render_template(\n            common_agent.load_template(\"planner_agent_with_reflection.txt\"),\n            reflection_data,\n        )\n        try:\n            web_result_markdown_text = common_agent.search_and_browse(\n                search_again_prompt\n            )\n        except:\n            printer.rule(f\"[red]TOTAL ITERATIONS: {iteration + 1}\")\n            printer.rule(f\"ALL SEARCH QUERIES\")\n            printer.log(common_agent.queryDB)\n            printer.rule(f\"[red]FINAL ANSWER\")\n            printer.log(common_agent_answer)\n\n            # we run out of searches for now, so we force the agent to give a final answer:\n            return f\"\\n{common_agent_answer}\\n\"\n\n        # Check if reached max iterations\n        if iteration == max_iterations - 1:\n            printer.rule(f\"[red]TOTAL ITERATIONS: {iteration + 1}\")\n            printer.rule(f\"ALL SEARCH QUERIES\")\n            printer.log(common_agent.queryDB)\n            printer.rule(f\"[red]FINAL ANSWER\")\n            printer.log(common_agent_answer)\n\n            return f\"\\n{common_agent_answer}\\n\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tasks_runner.py",
        "src/criticsearch/tasks_runner.py/execute_multiple_tasks"
      ],
      "reference_who": [
        "src/criticsearch/main.py/flatten_outline",
        "src/criticsearch/main.py/generate_content_for_section",
        "src/criticsearch/main.py/reconstruct_markdown",
        "src/criticsearch/main.py/extract_citations",
        "src/criticsearch/main.py/parse_markdown_to_structure",
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/base_agent.py/BaseAgent/receive_task",
        "src/criticsearch/base_agent.py/BaseAgent/extract_and_validate_yaml",
        "src/criticsearch/base_agent.py/BaseAgent/extract_and_validate_json",
        "src/criticsearch/models.py/ConversationManager/append_to_history",
        "src/criticsearch/rich_output.py/RichPrinter/rule",
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/rich_output.py/RichPrinter/print",
        "src/criticsearch/rich_output.py/RichPrinter/print_exception",
        "src/criticsearch/utils.py/extract_queries_from_response",
        "src/criticsearch/utils.py/extract_thought_from_response",
        "src/criticsearch/utils.py/extract_answer_from_response",
        "src/criticsearch/critic_agent.py/CriticAgent",
        "src/criticsearch/critic_agent.py/CriticAgent/receive_agent_answer",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_benchmark_item",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/verify_section"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/main_old_version.py": [
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: The function of main is to orchestrate the conversation process between a user and an intelligent agent, managing iterations of task handling, confidence evaluation, information retrieval, and response refinement.\n\n**parameters**: The parameters of this Function.\n· TASK: A string representing the task or question posed by the user that the agent needs to address.\n· MAX_ITERATION: An integer specifying the maximum number of iterations the agent should perform while refining its response.\n\n**Code Description**: The main function serves as the entry point for processing user tasks within the intelligent agent framework. It begins by initializing a BaseAgent instance, which is responsible for managing the conversation and handling user queries. The user question is assigned to the agent's `user_question` attribute, and the logging level is set based on the configuration.\n\nThe function logs the start of the conversation and appends the initial user question to the conversation history. It then enters a loop that iterates up to the specified `MAX_ITERATION`. In each iteration, the function performs several critical tasks:\n\n1. **Confidence Evaluation**: In the first iteration, the agent checks its confidence in answering the user's question by invoking the `model_confident` method. This method assesses whether the agent is confident enough to provide a direct answer or if it needs to gather more information.\n\n2. **Information Retrieval**: If the agent is not confident, it constructs a search prompt using a template and performs a search operation through the `search_and_browse` method. This method interacts with a search aggregator to retrieve relevant information from the web, which is then used to formulate a more informed response.\n\n3. **Response Generation**: The agent generates a response based on the gathered information or directly if it is confident. This is done through the `common_chat` method, which communicates with the language model to produce a response.\n\n4. **Critique and Refinement**: After generating a response, the function instantiates a CriticAgent, which evaluates the agent's answer. The critique is processed, and if the feedback indicates that the conversation should stop (as determined by the YAML response), the function logs the total iterations and the final answer before returning it.\n\n5. **Iterative Improvement**: If the critique suggests further refinement, the agent constructs a new search prompt based on the feedback and previous answers, repeating the search and response generation process until the maximum iterations are reached or a stopping condition is met.\n\nThroughout this process, the function utilizes various methods from the BaseAgent class, including `load_template`, `render_template`, and `update_answer`, to manage the conversation flow and enhance the quality of responses based on user feedback and search results.\n\n**Note**: It is crucial to ensure that the TASK parameter is well-defined and relevant to the agent's capabilities. The MAX_ITERATION parameter should be set appropriately to balance between thoroughness and efficiency in response generation.\n\n**Output Example**: A possible return value from the main function could be a string such as:\n```\n\"The capital of France is Paris, based on the latest information gathered from reliable sources.\"\n```"
      ],
      "code_start_line": 11,
      "code_end_line": 185,
      "params": [
        "TASK",
        "MAX_ITERATION"
      ],
      "have_return": true,
      "code_content": "def main(TASK, MAX_ITERATION):\n    # Initialize agents\n    common_agent = BaseAgent()\n\n    # initialize the task\n    common_agent.user_question = TASK\n\n    set_logger_level_from_config(log_level=settings.log_level.upper())\n\n    logger.success(f\"Starting the conversation with task: {TASK}\")\n\n    BaseAgent.conversation_manager.append_to_history(role=\"user\", content=TASK)\n\n    for iteration in range(MAX_ITERATION):\n        colorize_message(\n            message_title=f\"ITERATION {iteration + 1}\", color=\"cyan\", style=\"bold\"\n        )\n\n        if iteration == 0:\n            # Initialize search_results as None\n            search_results = None\n\n            # Model confidence check - yellow\n            agent_confident = common_agent.model_confident(TASK)\n            agent_confident_yaml = common_agent.extract_and_validate_yaml(\n                agent_confident\n            )\n\n            if agent_confident_yaml is None:\n                logger.warning(\n                    \"Failed to extract valid YAML content. Defaulting to 'false'.\"\n                )\n                agent_confident = False\n            else:\n                agent_confident_dict = yaml.safe_load(agent_confident_yaml)\n                agent_confident = (\n                    agent_confident_dict.get(\"confidence\", \"true\").lower() == \"true\"\n                )\n\n            if agent_confident:\n                # When confident, only get the answer\n                common_agent_answer = common_agent.common_chat(usr_prompt=TASK)\n            else:\n                # When not confident, start searching information\n                data = {\n                    \"user_question\": TASK,\n                }\n                initial_search_prompt = common_agent.load_template(\n                    \"planner_agent_initial_search_plan.txt\"\n                )\n                initial_search_rendered_prompt = common_agent.render_template(\n                    initial_search_prompt, data\n                )\n                logger.info(\n                    f\"initial_search_rendered_prompt: {initial_search_rendered_prompt}\"\n                )\n\n                initial_web_result_markdown_text = common_agent.search_and_browse(\n                    initial_search_rendered_prompt\n                )\n                logger.info(f\"Initial web result: {initial_web_result_markdown_text}\")\n\n                rag_based_answer_prompt = common_agent.render_template(\n                    common_agent.load_template(\"rag_based_answer.txt\"),\n                    {\n                        \"user_question\": common_agent.user_question,\n                        \"web_result_markdown_text\": initial_web_result_markdown_text,\n                    },\n                )\n\n                common_agent_answer = common_agent.common_chat(\n                    usr_prompt=rag_based_answer_prompt,\n                )\n\n        else:\n            # 前面根据critc的返回得到了新的网页搜索结果web_result_markdown_text\n            common_agent_answer = common_agent.update_answer(\n                query=TASK,\n                previous_answer=common_agent_answer,\n                search_results=web_result_markdown_text,\n                critic_feedback=critic_agent_response,\n            )\n            time.sleep(0.5)  # hitting rate limits for gpt mini\n\n        colorize_message(\n            message_title=\"COMMON AGENT ANSWER\",\n            color=\"magenta\",\n            message_content=common_agent_answer,\n        )\n\n        # Critic evaluation - blue\n        critic_agent = CriticAgent()\n        critic_agent.receive_task(TASK)\n        critic_agent.receive_agent_answer(common_agent_answer)\n        critic_agent_response = critic_agent.critic()\n\n        colorize_message(\n            message_title=\"CRITIC_AGENT_RESPONSE\",\n            color=\"blue\",\n            message_content=critic_agent_response,\n        )\n\n        if yaml.safe_load(critic_agent_response).get(\"Stop\", {}).lower() == \"true\":\n            colorize_message(\n                message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n            )\n\n            colorize_message(\n                message_title=\"ALL SEARCH QUERIES\",\n                color=\"black\",\n                message_content=\", \".join(map(str, common_agent.queryDB)),\n            )\n            colorize_message(\n                message_title=\"FINAL ANSWER\",\n                color=\"red\",\n                message_content=common_agent_answer,\n            )\n\n            return f\"\\n{common_agent_answer}\\n\"\n\n        # 根据critic的建议再执行一次搜索和爬虫操作\n        # 先构建rendered_prompt\n        reflection_data = {\n            \"user_question\": TASK,\n            \"previous_answer\": common_agent_answer,\n            \"user_feedback\": critic_agent_response,\n            \"search_history\": common_agent.queryDB,\n        }\n        search_again_prompt = common_agent.render_template(\n            common_agent.load_template(\"planner_agent_with_reflection.txt\"),\n            reflection_data,\n        )\n        try:\n            web_result_markdown_text = common_agent.search_and_browse(\n                search_again_prompt\n            )\n        except:\n            colorize_message(\n                message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n            )\n\n            colorize_message(\n                message_title=\"ALL SEARCH QUERIES\",\n                color=\"black\",\n                message_content=\", \".join(map(str, common_agent.queryDB)),\n            )\n\n            colorize_message(\n                message_title=\"FINAL ANSWER\",\n                color=\"red\",\n                message_content=common_agent_answer,\n            )\n\n            # we run out of searches for now, so we force the agent to give a final answer:\n            return f\"\\n{common_agent_answer}\\n\"\n\n        # Check if reached max iterations\n        if iteration == MAX_ITERATION - 1:\n            colorize_message(\n                message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n            )\n\n            colorize_message(\n                message_title=\"ALL SEARCH QUERIES\",\n                color=\"black\",\n                message_content=\", \".join(map(str, common_agent.queryDB)),\n            )\n\n            colorize_message(\n                message_title=\"FINAL ANSWER\",\n                color=\"red\",\n                message_content=common_agent_answer,\n            )\n\n            return f\"\\n{common_agent_answer}\\n\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/base_agent.py/BaseAgent/receive_task",
        "src/criticsearch/base_agent.py/BaseAgent/extract_and_validate_yaml",
        "src/criticsearch/models.py/ConversationManager/append_to_history",
        "src/criticsearch/critic_agent.py/CriticAgent",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/critic_agent.py/CriticAgent/receive_agent_answer"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/base_agent.py": [
    {
      "type": "ClassDef",
      "name": "BaseAgent",
      "md_content": [
        "**BaseAgent**: The function of BaseAgent is to serve as a foundational class for implementing an intelligent agent that can manage conversations, perform searches, and interact with various tools.\n\n**attributes**: The attributes of this Class.\n· queryDB: A set to store unique queries made by the agent during its operation.  \n· tool_registry: An instance of ToolRegistry that manages the schemas for tools used by the agent.  \n· user_question: A string that holds the current question posed by the user.  \n· conversation_manager: An instance of ConversationManager responsible for maintaining the history of the conversation.  \n· prompts_dir: A string that specifies the directory path where template prompt files are stored.  \n· citationDB: A list of dictionaries that contains search queries and their corresponding results, specifically those praised by a critic.  \n· search_aggregator: An instance of SearchAggregator that facilitates search operations.  \n· search_aggregator_schema: A schema representation of the search aggregator tool, retrieved from the tool registry.  \n· content_scraper: An instance of ContentScraper that handles web scraping tasks.  \n· content_scraper_schema: A schema representation of the content scraper tool, retrieved from the tool registry.  \n· repeat_turns: An integer that defines the number of times the agent will repeat its search and interaction process.\n\n**Code Description**: The BaseAgent class is designed to provide a structured framework for building intelligent agents that can engage in conversations, perform searches, and utilize various tools effectively. Upon initialization, the class sets up several key components, including the conversation manager, tool registry, and search aggregator. \n\nThe conversation manager is responsible for tracking the history of interactions, while the tool registry allows the agent to manage and retrieve schemas for different tools it may use. The citationDB is specifically designed to store search results that have received positive feedback from a critic, ensuring that the agent can reference high-quality information.\n\nThe class provides several methods for loading templates, rendering them with data, and managing conversations. The `common_chat` method is overloaded to handle different types of user prompts, allowing for flexible interactions. The `update_answer` method enables the agent to refine its responses based on previous answers and feedback from critics.\n\nAdditionally, the `search_and_browse` method integrates both search and web scraping functionalities, allowing the agent to gather information from various sources and present it to the user. The `model_confident` method checks the agent's confidence in its responses, guiding its decision-making process on whether to provide an answer or seek additional information.\n\nThe BaseAgent class is utilized in various parts of the project, including the CriticAgent, which extends its functionality to generate critiques based on the agent's responses. The main function in the project initializes an instance of BaseAgent to handle user tasks, demonstrating its role as a central component in the overall architecture.\n\n**Note**: It is essential to ensure that the tool registry is populated with the necessary schemas for the tools being used, as the agent relies on these schemas for proper functionality. Additionally, the citationDB should be managed carefully to maintain the quality of information referenced by the agent.\n\n**Output Example**: A possible appearance of the code's return value when performing a search might look like this:\n```json\n{\n  \"search_results\": [\n    {\n      \"document_id\": \"12345\",\n      \"url\": \"https://example.com/article\",\n      \"title\": \"Understanding the Challenges Faced by Google in 2019\",\n      \"content\": \"In 2019, Google faced several challenges including...\"\n    }\n  ],\n  \"conversation_history\": [\n    {\"role\": \"user\", \"content\": \"What challenges did Google face in 2019?\"},\n    {\"role\": \"assistant\", \"content\": \"Google faced several challenges including...\"}\n  ]\n}\n```",
        "**BaseAgent**: The function of BaseAgent is to serve as a foundational class for intelligent agents, providing essential functionalities for managing conversations, handling templates, and integrating various tools for search and content scraping.\n\n**attributes**: The attributes of this Class.\n· queryDB: A set to store queries made by the agent, ensuring uniqueness and facilitating tracking of search queries.  \n· tool_registry: An instance of ToolRegistry that manages the schemas for tools used by the agent.  \n· user_question: A string that holds the current question posed by the user.  \n· conversation_manager: An instance of ConversationManager responsible for managing the conversation history.  \n· citationDB: A list of dictionaries that stores search queries and their corresponding results, specifically those praised by critics.  \n· search_aggregator: An instance of SearchAggregator that aggregates search results from various sources.  \n· search_aggregator_schema: A schema for the search aggregator tool, retrieved or created from the tool registry.  \n· content_scraper: An instance of ContentScraper that extracts content from web pages.  \n· content_scraper_schema: A schema for the content scraper tool, retrieved or created from the tool registry.  \n· repeat_turns: An integer that indicates the number of times the agent should repeat its search or response process.\n\n**Code Description**: The BaseAgent class is designed to facilitate the operation of intelligent agents by providing a structured approach to managing user interactions and integrating various tools for enhanced functionality. Upon initialization, the class sets up its environment by determining the directory of the current script and establishing paths for template files. It initializes several key components, including a conversation manager to track the history of interactions, a tool registry to manage schemas for tools, and a citation database to store relevant search results.\n\nThe class includes methods for loading and rendering templates, which are essential for generating dynamic responses based on user input. The `chat_with_template` and `chat_with_tools` methods allow the agent to interact with users and tools by rendering prompts and managing the conversation flow. The `common_chat` method serves as a core function that handles communication with the language model, allowing for flexible interactions based on user prompts and available tools.\n\nAdditionally, the BaseAgent class provides methods for updating answers based on user feedback, checking the model's confidence in its responses, and scraping web content from search results. The `search_and_browse` method integrates search functionality with web scraping, enabling the agent to gather and process information effectively.\n\nThe BaseAgent is utilized by other classes, such as CriticAgent, which extends its capabilities to focus on generating critiques of responses. The CriticAgent leverages the functionalities provided by BaseAgent to manage conversations and interact with the language model, demonstrating the foundational role of BaseAgent in the overall architecture of the project.\n\n**Note**: It is important to ensure that the templates used for rendering responses are correctly formatted and accessible, as the BaseAgent relies on these templates for generating dynamic content. Additionally, proper management of the conversation history and tool schemas is crucial for maintaining the integrity and efficiency of the agent's operations.\n\n**Output Example**: A possible appearance of the code's return value when rendering a template might look like this:\n```json\n{\n  \"response\": \"Based on the search results, Google faced several challenges in 2019, including increased competition and regulatory scrutiny.\"\n}\n```"
      ],
      "code_start_line": 20,
      "code_end_line": 313,
      "params": [],
      "have_return": true,
      "code_content": "class BaseAgent:\n    # Class-level attributes, shared across all instances\n    queryDB = set()  # A set to store queries\n    tool_registry = ToolRegistry()  # Registry for tools\n    user_question = \"\"\n    conversation_manager = ConversationManager()\n\n    def __init__(self):\n        base_dir = os.path.dirname(\n            os.path.abspath(__file__)\n        )  # Directory of the current script\n        self.prompts_dir = os.path.join(base_dir, \"prompts\")\n        # self.env = Environment(loader=FileSystemLoader(self.prompts_dir))\n\n        # 对于citationDB,应该是一个字典，key是query，value是内容和来源\n        # 这个列表中的每个元素都是一个字典，代表一个搜索的问题以及对应的搜索结果\n        self.citationDB = [\n            {  # citationDB中只会把受到critic表扬的搜索结果加入\n                \"why do we say google was facing challenges in 2019?\": {\n                    \"document_id\": {  # 这个document_id是一个唯一的标识符，用于标识这个文档\n                        \"url\": \"\",\n                        \"title\": \"\",\n                        \"content\": \"\",\n                    }\n                }\n            }\n        ]\n        self.search_aggregator = SearchAggregator()\n\n        self.search_aggregator_schema = (\n            BaseAgent.tool_registry.get_or_create_tool_schema(\n                self.search_aggregator.search\n            )\n        )\n\n        self.content_scraper = ContentScraper()\n\n        self.content_scraper_schema = BaseAgent.tool_registry.get_or_create_tool_schema(\n            self.content_scraper.scrape\n        )\n\n        BaseAgent.conversation_manager.available_tools = [\n            self.content_scraper_schema,\n            self.search_aggregator_schema,\n        ]\n\n        self.repeat_turns = 10\n\n    def load_template(self, filename):\n        \"\"\"\n        Loads a template file from the prompts directory.\n\n        :param filename: The name of the template file to load.\n        :return: The content of the file as a string.\n        \"\"\"\n        filepath = os.path.join(self.prompts_dir, filename)\n\n        # Ensure the file exists\n        if not os.path.exists(filepath):\n            raise FileNotFoundError(\n                f\"Template file '{filename}' not found in {self.prompts_dir}\"\n            )\n\n        # Read and return the content of the file\n        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n            return file.read()\n\n    def render_template(self, template_str, data):\n        \"\"\"\n        Render a template using string formatting.\n\n        :param template_str: Template content as a string.\n        :param data: Dictionary of variables to replace in the template.\n        :return: Rendered string.\n        \"\"\"\n        template = Template(template_str)\n        return template.render(**data)\n\n    def chat_with_template(\n        self, template_name: str, template_data: dict, model: str = None\n    ) -> str:\n        \"\"\"Unified helper method to handle template rendering and chat calling\n\n        Args:\n            template_name: Name of template file\n            template_data: Data to render template with\n            model: Optional model override\n\n        Returns:\n            Chat response content\n        \"\"\"\n        template = self.load_template(template_name)\n        rendered_prompt = self.render_template(template, template_data)\n        return self.common_chat(\n            usr_prompt=rendered_prompt, model=model or settings.default_model\n        )\n\n    def chat_with_tools(\n        self, template_name: str, template_data: dict, tools: List, model: str = None\n    ) -> ChatCompletionMessage:\n        \"\"\"Helper method for chat with tools\"\"\"\n        template = self.load_template(template_name)\n        rendered_prompt = self.render_template(template, template_data)\n        return self.common_chat(\n            usr_prompt=rendered_prompt,\n            tools=tools,\n            model=model or settings.default_model,\n        )\n\n    @overload\n    def common_chat(\n        self, usr_prompt: List, tools: None = None\n    ) -> ChatCompletionMessage: ...\n\n    @overload\n    def common_chat(self, usr_prompt: str, tools: List) -> ChatCompletionMessage: ...\n\n    @overload\n    def common_chat(self, usr_prompt: str, tools: None = None) -> str: ...\n\n    def common_chat(\n        self,\n        usr_prompt: str | List,\n        tools: Optional[List] = None,\n        role: str = \"assistant\",\n        model: str = settings.default_model,  # 默认使用配置文件中的默认模型\n    ) -> ChatCompletionMessage | str | None:\n        llm_response = call_llm(\n            model=model,  # 使用传入的model / 默认model\n            usr_prompt=usr_prompt,\n            config=settings,\n            tools=tools,\n        )\n\n        if tools is not None:\n            return llm_response\n\n        BaseAgent.conversation_manager.append_to_history(\n            role=role, content=llm_response.content\n        )\n\n        return llm_response.content\n\n    def update_answer(self, query, previous_answer, search_results, critic_feedback):\n        data = {\n            \"query\": query,\n            \"previous_answer\": previous_answer,\n            \"search_results\": search_results,\n            \"critic_feedback\": critic_feedback,\n        }\n\n        agent_update_answer_prompt = self.load_template(\"agent_update_answer.txt\")\n        rendered_prompt = self.render_template(agent_update_answer_prompt, data)\n\n        agent_update_answer_response = self.common_chat(usr_prompt=rendered_prompt)\n\n        return agent_update_answer_response\n\n    def model_confident(self, query):\n        \"\"\"\n        检查模型是否对当前问题有信心。\n        \"\"\"\n        data = {\"user_question\": query}\n        agent_confidence_prompt = self.load_template(\"agent_confidence.txt\")\n\n        rendered_prompt = self.render_template(agent_confidence_prompt, data)\n        agent_confidence_response = self.common_chat(usr_prompt=rendered_prompt)\n\n        return agent_confidence_response\n\n    def web_scrape_results(self, search_results: str) -> str | None:\n        \"\"\"Extract web content from search results using web scraper\n\n        Args:\n            search_results: Initial search results to scrape from\n\n        Returns:\n            Scraped web content or None if scraping failed\n        \"\"\"\n        web_scraper_prompt = self.load_template(\"web_scraper.txt\")\n        web_scraper_rendered_prompt = self.render_template(\n            web_scraper_prompt,\n            {\n                \"user_question\": self.user_question,\n                \"initial_search_results\": search_results,\n            },\n        )\n\n        # Interact with the model for web scraping\n        web_scraper_response = self.common_chat(\n            usr_prompt=web_scraper_rendered_prompt,\n            tools=self.content_scraper_schema,\n        )\n\n        # If no tool calls, return the response immediately\n        if web_scraper_response.tool_calls is None:\n            return web_scraper_response.content\n\n        BaseAgent.conversation_manager.append_tool_call_to_history(\n            web_scraper_response.tool_calls\n        )\n\n        final_web_scraper_results = \"\"\n\n        for tool_call in web_scraper_response.tool_calls:\n            urls = json.loads(tool_call.function.arguments).get(\"urls\", [])\n            web_scraper_results = asyncio.run(self.content_scraper.scrape(urls=urls))\n            BaseAgent.conversation_manager.append_tool_call_result_to_history(\n                tool_call_id=tool_call.id,\n                name=\"scrape\",\n                content=web_scraper_results,\n            )\n            final_web_scraper_results += web_scraper_results\n\n        return final_web_scraper_results\n\n    def search_and_browse(self, rendered_prompt) -> str | None:\n        search_with_tool_response = self.common_chat(\n            usr_prompt=rendered_prompt, tools=self.search_aggregator_schema\n        )\n\n        printer.log(f\"search_with_tool_response:\\n{search_with_tool_response}\")\n\n        # If no tool calls, return the response immediately\n        if search_with_tool_response.tool_calls is None:\n            return search_with_tool_response.content\n\n        BaseAgent.conversation_manager.append_tool_call_to_history(\n            search_with_tool_response.tool_calls\n        )\n\n        final_search_results = \"\"\n\n        for tool_call in search_with_tool_response.tool_calls:\n            query = json.loads(tool_call.function.arguments).get(\"query\", \"\")\n\n            search_results = asyncio.run(self.search_aggregator.search(query=query))\n\n            time.sleep(0.2)\n\n            BaseAgent.conversation_manager.append_tool_call_result_to_history(\n                tool_call_id=tool_call.id,\n                name=\"search\",\n                content=search_results,\n            )\n\n            BaseAgent.queryDB.update(query)\n\n            final_search_results += f\"{search_results}\"\n\n        return self.web_scrape_results(final_search_results)\n\n    def receive_task(self, task):\n        \"\"\"\n        接收原始任务。\n        \"\"\"\n        self.original_task = task\n\n    def extract_and_validate_yaml(self, model_response):\n        # 正则表达式匹配包裹在```yaml```之间的内容\n        import re\n\n        match = re.search(r\"```yaml\\n([\\s\\S]*?)\\n```\", model_response, re.DOTALL)\n\n        if not match:\n            return None  # 如果没有找到匹配的内容，返回None\n\n        model_response = match.group(1).strip()\n\n        try:\n            # 尝试解析YAML内容\n            parsed_yaml = yaml.safe_load(model_response)\n            return yaml.dump(parsed_yaml, default_flow_style=False)\n\n        except yaml.YAMLError as exc:\n            print(f\"Invalid YAML content: {exc}\")\n            return None\n\n    def extract_and_validate_json(self, model_response):\n        # Try to extract JSON data wrapped in ```json``` blocks\n        # and return the parsed JSON content\n        match = re.search(r\"```json\\n([\\s\\S]*?)\\n```\", model_response, re.DOTALL)\n        if match:\n            json_content = match.group(1).strip()\n        else:\n            json_content = model_response.strip()\n\n        try:\n            parsed_json = json.loads(json_content)\n            return parsed_json\n\n        except json.JSONDecodeError as exc:\n            print(f\"Invalid JSON content: {exc}\")\n            return None\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/critic_agent.py",
        "src/criticsearch/critic_agent.py/CriticAgent",
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/__init__",
        "src/criticsearch/tasks_runner.py",
        "src/criticsearch/tasks_runner.py/execute_multiple_tasks"
      ],
      "reference_who": [
        "src/criticsearch/models.py/ConversationManager",
        "src/criticsearch/tools/tool_registry.py/ToolRegistry"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the BaseAgent class, setting up necessary directories, databases, and tools for the agent's operation.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The __init__ method is the constructor for the BaseAgent class. It is responsible for initializing various components that the agent will use during its operation. \n\n1. **Directory Setup**: The method begins by determining the base directory of the current script using `os.path.dirname(os.path.abspath(__file__))`. This directory is essential for locating resources related to the agent, specifically the prompts directory, which is constructed by joining the base directory with the string \"prompts\".\n\n2. **Citation Database Initialization**: The `citationDB` attribute is initialized as a list containing a single dictionary. This dictionary is designed to store search queries as keys and their corresponding results as values. The comment indicates that only search results praised by a critic will be included in this database. Each entry in the citationDB is structured to hold a unique document identifier along with its associated metadata, such as URL, title, and content.\n\n3. **Search Aggregator Setup**: An instance of the `SearchAggregator` class is created and assigned to the `search_aggregator` attribute. This component is responsible for managing search queries across multiple search engines.\n\n4. **Tool Schema Creation**: The method retrieves or creates schemas for the search aggregator and content scraper tools using the `get_or_create_tool_schema` method from the `tool_registry`. This method ensures that the schemas for these tools are registered and available for use in the agent's operations. The schemas are stored in `search_aggregator_schema` and `content_scraper_schema` attributes, respectively.\n\n5. **Content Scraper Initialization**: An instance of the `ContentScraper` class is created and assigned to the `content_scraper` attribute. This component is responsible for scraping content from specified URLs.\n\n6. **Updating Available Tools**: The method updates the `available_tools` attribute of the `conversation_manager` class variable in BaseAgent to include the schemas for both the content scraper and the search aggregator. This allows the agent to utilize these tools during interactions.\n\n7. **Repeat Turns Configuration**: Finally, the `repeat_turns` attribute is initialized to 10, which likely indicates the number of times the agent can repeat a search or interaction before stopping.\n\nThe __init__ method is crucial for setting up the BaseAgent instance with all necessary components and configurations, ensuring that it is ready to perform its intended functions effectively.\n\n**Note**: When using this method, it is important to ensure that the required directories and tools are correctly set up in the environment. Additionally, the proper functioning of the search aggregator and content scraper depends on the availability of their respective configurations and API keys."
      ],
      "code_start_line": 27,
      "code_end_line": 66,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        base_dir = os.path.dirname(\n            os.path.abspath(__file__)\n        )  # Directory of the current script\n        self.prompts_dir = os.path.join(base_dir, \"prompts\")\n        # self.env = Environment(loader=FileSystemLoader(self.prompts_dir))\n\n        # 对于citationDB,应该是一个字典，key是query，value是内容和来源\n        # 这个列表中的每个元素都是一个字典，代表一个搜索的问题以及对应的搜索结果\n        self.citationDB = [\n            {  # citationDB中只会把受到critic表扬的搜索结果加入\n                \"why do we say google was facing challenges in 2019?\": {\n                    \"document_id\": {  # 这个document_id是一个唯一的标识符，用于标识这个文档\n                        \"url\": \"\",\n                        \"title\": \"\",\n                        \"content\": \"\",\n                    }\n                }\n            }\n        ]\n        self.search_aggregator = SearchAggregator()\n\n        self.search_aggregator_schema = (\n            BaseAgent.tool_registry.get_or_create_tool_schema(\n                self.search_aggregator.search\n            )\n        )\n\n        self.content_scraper = ContentScraper()\n\n        self.content_scraper_schema = BaseAgent.tool_registry.get_or_create_tool_schema(\n            self.content_scraper.scrape\n        )\n\n        BaseAgent.conversation_manager.available_tools = [\n            self.content_scraper_schema,\n            self.search_aggregator_schema,\n        ]\n\n        self.repeat_turns = 10\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/tool_registry.py/ToolRegistry/get_or_create_tool_schema",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "load_template",
      "md_content": [
        "## Function: `load_template`\n\n### Overview:\nThe `load_template` function is responsible for loading a template file from a predefined directory and returning its contents as a string. This method is particularly useful for retrieving text-based templates that can be rendered or processed further in the application.\n\n### Method Signature:\n```python\ndef load_template(self, filename)\n```\n\n### Parameters:\n- **`filename`** (str): The name of the template file to be loaded. This should be a valid filename present within the specified prompts directory.\n\n### Returns:\n- **str**: The content of the template file as a string.\n\n### Raises:\n- **FileNotFoundError**: If the specified template file does not exist in the prompts directory, this error is raised with a descriptive message indicating the missing file.\n\n### Detailed Description:\nThe `load_template` function performs the following steps:\n1. It constructs the full file path by combining the predefined prompts directory (`self.prompts_dir`) with the provided `filename`.\n2. It checks if the file exists at the constructed path.\n3. If the file does not exist, it raises a `FileNotFoundError` with a detailed message about the missing file.\n4. If the file is found, it opens the file in read mode, reads its content, and returns the content as a string.\n\nThis function ensures that template files are loaded correctly, and if a template file is missing, the user is informed with a clear error message.\n\n### Example Usage:\n```python\ntemplate_content = self.load_template(\"example_template.txt\")\n```\n\nIn this example, the function loads the content of `example_template.txt` from the predefined directory and stores it in the `template_content` variable for further use.",
        "**load_template**: The function of load_template is to load a template file from the prompts directory.\n\n**parameters**: The parameters of this Function.\n· filename: The name of the template file to load.\n\n**Code Description**: The load_template function is designed to retrieve the content of a specified template file from a predefined directory, referred to as prompts_dir. The function takes a single parameter, filename, which represents the name of the template file to be loaded.\n\nThe function constructs the full file path by joining the prompts_dir with the provided filename using the os.path.join method. It then checks if the constructed file path exists using os.path.exists. If the file does not exist, a FileNotFoundError is raised, providing a clear error message indicating that the specified template file could not be found in the prompts directory.\n\nIf the file exists, the function proceeds to open the file in read mode with UTF-8 encoding. It reads the content of the file and returns it as a string. This functionality is essential for the BaseAgent class, as it allows the agent to dynamically load templates that are used for generating prompts and responses in various interactions.\n\nThe load_template function is called by several other methods within the BaseAgent class, including chat_with_template, chat_with_tools, and update_answer. These methods rely on load_template to fetch the appropriate template files needed for rendering prompts based on user queries and other contextual data. For example, in the chat_with_template method, load_template is used to retrieve the template specified by template_name, which is then rendered with the provided template_data to create a prompt for the chat model.\n\n**Note**: It is crucial to ensure that the filename passed to the load_template function corresponds to an existing template file in the prompts directory. Failure to do so will result in a FileNotFoundError, which will halt the execution of the calling function until the issue is resolved.\n\n**Output Example**: A possible return value from the load_template function could be a string containing the content of the template file, such as:\n```\n\"Hello, {{ user_name }}! How can I assist you today?\"\n```"
      ],
      "code_start_line": 68,
      "code_end_line": 85,
      "params": [
        "self",
        "filename"
      ],
      "have_return": true,
      "code_content": "    def load_template(self, filename):\n        \"\"\"\n        Loads a template file from the prompts directory.\n\n        :param filename: The name of the template file to load.\n        :return: The content of the file as a string.\n        \"\"\"\n        filepath = os.path.join(self.prompts_dir, filename)\n\n        # Ensure the file exists\n        if not os.path.exists(filepath):\n            raise FileNotFoundError(\n                f\"Template file '{filename}' not found in {self.prompts_dir}\"\n            )\n\n        # Read and return the content of the file\n        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n            return file.read()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_tools",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/critic_agent.py/CriticAgent/__init__",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_fact_extraction/process_section/attempt",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_factualqa",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section/attempt"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "render_template",
      "md_content": [
        "**render_template**: The function of render_template is to render a template using string formatting.\n\n**parameters**: The parameters of this Function.\n· template_str: Template content as a string.  \n· data: Dictionary of variables to replace in the template.  \n\n**Code Description**: The render_template function is designed to take a string representation of a template and a dictionary of data, which contains key-value pairs that will be used to replace placeholders in the template. The function utilizes the Template class to create a template object from the provided template string. It then calls the render method on this template object, unpacking the data dictionary to replace the placeholders with the corresponding values. The result is a fully rendered string that incorporates the provided data.\n\nThis function is called by various methods within the project, specifically in the ReportBenchmark and ReportEvaluation classes. For instance, in the attempt method of the ReportBenchmark class, render_template is used to create a prompt for a chat interaction by loading a specific template and populating it with relevant data such as wiki_text and UserQuery. Similarly, in the run_factualqa method, it renders a template for FactualQA evaluation, incorporating user queries and ground truth data. The rendered output is then used to generate responses from a common chat function, which is a critical part of the application's functionality.\n\nIn the ReportEvaluation class, methods like examinees_outline_generation and evaluate_factualqa also leverage render_template to prepare prompts for generating outlines and evaluating factual questions, respectively. Each of these methods relies on the rendered output to facilitate interactions with the chat system, ensuring that the responses are contextually relevant and tailored to the specific queries being addressed.\n\n**Note**: It is important to ensure that the data dictionary passed to the render_template function contains all necessary keys that correspond to placeholders in the template string. Failure to provide the correct keys may result in rendering errors or incomplete output.\n\n**Output Example**: An example of the rendered output might look like this:\n\nIf the template_str is \"Hello, {{ UserQuery }}! Here is your information: {{ wiki_text }}.\" and the data is {\"UserQuery\": \"What is AI?\", \"wiki_text\": \"AI stands for Artificial Intelligence.\"}, the output would be:\n\n\"Hello, What is AI?! Here is your information: AI stands for Artificial Intelligence.\"",
        "**render_template**: The function of render_template is to render a template using string formatting.\n\n**parameters**: The parameters of this Function.\n· template_str: Template content as a string.  \n· data: Dictionary of variables to replace in the template.  \n\n**Code Description**: The render_template function is designed to take a template string and a dictionary of data, and then produce a formatted string by replacing placeholders in the template with corresponding values from the data dictionary. It utilizes the Template class to create a template object from the provided template string. The method then calls the render function on this template object, unpacking the data dictionary to replace the placeholders with actual values.\n\nThis function is called by several other methods within the BaseAgent class, including chat_with_template, chat_with_tools, and update_answer. Each of these methods relies on render_template to generate a prompt that is sent to a conversational model. For instance, in chat_with_template, the method first loads a template based on the template_name provided, then uses render_template to format this template with the provided template_data before passing the rendered prompt to the common_chat method for interaction with the model. Similarly, chat_with_tools and update_answer also use render_template to prepare prompts that incorporate dynamic data, ensuring that the responses generated by the model are contextually relevant and tailored to the specific interaction.\n\nThe render_template function is crucial for maintaining the flexibility and adaptability of the agent's responses, allowing it to generate contextually appropriate prompts based on varying inputs.\n\n**Note**: It is essential to ensure that the template_str provided is correctly formatted and that the data dictionary contains all necessary keys corresponding to the placeholders in the template. Failure to do so may result in runtime errors during the rendering process.\n\n**Output Example**: A possible return value from the render_template function could be a string such as \"Hello, John! Your balance is $100.\" if the template_str was \"Hello, {name}! Your balance is ${balance}.\" and the data dictionary was {\"name\": \"John\", \"balance\": 100}."
      ],
      "code_start_line": 87,
      "code_end_line": 96,
      "params": [
        "self",
        "template_str",
        "data"
      ],
      "have_return": true,
      "code_content": "    def render_template(self, template_str, data):\n        \"\"\"\n        Render a template using string formatting.\n\n        :param template_str: Template content as a string.\n        :param data: Dictionary of variables to replace in the template.\n        :return: Rendered string.\n        \"\"\"\n        template = Template(template_str)\n        return template.render(**data)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_tools",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_fact_extraction/process_section/attempt",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_factualqa",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section/attempt"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "chat_with_template",
      "md_content": [
        "**chat_with_template**: The function of chat_with_template is to provide a unified method for rendering templates and facilitating chat interactions with a conversational model.\n\n**parameters**: The parameters of this Function.\n· template_name: The name of the template file to be used for rendering the prompt.  \n· template_data: A dictionary containing the data that will be used to populate the template.  \n· model: An optional parameter that allows for overriding the default model used for the chat interaction.\n\n**Code Description**: The chat_with_template function is a method within the BaseAgent class that serves as a comprehensive utility for managing interactions with a conversational model through template rendering. This function first loads a specified template file using the load_template method, which retrieves the content of the template from a predefined directory. The template_name parameter is passed to load_template to identify the correct file.\n\nOnce the template is loaded, the function utilizes the render_template method to format the template content by replacing placeholders with actual values from the template_data dictionary. This step is crucial as it ensures that the prompt sent to the conversational model is contextually relevant and tailored to the user's input.\n\nAfter rendering the prompt, chat_with_template calls the common_chat method, passing the rendered prompt along with the specified model (or the default model if none is provided). The common_chat method is responsible for sending the prompt to the conversational model and retrieving the generated response.\n\nThe chat_with_template function is invoked in various contexts within the project, such as in the process_single_task function, where it is used to assess the confidence of the agent and to generate responses based on user queries. For example, it is called to create prompts for evaluating the agent's confidence level and for generating direct responses to user tasks. Additionally, it is utilized in the ReportVerifier class to verify questions against factual data, demonstrating its versatility and importance in ensuring accurate and context-aware interactions with the conversational model.\n\n**Note**: It is essential to ensure that the template_name provided corresponds to an existing template file in the prompts directory, and that the template_data dictionary contains all necessary keys for rendering. Failure to do so may result in errors during the template loading or rendering process.\n\n**Output Example**: A possible return value from the chat_with_template function could be a string such as:\n```\n\"Hello, John! How can I assist you today?\"\n```"
      ],
      "code_start_line": 98,
      "code_end_line": 115,
      "params": [
        "self",
        "template_name",
        "template_data",
        "model"
      ],
      "have_return": true,
      "code_content": "    def chat_with_template(\n        self, template_name: str, template_data: dict, model: str = None\n    ) -> str:\n        \"\"\"Unified helper method to handle template rendering and chat calling\n\n        Args:\n            template_name: Name of template file\n            template_data: Data to render template with\n            model: Optional model override\n\n        Returns:\n            Chat response content\n        \"\"\"\n        template = self.load_template(template_name)\n        rendered_prompt = self.render_template(template, template_data)\n        return self.common_chat(\n            usr_prompt=rendered_prompt, model=model or settings.default_model\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/verify_section/verify_single_question"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "chat_with_tools",
      "md_content": [
        "### `chat_with_tools` Method Documentation\n\n#### Description\nThe `chat_with_tools` method is a helper function within the `BaseAgent` class designed to facilitate communication with external tools during a conversation. This method is responsible for loading a template, rendering it with the provided data, and initiating a chat with specified tools using the rendered prompt. It abstracts the process of preparing a prompt from a template and interacting with the tools in a standardized manner.\n\n#### Parameters\n- **template_name** (`str`): The name of the template file to be loaded. This template defines the structure of the prompt to be used in the conversation.\n- **template_data** (`dict`): A dictionary containing the data that will be used to render the template. The dictionary's keys correspond to placeholders in the template, and the values are inserted into the template during the rendering process.\n- **tools** (`List`): A list of tools that will be used in the chat interaction. These tools may include models, external APIs, or other utilities that assist in processing the conversation.\n- **model** (`str`, optional): The model to be used for the chat interaction. If not provided, the method will default to using the model specified in the application's settings.\n\n#### Return Type\n- **ChatCompletionMessage**: The method returns a `ChatCompletionMessage` object that represents the result of the chat interaction. This message encapsulates the response from the tools based on the rendered prompt.\n\n#### Functionality Overview\n1. **Template Loading**: The method first loads the specified template using the `load_template` function. This function retrieves the content of the template file from a predefined directory.\n2. **Template Rendering**: After loading the template, it is rendered with the provided `template_data` using the `render_template` function. This process replaces placeholders in the template with actual data from the `template_data` dictionary.\n3. **Chat Interaction**: Finally, the rendered prompt is passed to the `common_chat` method along with the provided tools and model. The `common_chat` method handles the interaction with the tools, generating the response based on the prompt.\n\n#### Usage Example\n```python\nresponse = agent.chat_with_tools(\n    template_name=\"user_query_template.txt\",\n    template_data={\"user_name\": \"John\", \"query\": \"What is the weather like today?\"},\n    tools=[weather_tool],\n    model=\"gpt-4\"\n)\n```\n\n#### Related Functions\n- **`load_template`**: Loads a template file based on the `template_name` provided.\n- **`render_template`**: Renders the template with the provided data, substituting placeholders with actual values.\n- **`common_chat`**: Handles the chat interaction with the specified tools and model, using the rendered prompt.\n\n#### Notes\n- Ensure that the template file specified in `template_name` exists in the appropriate directory, as the method will raise an error if the file cannot be found.\n- If no model is specified, the method will default to the model set in the application's settings."
      ],
      "code_start_line": 117,
      "code_end_line": 127,
      "params": [
        "self",
        "template_name",
        "template_data",
        "tools",
        "model"
      ],
      "have_return": true,
      "code_content": "    def chat_with_tools(\n        self, template_name: str, template_data: dict, tools: List, model: str = None\n    ) -> ChatCompletionMessage:\n        \"\"\"Helper method for chat with tools\"\"\"\n        template = self.load_template(template_name)\n        rendered_prompt = self.render_template(template, template_data)\n        return self.common_chat(\n            usr_prompt=rendered_prompt,\n            tools=tools,\n            model=model or settings.default_model,\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "common_chat",
      "md_content": [
        "**common_chat**: The function of common_chat is to facilitate a conversation with the model by sending user prompts and receiving responses.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A list containing the user prompt that is to be sent to the model for processing.\n· tools: An optional parameter that can be set to None, which may be used to specify any tools that the model can utilize during the conversation.\n\n**Code Description**: The common_chat function is designed to interact with a conversational model, taking a user prompt as input and returning a response in the form of a ChatCompletionMessage. This function is integral to the operation of various components within the project, as it serves as the primary means of communication with the model. It is invoked by several methods across different classes, such as attempt in ReportBenchmark, run_factualqa, examinees_outline_generation, evaluate_factualqa, and extract_student_tree_structure in the ReportEvaluation class, as well as update_answer and model_confident in the BaseAgent class.\n\nIn each of these instances, common_chat is called with a rendered prompt that is generated based on specific templates and user data. For example, in the attempt method, the function is called after loading and rendering a template for fact extraction, ensuring that the model receives a well-structured query. Similarly, in the run_factualqa method, the function is used to evaluate factual questions by passing a prompt that includes user queries and ground truth data.\n\nThe common_chat function is also utilized in the context of updating answers and checking model confidence, where it processes prompts that include previous answers and user feedback. This highlights its role in maintaining an ongoing dialogue with the model, allowing for iterative improvements based on user interactions and feedback.\n\n**Note**: It is important to ensure that the usr_prompt parameter is properly formatted and contains relevant information for the model to generate an appropriate response. Additionally, while the tools parameter is optional, its inclusion may enhance the model's capabilities depending on the context of the conversation.",
        "**common_chat**: The function of common_chat is to facilitate communication with a conversational model by sending a user prompt and optionally utilizing specific tools.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A list containing the prompt or message that the user wants to send to the conversational model.  \n· tools: An optional parameter that can be set to None or a list of tools that may be used during the chat interaction.\n\n**Code Description**: The common_chat function is a method within the BaseAgent class designed to interact with a conversational model, processing user prompts and potentially utilizing additional tools to enhance the interaction. This function takes in a user prompt, which is expected to be a list, and an optional tools parameter that can specify any tools to be employed during the chat.\n\nThe common_chat function is invoked by several other methods within the BaseAgent class, showcasing its central role in the communication process. For instance, the chat_with_template method utilizes common_chat to send a rendered prompt generated from a specified template and associated data. Similarly, the chat_with_tools method also calls common_chat, passing a rendered prompt along with a list of tools to facilitate a more enriched interaction.\n\nIn the context of the update_answer function, common_chat is called to refine the agent's response based on new information and feedback. This highlights the function's importance in ensuring that the agent's replies are not only relevant but also informed by the latest data and user interactions.\n\nThe model_confident function also relies on common_chat to assess the confidence level of the model regarding a specific user query. By sending a rendered prompt that includes the user's question, common_chat helps determine whether the model is confident enough to provide a direct answer or if further actions are necessary.\n\nFurthermore, the web_scrape_results function employs common_chat to interact with a web scraper, sending a prompt that includes the user's question and initial search results. This demonstrates the versatility of common_chat in handling various types of interactions, from general conversation to specific tasks like web scraping.\n\nOverall, common_chat serves as a fundamental method within the BaseAgent class, enabling seamless communication with the conversational model and supporting various functionalities that enhance the agent's capabilities.\n\n**Note**: It is essential to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation. Additionally, the tools parameter should be appropriately defined if used, as it can influence the interaction with the model."
      ],
      "code_start_line": 130,
      "code_end_line": 132,
      "params": [
        "self",
        "usr_prompt",
        "tools"
      ],
      "have_return": false,
      "code_content": "    def common_chat(\n        self, usr_prompt: List, tools: None = None\n    ) -> ChatCompletionMessage: ...\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_tools",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_fact_extraction/process_section/attempt",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_factualqa",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section/attempt"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "common_chat",
      "md_content": [
        "**common_chat**: The function of common_chat is to facilitate a chat interaction by processing a user prompt and potentially utilizing additional tools.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string representing the user's prompt that initiates the chat interaction.\n· tools: A list of tools that may be used during the chat interaction.\n\n**Code Description**: The common_chat function is designed to handle chat interactions within the BaseAgent class. It takes a user prompt (usr_prompt) as input, which is a string that contains the user's query or message. Additionally, it accepts a list of tools that can be employed during the chat process. The function is expected to return a ChatCompletionMessage, which encapsulates the response generated from the chat interaction.\n\nThis function plays a crucial role in various methods across the project, serving as a core component for generating responses based on user queries. For instance, in the ReportBenchmark class, the common_chat function is invoked in methods such as run_fact_extraction and run_factualqa. In these contexts, it processes prompts that are dynamically created from templates and user data, ensuring that the responses are contextually relevant and accurate.\n\nMoreover, the common_chat function is also utilized in the ReportEvaluation class, specifically in methods like examinees_outline_generation and evaluate_factualqa. Here, it aids in generating outlines and evaluations based on student reports and user queries, demonstrating its versatility in handling different types of interactions.\n\nIn the context of the BaseAgent class, the common_chat function is called by other methods such as update_answer and model_confident. These methods leverage common_chat to obtain responses that are then used to update previous answers or assess the model's confidence in its responses.\n\nOverall, the common_chat function serves as a foundational element in the communication framework of the project, enabling seamless interactions between users and the system while ensuring that the responses are generated based on the provided prompts and tools.\n\n**Note**: It is important to ensure that the usr_prompt is well-formed and relevant to the context of the conversation to receive meaningful responses. Additionally, the tools parameter should be populated with appropriate tools that can enhance the chat interaction if needed.",
        "**common_chat**: The function of common_chat is to facilitate communication with a chat model by sending a user prompt and receiving a response.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string representing the prompt or message from the user that is to be sent to the chat model.  \n· tools: A list of tools that may be utilized during the chat interaction, which can enhance the model's response capabilities.\n\n**Code Description**: The common_chat function is a core method within the BaseAgent class that serves as an interface for interacting with a chat model. It takes two parameters: `usr_prompt`, which is the input message from the user, and `tools`, which is an optional list of tools that can be employed to assist in generating a more informed response.\n\nThe function is designed to send the user prompt to the chat model and retrieve a response, encapsulating the interaction logic necessary for effective communication. This function is pivotal in various methods throughout the BaseAgent class, where it is invoked to handle user queries and generate responses based on templates and rendered prompts.\n\nFor instance, the common_chat function is called within the chat_with_template method, where it receives a rendered prompt created from a specific template and associated data. This allows the agent to provide contextually relevant responses based on user input. Similarly, in the chat_with_tools method, common_chat is utilized to send a rendered prompt along with a list of tools, enabling the agent to leverage additional functionalities during the chat interaction.\n\nMoreover, the common_chat function is also employed in other methods such as update_answer, model_confident, web_scrape_results, and search_and_browse, demonstrating its versatility and central role in the agent's operations. Each of these methods relies on common_chat to communicate with the chat model, ensuring that the agent can effectively process user queries, update responses based on new information, and interact with external tools.\n\n**Note**: It is essential to ensure that the `usr_prompt` parameter is well-formed and relevant to the context of the conversation. Additionally, the `tools` parameter should be appropriately populated with valid tools to enhance the response generation process when applicable. Proper error handling should also be implemented to manage cases where the chat model does not return a valid response."
      ],
      "code_start_line": 135,
      "code_end_line": 135,
      "params": [
        "self",
        "usr_prompt",
        "tools"
      ],
      "have_return": false,
      "code_content": "    def common_chat(self, usr_prompt: str, tools: List) -> ChatCompletionMessage: ...\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_tools",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_fact_extraction/process_section/attempt",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_factualqa",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section/attempt"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "common_chat",
      "md_content": [
        "**common_chat**: The function of common_chat is to facilitate a conversation with the user by processing a given prompt and returning a response.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string that contains the user's prompt or query that needs to be processed.\n· tools: An optional parameter that can be set to None, which may be used to specify additional tools for processing the prompt.\n\n**Code Description**: The common_chat function is designed to handle user interactions by taking a user prompt as input and generating a response. It is a core component of the BaseAgent class, which serves as a foundational element for various agents in the project. The function is invoked in multiple contexts throughout the project, primarily within methods that require interaction with the user or the processing of user queries.\n\nFor instance, in the ReportBenchmark class, the common_chat function is called within the run_fact_extraction and run_factualqa methods. In these cases, it processes a rendered template prompt that includes user queries and relevant data, returning a response that is expected to be in a specific format (e.g., JSON). If the response is not in the expected format or is empty, exceptions are raised to handle these scenarios appropriately.\n\nSimilarly, in the ReportEvaluation class, methods such as examinees_outline_generation and evaluate_factualqa also utilize common_chat to generate responses based on user queries. The responses from common_chat are critical for further processing and evaluation, as they often serve as input for subsequent logic or decision-making steps.\n\nThe function is also utilized in the update_answer and model_confident methods of the BaseAgent class, where it processes prompts related to updating answers based on user feedback and checking the model's confidence in its responses.\n\nOverall, common_chat acts as a communication bridge between the user and the agent, ensuring that user queries are effectively processed and that the agents can respond appropriately based on the context provided.\n\n**Note**: When using the common_chat function, it is essential to ensure that the usr_prompt is well-formed and that any expected tools are correctly specified if applicable. Proper error handling should be implemented to manage cases where the response does not meet the expected criteria.",
        "**common_chat**: The function of common_chat is to facilitate communication with a conversational model by sending a user prompt and receiving a response.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string that represents the prompt or question provided by the user to the conversational model.  \n· tools: An optional parameter that can be used to specify any tools that may be required for the chat interaction. The default value is None.\n\n**Code Description**: The common_chat function is a method within the BaseAgent class that serves as a core component for interacting with a conversational model. This function takes a user-defined prompt (usr_prompt) and an optional tools parameter to facilitate the communication process. The primary role of common_chat is to send the rendered prompt to the model and retrieve the generated response.\n\nThis function is called by several other methods within the BaseAgent class, including chat_with_template, chat_with_tools, update_answer, model_confident, web_scrape_results, and search_and_browse. Each of these methods utilizes common_chat to handle the interaction with the conversational model, ensuring that the prompts generated from various contexts are processed consistently.\n\nFor instance, in the chat_with_template method, common_chat is invoked after rendering a template with specific data. The rendered prompt is passed to common_chat, which then communicates with the model to obtain a chat response. Similarly, in chat_with_tools, the rendered prompt is sent alongside any specified tools, allowing for a more dynamic interaction based on the user's needs.\n\nThe update_answer method relies on common_chat to refine the agent's response by sending a newly constructed prompt that incorporates user feedback and search results. The model_confident method uses common_chat to assess the model's confidence level regarding a user query, while web_scrape_results and search_and_browse leverage common_chat to interact with the model for web scraping and searching tasks.\n\nOverall, common_chat acts as a bridge between the user prompts generated by various methods and the conversational model, ensuring that the interaction is seamless and effective.\n\n**Note**: It is important to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation to receive an accurate response from the model. Additionally, the tools parameter should be used judiciously to enhance the functionality of the chat interaction when necessary."
      ],
      "code_start_line": 138,
      "code_end_line": 138,
      "params": [
        "self",
        "usr_prompt",
        "tools"
      ],
      "have_return": false,
      "code_content": "    def common_chat(self, usr_prompt: str, tools: None = None) -> str: ...\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_tools",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_fact_extraction/process_section/attempt",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_factualqa",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section/attempt"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "common_chat",
      "md_content": [
        "**common_chat**: The function of common_chat is to facilitate interaction with a language model by sending user prompts and managing the conversation history.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string or list representing the user's prompt to the language model.\n· tools: An optional list of tools that can be utilized during the interaction.\n· role: A string indicating the role of the entity sending the message, defaulting to \"assistant\".\n\n**Code Description**: The common_chat function is designed to interact with a language model by sending a user-defined prompt and processing the response. It first calls the call_llm function, which handles the communication with the language model API. This function takes the model specified in the settings, the user prompt, and any tools that may be needed for the interaction.\n\nIf tools are provided, the function returns the model's response directly without appending it to the conversation history. However, if no tools are specified, the response content is appended to the conversation history managed by the BaseAgent's conversation_manager, using the specified role (defaulting to \"assistant\"). The function ultimately returns the content of the model's response.\n\nThe common_chat function is called by various methods throughout the project, including:\n- In the attempt method of the ReportBenchmark class, where it generates a prompt based on a template and user query, then retrieves a response from the language model.\n- In the run_factualqa method, where it similarly generates a prompt for factual question answering and retrieves the model's response.\n- In the examinees_outline_generation and evaluate_factualqa methods, where it generates prompts for outline generation and factual question evaluation, respectively.\n- In the update_answer and model_confident methods, where it is used to update answers based on user queries and to check the model's confidence in its responses.\n- In the search_and_browse method, where it interacts with tools for searching and browsing based on the rendered prompt.\n\nThis function plays a critical role in maintaining the flow of conversation and ensuring that responses from the language model are appropriately handled and recorded.\n\n**Note**: It is important to ensure that the usr_prompt is well-formed and that the tools, if used, are compatible with the language model being called. The function's behavior may vary based on whether tools are provided or not, affecting how the response is processed and stored.\n\n**Output Example**: A possible return value from the common_chat function could be a string such as \"The capital of France is Paris.\" or a structured response from the language model, depending on the prompt and context provided.",
        "**common_chat**: The function of common_chat is to facilitate interaction with a language model by processing user prompts and returning generated responses.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string or list representing the user's prompt to the model, which serves as the basis for generating a response.  \n· tools: An optional list of tools that the model can utilize during the interaction. If not provided, it defaults to None.  \n· role: A string indicating the role of the entity interacting with the model, defaulting to \"assistant\".  \n· model: A string specifying the model to be used for generating the response, defaulting to the model defined in the settings.\n\n**Code Description**: The common_chat function is a method within the BaseAgent class that serves as a bridge for communication between the user and a language model. It begins by invoking the call_llm function, which interacts with the language model API to generate a response based on the provided user prompt (usr_prompt). The model parameter allows for flexibility in specifying which model to use, while the tools parameter enables the use of additional functionalities during the interaction.\n\nUpon receiving the response from the call_llm function, the common_chat function checks if any tools were specified. If tools are present, the function returns the model's response directly. If no tools are specified, it appends the model's response to the conversation history using the append_to_history method from the ConversationManager class. This method records the response along with the specified role, ensuring that the conversation flow is maintained.\n\nThe common_chat function is called by various methods within the BaseAgent class, such as chat_with_template and chat_with_tools. These methods utilize common_chat to handle user prompts that have been processed through templates or require the use of specific tools. Additionally, it is invoked in the update_answer and model_confident methods, where it plays a crucial role in refining the agent's responses based on user queries and feedback.\n\nIn summary, common_chat is essential for managing interactions with the language model, ensuring that user prompts are effectively processed and responses are accurately recorded in the conversation history. This functionality is vital for applications that rely on conversational agents, as it provides a structured approach to dialogue management.\n\n**Note**: It is important to ensure that the usr_prompt parameter is well-formed and relevant to the context of the conversation. The tools parameter should only be used if supported by the model, and the role parameter should accurately reflect the entity's role in the conversation to avoid any inconsistencies.\n\n**Output Example**: A possible return value from the common_chat function could be a string such as \"The capital of France is Paris.\" This output reflects the model's generated response based on the user's prompt."
      ],
      "code_start_line": 140,
      "code_end_line": 161,
      "params": [
        "self",
        "usr_prompt",
        "tools",
        "role",
        "model"
      ],
      "have_return": true,
      "code_content": "    def common_chat(\n        self,\n        usr_prompt: str | List,\n        tools: Optional[List] = None,\n        role: str = \"assistant\",\n        model: str = settings.default_model,  # 默认使用配置文件中的默认模型\n    ) -> ChatCompletionMessage | str | None:\n        llm_response = call_llm(\n            model=model,  # 使用传入的model / 默认model\n            usr_prompt=usr_prompt,\n            config=settings,\n            tools=tools,\n        )\n\n        if tools is not None:\n            return llm_response\n\n        BaseAgent.conversation_manager.append_to_history(\n            role=role, content=llm_response.content\n        )\n\n        return llm_response.content\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_tools",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_fact_extraction/process_section/attempt",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/run_factualqa",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section/attempt"
      ],
      "reference_who": [
        "src/criticsearch/models.py/ConversationManager/append_to_history",
        "src/criticsearch/llm_service.py/call_llm"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "update_answer",
      "md_content": [
        "**update_answer**: The function of update_answer is to update the agent's response based on a new query, previous answer, search results, and feedback from a critic.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the new question or query that the agent needs to address.  \n· previous_answer: A string containing the previous answer provided by the agent to the query.  \n· search_results: A string or data structure that holds the results obtained from a search operation related to the query.  \n· critic_feedback: A string containing feedback from a critic that may influence the update of the answer.\n\n**Code Description**: The update_answer function is designed to refine the agent's response to a user query by incorporating new information and feedback. The function begins by organizing the input parameters into a dictionary named `data`, which includes the current query, the previous answer, the search results, and the critic's feedback. \n\nNext, the function calls the load_template method to retrieve a specific template file named \"agent_update_answer.txt\". This template is expected to contain a structured format for generating a prompt that will be sent to the conversational model. The retrieved template is then rendered using the render_template method, which replaces placeholders in the template with the actual values from the `data` dictionary. This step ensures that the prompt is contextually relevant and tailored to the current interaction.\n\nAfter rendering the prompt, the function invokes the common_chat method, passing the rendered prompt as an argument. This method facilitates communication with the language model, sending the prompt and receiving a response. The response generated by the common_chat method is then returned as the output of the update_answer function.\n\nThe update_answer function is called within the main function of the project, specifically during the iterative process of refining answers based on user queries and feedback from a critic agent. In the main function, after the initial answer is generated, the update_answer function is invoked to incorporate the latest search results and feedback from the critic. This iterative approach allows the agent to improve its responses over multiple iterations, ensuring that the final answer is well-informed and aligned with user expectations.\n\n**Note**: It is essential to ensure that the parameters passed to the update_answer function are valid and appropriately formatted. The function relies on the successful loading of the template and the proper functioning of the common_chat method to generate an accurate response.\n\n**Output Example**: A possible return value from the update_answer function could be a string such as \"Based on the latest search results and feedback, the updated answer is: [new answer].\" This output reflects the agent's refined response after considering the provided inputs.",
        "**update_answer**: The function of update_answer is to refine the agent's response based on user feedback, previous answers, and search results.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the user's current question or prompt that needs to be addressed.  \n· previous_answer: A string containing the agent's last response to the user's query, which serves as a reference for improvement.  \n· search_results: A string or structured data containing the results obtained from a search operation that may provide additional context or information relevant to the query.  \n· critic_feedback: A string containing feedback from a critic agent, which may include suggestions or evaluations of the previous answer.\n\n**Code Description**: The update_answer function is a method within the BaseAgent class designed to enhance the quality of the agent's responses by integrating new information and feedback. This function takes four parameters: `query`, `previous_answer`, `search_results`, and `critic_feedback`. \n\nThe function begins by organizing these parameters into a dictionary called `data`, which serves as a structured format for the information that will be utilized in generating a new response. The next step involves loading a template for the update process by calling the load_template method with the filename \"agent_update_answer.txt\". This template is crucial as it provides a predefined structure for the prompt that will be sent to the conversational model.\n\nOnce the template is loaded, the function renders it using the render_template method, passing in the `data` dictionary. This step replaces placeholders in the template with actual values from the provided parameters, creating a contextually relevant prompt.\n\nThe rendered prompt is then sent to the common_chat method, which facilitates communication with the conversational model. This method processes the prompt and returns a response based on the updated context, effectively allowing the agent to generate a more informed and relevant answer.\n\nThe update_answer function is called within the main function of the project, specifically during the iterative process of refining the agent's responses based on feedback from a critic agent. After the initial response is generated, the update_answer method is invoked to incorporate new search results and feedback, ensuring that the agent's final output is polished and aligned with user expectations.\n\n**Note**: It is important to ensure that the parameters passed to the update_answer function are well-formed and relevant to the context of the conversation. The effectiveness of the function relies on the quality of the previous answer, search results, and critic feedback provided.\n\n**Output Example**: A possible return value from the update_answer function could be a string such as \"Based on the latest search results and your feedback, the updated answer is: The capital of France is Paris.\""
      ],
      "code_start_line": 163,
      "code_end_line": 176,
      "params": [
        "self",
        "query",
        "previous_answer",
        "search_results",
        "critic_feedback"
      ],
      "have_return": true,
      "code_content": "    def update_answer(self, query, previous_answer, search_results, critic_feedback):\n        data = {\n            \"query\": query,\n            \"previous_answer\": previous_answer,\n            \"search_results\": search_results,\n            \"critic_feedback\": critic_feedback,\n        }\n\n        agent_update_answer_prompt = self.load_template(\"agent_update_answer.txt\")\n        rendered_prompt = self.render_template(agent_update_answer_prompt, data)\n\n        agent_update_answer_response = self.common_chat(usr_prompt=rendered_prompt)\n\n        return agent_update_answer_response\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "model_confident",
      "md_content": [
        "**model_confident**: The function of model_confident is to check whether the model is confident in its response to the current user query.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the user's question that needs to be evaluated for model confidence.\n\n**Code Description**: The model_confident function is designed to assess the confidence level of the model regarding a specific user query. It begins by constructing a data dictionary that includes the user's question under the key \"user_question\". The function then loads a confidence assessment template from a file named \"agent_confidence.txt\" using the load_template method. This template serves as a structured prompt for the model to evaluate its confidence.\n\nNext, the function renders the loaded template by passing the data dictionary to the render_template method, which replaces any placeholders in the template with the corresponding values from the dictionary. The rendered prompt is then sent to the model through the common_chat method, which facilitates the interaction with the model and retrieves the response.\n\nThe response obtained from common_chat is returned as the output of the model_confident function. This output indicates the model's confidence level regarding the provided query.\n\nThe model_confident function is called within the main function of the project, specifically during the first iteration of a loop that manages multiple interactions with the model. In this context, it is used to determine whether the model is confident enough to provide an answer directly or if further actions, such as searching for additional information, are necessary. If the model is deemed confident, the common_chat function is invoked to obtain the answer. Conversely, if the model lacks confidence, the process involves generating a search prompt and retrieving supplementary data before answering the user.\n\n**Note**: It is essential to ensure that the query parameter passed to the model_confident function is well-formed and relevant to the context of the conversation to receive an accurate confidence assessment.\n\n**Output Example**: A possible return value from the model_confident function could be a string indicating the model's confidence level, such as \"true\" or \"false\", depending on the evaluation of the user query.",
        "**model_confident**: The function of model_confident is to check whether the model is confident in its response to the current user query.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the user's question that needs to be evaluated for the model's confidence.\n\n**Code Description**: The model_confident function is a method within the BaseAgent class that assesses the confidence level of the model regarding a specific user query. It takes a single parameter, query, which is the user's question. The function begins by creating a data dictionary that includes the user's question under the key \"user_question\". \n\nNext, it calls the load_template method to retrieve a template from the prompts directory, specifically the \"agent_confidence.txt\" file. This template is used to formulate a prompt that will be sent to the conversational model. The rendered prompt is generated by passing the loaded template and the data dictionary to the render_template method, which replaces placeholders in the template with actual values from the data dictionary.\n\nOnce the rendered prompt is prepared, the function invokes the common_chat method, passing the rendered prompt as the usr_prompt parameter. This method facilitates communication with the conversational model, sending the prompt and receiving a response that indicates the model's confidence level regarding the user's query.\n\nThe model_confident function is called within the main function of the project, specifically during the first iteration of a loop that processes user tasks. After initializing the task, the model_confident function is invoked to determine if the model is confident enough to provide a direct answer to the user's question. If the model indicates confidence, the common_chat method is called again to retrieve the answer. Conversely, if the model is not confident, the function initiates a search process to gather more information before attempting to answer the query.\n\nThis function plays a critical role in ensuring that the agent only provides answers when it is confident in the information, thereby enhancing the reliability of the responses generated by the conversational model.\n\n**Note**: It is essential to ensure that the query parameter passed to the model_confident function is well-formed and relevant to the context of the conversation. This will help in obtaining an accurate assessment of the model's confidence level.\n\n**Output Example**: A possible return value from the model_confident function could be a string indicating the model's confidence level, such as:\n```\n\"Confidence: true\"\n```"
      ],
      "code_start_line": 178,
      "code_end_line": 188,
      "params": [
        "self",
        "query"
      ],
      "have_return": true,
      "code_content": "    def model_confident(self, query):\n        \"\"\"\n        检查模型是否对当前问题有信心。\n        \"\"\"\n        data = {\"user_question\": query}\n        agent_confidence_prompt = self.load_template(\"agent_confidence.txt\")\n\n        rendered_prompt = self.render_template(agent_confidence_prompt, data)\n        agent_confidence_response = self.common_chat(usr_prompt=rendered_prompt)\n\n        return agent_confidence_response\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "web_scrape_results",
      "md_content": [
        "**web_scrape_results**: The function of web_scrape_results is to extract web content from search results using a web scraper.\n\n**parameters**: The parameters of this Function.\n· search_results: A string representing the initial search results to scrape from.\n\n**Code Description**: The web_scrape_results function is a method within the BaseAgent class that facilitates the extraction of web content based on initial search results provided as input. The function begins by loading a template for the web scraper using the load_template method, which retrieves the content of a specified template file from the prompts directory. This template is then rendered with the user question and the initial search results using the render_template method, which formats the template string by replacing placeholders with actual values from the provided data.\n\nFollowing the preparation of the rendered prompt, the function interacts with a conversational model through the common_chat method, sending the rendered prompt along with any necessary tools defined in the content_scraper_schema. The response from this interaction is analyzed to determine if any tool calls were made. If no tool calls are present, the function returns the content of the response directly.\n\nIn cases where tool calls are included in the response, the function appends these calls to the conversation history using the append_tool_call_to_history method. It then initializes an empty string to accumulate the final web scraping results. For each tool call, the function extracts the URLs from the arguments and invokes the content scraper's scrape method asynchronously to retrieve the content from the specified URLs. The results of these scraping operations are logged into the conversation history using the append_tool_call_result_to_history method.\n\nThe web_scrape_results function is called within the search_and_browse method of the BaseAgent class, which orchestrates a two-step process of searching for information and subsequently scraping content based on the search results. This highlights the function's role in enhancing the agent's ability to provide accurate and relevant responses by leveraging real-time data from web sources.\n\n**Note**: It is essential to ensure that the search_results parameter is well-formed and contains relevant queries for the scraping process to function effectively. Additionally, proper error handling should be implemented to manage cases where the scraping operations do not yield results.\n\n**Output Example**: A possible return value from the web_scrape_results function could be a string formatted as follows:\n```\n\"Here are the results from your search: 1. Title: Example Article, URL: http://example.com/article1, Content: This is a summary of the article. 2. Title: Another Example, URL: http://example.com/article2, Content: This is another summary.\"\n```"
      ],
      "code_start_line": 190,
      "code_end_line": 234,
      "params": [
        "self",
        "search_results"
      ],
      "have_return": true,
      "code_content": "    def web_scrape_results(self, search_results: str) -> str | None:\n        \"\"\"Extract web content from search results using web scraper\n\n        Args:\n            search_results: Initial search results to scrape from\n\n        Returns:\n            Scraped web content or None if scraping failed\n        \"\"\"\n        web_scraper_prompt = self.load_template(\"web_scraper.txt\")\n        web_scraper_rendered_prompt = self.render_template(\n            web_scraper_prompt,\n            {\n                \"user_question\": self.user_question,\n                \"initial_search_results\": search_results,\n            },\n        )\n\n        # Interact with the model for web scraping\n        web_scraper_response = self.common_chat(\n            usr_prompt=web_scraper_rendered_prompt,\n            tools=self.content_scraper_schema,\n        )\n\n        # If no tool calls, return the response immediately\n        if web_scraper_response.tool_calls is None:\n            return web_scraper_response.content\n\n        BaseAgent.conversation_manager.append_tool_call_to_history(\n            web_scraper_response.tool_calls\n        )\n\n        final_web_scraper_results = \"\"\n\n        for tool_call in web_scraper_response.tool_calls:\n            urls = json.loads(tool_call.function.arguments).get(\"urls\", [])\n            web_scraper_results = asyncio.run(self.content_scraper.scrape(urls=urls))\n            BaseAgent.conversation_manager.append_tool_call_result_to_history(\n                tool_call_id=tool_call.id,\n                name=\"scrape\",\n                content=web_scraper_results,\n            )\n            final_web_scraper_results += web_scraper_results\n\n        return final_web_scraper_results\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)",
        "src/criticsearch/models.py/ConversationManager/append_tool_call_to_history",
        "src/criticsearch/models.py/ConversationManager/append_tool_call_result_to_history",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "search_and_browse",
      "md_content": [
        "**search_and_browse**: The function of search_and_browse is to perform a search and web scraping operation based on a rendered prompt, returning the final results as a string.\n\n**parameters**: The parameters of this Function.\n· rendered_prompt: A string that contains the prompt to be processed for searching and browsing.\n\n**Code Description**: The search_and_browse function is a method within the BaseAgent class that orchestrates a two-step process: searching for information using a search aggregator and then scraping content from the web based on the search results. \n\n1. The function begins by invoking the common_chat method with the provided rendered_prompt and the search_aggregator_schema. This interaction initiates a search operation, and the response is logged for debugging purposes.\n\n2. If the search_with_tool_response indicates that no tool calls were made, the function immediately returns the content of the response. This allows for quick exit when no further action is necessary.\n\n3. If tool calls are present, the function appends these calls to the conversation history using the append_tool_call_to_history method. This ensures that all interactions with tools are recorded for future reference.\n\n4. The function then initializes an empty string to accumulate the final search results. It iterates over each tool call, extracting the query from the tool call's arguments. For each query, it calls the search method of the search_aggregator to perform the search asynchronously.\n\n5. After a brief pause to manage rate limits, the results from each search are appended to the conversation history using append_tool_call_result_to_history, and the query is updated in the query database.\n\n6. Once all search results are collected, the function loads a web scraper template and renders it with the user's question and the initial search results. This rendered prompt is then sent to the common_chat method to interact with the content scraper.\n\n7. Similar to the search step, if the web_scraper_response contains no tool calls, the function returns the content directly. If tool calls are present, they are appended to the conversation history.\n\n8. The function then processes each tool call related to web scraping, extracting URLs from the arguments and calling the scrape method of the content scraper to gather content from these URLs.\n\n9. The results from the scraping operation are also logged into the conversation history, and the final results are concatenated into a single string, which is returned at the end of the function.\n\nThe search_and_browse function is called within the main function of the project, specifically when the agent is not confident in its initial answer and needs to gather additional information through searching and scraping. This highlights its role in enhancing the agent's ability to provide accurate and relevant responses based on real-time data.\n\n**Note**: It is essential to ensure that the rendered_prompt is well-formed and contains relevant queries for both the search and scraping processes to function effectively. Additionally, proper error handling should be implemented to manage cases where the search or scraping operations do not yield results.\n\n**Output Example**: A possible return value of the search_and_browse function could be a string formatted as follows:\n```\n\"Here are the results from your search: 1. Title: Example Article, URL: http://example.com/article1, Content: This is a summary of the article. 2. Title: Another Example, URL: http://example.com/article2, Content: This is another summary.\"\n```",
        "**search_and_browse**: The function of search_and_browse is to perform a search based on a user-provided prompt and subsequently scrape web content from the search results.\n\n**parameters**: The parameters of this Function.\n· rendered_prompt: A string that contains the user prompt formatted for the search operation.\n\n**Code Description**: The search_and_browse function is a method within the BaseAgent class that orchestrates a two-step process: first, it conducts a search using a conversational model based on the provided user prompt, and second, it scrapes web content from the results of that search. \n\nThe function begins by invoking the common_chat method, passing the rendered_prompt and a schema of tools (search_aggregator_schema) that may be utilized during the search interaction. The response from this method, search_with_tool_response, contains the results of the search operation.\n\nIf the search_with_tool_response indicates that no tool calls were made (i.e., tool_calls is None), the function returns the content of the response directly. This allows for immediate feedback to the user without further processing.\n\nIn cases where tool calls are present, the function appends these calls to the conversation history using the append_tool_call_to_history method from the ConversationManager class. This ensures that all interactions with external tools are logged for future reference.\n\nThe function then initializes an empty string, final_search_results, to accumulate the results from each tool call. It iterates through the tool calls, extracting the search query from the tool call's arguments. For each query, it invokes the search method from the SearchAggregator class asynchronously to perform the actual search. The results are awaited, and a brief sleep is introduced to manage rate limits.\n\nAfter obtaining the search results, the function logs the results into the conversation history using the append_tool_call_result_to_history method. It also updates the query database with the executed query using the update method from the queryDB.\n\nFinally, the function compiles all the search results into the final_search_results string and returns the results after processing them through the web_scrape_results method. This method is responsible for extracting relevant web content based on the search results, enhancing the overall response provided to the user.\n\nThe search_and_browse function is called within the process_single_task function, where it is utilized to refine the agent's responses based on user feedback and additional search queries. This highlights its role in ensuring that the agent can provide accurate and relevant information by leveraging real-time data from web sources.\n\n**Note**: It is important to ensure that the rendered_prompt parameter is well-formed and relevant to the context of the search. Proper error handling should be implemented to manage cases where the search operation does not yield valid results.\n\n**Output Example**: A possible return value from the search_and_browse function could be a string formatted as follows:\n```\n\"Here are the results from your search: 1. Title: Latest Tech Innovations, URL: http://example.com/latest-tech, Content: Discover the newest advancements in technology. 2. Title: AI in Healthcare, URL: http://example.com/ai-healthcare, Content: Explore how AI is transforming the healthcare industry.\"\n```"
      ],
      "code_start_line": 236,
      "code_end_line": 270,
      "params": [
        "self",
        "rendered_prompt"
      ],
      "have_return": true,
      "code_content": "    def search_and_browse(self, rendered_prompt) -> str | None:\n        search_with_tool_response = self.common_chat(\n            usr_prompt=rendered_prompt, tools=self.search_aggregator_schema\n        )\n\n        printer.log(f\"search_with_tool_response:\\n{search_with_tool_response}\")\n\n        # If no tool calls, return the response immediately\n        if search_with_tool_response.tool_calls is None:\n            return search_with_tool_response.content\n\n        BaseAgent.conversation_manager.append_tool_call_to_history(\n            search_with_tool_response.tool_calls\n        )\n\n        final_search_results = \"\"\n\n        for tool_call in search_with_tool_response.tool_calls:\n            query = json.loads(tool_call.function.arguments).get(\"query\", \"\")\n\n            search_results = asyncio.run(self.search_aggregator.search(query=query))\n\n            time.sleep(0.2)\n\n            BaseAgent.conversation_manager.append_tool_call_result_to_history(\n                tool_call_id=tool_call.id,\n                name=\"search\",\n                content=search_results,\n            )\n\n            BaseAgent.queryDB.update(query)\n\n            final_search_results += f\"{search_results}\"\n\n        return self.web_scrape_results(final_search_results)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/common_chat",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/models.py/ConversationManager/append_tool_call_to_history",
        "src/criticsearch/models.py/ConversationManager/append_tool_call_result_to_history",
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "receive_task",
      "md_content": [
        "**receive_task**: The function of receive_task is to accept and store the original task provided to the agent.\n\n**parameters**: The parameters of this Function.\n· task: The original task that needs to be received and stored by the agent.\n\n**Code Description**: The receive_task function is a method of the BaseAgent class, designed to receive a task as input and store it in the instance variable original_task. This function is crucial for the operation of the agent, as it allows the agent to keep track of the task it is currently handling. When the function is called, it takes the task parameter and assigns it to the instance variable self.original_task, effectively saving the task for future reference or processing.\n\nIn the context of the project, the receive_task function is invoked by the CriticAgent within the main function of the application. Specifically, after the common agent generates an answer to the user's question, the CriticAgent receives the original task (TASK) using the receive_task method. This step is essential for the CriticAgent to evaluate the performance of the common agent based on the task it was given. By storing the original task, the CriticAgent can provide feedback and suggestions for improvement, thereby enhancing the overall interaction and effectiveness of the agents involved in the task processing.\n\n**Note**: It is important to ensure that the task passed to the receive_task function is well-defined and relevant to the agent's capabilities, as this will directly influence the quality of the agent's performance and the feedback provided by the CriticAgent.",
        "**receive_task**: The function of receive_task is to accept and store the original task provided to the agent.\n\n**parameters**: The parameters of this Function.\n· task: The original task that is being received and stored by the agent.\n\n**Code Description**: The receive_task function is a method defined within the BaseAgent class. Its primary purpose is to accept a task input, which is expected to be a string or a structured object representing the task details. Upon invocation, the function assigns the input task to the instance variable original_task, effectively storing it for later use within the agent's operations.\n\nThis function plays a critical role in the workflow of the BaseAgent, as it serves as the initial point of interaction where the agent receives the task it needs to process. The stored task can later be utilized in various methods of the BaseAgent, such as when generating responses or conducting searches based on the task's requirements.\n\nThe receive_task function is called by the CriticAgent within the process_single_task function located in the src/criticsearch/main.py file. In this context, the CriticAgent is responsible for evaluating the task and the responses generated by the common agent. By invoking receive_task, the CriticAgent ensures that it has access to the original task, which is essential for providing accurate feedback and evaluations based on the task's context.\n\n**Note**: It is important to ensure that the task parameter passed to receive_task is well-structured and relevant to the agent's capabilities, as this will directly influence the effectiveness of the agent's subsequent operations."
      ],
      "code_start_line": 272,
      "code_end_line": 276,
      "params": [
        "self",
        "task"
      ],
      "have_return": false,
      "code_content": "    def receive_task(self, task):\n        \"\"\"\n        接收原始任务。\n        \"\"\"\n        self.original_task = task\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_and_validate_yaml",
      "md_content": [
        "**extract_and_validate_yaml**: The function of extract_and_validate_yaml is to extract YAML content from a string and validate it.\n\n**parameters**: The parameters of this Function.\n· model_response: A string containing the response from a model which may include YAML content wrapped in code block markers.\n\n**Code Description**: \nThe function `extract_and_validate_yaml` is designed to extract YAML data from a given string, `model_response`, and return it in a structured format if valid. It performs the following steps:\n\n1. **Regular Expression Matching**: The function begins by using a regular expression to search for content between triple backticks ` ```yaml ` and ` ``` `, which is expected to be YAML content. The regular expression `r\"```yaml\\n([\\s\\S]*?)\\n```\"` is used to identify the YAML content enclosed within the code block. If no match is found, it returns `None`.\n\n2. **YAML Parsing**: Once the YAML content is extracted, the function attempts to parse it using `yaml.safe_load`. This function safely loads the YAML content into a Python dictionary or data structure. If the content is not valid YAML, a `yaml.YAMLError` is caught and the error message is printed, returning `None`.\n\n3. **Formatting and Returning**: If the YAML content is successfully parsed, it is re-serialized into a YAML formatted string using `yaml.dump`, and the result is returned. This output is presented in a human-readable YAML format, with the default flow style set to `False`.\n\nIn the context of the broader project, this function is typically used in situations where model responses or other content need to be processed for YAML data. For example, in the `critic` method of the `CriticAgent` class, this function is called to extract and validate YAML content from a model response. Similarly, in the `main` function, it is used to process agent confidence data in YAML format.\n\n**Note**: It is important to ensure that the model response contains valid YAML content, as invalid or improperly formatted YAML will cause the function to return `None` and may trigger error handling in the calling code.\n\n**Output Example**: \nGiven a valid `model_response` such as:\n\n```\n```yaml\nconfidence: true\n```\n```\n\nThe function would return:\n\n```\nconfidence: true\n```",
        "**extract_and_validate_yaml**: The function of extract_and_validate_yaml is to extract YAML content from a given string and validate its format.\n\n**parameters**: The parameters of this Function.\n· model_response: A string containing the response from a model, which is expected to include YAML content wrapped in specific delimiters.\n\n**Code Description**: The extract_and_validate_yaml function is a method within the BaseAgent class that processes a string input, searching for YAML content encapsulated within triple backticks (```yaml```). The function utilizes regular expressions to identify and extract the relevant portion of the string. If the expected YAML content is not found, the function returns None, indicating a failure to extract valid content.\n\nOnce the YAML content is extracted, the function attempts to parse it using the yaml.safe_load method. This method is designed to safely parse YAML strings into Python objects. If the parsing is successful, the function returns a formatted YAML string using yaml.dump, which can be utilized for further processing or output. However, if a yaml.YAMLError occurs during parsing, the function catches this exception, prints an error message indicating the invalid content, and returns None.\n\nThe extract_and_validate_yaml function is called by other methods within the project, such as the critic method in the CriticAgent class. In this context, it is used to validate and extract YAML feedback from the model's response after generating a critique based on user input and agent responses. The successful extraction and validation of YAML content are crucial for the flow of information between the CriticAgent and the BaseAgent, as it directly influences the feedback mechanism and the overall interaction quality.\n\n**Note**: It is essential to ensure that the model_response string contains valid YAML content wrapped in the correct delimiters. If the content is not valid YAML, the function will return None, which may disrupt the expected flow of the application.\n\n**Output Example**: A possible return value from the extract_and_validate_yaml function could be a YAML formatted string such as:\n\n```yaml\nfeedback: \"The agent's answer is comprehensive but lacks specific examples.\"\nsuggestions:\n  - \"Include more detailed explanations.\"\n  - \"Provide references to support claims.\"\n```"
      ],
      "code_start_line": 278,
      "code_end_line": 296,
      "params": [
        "self",
        "model_response"
      ],
      "have_return": true,
      "code_content": "    def extract_and_validate_yaml(self, model_response):\n        # 正则表达式匹配包裹在```yaml```之间的内容\n        import re\n\n        match = re.search(r\"```yaml\\n([\\s\\S]*?)\\n```\", model_response, re.DOTALL)\n\n        if not match:\n            return None  # 如果没有找到匹配的内容，返回None\n\n        model_response = match.group(1).strip()\n\n        try:\n            # 尝试解析YAML内容\n            parsed_yaml = yaml.safe_load(model_response)\n            return yaml.dump(parsed_yaml, default_flow_style=False)\n\n        except yaml.YAMLError as exc:\n            print(f\"Invalid YAML content: {exc}\")\n            return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_and_validate_json",
      "md_content": [
        "**extract_and_validate_json**: The function of extract_and_validate_json is to extract JSON data from a given string and validate its correctness.\n\n**parameters**: The parameters of this Function.\n· model_response: A string that may contain JSON data, potentially wrapped in specific formatting.\n\n**Code Description**: The extract_and_validate_json function is designed to process a string input, model_response, which is expected to contain JSON data. The function first attempts to locate JSON content that is enclosed within ```json``` code blocks using a regular expression. If such a block is found, it extracts the content and removes any leading or trailing whitespace. If no such block is found, it assumes that the entire input string is the JSON content and trims any whitespace accordingly.\n\nOnce the JSON content is identified, the function attempts to parse it using the json.loads method. If the parsing is successful, the parsed JSON object is returned. However, if a json.JSONDecodeError occurs during parsing, indicating that the content is not valid JSON, the function prints an error message detailing the issue and returns None.\n\nThis function is called within the context of the BaseAgent class, specifically in methods such as process_single_task and main. In these methods, extract_and_validate_json is used to validate the structure of outlines generated from user queries or model responses. By ensuring that the outlines are valid JSON, the function plays a crucial role in maintaining the integrity of the data being processed, which is essential for subsequent operations such as flattening the outline and generating content for various sections.\n\n**Note**: It is important to ensure that the input string is formatted correctly to avoid JSON parsing errors. The function will return None if the input does not contain valid JSON, which should be handled appropriately by the calling functions to prevent further issues in the workflow.\n\n**Output Example**: A possible return value of the function could be a dictionary representing the parsed JSON, such as:\n{\n    \"sections\": [\n        {\n            \"title\": \"Introduction\",\n            \"content\": \"This is the introduction section.\"\n        },\n        {\n            \"title\": \"Conclusion\",\n            \"content\": \"This is the conclusion section.\"\n        }\n    ]\n}"
      ],
      "code_start_line": 298,
      "code_end_line": 313,
      "params": [
        "self",
        "model_response"
      ],
      "have_return": true,
      "code_content": "    def extract_and_validate_json(self, model_response):\n        # Try to extract JSON data wrapped in ```json``` blocks\n        # and return the parsed JSON content\n        match = re.search(r\"```json\\n([\\s\\S]*?)\\n```\", model_response, re.DOTALL)\n        if match:\n            json_content = match.group(1).strip()\n        else:\n            json_content = model_response.strip()\n\n        try:\n            parsed_json = json.loads(json_content)\n            return parsed_json\n\n        except json.JSONDecodeError as exc:\n            print(f\"Invalid JSON content: {exc}\")\n            return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/models.py": [
    {
      "type": "ClassDef",
      "name": "HistoryItem",
      "md_content": [
        "**HistoryItem**: The function of HistoryItem is to represent an individual entry in a conversation history.\n\n**attributes**: The attributes of this Class.\n· role: Literal[\"user\", \"assistant\", \"tool\", \"critic\"]  \n   This attribute defines the role of the entity in the conversation, such as \"user\", \"assistant\", \"tool\", or \"critic\". The role helps categorize the message within the conversation history.  \n· content: Optional[str]  \n   This attribute holds the actual content of the message. It is optional and may be `None` if not provided.  \n· tool_calls: Optional[List[ChatCompletionMessageToolCall]]  \n   This attribute stores a list of tool calls associated with the message. It is optional and only used when the message involves invoking tools.  \n· tool_call_id: Optional[str]  \n   This attribute holds a unique identifier for a tool call. It is optional and is used to track the specific tool call in case the message involves a tool.  \n· name: Optional[str]  \n   This attribute is used to store the name associated with the history item. It is optional and can be used to provide additional context about the message.\n\n**Code Description**:  \nThe `HistoryItem` class is a data model that encapsulates a single entry within a conversation. It has a `role` attribute that defines who or what is contributing to the conversation, such as the user, assistant, tool, or critic. The `content` attribute contains the message's text, and it can be optional if no message content is provided. The `tool_calls` attribute, also optional, is used when a message involves calling a tool or a function, storing details about the tool calls made. Similarly, the `tool_call_id` serves as a unique identifier for each tool call, providing a way to trace or reference specific tool interactions. Finally, the `name` attribute can store a custom name for the entry, providing further context if required.\n\nThis class plays a vital role in managing conversation history, particularly in the context of applications where various entities interact through a series of messages. It integrates directly with the `ConversationManager` class, which manages the overall conversation history and tools used within it. Specifically, the `HistoryItem` class is used when appending new entries to the conversation history, either through regular messages or tool calls, ensuring that each entry is well-defined with relevant context (such as role, content, tool calls, etc.).\n\nIn the `ConversationManager` class, the `HistoryItem` is instantiated and appended to the `history` attribute whenever a new message is added to the conversation. For instance, the `append_to_history` method in `ConversationManager` creates a new `HistoryItem` with the given role and content, and then adds it to the history list. The `serialize_history` method, which handles the serialization of the conversation history, processes these `HistoryItem` instances to ensure that the history is accurately saved, potentially excluding any `None` values during serialization.\n\nFurthermore, in specific cases where tool calls are involved, the `HistoryItem` can hold a list of `tool_calls` and a `tool_call_id`, providing a way to capture the specific details of any tools or functions invoked during the conversation.\n\n**Note**:  \n- The `content` attribute can be omitted for certain roles, such as \"tool\" or \"critic\", where the primary focus is on the interaction rather than the message content.  \n- The use of `tool_calls` and `tool_call_id` is essential when the conversation involves automated tools or functions, ensuring that these interactions are tracked and referenced correctly.  \n- The optional nature of many attributes (like `content`, `tool_calls`, `tool_call_id`, and `name`) allows for flexibility, enabling this class to accommodate various types of messages and interactions in a conversation history."
      ],
      "code_start_line": 21,
      "code_end_line": 26,
      "params": [],
      "have_return": false,
      "code_content": "class HistoryItem(BaseModel):\n    role: Literal[\"user\", \"assistant\", \"tool\", \"critic\"]\n    content: Optional[str] = None\n    tool_calls: Optional[List[ChatCompletionMessageToolCall]] = None\n    tool_call_id: Optional[str] = None\n    name: Optional[str] = None\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/models.py/ConversationManager",
        "src/criticsearch/models.py/ConversationManager/serialize_history",
        "src/criticsearch/models.py/ConversationManager/append_to_history"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "ConversationManager",
      "md_content": [
        "**ConversationManager**: The function of ConversationManager is to manage and maintain the conversation history, including the ability to serialize this history, append new messages, and handle tool calls within the conversation.\n\n**attributes**: The attributes of this Class.\n· history: List[HistoryItem]  \n   This attribute stores the conversation history as a list of HistoryItem instances, which represent individual entries in the conversation.\n\n· max_history_length: int  \n   This attribute defines the maximum number of entries to retain in the conversation history, limiting the size of the history to the most recent entries.\n\n· available_tools: List  \n   This attribute holds a list of tools that can be utilized during the conversation, allowing for integration of various functionalities.\n\n· save_path: Path  \n   This attribute specifies the file path where the conversation history will be saved in JSON Lines format.\n\n· delete_on_init: bool  \n   This attribute is a flag that indicates whether to delete the existing conversation history file upon initialization of the ConversationManager.\n\n**Code Description**: The ConversationManager class is designed to facilitate the management of conversation history in applications that involve interactions between users and automated systems. It extends from BaseModel, allowing it to leverage data modeling capabilities. Upon initialization, the class checks if the specified save_path exists and deletes it if the delete_on_init flag is set to True. This ensures that any previous conversation history is cleared when a new instance is created.\n\nThe class provides methods to append messages to the history, including the append_to_history method, which allows adding new entries with specified roles (user, assistant, tool, or critic) and optional content. It also includes specialized methods for handling tool calls, such as append_tool_call_to_history and append_tool_call_result_to_history, which ensure that interactions with tools are accurately recorded in the history.\n\nSerialization of the conversation history is handled through the serialize_history method, which prepares the history for saving by excluding any None values and limiting the output to the most recent entries based on max_history_length. Additionally, the custom_serialize method allows for context-specific serialization, particularly for formats like ShareGPT, transforming the history into a structured format suitable for sharing.\n\nThe write method is responsible for saving the conversation history to the specified file, handling both the creation of the file and the appending of new data to existing content. The _auto_save method ensures that the most recent entry in the history is automatically saved after each update.\n\nThe ConversationManager is utilized within the BaseAgent class, where it serves as a central component for managing conversation interactions. The BaseAgent class creates an instance of ConversationManager, allowing it to append messages and tool calls to the history as part of its operations. This integration ensures that all interactions, whether user prompts or tool responses, are systematically recorded, providing a comprehensive log of the conversation flow.\n\n**Note**: It is important to ensure that the max_history_length is set appropriately to avoid excessive memory usage, especially in long-running applications. The delete_on_init flag should be used with caution, as it will permanently remove any existing conversation history upon initialization.\n\n**Output Example**: A possible appearance of the code's return value when serializing the conversation history might look like this:\n```json\n{\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": \"Hello, how can I help you?\"},\n    {\"from\": \"function_call\", \"value\": \"{\\\"name\\\": \\\"search\\\", \\\"arguments\\\": {\\\"query\\\": \\\"latest news\\\"}}\"},\n    {\"from\": \"observation\", \"value\": \"{\\\"result\\\": \\\"Here are the latest news articles...\\\"}\"}\n  ],\n  \"tools\": \"[{\\\"name\\\": \\\"search\\\", \\\"description\\\": \\\"Search for information\\\"}]\"\n}\n```",
        "**ConversationManager**: The function of ConversationManager is to manage the conversation history, including storing, serializing, and manipulating entries of conversation.\n\n**attributes**: The attributes of this Class.\n· history: List[HistoryItem]  \n   This attribute holds the conversation history as a list of HistoryItem instances, representing individual entries in the conversation.  \n· max_history_length: int  \n   This attribute sets a limit on the number of entries that can be stored in the conversation history, with a default value of 10.  \n· available_tools: List  \n   This attribute maintains a list of tools that are available for use within the conversation context.  \n· save_path: Path  \n   This attribute specifies the file path where the conversation history will be saved, defaulting to \"conversation_history.jsonl\".  \n· delete_on_init: bool  \n   This attribute is a flag that indicates whether to delete the existing conversation history file upon initialization of the ConversationManager instance.\n\n**Code Description**: The ConversationManager class is designed to facilitate the management of conversation history in applications that involve interactive dialogue, such as chatbots or virtual assistants. Upon initialization, the class checks if the specified save path for the conversation history file exists and, if the delete_on_init flag is set to True, deletes the file to start fresh. This ensures that each instance of ConversationManager can begin with a clean slate if desired.\n\nThe class provides several key methods for handling conversation history. The `serialize_history` method is responsible for serializing the history entries, ensuring that only the most recent entries, up to the defined max_history_length, are included in the output. This is particularly useful for maintaining a manageable size of the history data.\n\nThe `custom_serialize` method allows for tailored serialization logic based on the context, such as transforming the history into a specific format for external systems like ShareGPT. This method processes each HistoryItem, mapping roles to specific formats and handling tool calls appropriately.\n\nThe `write` method is used to save the conversation history to a specified file path, creating the necessary directories if they do not exist. It reads existing data from the file, appends new entries, and writes the updated history back to the file in JSON format.\n\nThe `_auto_save` method automatically saves the latest entry to the specified save path after each update, ensuring that the conversation history is consistently backed up.\n\nThe `append_to_history` method allows for adding new messages to the conversation history, while `append_tool_call_to_history` and `append_tool_call_result_to_history` specifically handle entries related to tool calls, ensuring that all interactions are accurately recorded.\n\nThe `clear_history` method provides functionality to clear the entire conversation history, resetting the state of the ConversationManager.\n\nThe ConversationManager is utilized within the BaseAgent class, which serves as a foundational class for intelligent agents. The conversation_manager attribute in BaseAgent is an instance of ConversationManager, allowing the agent to maintain a record of interactions with users and tools. This integration enables the agent to append messages and tool calls to the history, facilitating a coherent dialogue experience.\n\n**Note**: It is important to manage the max_history_length attribute to prevent excessive memory usage and ensure that only relevant conversation entries are retained. The delete_on_init flag should be used cautiously, as it will permanently remove any existing conversation history upon initialization. Proper handling of the save_path is also crucial to avoid file access issues.\n\n**Output Example**: A possible appearance of the code's return value when serializing the conversation history might look like this:\n```json\n{\n  \"history\": [\n    {\"role\": \"user\", \"content\": \"Hello, how can I help you?\"},\n    {\"role\": \"assistant\", \"content\": \"I need assistance with my order.\"}\n  ],\n  \"tools\": []\n}\n```"
      ],
      "code_start_line": 30,
      "code_end_line": 191,
      "params": [],
      "have_return": true,
      "code_content": "class ConversationManager(BaseModel):\n    history: List[HistoryItem] = []\n    max_history_length: int = 10  # Limit for conversation history\n    available_tools: List = []\n    save_path: Path = Path(\"conversation_history.jsonl\")\n    delete_on_init: bool = True  # Flag to delete file on initialization\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        if self.delete_on_init and self.save_path.exists():\n            try:\n                self.save_path.unlink(missing_ok=True)  # Delete the file if it exists\n                printer.log(f\"Deleted existing file: {self.save_path}\")\n            except Exception:\n                printer.print_exception(f\"Failed to delete file {self.save_path}\")\n                raise\n        # Set the flag to False to avoid further deletions\n        self.delete_on_init = False\n\n    @field_serializer(\"history\")\n    def serialize_history(self, history: List[HistoryItem]):\n        serialized_history = []\n        for item in history[-self.max_history_length :]:\n            serialized_history.append(item.model_dump(exclude_none=True))\n        return serialized_history\n\n    @model_serializer(mode=\"wrap\")\n    def custom_serialize(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ):\n        \"\"\"\n        Custom serialization logic that handles different contexts, such as 'sharegpt'.\n        \"\"\"\n        # Perform default serialization\n        result = handler(self)\n\n        result = result[\"history\"]\n\n        if info.context and info.context.get(\"sharegpt\"):\n            # Transform history into ShareGPT format\n            # TODO: human 和 observation 必须出现在奇数位置，gpt 和 function 必须出现在偶数位置\n            conversations = []\n            for item in self.history:\n                role_mapping = {\n                    \"user\": \"human\",\n                    \"assistant\": \"function_call\",\n                    \"tool\": \"observation\",\n                    \"critic\": \"critic\",\n                }\n\n                role = role_mapping.get(item.role, item.role)\n                value = item.content if item.content else None\n\n                # For tool calls, include the tool_call_id or arguments\n                if role == \"function_call\":\n                    if item.tool_calls:\n                        for tool_call in item.tool_calls:\n                            value = json.dumps(\n                                {\n                                    \"name\": tool_call.function.name,\n                                    \"arguments\": tool_call.function.arguments,\n                                },\n                                ensure_ascii=True,\n                            )\n\n                            conversations.append({\"from\": role, \"value\": value})\n                    else:\n                        conversations.append({\"from\": \"gpt\", \"value\": value})\n\n                elif role == \"observation\":\n                    conversations.append(\n                        {\"from\": role, \"value\": json.dumps(value, ensure_ascii=True)}\n                    )\n\n                else:\n                    conversations.append({\"from\": role, \"value\": value})\n\n            # Build final output structure\n            result = {\n                \"conversations\": conversations,\n                \"tools\": json.dumps(self.available_tools, ensure_ascii=True),\n            }\n\n        return result\n\n    def write(self, data: dict, path: Path | str):\n        if isinstance(path, str):\n            path = Path(path)\n\n        try:\n            # Create parent directories if they do not exist\n            path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Check if the file exists and read existing data\n            if path.exists():\n                with path.open(\"r\", encoding=\"utf-8\") as f:\n                    try:\n                        # Try to parse the existing JSON data (expecting a list at the top level)\n                        existing_data = json.load(f)\n                        if not isinstance(existing_data, list):\n                            existing_data = []  # Ensure it's a list\n                    except json.JSONDecodeError:\n                        # If the file is empty or corrupt, start with an empty list\n                        existing_data = []\n            else:\n                existing_data = []\n\n            # Append the new data to the existing array\n            existing_data.append(data)\n\n            # Write the updated array back to the file\n            with path.open(\"w\", encoding=\"utf-8\") as f:\n                json.dump(existing_data, f, ensure_ascii=True, indent=2)\n\n        except Exception:\n            printer.print_exception(f\"Failed to write to {path}\")\n            raise\n\n    def _auto_save(self):\n        \"\"\"Auto save after each update if save_path is set\"\"\"\n        if settings.save_sharegpt:\n            self.write(\n                path=self.save_path, data=self.history[-1].model_dump(exclude_none=True)\n            )\n\n    def append_to_history(\n        self,\n        role: Literal[\"user\", \"assistant\", \"tool\", \"critic\"],\n        content: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Add a new message to the conversation history.\n        \"\"\"\n        self.history.append(HistoryItem(role=role, content=content, **kwargs))\n        self._auto_save()\n\n    def append_tool_call_to_history(\n        self,\n        tool_calls: List[ChatCompletionMessageToolCall],\n        content: Optional[str] = None,\n    ):\n        \"\"\"\n        Add a tool call entry to the conversation history.\n        \"\"\"\n        self.append_to_history(role=\"assistant\", tool_calls=tool_calls, content=content)\n\n    def append_tool_call_result_to_history(\n        self, tool_call_id: str, name: str, content: str\n    ):\n        \"\"\"\n        Add a tool call result to the conversation history.\n        \"\"\"\n        self.append_to_history(\n            role=\"tool\", tool_call_id=tool_call_id, name=name, content=content\n        )\n\n    def clear_history(self):\n        \"\"\"\n        Clear the entire conversation history.\n        \"\"\"\n        self.history.clear()\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py",
        "src/criticsearch/base_agent.py/BaseAgent"
      ],
      "reference_who": [
        "src/criticsearch/models.py/HistoryItem"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the object and manage file deletion if certain conditions are met.\n\n**parameters**: \n· data: This parameter is passed as keyword arguments (**kwargs**) to initialize the base class.\n\n**Code Description**: \nThe `__init__` function in this case is a constructor that serves to initialize an object. It first calls the constructor of the superclass using `super().__init__(**data)` to ensure that any initialization defined in the parent class is executed. The keyword arguments passed to this constructor are forwarded to the parent class.\n\nThe function then checks the condition `if self.delete_on_init and self.save_path.exists()`. If the `delete_on_init` flag is set to `True` and a file at the path specified by `self.save_path` exists, the function attempts to delete the file using `self.save_path.unlink()`. If the file is successfully deleted, a log message is created with the information that the file was deleted. If the deletion operation fails (e.g., due to insufficient permissions or other issues), the function catches the exception and logs an error message containing the exception details.\n\nAfter attempting the deletion, the function sets `self.delete_on_init = False` to ensure that no further deletion attempts are made in subsequent invocations of the object. This is done to prevent repeated deletions of the same file unless explicitly required elsewhere in the class.\n\n**Note**: \n- The method assumes that `self.save_path` is a valid file path object that supports `.exists()` and `.unlink()` methods.\n- It is important that the `delete_on_init` flag is properly managed to avoid unintended file deletion during object initialization.\n- The use of `logger` for logging information and errors is crucial for debugging and understanding the state of the file deletion process.",
        "**__init__**: The function of __init__ is to initialize an instance of the class and manage the deletion of an existing file if certain conditions are met.\n\n**parameters**: The parameters of this Function.\n· data: dict - A variable-length keyword argument dictionary that contains initialization data for the instance.\n\n**Code Description**: The __init__ method is a constructor that is called when an instance of the class is created. It first invokes the superclass's __init__ method using `super().__init__(**data)`, which allows the parent class to initialize its attributes with the provided data. This is essential for ensuring that the instance inherits and properly initializes any attributes defined in the parent class.\n\nFollowing the superclass initialization, the method checks the `delete_on_init` attribute. If this attribute is set to True and the `save_path` (presumably a Path object) exists, the method attempts to delete the file at that path using `unlink(missing_ok=True)`. This operation removes the file if it exists, and if successful, it logs a message indicating the deletion using the `printer.log` method. The `printer.log` method is designed to print styled log messages to the console, enhancing visibility into the application's operations.\n\nIf an exception occurs during the file deletion process, the method catches the exception and calls `printer.print_exception`, which logs an error message and prints the exception details to the console. This ensures that any issues encountered during the deletion process are communicated clearly to the developer or user.\n\nAfter attempting to delete the file, the method sets the `delete_on_init` attribute to False. This action prevents further deletions from occurring during the lifecycle of the instance, ensuring that the file will not be deleted again unless explicitly handled.\n\nThe relationship with the `printer.log` and `printer.print_exception` methods is significant, as they provide a robust logging mechanism that aids in debugging and monitoring the application's behavior. The logging of both successful deletions and exceptions contributes to a clear understanding of the instance's initialization process and any potential issues that may arise.\n\n**Note**: It is important to ensure that the `delete_on_init` attribute is appropriately set before creating an instance of this class. Additionally, the `save_path` should be a valid Path object that points to a file location, as the deletion operation relies on its existence. Proper usage of this constructor contributes to effective resource management and error handling within the application."
      ],
      "code_start_line": 37,
      "code_end_line": 47,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, **data):\n        super().__init__(**data)\n        if self.delete_on_init and self.save_path.exists():\n            try:\n                self.save_path.unlink(missing_ok=True)  # Delete the file if it exists\n                printer.log(f\"Deleted existing file: {self.save_path}\")\n            except Exception:\n                printer.print_exception(f\"Failed to delete file {self.save_path}\")\n                raise\n        # Set the flag to False to avoid further deletions\n        self.delete_on_init = False\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/rich_output.py/RichPrinter/print_exception"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "serialize_history",
      "md_content": [
        "**serialize_history**: The function of serialize_history is to convert a list of conversation history items into a serialized format, excluding any attributes that are set to None.\n\n**parameters**: The parameters of this Function.\n· history: List[HistoryItem]  \n   This parameter represents a list of `HistoryItem` objects that encapsulate individual entries in a conversation history.\n\n**Code Description**: The `serialize_history` method is designed to process a list of `HistoryItem` instances, which represent entries in a conversation. The method takes a single parameter, `history`, which is expected to be a list of `HistoryItem` objects. The function first initializes an empty list called `serialized_history` to hold the serialized representations of the history items.\n\nThe method then iterates over the last `max_history_length` number of items from the `history` list. This is achieved by slicing the list with `history[-self.max_history_length :]`, which ensures that only the most recent entries are considered for serialization. For each `HistoryItem` in this sliced list, the method calls the `model_dump` method on the item, passing `exclude_none=True` as an argument. This call to `model_dump` is responsible for converting the `HistoryItem` into a dictionary format while omitting any attributes that have a value of None.\n\nFinally, the method returns the `serialized_history` list, which contains the serialized representations of the selected `HistoryItem` instances. This serialized data can be useful for saving conversation history in a structured format, such as for storage in a database or for transmission over a network.\n\nThe `serialize_history` method is closely related to the `HistoryItem` class, which defines the structure of each entry in the conversation history. By utilizing the `model_dump` method from the `HistoryItem`, this function ensures that the serialization process respects the attributes defined in the `HistoryItem` class, providing a consistent and accurate representation of the conversation history.\n\n**Note**: It is important to ensure that the `max_history_length` attribute is defined within the class that contains the `serialize_history` method, as it dictates how many of the most recent history items will be serialized. The method is particularly useful in scenarios where only a limited portion of the conversation history is needed, such as when displaying recent interactions to a user or when preparing data for logging purposes.\n\n**Output Example**: A possible appearance of the code's return value could be as follows:\n```json\n[\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the weather like today?\",\n        \"tool_calls\": null,\n        \"tool_call_id\": null,\n        \"name\": null\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"The weather is sunny with a high of 75°F.\",\n        \"tool_calls\": null,\n        \"tool_call_id\": null,\n        \"name\": null\n    }\n]\n```"
      ],
      "code_start_line": 50,
      "code_end_line": 54,
      "params": [
        "self",
        "history"
      ],
      "have_return": true,
      "code_content": "    def serialize_history(self, history: List[HistoryItem]):\n        serialized_history = []\n        for item in history[-self.max_history_length :]:\n            serialized_history.append(item.model_dump(exclude_none=True))\n        return serialized_history\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/models.py/HistoryItem"
      ],
      "special_reference_type": [
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "custom_serialize",
      "md_content": [
        "**custom_serialize**: The function of custom_serialize is to apply custom serialization logic to handle different contexts, particularly focusing on transforming the history into a ShareGPT format when specified.\n\n**parameters**:\n· handler: A SerializerFunctionWrapHandler responsible for performing default serialization of the object.\n· info: A SerializationInfo object containing the context information, which can determine whether the ShareGPT format should be applied.\n\n**Code Description**:  \nThe `custom_serialize` function begins by invoking the `handler` to perform default serialization on the object. This results in the `result` variable, which contains the serialized data, including the `history` attribute.\n\nIf the `info` parameter contains a context with a key \"sharegpt\", the function processes the history attribute further to format it for the ShareGPT use case. The logic involves iterating over each item in the `history` and remapping the roles using a predefined `role_mapping` dictionary. The roles are mapped from:\n- `\"user\"` to `\"human\"`\n- `\"assistant\"` to `\"function_call\"`\n- `\"tool\"` to `\"observation\"`\n- `\"critic\"` to `\"critic\"`\n\nFor each item, the corresponding value (either content or tool call data) is extracted. If the role is `\"function_call\"`, the function checks if tool calls are associated with the item and formats them as JSON, including the function name and its arguments. If no tool calls are present, the value is directly assigned with the content associated with the \"gpt\" role.\n\nFor `\"observation\"` roles, the content is serialized as a JSON string.\n\nAfter processing all items in the history, the `conversations` list is populated with the formatted role-value pairs. Finally, the `result` object is updated to include the conversations and a serialized version of the `available_tools` attribute. This final output is then returned.\n\n**Note**:  \n- The `result[\"history\"]` is assumed to be a list that contains historical interactions with objects that have `role` and `content` attributes.\n- The handling of tool calls in the `\"function_call\"` role relies on the presence of a `tool_calls` attribute, which is expected to be a list of tool call objects.\n- The format of the final output is specifically structured to accommodate ShareGPT's data format, which includes the key `\"conversations\"` and `\"tools\"`.\n\n**Output Example**:  \nHere is a mock-up of what the final output might look like when the `sharegpt` context is provided:\n\n```json\n{\n  \"conversations\": [\n    {\n      \"from\": \"human\",\n      \"value\": \"Hello, how are you?\"\n    },\n    {\n      \"from\": \"gpt\",\n      \"value\": \"I'm doing well, thank you for asking!\"\n    },\n    {\n      \"from\": \"function_call\",\n      \"value\": \"{\\\"name\\\": \\\"weatherApi.getForecast\\\", \\\"arguments\\\": {\\\"location\\\": \\\"New York\\\"}}\"\n    },\n    {\n      \"from\": \"observation\",\n      \"value\": \"{\\\"temperature\\\": 72, \\\"condition\\\": \\\"Sunny\\\"}\"\n    }\n  ],\n  \"tools\": \"{\\\"weatherApi\\\": {\\\"name\\\": \\\"weatherApi\\\", \\\"description\\\": \\\"Provides weather data\\\"}}\"\n}\n```"
      ],
      "code_start_line": 57,
      "code_end_line": 113,
      "params": [
        "self",
        "handler",
        "info"
      ],
      "have_return": true,
      "code_content": "    def custom_serialize(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ):\n        \"\"\"\n        Custom serialization logic that handles different contexts, such as 'sharegpt'.\n        \"\"\"\n        # Perform default serialization\n        result = handler(self)\n\n        result = result[\"history\"]\n\n        if info.context and info.context.get(\"sharegpt\"):\n            # Transform history into ShareGPT format\n            # TODO: human 和 observation 必须出现在奇数位置，gpt 和 function 必须出现在偶数位置\n            conversations = []\n            for item in self.history:\n                role_mapping = {\n                    \"user\": \"human\",\n                    \"assistant\": \"function_call\",\n                    \"tool\": \"observation\",\n                    \"critic\": \"critic\",\n                }\n\n                role = role_mapping.get(item.role, item.role)\n                value = item.content if item.content else None\n\n                # For tool calls, include the tool_call_id or arguments\n                if role == \"function_call\":\n                    if item.tool_calls:\n                        for tool_call in item.tool_calls:\n                            value = json.dumps(\n                                {\n                                    \"name\": tool_call.function.name,\n                                    \"arguments\": tool_call.function.arguments,\n                                },\n                                ensure_ascii=True,\n                            )\n\n                            conversations.append({\"from\": role, \"value\": value})\n                    else:\n                        conversations.append({\"from\": \"gpt\", \"value\": value})\n\n                elif role == \"observation\":\n                    conversations.append(\n                        {\"from\": role, \"value\": json.dumps(value, ensure_ascii=True)}\n                    )\n\n                else:\n                    conversations.append({\"from\": role, \"value\": value})\n\n            # Build final output structure\n            result = {\n                \"conversations\": conversations,\n                \"tools\": json.dumps(self.available_tools, ensure_ascii=True),\n            }\n\n        return result\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "write",
      "md_content": [
        "**write**: The function of write is to append data to a JSON file, creating the necessary directories and handling potential issues with existing file data.\n\n**parameters**: The parameters of this Function.\n· data: dict - A dictionary containing the data to be written to the file.\n· path: Path | str - The path (as a string or Path object) to the file where the data will be saved.\n\n**Code Description**: The `write` function is responsible for managing the process of appending data to a specified JSON file. The function first ensures that the provided file path is a `Path` object, converting it from a string if necessary. It then verifies that the parent directories of the file exist, creating them if needed. If the file already exists, it attempts to read the existing data and ensures that it is a list (or initializes it as an empty list if the data is invalid or absent). The new data is then appended to this list, and the updated list is written back to the file in a JSON format, ensuring that the data is properly formatted (with indentation for readability).\n\nThe function handles potential errors that might occur during these operations, such as issues with file access or invalid data formats. In such cases, it logs the error message and raises the exception.\n\nFrom a project perspective, the `write` function is utilized by other parts of the code to persist conversation history or similar data. For instance, it is called by the `_auto_save` method in the `ConversationManager` class, where it is used to save the most recent conversation history to a file, if the `save_path` attribute is set. Additionally, the `write` function is called in the `run_tasks` function, which processes a list of tasks and logs conversation data after each task is completed.\n\n**Note**: When using this function, ensure that the path provided is correct, and be aware that it appends data to the file. This means that the file will grow in size over time as more data is added. Also, if the file contains corrupt or non-JSON data, the function will start with an empty list, potentially overwriting previous content.",
        "**write**: The function of write is to append a new data entry to a specified JSON file, ensuring that the file and its parent directories exist.\n\n**parameters**: The parameters of this Function.\n· data: dict - The data to be appended to the JSON file, expected to be in dictionary format.  \n· path: Path | str - The file path where the data will be written, which can be provided as a string or a Path object.\n\n**Code Description**: The write method is a function within the ConversationManager class that facilitates the process of saving conversation data to a JSON file. It begins by checking the type of the `path` parameter; if it is a string, it converts it to a Path object for consistency in file handling.\n\nThe method then attempts to create any necessary parent directories for the specified file path using the `mkdir` method with the `parents=True` and `exist_ok=True` flags. This ensures that the directory structure is in place before any file operations are attempted.\n\nNext, the method checks if the file at the specified path already exists. If it does, it opens the file in read mode and attempts to load the existing data as JSON. The method expects the data to be a list at the top level; if the data is not a list or if a JSONDecodeError occurs (indicating that the file is empty or corrupt), it initializes `existing_data` as an empty list.\n\nAfter ensuring that `existing_data` is a list, the method appends the new `data` entry to this list. It then opens the file in write mode and writes the updated list back to the file using `json.dump`, ensuring that the output is formatted correctly with indentation for readability.\n\nIf any exceptions occur during this process, the method captures them and utilizes the `print_exception` method from the RichPrinter class to log the error message and print the exception details to the console. This error handling mechanism is crucial for debugging and maintaining the robustness of the application.\n\nThe write method is called by the `_auto_save` method within the same class, which is responsible for automatically saving the most recent entry in the conversation history whenever a new message is added. The `_auto_save` method retrieves the last entry from the `history` list and prepares it for saving by invoking the `model_dump` method to exclude any None values. It then calls the write method with the prepared data and the specified save path.\n\nAdditionally, the write method is also invoked by the `execute_multiple_tasks` function in the tasks_runner module. This function processes a list of tasks and, if the save option is enabled, it retrieves the conversation data from the BaseAgent's conversation manager and saves it using the write method.\n\n**Note**: It is essential to ensure that the `path` parameter is valid and that the data being passed is in the correct dictionary format. If the file path is invalid or if the data is not structured as expected, the write operation may fail, leading to exceptions being raised. Proper error handling and validation of inputs are recommended to ensure smooth operation."
      ],
      "code_start_line": 115,
      "code_end_line": 146,
      "params": [
        "self",
        "data",
        "path"
      ],
      "have_return": false,
      "code_content": "    def write(self, data: dict, path: Path | str):\n        if isinstance(path, str):\n            path = Path(path)\n\n        try:\n            # Create parent directories if they do not exist\n            path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Check if the file exists and read existing data\n            if path.exists():\n                with path.open(\"r\", encoding=\"utf-8\") as f:\n                    try:\n                        # Try to parse the existing JSON data (expecting a list at the top level)\n                        existing_data = json.load(f)\n                        if not isinstance(existing_data, list):\n                            existing_data = []  # Ensure it's a list\n                    except json.JSONDecodeError:\n                        # If the file is empty or corrupt, start with an empty list\n                        existing_data = []\n            else:\n                existing_data = []\n\n            # Append the new data to the existing array\n            existing_data.append(data)\n\n            # Write the updated array back to the file\n            with path.open(\"w\", encoding=\"utf-8\") as f:\n                json.dump(existing_data, f, ensure_ascii=True, indent=2)\n\n        except Exception:\n            printer.print_exception(f\"Failed to write to {path}\")\n            raise\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/models.py/ConversationManager/_auto_save",
        "src/criticsearch/tasks_runner.py/execute_multiple_tasks"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print_exception"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_auto_save",
      "md_content": [
        "**_auto_save**: The function of _auto_save is to automatically save the most recent entry in the conversation history to a specified file path, provided the save path is set.\n\n**parameters**:  \n- None\n\n**Code Description**: The `_auto_save` method is a function within the `ConversationManager` class that is responsible for saving the most recent conversation history to a file, specifically the last entry in the `history` list. The method checks whether the `save_path` attribute is defined, and if it is, it uses the `write` method to persist the most recent entry in the conversation history. This entry is accessed using `self.history[-1]`, which fetches the last item in the `history` list, and then the `model_dump(exclude_none=True)` method is invoked on this entry to prepare it for saving. This method ensures that no `None` values are included in the saved data. The resulting data is then passed to the `write` method, which handles the task of appending this data to a file at the location specified by the `save_path`.\n\nThe `_auto_save` function is called automatically whenever a new message is added to the conversation history. Specifically, it is invoked by the `append_to_history` method, which adds new conversation entries to the `history` list. Once a new message is appended to the history, `_auto_save` is triggered to ensure that the updated history is saved immediately. This process facilitates the continuous preservation of the conversation data without requiring manual intervention from the user.\n\nThe `write` method, which is invoked by `_auto_save`, is responsible for appending data to a JSON file. It manages the creation of necessary directories, reads any existing data from the file, and ensures that the data is in the correct format before appending new content. This integration ensures that the conversation history is consistently updated and saved to disk.\n\n**Note**: It is important to ensure that the `save_path` attribute is set correctly in order for the `_auto_save` function to work as expected. If the `save_path` is not provided or is invalid, no data will be saved. Additionally, the data being saved is the most recent entry in the `history` list, meaning only the latest message or event is preserved during each save operation.",
        "**_auto_save**: The function of _auto_save is to automatically save the most recent conversation history entry if a valid save path is configured.\n\n**parameters**:  \n- None.\n\n**Code Description**:  \nThe `_auto_save` function is responsible for ensuring that the latest conversation history entry is saved to a specified file path after each update, provided that a valid save path (`self.save_path`) is configured and that the `settings.save_sharegpt` is enabled.\n\nThe function first checks the `settings.save_sharegpt` flag. If this flag is `True`, the function proceeds to save the data. It does so by calling the `write` method of the `ConversationManager` class. The data to be saved is retrieved by accessing the last entry in the `self.history` list, which stores the conversation history. The most recent history entry is passed through the `model_dump` method, which serializes it while excluding any `None` values. This ensures that only meaningful data is saved. The data, along with the path specified by `self.save_path`, is then passed to the `write` method.\n\nThe `write` method, which is called within `_auto_save`, handles the task of appending the provided data to the specified file. It ensures that the file's parent directories are created if they do not already exist, reads the existing file data if available, appends the new data, and writes the updated content back to the file in JSON format.\n\nThe `_auto_save` function is called indirectly from the `append_to_history` method, which is responsible for adding new messages to the conversation history. After a new history item is appended, `_auto_save` is invoked to automatically save the updated history to the file. This creates an automated mechanism for persisting the conversation's progress after each new update.\n\n**Note**:  \n- The function will only save the conversation history if the `settings.save_sharegpt` flag is set to `True` and if a valid `save_path` is provided.  \n- The `self.history[-1]` is expected to be a valid entry in the conversation history, and it must be serializable by the `model_dump` method.  \n- If `self.save_path` is not properly set or if the `settings.save_sharegpt` flag is `False`, the function will not perform any saving operations.  \n- Proper configuration of the `save_path` and `settings.save_sharegpt` is necessary for the `_auto_save` function to operate correctly."
      ],
      "code_start_line": 148,
      "code_end_line": 153,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def _auto_save(self):\n        \"\"\"Auto save after each update if save_path is set\"\"\"\n        if settings.save_sharegpt:\n            self.write(\n                path=self.save_path, data=self.history[-1].model_dump(exclude_none=True)\n            )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/models.py/ConversationManager/append_to_history"
      ],
      "reference_who": [
        "src/criticsearch/models.py/ConversationManager/write"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "append_to_history",
      "md_content": [
        "### Function: `append_to_history`\n\n**Purpose**:  \nThe `append_to_history` function is designed to add a new message to the conversation history in a `ConversationManager` instance.\n\n**Parameters**:\n- `role` (Literal[\"user\", \"assistant\", \"tool\", \"critic\"]):  \n  Specifies the role of the entity contributing to the conversation. The value can be one of the following:\n  - `\"user\"`: Represents the user initiating the message.\n  - `\"assistant\"`: Represents the assistant responding to the user.\n  - `\"tool\"`: Represents an automated tool interacting within the conversation.\n  - `\"critic\"`: Represents a critic providing feedback or evaluation.\n\n- `content` (Optional[str]):  \n  The actual content of the message. This is an optional parameter and can be set to `None` if no content is provided.\n\n- `**kwargs`:  \n  Additional optional keyword arguments that can be passed to the `HistoryItem` constructor. These arguments can be used for providing extra context, such as tool calls or identifiers.\n\n**Functionality**:  \nThis method creates a new entry in the conversation history by instantiating a `HistoryItem` object with the provided `role`, `content`, and any additional keyword arguments. The newly created `HistoryItem` is appended to the `history` attribute, which is a list that stores the conversation history. After adding the new history item, the method triggers the `_auto_save` function to automatically save the updated conversation history.\n\n**Usage**:  \nThis function is used to maintain and update the flow of a conversation by systematically adding new messages from different participants (user, assistant, tool, or critic) and ensuring that the history is saved for future reference. It plays a crucial role in tracking the progression of conversations over time.\n\n**Example**:  \n```python\nconversation_manager.append_to_history(role=\"user\", content=\"Hello, how can I assist you today?\")\n```\n\nIn this example, a new message from the user is added to the conversation history. The content of the message is \"Hello, how can I assist you today?\" and the role is set as `\"user\"`.\n\n**Related Methods**:  \n- `_auto_save`: Automatically saves the most recent conversation entry to a file after the message is added to history.",
        "**append_to_history**: The function of append_to_history is to add a new message to the conversation history.\n\n**parameters**: The parameters of this Function.\n· role: Literal[\"user\", \"assistant\", \"tool\", \"critic\"] - This parameter defines the role of the entity contributing to the conversation, which can be one of the following: \"user\", \"assistant\", \"tool\", or \"critic\".\n· content: Optional[str] - This parameter holds the actual content of the message. It is optional and may be `None` if not provided.\n· kwargs: Additional keyword arguments that can be passed to provide more context or information related to the message being appended.\n\n**Code Description**: The append_to_history function is a method within the ConversationManager class that is responsible for appending a new entry to the conversation history. When invoked, it creates a new instance of the HistoryItem class, which represents an individual entry in the conversation history. The function takes in the role of the entity sending the message, the content of the message, and any additional keyword arguments that may be relevant.\n\nUpon execution, the function appends the newly created HistoryItem instance to the history attribute of the ConversationManager. This history attribute is a list that maintains the sequence of messages exchanged during the conversation. After appending the new message, the function calls the _auto_save method to ensure that the updated conversation history is saved automatically, provided that a valid save path is configured.\n\nThe append_to_history function is called in various contexts throughout the project. For instance, it is invoked within the common_chat function of the BaseAgent class, where it appends the model's response to the conversation history after processing a user prompt. Additionally, it is utilized in the process_single_task function, where user queries and responses from the agent are logged into the conversation history. This systematic logging of messages is crucial for maintaining a coherent and traceable dialogue flow, allowing for effective analysis and debugging of the conversation interactions.\n\nIn summary, append_to_history plays a vital role in managing the conversation history by ensuring that each message is accurately recorded along with its associated role and content. This functionality is essential for applications that rely on conversational agents, as it provides a structured way to track interactions over time.\n\n**Note**: It is important to ensure that the role parameter is correctly specified as one of the allowed literals (\"user\", \"assistant\", \"tool\", \"critic\") to avoid errors. The content parameter can be omitted if there is no message content to log, but it should be provided whenever applicable to maintain a complete conversation history. The use of additional keyword arguments (kwargs) allows for flexibility in capturing various aspects of the conversation, enhancing the richness of the logged data."
      ],
      "code_start_line": 155,
      "code_end_line": 165,
      "params": [
        "self",
        "role",
        "content"
      ],
      "have_return": false,
      "code_content": "    def append_to_history(\n        self,\n        role: Literal[\"user\", \"assistant\", \"tool\", \"critic\"],\n        content: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Add a new message to the conversation history.\n        \"\"\"\n        self.history.append(HistoryItem(role=role, content=content, **kwargs))\n        self._auto_save()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main",
        "src/criticsearch/models.py/ConversationManager/append_tool_call_to_history",
        "src/criticsearch/models.py/ConversationManager/append_tool_call_result_to_history"
      ],
      "reference_who": [
        "src/criticsearch/models.py/HistoryItem",
        "src/criticsearch/models.py/ConversationManager/_auto_save"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "append_tool_call_to_history",
      "md_content": [
        "**append_tool_call_to_history**: The function of append_tool_call_to_history is to add a tool call entry to the conversation history.\n\n**parameters**: The parameters of this Function.\n· tool_calls: List[ChatCompletionMessageToolCall] - A list of tool call entries that are to be added to the conversation history.\n· content: Optional[str] - An optional string that represents additional content related to the tool calls. If not provided, it defaults to None.\n\n**Code Description**: The append_tool_call_to_history function is a method within the ConversationManager class that is responsible for appending tool call entries to the conversation history. It takes two parameters: a list of tool calls and an optional content string. The primary purpose of this function is to maintain a record of interactions that involve automated tools within the conversation flow.\n\nWhen invoked, the function calls another method, append_to_history, passing along the role of \"assistant\", the list of tool calls, and any additional content. The append_to_history method is responsible for creating a new entry in the conversation history, which is stored as a list of HistoryItem objects. This ensures that every interaction involving tools is documented, allowing for a comprehensive view of the conversation's progression.\n\nThe append_tool_call_to_history function is called within the search_and_browse method of the BaseAgent class. In this context, it is used to log the tool calls generated during a search operation. If the search_with_tool_response contains tool calls, the function is invoked to append these calls to the conversation history. This is crucial for tracking the actions taken by the assistant and the tools it interacts with, ensuring that the conversation history remains accurate and up-to-date.\n\nIn summary, append_tool_call_to_history plays a vital role in maintaining the integrity of the conversation history by systematically recording tool interactions, which can be referenced later for analysis or debugging.\n\n**Note**: It is important to ensure that the tool_calls parameter is always provided as a list of ChatCompletionMessageToolCall objects to avoid errors during the history appending process. Additionally, the content parameter is optional and can be omitted if there is no additional information to record.",
        "**append_tool_call_to_history**: The function of append_tool_call_to_history is to add a tool call entry to the conversation history.\n\n**parameters**:\n- tool_calls: List[ChatCompletionMessageToolCall] - A list of tool calls to be appended to the conversation history.\n- content: Optional[str] - Optional content to include with the tool calls. This can be `None` if not required.\n\n**Code Description**:  \nThe append_tool_call_to_history function is a method within the ConversationManager class, designed to record tool call entries into the conversation history. It accepts two main parameters: `tool_calls`, which is a list of tool call objects, and an optional `content` parameter. The purpose of this function is to ensure that any tool call interaction made by the assistant is captured and stored in the conversation history for future reference.\n\nUpon invocation, the function delegates the task of appending the tool calls to the history by invoking the `append_to_history` method. It passes along the role as \"assistant\", indicating that the tool calls are being made by the assistant in the context of the conversation. The `tool_calls` are passed as-is, and any additional `content` provided will be included with the entry if it is not `None`. \n\nThis method ensures that every interaction with external tools or services is systematically logged into the conversation history, which is crucial for maintaining a coherent dialogue flow and for later analysis. The `append_to_history` method, which is called inside this function, further appends the tool call details into the conversation history and ensures that the information is saved if auto-save functionality is enabled.\n\nThe `append_tool_call_to_history` method plays a crucial role in contexts where the assistant interacts with external tools to gather or process data. For example, when a web scraping tool is called to extract content based on user queries or search results, the tool calls related to this action will be captured through this method, allowing the conversation history to retain full visibility of tool usage.\n\n**Reference Relationships in the Project**:\n- The `append_tool_call_to_history` function is called in methods such as `web_scrape_results` and `search_and_browse`, which involve web scraping and search operations. In both methods, the tool calls generated during these processes are passed to `append_tool_call_to_history` to ensure they are logged into the conversation history.\n- Specifically, in the `search_and_browse` function, after initiating a search through the common chat interface, the tool calls (if any) are appended to the history using this function. Similarly, after web scraping, any tool calls related to the scraping process are logged in the same manner.\n\n**Note**: \n- It is important to ensure that the `tool_calls` parameter is provided as a valid list of tool call objects. The content parameter is optional and should be included when additional context or information needs to be logged along with the tool calls.\n- This function ensures that all interactions with tools are properly recorded, which is essential for tracking the flow of the conversation, debugging, and improving the system’s response accuracy."
      ],
      "code_start_line": 167,
      "code_end_line": 175,
      "params": [
        "self",
        "tool_calls",
        "content"
      ],
      "have_return": false,
      "code_content": "    def append_tool_call_to_history(\n        self,\n        tool_calls: List[ChatCompletionMessageToolCall],\n        content: Optional[str] = None,\n    ):\n        \"\"\"\n        Add a tool call entry to the conversation history.\n        \"\"\"\n        self.append_to_history(role=\"assistant\", tool_calls=tool_calls, content=content)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse"
      ],
      "reference_who": [
        "src/criticsearch/models.py/ConversationManager/append_to_history"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "append_tool_call_result_to_history",
      "md_content": [
        "**append_tool_call_result_to_history**: The function of `append_tool_call_result_to_history` is to add the result of a tool call to the conversation history.\n\n**parameters**: The parameters of this function are:\n- `tool_call_id`: A string representing the unique identifier of the tool call.\n- `name`: A string representing the name of the tool involved in the call.\n- `content`: A string representing the result or content generated from the tool call.\n\n**Code Description**: \nThe `append_tool_call_result_to_history` function is designed to add the result of a tool call to the conversation history in a structured manner. This is achieved by calling the `append_to_history` method, which is responsible for adding an entry to the conversation history. The `role` parameter in `append_to_history` is set to `\"tool\"`, signifying that the message is coming from an automated tool. Along with this, the `tool_call_id`, `name`, and `content` are passed as keyword arguments to ensure that the specific details of the tool call are included in the history entry.\n\nThis function is called within the `search_and_browse` method of the `BaseAgent` class, which handles the interaction between the agent and external tools. Specifically, after making a tool call to search or scrape data, the results are passed to `append_tool_call_result_to_history` to be logged into the conversation history. This is crucial for maintaining a structured log of tool activities in the ongoing conversation.\n\nWhen tool calls occur during the search and web scraping processes, `append_tool_call_result_to_history` ensures that the relevant tool call results are captured and saved in the conversation history. This allows for easy tracking of the results generated by tools like search aggregators and content scrapers.\n\nIn summary, this function is used to log the results of tool calls in the conversation history, providing a complete and organized record of the interactions involving automated tools.\n\n**Note**: The content parameter passed to this function should contain the result of the tool's operation (such as search results or web scraping data) and should be a string that accurately reflects the output generated by the tool.",
        "**append_tool_call_result_to_history**: The function of append_tool_call_result_to_history is to add the result of a tool call to the conversation history.\n\n**parameters**: The parameters of this Function.\n· tool_call_id: str - This parameter represents the unique identifier of the tool call being recorded in the conversation history.\n· name: str - This parameter specifies the name of the tool that was called.\n· content: str - This parameter contains the result or output generated by the tool call.\n\n**Code Description**: The append_tool_call_result_to_history function is a method within the ConversationManager class that facilitates the logging of results from tool calls into the conversation history. When this function is invoked, it takes three parameters: tool_call_id, name, and content. These parameters are essential for accurately documenting the interaction with the tool.\n\nThe function operates by calling the append_to_history method, which is responsible for appending a new entry to the conversation history. In this context, the role is set to \"tool,\" indicating that the entry pertains to a tool's output. The tool_call_id, name, and content are passed as arguments to provide a comprehensive record of the tool's activity.\n\nThe append_to_history method, which is invoked within append_tool_call_result_to_history, plays a critical role in maintaining the integrity of the conversation history. It ensures that each tool interaction is logged systematically, allowing for effective tracking and analysis of the conversation flow. The history attribute of the ConversationManager class is a list that stores all entries, and each entry is represented by an instance of the HistoryItem class.\n\nThis function is called in various contexts, specifically within the web_scrape_results and search_and_browse methods of the BaseAgent class. In web_scrape_results, after executing a tool call for web scraping, the results are logged using append_tool_call_result_to_history. Similarly, in search_and_browse, the function is used to log the results of search queries processed by the agent. This systematic logging is crucial for maintaining a coherent dialogue and ensuring that all interactions with tools are recorded for future reference.\n\n**Note**: It is important to ensure that the parameters provided to the function are accurate and relevant. The tool_call_id should correspond to a valid tool call, the name should reflect the tool's identity, and the content should contain the actual results generated by the tool. Proper usage of this function contributes to a well-structured conversation history, which is essential for applications relying on conversational agents."
      ],
      "code_start_line": 177,
      "code_end_line": 185,
      "params": [
        "self",
        "tool_call_id",
        "name",
        "content"
      ],
      "have_return": false,
      "code_content": "    def append_tool_call_result_to_history(\n        self, tool_call_id: str, name: str, content: str\n    ):\n        \"\"\"\n        Add a tool call result to the conversation history.\n        \"\"\"\n        self.append_to_history(\n            role=\"tool\", tool_call_id=tool_call_id, name=name, content=content\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse"
      ],
      "reference_who": [
        "src/criticsearch/models.py/ConversationManager/append_to_history"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "clear_history",
      "md_content": [
        "**clear_history**: The function of clear_history is to remove all entries from the conversation history.\n\n**parameters**: The clear_history function does not take any parameters.\n\n**Code Description**: The clear_history function is a method that belongs to a class, presumably related to managing conversations. When invoked, this method performs a single action: it clears the entire conversation history stored in the object's history attribute. The history attribute is expected to be a data structure that supports the clear operation, such as a list or a similar collection. By calling self.history.clear(), the method effectively empties this collection, ensuring that no previous conversation data remains accessible. This functionality is particularly useful in scenarios where a fresh start is required, such as resetting a chat interface or preparing for a new session without any prior context.\n\n**Note**: It is important to be aware that once the clear_history method is executed, all conversation data will be permanently deleted and cannot be recovered. Therefore, it is advisable to use this method with caution, especially in applications where conversation history may be needed for reference or analysis."
      ],
      "code_start_line": 187,
      "code_end_line": 191,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def clear_history(self):\n        \"\"\"\n        Clear the entire conversation history.\n        \"\"\"\n        self.history.clear()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/__main__.py": [],
  "src/criticsearch/main_old_paralell.py": [
    {
      "type": "FunctionDef",
      "name": "flatten_outline",
      "md_content": [
        "**flatten_outline**: The function of flatten_outline is to flatten a hierarchical outline structure into a list of sections, each annotated with its depth and path.\n\n**parameters**: The parameters of this Function.\n· section: A dictionary representing a section of the outline, which may contain a title and potentially children sections.\n· depth: An integer indicating the current depth level in the outline hierarchy, defaulting to 1.\n· path: A list that tracks the path of titles leading to the current section, defaulting to None.\n\n**Code Description**: The flatten_outline function is designed to transform a nested outline structure, typically represented in JSON format, into a flat list. Each entry in this list contains a dictionary with three keys: \"path\", \"section\", and \"depth\". The \"path\" key holds a list of titles leading to the current section, the \"section\" key contains the original section data, and the \"depth\" key indicates how deep the section is within the hierarchy.\n\nThe function begins by checking if the path parameter is None; if so, it initializes it as an empty list. It then constructs a dictionary for the current section, appending the section's title to the path and setting the depth. This dictionary is added to a list called flat, which will ultimately be returned.\n\nIf the current section contains children (indicating that it is not a leaf node), the function iterates over each child section. For each child, it recursively calls flatten_outline, increasing the depth by one and passing the updated path. The results from these recursive calls are extended into the flat list.\n\nThe flatten_outline function is called within the main function of the project, specifically after the outline has been generated from search results. The resulting flat_sections list is then used to generate content for each section in parallel, allowing for efficient processing of potentially large outlines. This integration highlights the function's role in transforming complex hierarchical data into a manageable format for further processing.\n\n**Note**: It is important to ensure that the input section is structured correctly as a JSON object with the expected keys (\"title\" and \"children\") for the function to operate effectively.\n\n**Output Example**: An example return value of flatten_outline might look like this for a given outline:\n[\n    {\"path\": [\"Introduction\"], \"section\": {\"title\": \"Introduction\", \"children\": [...]}, \"depth\": 1},\n    {\"path\": [\"Introduction\", \"Background\"], \"section\": {\"title\": \"Background\", \"children\": [...]}, \"depth\": 2},\n    {\"path\": [\"Introduction\", \"Background\", \"Details\"], \"section\": {\"title\": \"Details\"}, \"depth\": 3}\n]"
      ],
      "code_start_line": 15,
      "code_end_line": 24,
      "params": [
        "section",
        "depth",
        "path"
      ],
      "have_return": true,
      "code_content": "def flatten_outline(section, depth=1, path=None):\n    # 将 outline_json 展平，记录每个节点及其层级和路径\n    if path is None:\n        path = []\n    current = {\"path\": path + [section.get(\"title\")], \"section\": section, \"depth\": depth}\n    flat = [current]\n    if \"children\" in section:\n        for child in section[\"children\"]:\n            flat.extend(flatten_outline(child, depth + 1, path + [section.get(\"title\")]))\n    return flat\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "generate_content_for_section",
      "md_content": [
        "**generate_content_for_section**: The function of generate_content_for_section is to generate detailed textual content for a specified section based on a search query and a given task.\n\n**parameters**: The parameters of this Function.\n· common_agent: An instance of the BaseAgent class that facilitates search and chat functionalities.\n· section: A dictionary containing information about the section, specifically its title.\n· TASK: A string representing the overarching task or topic under which the content is to be generated.\n\n**Code Description**: The generate_content_for_section function is designed to create content for a specific section of a report or document. It begins by extracting the title from the provided section dictionary. Using this title, it formulates a search query that prompts the common_agent to generate relevant search queries related to the title within the context of the specified TASK. The search results obtained from the common_agent's search_and_browse method are then utilized to construct a detailed prompt for generating content.\n\nThis prompt instructs the common_agent to write one or several paragraphs that logically present data and facts about the section's title, ensuring that all information is cited correctly using a specified format. The content is generated through the common_agent's common_chat method, which processes the prompt and returns the generated paragraphs.\n\nThe function is called within a multi-threaded context in the main function of the project. Specifically, it is invoked for each section of a flattened outline generated from initial search results. This allows for parallel content generation across multiple sections, enhancing efficiency and reducing the overall time required to compile the report. The results from each call to generate_content_for_section are collected and later reconstructed into a coherent markdown format.\n\n**Note**: It is important to ensure that the common_agent is properly initialized and that the section parameter contains a valid title to avoid errors during content generation. Additionally, the function's output should be formatted correctly to maintain consistency in citation style.\n\n**Output Example**: A possible appearance of the code's return value could be:\n\"Climate change is a pressing global issue that affects various aspects of life on Earth. According to the Intergovernmental Panel on Climate Change (IPCC), the average global temperature has risen by approximately 1.2 degrees Celsius since the late 19th century<\\cite>https://www.ipcc.ch/report/ar6/wg1/#:~:text=The%20average%20global%20temperature%20has%20risen%20by%20approximately%201.2%20degrees%20Celsius%20since%20the%20late%2019th%20century<\\cite>. This increase has led to more frequent and severe weather events, including hurricanes, droughts, and floods<\\cite>https://www.ncdc.noaa.gov/sotc/global/202012#climate<\\cite>.\""
      ],
      "code_start_line": 26,
      "code_end_line": 39,
      "params": [
        "common_agent",
        "section",
        "TASK"
      ],
      "have_return": true,
      "code_content": "def generate_content_for_section(common_agent, section, TASK):\n    # 保持 prompt 不变，仅生成单层章节内容，不递归\n    title = section.get(\"title\")\n    search_query = f\"generate some search queries about '{title}' under the background of this TOPIC/TASK: '{TASK}'.\"\n    search_results = common_agent.search_and_browse(search_query)\n    prompt = (\n        f\"Using the following search results:\\n\\n{search_results}\\n\\n\"\n        f\"Write one or several detailed paragraphs with data and facts in a logical way about '{title}' under the background of this TOPIC/TASK: '{TASK}', formatted in pure text, without summary sentences.\"\n        f\"Please make sure you are always obeying and using '<\\cite>The url link that you used for supporting the previous statement<\\cite>' format in every sentence that you are using data from the web.\"\n    )\n    paragraph = common_agent.common_chat(usr_prompt=prompt)\n    print(f\"--- Generated content for '{title}' ---\")\n    print(paragraph)\n    return paragraph\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "reconstruct_markdown",
      "md_content": [
        "**reconstruct_markdown**: The function of reconstruct_markdown is to generate a final Markdown document based on a hierarchical outline and corresponding content.\n\n**parameters**: The parameters of this Function.\n- outline: A dictionary that represents the hierarchical structure of the document, including sections and their titles.\n- flat_contents: A list of tuples where each tuple contains a dictionary representing a section and the corresponding content for that section.\n\n**Code Description**: \nThe `reconstruct_markdown` function is responsible for assembling a Markdown document by combining the hierarchical structure of a document (provided in the `outline` parameter) with corresponding content (provided in the `flat_contents` parameter). It follows these steps:\n1. **Mapping Content**: It first creates a dictionary (`content_map`) where each key is the full path of a section (represented as a tuple of strings), and the value is the content associated with that section. This is done by iterating over the `flat_contents` and extracting the path and content for each section.\n   \n2. **Recursive Content Generation**: The function defines a helper function `helper` that is recursively used to generate Markdown for each section of the outline. This function takes a section and the current path (starting as an empty list) as input.\n   - For each section, it combines the title (from the `title` field) with the correct level of Markdown headers (`#` symbols based on the depth of the section in the outline).\n   - If content for the section exists (i.e., its path is found in the `content_map`), the content is appended to the Markdown string for that section.\n   - If the section has child sections (i.e., if `children` exist), the function recursively processes each child section, appending their corresponding Markdown to the current section.\n\n3. **Result Construction**: The `helper` function is initially called for the top-level sections of the document, and the resulting Markdown for all sections is concatenated into the `result` string. If the `outline` contains a title, it is added at the beginning of the Markdown document with the appropriate header (`#`).\n\n4. **Final Markdown Output**: After all sections and their content have been processed, the final Markdown string is returned.\n\nThis function is integral to the document generation pipeline. Specifically, it is used in the context of a broader workflow where an agent gathers search results and creates an outline, followed by generating content for individual sections. Once content is generated in parallel, `reconstruct_markdown` is called to structure that content into a final Markdown document that is later polished and saved.\n\nThe function is invoked within the `main` function as part of a multi-step document generation process. After gathering search results and content for individual sections, `reconstruct_markdown` is used to combine the content and hierarchical structure into a Markdown report, which is further polished and stored.\n\n**Note**: The function relies on the assumption that `flat_contents` contains content for each section as a tuple where the first element is the section's metadata (containing a \"path\") and the second element is the content itself. The path is crucial for matching the content to the correct section in the `outline`. Additionally, the recursive nature of the `helper` function means that deeply nested sections are properly handled by the same logic.\n\n**Output Example**: \nFor an outline like:\n```json\n{\n    \"title\": \"Main Title\",\n    \"children\": [\n        {\n            \"title\": \"Section 1\",\n            \"children\": [\n                {\"title\": \"Subsection 1.1\", \"children\": []},\n                {\"title\": \"Subsection 1.2\", \"children\": []}\n            ]\n        },\n        {\n            \"title\": \"Section 2\",\n            \"children\": []\n        }\n    ]\n}\n```\nand corresponding content for each section, the generated Markdown might look like:\n```\n# Main Title\n\n## Section 1\n\n### Subsection 1.1\n\nContent for Subsection 1.1\n\n### Subsection 1.2\n\nContent for Subsection 1.2\n\n## Section 2\n\nContent for Section 2\n```"
      ],
      "code_start_line": 41,
      "code_end_line": 77,
      "params": [
        "outline",
        "flat_contents"
      ],
      "have_return": true,
      "code_content": "def reconstruct_markdown(outline, flat_contents):\n    \"\"\"\n    根据展平后的内容与原 outline 结构，拼接生成最终 Markdown 文本\n    使用完整路径为key确保唯一性\n    \"\"\"\n    # 构建以完整路径为键的映射字典\n    content_map = {}\n    for item, content in flat_contents:\n        path_key = tuple(item[\"path\"])  # 使用完整路径作为key\n        content_map[path_key] = content\n\n    def helper(section, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n        \n        # 生成标题（深度就是路径的长度）\n        depth = len(current_path)\n        md = f\"{'#' * depth} {section.get('title')}\\n\\n\"\n        \n        # 添加该节的内容（如果有）\n        if path_key in content_map:\n            md += f\"{content_map[path_key]}\\n\\n\"\n            \n        # 处理子节点\n        if \"children\" in section:\n            for child in section[\"children\"]:\n                md += helper(child, current_path)\n        return md\n\n    result = \"\"\n    if \"title\" in outline:\n        result += f\"# {outline['title']}\\n\\n\"\n    \n    for section in outline.get(\"children\", []):\n        result += helper(section, [outline.get(\"title\")] if \"title\" in outline else [])\n    \n    return result\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "helper",
      "md_content": [
        "**helper**: The function of helper is to recursively generate Markdown text for a given section and its nested sub-sections.\n\n**parameters**:\n- parameter1: section (dict) - A dictionary representing a section, containing a title and possibly children, which are nested sections.\n- parameter2: path (list, optional) - A list representing the current path of titles. Default is an empty list, and it is used to track the hierarchy of section titles.\n\n**Code Description**:  \nThe `helper` function is responsible for generating a Markdown representation of a section, including its title and any associated content. It handles nested sections by recursively calling itself for child sections, creating a hierarchical structure based on the depth of the section in the hierarchy. The function performs the following operations:\n\n1. **Initialize Path**: The function begins by constructing a `current_path`, which combines the existing `path` with the title of the current section. This path is used to track the hierarchical level of the section.\n\n2. **Path Key**: It then creates a `path_key`, which is a tuple formed by the `current_path`. This tuple is used to check if there is any associated content for the section in the `content_map`.\n\n3. **Generate Markdown for the Title**: The depth of the section is determined by the length of the `current_path`. The number of hashtags (`#`) in the Markdown title is equal to the depth of the section. The function then constructs the section title in Markdown format (`# Title`).\n\n4. **Include Content**: If there is any content mapped to the `path_key` in the `content_map`, this content is added below the section title. The content is added in Markdown format.\n\n5. **Handle Child Sections**: If the current section has any children (i.e., sub-sections), the function iterates over them and recursively calls itself on each child, appending the generated Markdown to the result.\n\n6. **Return the Result**: After processing the title, content, and children (if any), the function returns the generated Markdown string for the current section and its descendants.\n\n**Note**: \n- The `content_map` must be defined elsewhere in the code, as it is used to retrieve the content associated with each section's path.\n- The function is designed to handle hierarchical section structures where each section can have nested sub-sections. The depth of each section is represented in the generated Markdown by the number of `#` symbols in the title.\n- The `path` parameter allows the function to track the nesting level of each section and ensures that the correct content is included at each level of the hierarchy.\n\n**Output Example**:  \nGiven a sample input where `content_map` contains content for each section and its sub-sections, the output might look like this:\n\nFor a `section` with the structure:\n```python\n{\n    \"title\": \"Introduction\",\n    \"children\": [\n        {\n            \"title\": \"Background\",\n            \"children\": []\n        },\n        {\n            \"title\": \"Objective\",\n            \"children\": []\n        }\n    ]\n}\n```\n\nAnd assuming `content_map` has content for \"Introduction\", \"Background\", and \"Objective\", the generated output could look like:\n\n```\n# Introduction\n\nThis is the content for the Introduction section.\n\n## Background\n\nThis is the content for the Background section.\n\n## Objective\n\nThis is the content for the Objective section.\n```\n\nThis example illustrates how the function constructs the hierarchy using `#` for titles and includes content where available."
      ],
      "code_start_line": 52,
      "code_end_line": 68,
      "params": [
        "section",
        "path"
      ],
      "have_return": true,
      "code_content": "    def helper(section, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n        \n        # 生成标题（深度就是路径的长度）\n        depth = len(current_path)\n        md = f\"{'#' * depth} {section.get('title')}\\n\\n\"\n        \n        # 添加该节的内容（如果有）\n        if path_key in content_map:\n            md += f\"{content_map[path_key]}\\n\\n\"\n            \n        # 处理子节点\n        if \"children\" in section:\n            for child in section[\"children\"]:\n                md += helper(child, current_path)\n        return md\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_citations",
      "md_content": [
        "**extract_citations**: The function of extract_citations is to extract all citations (URLs) enclosed in `<cite>` tags from a given text.\n\n**parameters**:  \n· text: The input string in which citations need to be extracted.\n\n**Code Description**:  \nThe `extract_citations` function is responsible for identifying and extracting URLs that are enclosed within `<cite>` HTML tags in a given text. It uses a regular expression pattern to match these tags and captures the content inside them.\n\nThe function begins by initializing an empty list called `citations`, which will hold the URLs found in the input text. It then defines a regular expression pattern `r'<cite>(.*?)<\\/cite>'`. This pattern looks for text between `<cite>` and `</cite>` tags. The `re.findall()` function is then used to search the provided text for all matches of this pattern. Each match found is a string containing the URL or citation within the `<cite>` tag. The function returns a list of all these matches.\n\nThe function is used within the `process_section` and `parse_markdown_to_structure` functions to handle paragraph-level citation extraction. In `process_section`, it extracts citations from each paragraph of a section, while in `parse_markdown_to_structure`, it performs citation extraction as paragraphs are processed while parsing a markdown text structure. In both cases, the `extract_citations` function helps to collect URLs in the specified `<cite>` tags for further processing or inclusion in the returned document structure.\n\n**Note**: The function does not handle malformed or incomplete HTML tags and assumes that the input text will contain properly formatted `<cite>` tags. The function extracts all citations, regardless of the number or complexity of the tags, and returns them as a list.\n\n**Output Example**:  \nGiven an input text:\n```html\nThis is a reference to <cite>https://example.com</cite> in the text.\nAnother citation: <cite>https://another-example.com</cite>.\n```\n\nThe output would be:\n```python\n['https://example.com', 'https://another-example.com']\n```"
      ],
      "code_start_line": 79,
      "code_end_line": 84,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def extract_citations(text):\n    \"\"\"从文本中提取引用的URLs\"\"\"\n    citations = []\n    pattern = r'<cite>(.*?)<\\/cite>'\n    matches = re.findall(pattern, text)\n    return matches\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/create_document_structure/process_section",
        "src/criticsearch/main_old_paralell.py/parse_markdown_to_structure"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "create_document_structure",
      "md_content": [
        "**create_document_structure**: The function of create_document_structure is to generate a structured document based on a given outline and corresponding content.\n\n**parameters**:\n· outline_json: A JSON object representing the outline of the document, which contains hierarchical information about titles and subsections.\n· flat_contents: A list of tuples where each tuple contains a dictionary with a \"path\" key representing the hierarchical path and a content string associated with that path.\n\n**Code Description**:  \nThe function `create_document_structure` is designed to create a structured document based on an outline and flat content. It takes two inputs: `outline_json` and `flat_contents`.\n\n- `outline_json` is a JSON object that represents the hierarchical structure of the document, including the document title, subsections, and children sections.\n- `flat_contents` is a list of tuples, each of which pairs a dictionary containing the path to a section with the corresponding textual content for that section.\n\nThe function proceeds by first initializing a `document` dictionary with a title (extracted from `outline_json`), a level (set to 1), and an empty list of subsections. The main goal is to map each section in the outline to its respective content and organize this data in a structured way.\n\n1. **Mapping Content to Paths**:  \n   The function creates a `content_map` dictionary where each section's path is mapped to its respective content. The `path` is represented as a tuple derived from the \"path\" key in the input tuples of `flat_contents`.\n\n2. **Processing Sections**:  \n   The function defines an internal function `process_section` which is responsible for recursively processing each section of the outline. Each section is processed based on the depth of its position in the outline, and the content associated with that section is retrieved from `content_map` using the path.\n\n   For each section, the function:\n   - Retrieves the section title and assigns it to a new dictionary.\n   - If content exists for the section (found using its path in `content_map`), it splits the content into paragraphs, each of which is processed further to extract any citations using a helper function `extract_citations`.\n   - If the section contains children (subsections), the function recursively processes each child, appending the result to the `subsections` list for that section.\n\n3. **Document Structure Construction**:  \n   After processing all sections in the root of the outline (under the `children` key of `outline_json`), the function returns the fully constructed document, which consists of a hierarchical structure of titles, paragraphs, citations, and subsections.\n\n**Note**: \n- The content is assumed to be divided into paragraphs by empty lines (`\\n\\n`).\n- Citations within paragraphs are processed by a helper function `extract_citations`, although the implementation of this function is not provided here.\n- The structure of the outline is assumed to be consistent, where each section may have a title and children, and content is mapped by its hierarchical path.\n\n**Output Example**:  \nA possible output of this function could look like the following:\n\n```json\n{\n  \"document\": {\n    \"title\": \"Sample Document Title\",\n    \"level\": 1,\n    \"subsections\": [\n      {\n        \"title\": \"Introduction\",\n        \"level\": 2,\n        \"paragraphs\": [\n          {\n            \"text\": \"This is the introduction paragraph.\\n\\nIt introduces the topic.\",\n            \"citations\": [\"citation1\", \"citation2\"]\n          }\n        ],\n        \"subsections\": []\n      },\n      {\n        \"title\": \"Methodology\",\n        \"level\": 2,\n        \"paragraphs\": [\n          {\n            \"text\": \"In this section, we describe the methods used in the study.\",\n            \"citations\": []\n          }\n        ],\n        \"subsections\": [\n          {\n            \"title\": \"Data Collection\",\n            \"level\": 3,\n            \"paragraphs\": [\n              {\n                \"text\": \"Data was collected through surveys and interviews.\",\n                \"citations\": [\"citation3\"]\n              }\n            ],\n            \"subsections\": []\n          }\n        ]\n      }\n    ]\n  }\n}\n```"
      ],
      "code_start_line": 86,
      "code_end_line": 141,
      "params": [
        "outline_json",
        "flat_contents"
      ],
      "have_return": true,
      "code_content": "def create_document_structure(outline_json, flat_contents):\n    \"\"\"\n    基于outline结构和生成的内容创建文档结构\n    \"\"\"\n    document = {\n        \"document\": {\n            \"title\": outline_json.get(\"title\", \"\"),\n            \"level\": 1,\n            \"subsections\": []\n        }\n    }\n    \n    # 构建路径到内容的映射\n    content_map = {}\n    for item, content in flat_contents:\n        path_key = tuple(item[\"path\"])\n        content_map[path_key] = content\n        \n    def process_section(section, depth=1, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n        \n        section_data = {\n            \"title\": section.get(\"title\"),\n            \"level\": depth,\n            \"paragraphs\": []\n        }\n        \n        # 如果有内容，处理段落\n        if path_key in content_map:\n            content = content_map[path_key]\n            paragraphs = content.split('\\n\\n')  # 假设段落用空行分隔\n            for para in paragraphs:\n                if para.strip():  # 忽略空段落\n                    citations = extract_citations(para)\n                    paragraph_data = {\n                        \"text\": para.strip(),  # 保留原始文本，包括cite标记\n                        \"citations\": citations\n                    }\n                    section_data[\"paragraphs\"].append(paragraph_data)\n        \n        # 处理子节点\n        if \"children\" in section:\n            section_data[\"subsections\"] = []\n            for child in section[\"children\"]:\n                child_data = process_section(child, depth + 1, current_path)\n                section_data[\"subsections\"].append(child_data)\n                \n        return section_data\n    \n    # 处理根节点下的所有节点\n    for section in outline_json.get(\"children\", []):\n        doc_section = process_section(section, 2, [outline_json.get(\"title\")] if \"title\" in outline_json else [])\n        document[\"document\"][\"subsections\"].append(doc_section)\n    \n    return document\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_section",
      "md_content": [
        "**process_section**: The function of process_section is to process a section of content, extract paragraphs and citations, and recursively handle sub-sections.\n\n**parameters**: The parameters of this Function.\n- section: A dictionary representing a section, which contains keys like \"title\", \"children\", and possibly other content.\n- depth: An integer representing the current depth level of the section in a hierarchical structure, default value is 1.\n- path: A list used to track the path taken through the section hierarchy, default value is an empty list.\n\n**Code Description**:  \nThe `process_section` function is responsible for processing a single section within a larger content structure, extracting its relevant details, including paragraphs and citations, and recursively processing any child sections (subsections).\n\n1. **Section Title and Path Tracking**:\n   - The function starts by constructing a list `current_path`, which tracks the path taken to the current section. It appends the current section's title to this path.\n   - The `path_key` is formed as a tuple from the `current_path`. This key is then used to look up content for the section from a global `content_map`.\n\n2. **Paragraph Extraction and Citation Handling**:\n   - If content for the section is available in `content_map` (as indicated by `path_key`), the content is retrieved and split into paragraphs based on two consecutive newline characters (`\\n\\n`).\n   - For each paragraph, any non-empty paragraphs are processed. The function calls `extract_citations`, which extracts citations (URLs enclosed in `<cite>` tags) from the paragraph. The resulting citations are stored alongside the paragraph text.\n\n3. **Handling Subsections**:\n   - If the section contains child sections (subsections), the function creates a list under the key `\"subsections\"`. It then recursively calls `process_section` for each child, incrementing the `depth` by 1 and passing the updated `path`.\n\n4. **Return Value**:\n   - The function returns a dictionary containing:\n     - `\"title\"`: The section's title.\n     - `\"level\"`: The depth level of the section.\n     - `\"paragraphs\"`: A list of dictionaries, each representing a paragraph with associated citation data.\n     - `\"subsections\"`: A list of processed subsections, if any.\n\nThis function is designed to support nested sections, where each section can have multiple levels of sub-sections. It is useful for transforming a hierarchical content structure into a more structured format that includes both textual data and citations, making it easier to process or render in a desired output format.\n\nThe `process_section` function interacts with the `extract_citations` function to collect citations from paragraphs. This makes it particularly suited for handling structured content that includes references marked by `<cite>` tags.\n\n**Note**: \n- The function assumes that the input data is well-formed, with sections containing a \"title\" and optional \"children\".\n- The `content_map` must be pre-defined and should map section paths (as tuples) to corresponding content.\n- The function handles nested sections and citations extraction, making it suitable for documents with complex structures.\n\n**Output Example**:\nGiven the following input:\n```python\nsection = {\n    \"title\": \"Introduction\",\n    \"children\": [\n        {\"title\": \"Background\"},\n        {\"title\": \"Objective\"}\n    ]\n}\n```\nAssuming the `content_map` contains the relevant content for the sections, the output might look like:\n```python\n{\n    \"title\": \"Introduction\",\n    \"level\": 1,\n    \"paragraphs\": [\n        {\"text\": \"This is the introduction paragraph. <cite>https://example.com</cite>\", \"citations\": [\"https://example.com\"]}\n    ],\n    \"subsections\": [\n        {\"title\": \"Background\", \"level\": 2, \"paragraphs\": [], \"subsections\": []},\n        {\"title\": \"Objective\", \"level\": 2, \"paragraphs\": [], \"subsections\": []}\n    ]\n}\n```"
      ],
      "code_start_line": 104,
      "code_end_line": 134,
      "params": [
        "section",
        "depth",
        "path"
      ],
      "have_return": true,
      "code_content": "    def process_section(section, depth=1, path=[]):\n        current_path = path + [section.get(\"title\")]\n        path_key = tuple(current_path)\n        \n        section_data = {\n            \"title\": section.get(\"title\"),\n            \"level\": depth,\n            \"paragraphs\": []\n        }\n        \n        # 如果有内容，处理段落\n        if path_key in content_map:\n            content = content_map[path_key]\n            paragraphs = content.split('\\n\\n')  # 假设段落用空行分隔\n            for para in paragraphs:\n                if para.strip():  # 忽略空段落\n                    citations = extract_citations(para)\n                    paragraph_data = {\n                        \"text\": para.strip(),  # 保留原始文本，包括cite标记\n                        \"citations\": citations\n                    }\n                    section_data[\"paragraphs\"].append(paragraph_data)\n        \n        # 处理子节点\n        if \"children\" in section:\n            section_data[\"subsections\"] = []\n            for child in section[\"children\"]:\n                child_data = process_section(child, depth + 1, current_path)\n                section_data[\"subsections\"].append(child_data)\n                \n        return section_data\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/main_old_paralell.py/extract_citations"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "parse_markdown_to_structure",
      "md_content": [
        "### Function Documentation: `parse_markdown_to_structure`\n\n#### Overview:\nThe `parse_markdown_to_structure` function parses a given markdown text and converts it into a structured document format, organizing sections, subsections, and paragraphs based on the markdown syntax. It identifies headers marked with `#` to define sections and handles paragraph content, extracting citations where applicable.\n\n#### Parameters:\n- **markdown_text** (str): The input markdown text to be parsed into a structured document. This text can contain headers (denoted by `#`) and paragraphs, with citations included in `<cite>` tags.\n\n#### Returns:\n- **document** (dict): A dictionary representing the parsed structure of the markdown text. It includes a top-level document object with a title, level, subsections, and paragraphs. Each section in the markdown is represented as a subsection in the structure.\n\n#### Structure of the returned document:\n- **document** (dict):\n  - **title** (str): The title of the document, typically set from the first markdown header.\n  - **level** (int): The level of the current section. This is determined by the number of `#` characters at the start of a header.\n  - **subsections** (list): A list of subsections (if any), with each subsection being a dictionary containing:\n    - **title** (str): The title of the subsection.\n    - **level** (int): The level of the subsection based on its header.\n    - **subsections** (list): Further nested subsections.\n    - **paragraphs** (list): A list of paragraphs in the subsection, each represented as a dictionary with:\n      - **text** (str): The paragraph text.\n      - **citations** (list): A list of citations (URLs) extracted from the paragraph's text, if any.\n\n#### Functionality:\n1. **Splitting the Markdown**: The function first splits the input markdown text by line breaks, processing each line to identify headers and paragraphs.\n2. **Handling Headers**: Headers are identified by lines starting with `#`. The level of the header (i.e., how many `#` characters it contains) determines the depth of the section in the document structure. New sections are created as subsections of the current section. If the header level is less than or equal to the current section's level, the function adjusts the section stack to properly organize the hierarchy.\n3. **Handling Paragraphs**: Text between headers is treated as paragraph content. Paragraphs are collected until an empty line or another header is encountered. Citations within paragraphs, enclosed in `<cite>` tags, are extracted using the `extract_citations` function.\n4. **Final Processing**: After processing all lines, any remaining paragraph text is added to the document structure, ensuring that no content is missed.\n\n#### Example:\nGiven the following markdown input:\n```markdown\n# Title of Document\n## Introduction\nThis is an introduction paragraph with a citation <cite>https://example.com</cite>.\n## Main Content\nThis is the main content, which also contains a citation <cite>https://another-example.com</cite>.\n```\n\nThe function will return a document structure like this:\n```python\n{\n    \"document\": {\n        \"title\": \"Title of Document\",\n        \"level\": 1,\n        \"subsections\": [\n            {\n                \"title\": \"Introduction\",\n                \"level\": 2,\n                \"subsections\": [],\n                \"paragraphs\": [\n                    {\n                        \"text\": \"This is an introduction paragraph with a citation.\",\n                        \"citations\": [\"https://example.com\"]\n                    }\n                ]\n            },\n            {\n                \"title\": \"Main Content\",\n                \"level\": 2,\n                \"subsections\": [],\n                \"paragraphs\": [\n                    {\n                        \"text\": \"This is the main content, which also contains a citation.\",\n                        \"citations\": [\"https://another-example.com\"]\n                    }\n                ]\n            }\n        ]\n    }\n}\n```\n\n#### Dependencies:\n- **extract_citations**: The function relies on `extract_citations` to extract any citations embedded in `<cite>` HTML tags within paragraphs.\n\n#### Usage:\nThe `parse_markdown_to_structure` function is typically used when there's a need to convert a markdown document into a structured format, such as when processing or analyzing documents with hierarchical content. The parsed structure can be used for further processing, such as generating reports or extracting specific information from different sections."
      ],
      "code_start_line": 143,
      "code_end_line": 218,
      "params": [
        "markdown_text"
      ],
      "have_return": true,
      "code_content": "def parse_markdown_to_structure(markdown_text):\n    \"\"\"从markdown文本解析出文档结构\"\"\"\n    lines = markdown_text.split('\\n')\n    document = {\n        \"document\": {\n            \"title\": \"\",\n            \"level\": 1,\n            \"subsections\": []\n        }\n    }\n    \n    current_section = document[\"document\"]\n    section_stack = [current_section]\n    current_level = 1\n    current_text = []\n    \n    for line in lines:\n        if line.strip():\n            # 处理标题\n            if line.startswith('#'):\n                # 如果有待处理的段落文本，先��理完\n                if current_text:\n                    paragraph_text = ' '.join(current_text)\n                    if paragraph_text.strip():\n                        citations = extract_citations(paragraph_text)\n                        current_section.setdefault(\"paragraphs\", []).append({\n                            \"text\": paragraph_text.strip(),\n                            \"citations\": citations\n                        })\n                    current_text = []\n                \n                # 处理新标题\n                level = len(line.split()[0])  # 计算#的数量\n                title = ' '.join(line.split()[1:])\n                \n                # 根据层级调整当前section\n                while len(section_stack) > 1 and level <= section_stack[-1][\"level\"]:\n                    section_stack.pop()\n                \n                new_section = {\n                    \"title\": title,\n                    \"level\": level,\n                    \"subsections\": [],\n                    \"paragraphs\": []\n                }\n                \n                section_stack[-1].setdefault(\"subsections\", []).append(new_section)\n                section_stack.append(new_section)\n                current_section = new_section\n                \n            else:\n                # 收集段落文本\n                current_text.append(line)\n        else:\n            # 空行，处理当前段落\n            if current_text:\n                paragraph_text = ' '.join(current_text)\n                if paragraph_text.strip():\n                    citations = extract_citations(paragraph_text)\n                    current_section.setdefault(\"paragraphs\", []).append({\n                        \"text\": paragraph_text.strip(),\n                        \"citations\": citations\n                    })\n                current_text = []\n    \n    # 处理最后一个段落\n    if current_text:\n        paragraph_text = ' '.join(current_text)\n        if paragraph_text.strip():\n            citations = extract_citations(paragraph_text)\n            current_section.setdefault(\"paragraphs\", []).append({\n                \"text\": paragraph_text.strip(),\n                \"citations\": citations\n            })\n\n    return document\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main"
      ],
      "reference_who": [
        "src/criticsearch/main_old_paralell.py/extract_citations"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: The function of main is to orchestrate the conversation process between a user and an intelligent agent, managing iterations of task execution and response refinement.\n\n**parameters**: The parameters of this Function.\n· TASK: A string representing the user's question or task that the agent is expected to address.\n· MAX_ITERATION: An integer indicating the maximum number of iterations the agent should perform to refine its response.\n\n**Code Description**: The main function serves as the entry point for executing the conversation logic of an intelligent agent. It begins by initializing a common agent instance of the BaseAgent class, which is responsible for handling user interactions, managing conversation history, and integrating various tools for search and content generation.\n\nThe function sets the user question for the agent by assigning the TASK parameter to the agent's user_question attribute. It also configures the logging level based on the settings defined in the project, ensuring that relevant messages are logged throughout the execution.\n\nThe conversation process is structured around a loop that iterates up to MAX_ITERATION times. During each iteration, the function performs several key operations:\n\n1. It checks the agent's confidence in its response to the TASK. If the agent is confident, it retrieves an answer directly using the common_chat method. If not, it initiates a search process to gather more information.\n\n2. In the first iteration, if the agent is not confident, it constructs a search prompt and retrieves initial search results. These results are used to generate an outline for the report, which is then flattened into a list of sections for parallel content generation.\n\n3. The function employs a ThreadPoolExecutor to generate content for each section concurrently, enhancing efficiency. Each section's content is generated based on the search results and the TASK context.\n\n4. After generating content for all sections, the function reconstructs the final Markdown document using the reconstructed_markdown function, which combines the outline structure with the generated content.\n\n5. The generated report is then polished and saved to a Markdown file. Additionally, the document structure is parsed and saved in JSON format for further use.\n\n6. In subsequent iterations, the agent updates its answer based on feedback from a CriticAgent, which evaluates the agent's responses and provides suggestions for improvement. The process continues until either the maximum number of iterations is reached or the CriticAgent indicates that the conversation should stop.\n\nThe main function effectively coordinates the interaction between the user, the intelligent agent, and the CriticAgent, ensuring that responses are refined based on real-time feedback and additional information gathered through searches.\n\n**Note**: It is crucial to ensure that the TASK parameter is well-defined and relevant to the agent's capabilities. The MAX_ITERATION parameter should be set appropriately to balance the need for thoroughness with the efficiency of the conversation process.\n\n**Output Example**: A possible return value from the main function could be a string summarizing the final answer provided by the agent, such as:\n```\n\"The comprehensive report on climate change highlights the significant impacts observed globally, including rising temperatures and extreme weather events.\"\n```"
      ],
      "code_start_line": 220,
      "code_end_line": 452,
      "params": [
        "TASK",
        "MAX_ITERATION"
      ],
      "have_return": true,
      "code_content": "def main(TASK, MAX_ITERATION):\n    # Initialize agents\n    common_agent = BaseAgent()\n\n    # initialize the task\n    common_agent.user_question = TASK\n\n    set_logger_level_from_config(log_level=settings.log_level.upper())\n\n    logger.success(f\"Starting the conversation with task: {TASK}\")\n\n    BaseAgent.conversation_manager.append_to_history(role=\"user\", content=TASK)\n\n    for iteration in range(MAX_ITERATION):\n        colorize_message(\n            message_title=f\"ITERATION {iteration + 1}\", color=\"cyan\", style=\"bold\"\n        )\n\n        if iteration == 0:\n            # Initialize search_results as None\n            search_results = None\n\n            # Model confidence check - yellow\n            agent_confident = common_agent.model_confident(TASK)\n            agent_confident_yaml = common_agent.extract_and_validate_yaml(\n                agent_confident\n            )\n\n            if agent_confident_yaml is None:\n                logger.warning(\n                    \"Failed to extract valid YAML content. Defaulting to 'false'.\"\n                )\n                agent_confident = False\n            else:\n                agent_confident_dict = yaml.safe_load(agent_confident_yaml)\n                agent_confident = (\n                    agent_confident_dict.get(\"confidence\", \"true\").lower() == \"true\"\n                )\n\n            if agent_confident:\n                # When confident, only get the answer\n                common_agent_answer = common_agent.common_chat(usr_prompt=TASK)\n            else:\n                # When not confident, get both answer and search results\n                # 并且第一次全面的搜索后的结果用来构建一个report的结构\n                data = {\n                    \"user_question\": TASK,\n                }\n                initial_search_prompt = common_agent.load_template(\n                    \"planner_agent_initial_search_plan.txt\"\n                )\n                initial_search_rendered_prompt = common_agent.render_template(\n                    initial_search_prompt, data\n                )\n                logger.info(\n                    f\"initial_search_rendered_prompt: {initial_search_rendered_prompt}\"\n                )\n\n                initial_web_result_markdown_text = common_agent.search_and_browse(\n                    initial_search_rendered_prompt\n                )# 这里返回的是模型决定了访问哪些后的网页爬取extract的结果\n\n                logger.info(f\"Initial web result: {initial_web_result_markdown_text}\")\n\n                # Generate report outline based on search results\n                outline_prompt = common_agent.render_template(\n                    common_agent.load_template(\"outline_generation.txt\"),\n                    {\n                        \"user_question\": common_agent.user_question,\n                        \"web_result_markdown_text\": initial_web_result_markdown_text,\n                    },\n                )\n\n                outline = common_agent.common_chat(\n                    usr_prompt=outline_prompt,\n                )\n\n                # verify the outline is a json string   \n                outline_json = common_agent.extract_and_validate_json(outline)\n                \n                # Flatten the outline to get all sections at all levels\n                flat_sections = flatten_outline(outline_json)\n                \n                # Generate content for all sections in parallel\n                with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n                    future_to_section = {\n                        executor.submit(generate_content_for_section, common_agent, item[\"section\"], TASK): item \n                        for item in flat_sections\n                    }\n                    \n                    # Collect results as they complete\n                    flat_contents = []\n                    for future in concurrent.futures.as_completed(future_to_section):\n                        item = future_to_section[future]\n                        try:\n                            content = future.result()\n                            flat_contents.append((item, content))\n                        except Exception as e:\n                            logger.error(f\"Error generating content for section {item['section'].get('title')}: {e}\")\n                \n                # Reconstruct the markdown with proper hierarchy\n                common_agent_answer = reconstruct_markdown(outline_json, flat_contents)\n                print(common_agent_answer)  # 这里是第一次生成的report,还没有polish\n\n                # 用deepseek润色一下得到正式的version1 report\n                polish_prompt = common_agent.load_template(\"polish_first_version.txt\")\n                polish_rendered_prompt = common_agent.render_template(\n                    polish_prompt,\n                    {\n                        \"task\": TASK,\n                        \"report\": common_agent_answer,\n                    },\n                )\n                common_agent_answer = common_agent.common_chat(\n                    usr_prompt=polish_rendered_prompt,\n                    model=\"gpt-4o\"\n                )\n                print(common_agent_answer)  # 这里是第一次生成的正式的polished report\n                # 保存到一个md\n                with open(\"first_version_report.md\", \"w\") as f:\n                    f.write(common_agent_answer)\n\n                # Polish后从markdown解析出文档结构\n                document_structure = parse_markdown_to_structure(common_agent_answer)\n                common_agent.document_structure = document_structure\n\n                # 保存到一个json文件\n                with open(\"document_structure.json\", \"w\") as f:\n                    json.dump(document_structure, f, indent=4, ensure_ascii=False)\n \n        else:\n            # 前面根据critc的返回得到了新的网页搜索结果web_result_markdown_text\n            common_agent_answer = common_agent.update_answer(\n                query=TASK,\n                previous_answer=common_agent_answer,\n                search_results=web_result_markdown_text,\n                critic_feedback=critic_agent_response,\n            )\n            time.sleep(0.1)  # hitting rate limits for gpt mini\n\n        # ========================== #\n        ## 这里在if-else结构之外 ##\n        colorize_message(\n            message_title=\"COMMON AGENT ANSWER\",\n            color=\"magenta\",\n            message_content=common_agent_answer,\n        )\n\n        # Critic evaluation - blue\n        critic_agent = CriticAgent()\n        critic_agent.receive_task(TASK)\n        critic_agent.receive_agent_answer(common_agent_answer)\n        critic_agent_response = critic_agent.critic()\n\n        colorize_message(\n            message_title=\"CRITIC_AGENT_RESPONSE\",\n            color=\"blue\",\n            message_content=critic_agent_response,\n        )\n\n        if yaml.safe_load(critic_agent_response).get(\"Stop\", {}).lower() == \"true\":\n            colorize_message(\n                message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n            )\n\n            colorize_message(\n                message_title=\"ALL SEARCH QUERIES\",\n                color=\"black\",\n                message_content=\", \".join(map(str, common_agent.queryDB)),\n            )\n            colorize_message(\n                message_title=\"FINAL ANSWER\",\n                color=\"red\",\n                message_content=common_agent_answer,\n            )\n\n            return f\"\\n{common_agent_answer}\\n\"\n\n        # 根据critic的建议再执行一次搜索和爬虫操作\n        # 先构建rendered_prompt\n        reflection_data = {\n            \"user_question\": TASK,\n            \"previous_answer\": common_agent_answer,\n            \"user_feedback\": critic_agent_response,\n            \"search_history\": common_agent.queryDB,\n        }\n        search_again_prompt = common_agent.render_template(\n            common_agent.load_template(\"planner_agent_with_reflection.txt\"),\n            reflection_data,\n        )\n        try:\n            web_result_markdown_text = common_agent.search_and_browse(\n                search_again_prompt\n            )\n        except:\n            colorize_message(\n                message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n            )\n\n            colorize_message(\n                message_title=\"ALL SEARCH QUERIES\",\n                color=\"black\",\n                message_content=\", \".join(map(str, common_agent.queryDB)),\n            )\n\n            colorize_message(\n                message_title=\"FINAL ANSWER\",\n                color=\"red\",\n                message_content=common_agent_answer,\n            )\n\n            # we run out of searches for now, so we force the agent to give a final answer:\n            return f\"\\n{common_agent_answer}\\n\"\n\n        # Check if reached max iterations\n        if iteration == MAX_ITERATION - 1:\n            colorize_message(\n                message_title=f\"TOTAL ITERATIONS: {iteration + 1}\", color=\"red\"\n            )\n\n            colorize_message(\n                message_title=\"ALL SEARCH QUERIES\",\n                color=\"black\",\n                message_content=\", \".join(map(str, common_agent.queryDB)),\n            )\n\n            colorize_message(\n                message_title=\"FINAL ANSWER\",\n                color=\"red\",\n                message_content=common_agent_answer,\n            )\n\n            return f\"\\n{common_agent_answer}\\n\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/update_answer",
        "src/criticsearch/base_agent.py/BaseAgent/model_confident",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/base_agent.py/BaseAgent/receive_task",
        "src/criticsearch/base_agent.py/BaseAgent/extract_and_validate_yaml",
        "src/criticsearch/base_agent.py/BaseAgent/extract_and_validate_json",
        "src/criticsearch/models.py/ConversationManager/append_to_history",
        "src/criticsearch/main_old_paralell.py/flatten_outline",
        "src/criticsearch/main_old_paralell.py/generate_content_for_section",
        "src/criticsearch/main_old_paralell.py/reconstruct_markdown",
        "src/criticsearch/main_old_paralell.py/parse_markdown_to_structure",
        "src/criticsearch/critic_agent.py/CriticAgent",
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/critic_agent.py/CriticAgent/receive_agent_answer"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/rich_output.py": [
    {
      "type": "ClassDef",
      "name": "RichPrinter",
      "md_content": [
        "**RichPrinter**: The function of RichPrinter is to provide a set of methods for printing styled messages, handling exception printing, and saving console output to a file.\n\n**attributes**:\n· console: Console  \n· default_title_style: str  \n· default_line_characters: str  \n\n**Code Description**:  \nThe `RichPrinter` class is designed to work with the `Console` class, most likely from the `rich` library, to enable styled and formatted output to the terminal. The class also includes features for printing exceptions and saving console output to a file.\n\n- **console**: This attribute stores an instance of the `Console` class. If no `Console` instance is provided during the initialization of `RichPrinter`, a new `Console` instance is created with `record=True`. The `record` parameter indicates that the console should capture the output for later use or export.\n  \n- **default_title_style**: This string attribute sets the default style for titles printed using the `rule` method. The default style is `[cyan bold]`, which means titles will appear in cyan and bold when printed.\n  \n- **default_line_characters**: This attribute defines the default character used to create a line or separator in printed rules. By default, the character is the equal sign (`=`).\n\nThe class provides the following methods:\n\n1. **__init__(self, console: Console = None)**:  \n   The constructor method initializes the `RichPrinter` object. If a `Console` object is passed as a parameter, it uses that; otherwise, it creates a new `Console` instance with the `record` set to `True`.\n\n2. **rule(self, title: str)**:  \n   This method prints a decorative rule (a line) with a title. The title is printed with the default title style (`[cyan bold]`), and the line separating the title is formed using the default line characters (`=`). It calls the `console.rule` method to render the rule.\n\n3. **log(self, message: str, style: str = None)**:  \n   This method prints a log message using the `console.log` method. The message is printed with an optional style, which defaults to `None` if no style is provided.\n\n4. **print(self, message: str, style: str = None)**:  \n   Similar to the `log` method, this method prints a regular message to the console using the `console.print` method. Again, the message can be styled by passing an optional `style` argument.\n\n5. **print_exception(self, message: str, max_frames: int = 5)**:  \n   This method prints an exception message with a bold red style using the `console.print_exception` method. It also allows controlling the number of stack frames shown when printing the exception, defaulting to 5 frames. Before printing the exception, it uses the `log` method to display the exception message in red, bold.\n\n6. **save_output_to_file(self, file_path: Path = Path(\"output.txt\"))**:  \n   This method saves the output of the `Console` object to a specified file. It creates a new `Console` instance that writes to the file. It exports the current text output captured by the original console and saves it to the given file path. The method ensures that a rule is added at the end of the file to mark that the output has been successfully written.\n\n**Note**:  \n- The `save_output_to_file` method exports the content of the console as plain text. It is important to ensure that the file path provided is accessible and writable.  \n- The default behavior of printing to the console and exporting to a file is meant to be flexible, allowing users to either view the output directly or save it for later inspection."
      ],
      "code_start_line": 7,
      "code_end_line": 45,
      "params": [],
      "have_return": false,
      "code_content": "class RichPrinter:\n    def __init__(self, console: Console = None):\n        # 如果没有传入 Console 对象，默认使用一个新的 Console 实例\n        self.console = console or Console(record=True)\n\n        # 默认样式配置\n        self.default_title_style = \"[cyan bold]\"\n        self.default_line_characters = \"=\"  # 使用 `=` 作为分隔线样式\n\n    def rule(self, title: str):\n        self.console.rule(\n            f\"{self.default_title_style}{title}\",\n            characters=self.default_line_characters,  # 默认使用 `=` 符号的分隔线\n        )\n\n    def log(self, message: str, style: str = None):\n        \"\"\"打印带有样式的日志消息\"\"\"\n        self.console.log(message, style=style)\n\n    def print(self, message: str, style: str = None):\n        \"\"\"打印普通的消息\"\"\"\n        self.console.print(message, style=style)\n\n    def print_exception(self, message: str, max_frames: int = 5):\n        printer.log(f\"{message}\", style=\"bold red\")\n\n        \"\"\"打印异常信息\"\"\"\n        self.console.print_exception(max_frames=max_frames)\n\n    def save_output_to_file(self, file_path: Path = Path(\"output.txt\")):\n        with open(file_path, \"wt\", encoding=\"utf-8\") as report_file:\n            # Redirect Console output to the specified file\n            console_for_export = Console(file=report_file)\n\n            # Export the console's current text output and write it to the file\n            console_for_export.log(self.console.export_text())\n\n            # Add a rule at the end of the output to indicate the file has been generated\n            console_for_export.rule(\"Output file Generated.\")\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the instance of the `RichPrinter` class and configure its attributes, including setting up a default `Console` instance and assigning default styles for the title and line separator.\n\n**parameters**: \n· console: Optional. An instance of the `Console` class. If not provided, a new `Console` object will be created.\n\n**Code Description**: \nThe `__init__` function is the constructor method of the `RichPrinter` class. It initializes the object with two main responsibilities:\n1. It accepts an optional `console` parameter, which is an instance of the `Console` class. If no `console` is passed to the constructor, it defaults to creating a new `Console` instance with the `record=True` parameter, enabling the recording of the output.\n2. It sets up default style and formatting attributes:\n   - `default_title_style`: This is set to \"[cyan bold]\", defining the default style for titles to be cyan and bold.\n   - `default_line_characters`: This is set to `\"=\"`, which specifies that the default character for line separators will be the equals sign (`=`).\n\nThis constructor ensures that the `RichPrinter` class has the necessary setup for its output, including the ability to log or record the output and use consistent styling for its visual presentation.\n\n**Note**: If a custom `Console` instance is passed, it will be used instead of the default one. Additionally, the default style settings can be overridden later in the class, but they serve as initial values for title and line separator styling."
      ],
      "code_start_line": 8,
      "code_end_line": 14,
      "params": [
        "self",
        "console"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, console: Console = None):\n        # 如果没有传入 Console 对象，默认使用一个新的 Console 实例\n        self.console = console or Console(record=True)\n\n        # 默认样式配置\n        self.default_title_style = \"[cyan bold]\"\n        self.default_line_characters = \"=\"  # 使用 `=` 作为分隔线样式\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "rule",
      "md_content": [
        "**rule**: The function of rule is to create a formatted horizontal rule in the console output with a specified title.\n\n**parameters**: The parameters of this Function.\n· title: A string that represents the title to be displayed alongside the rule.\n\n**Code Description**: The rule function is a method defined within the RichPrinter class. Its primary purpose is to generate a visually distinct horizontal rule in the console output, which is useful for separating different sections of output for better readability. The function takes a single parameter, title, which is a string that specifies the text to be displayed as part of the rule. \n\nInternally, the function calls the console's rule method, passing a formatted string that combines a default title style with the provided title. The characters parameter is set to self.default_line_characters, which defaults to the '=' symbol, indicating that the horizontal rule will be represented by this character. This method enhances the visual structure of console output, making it easier for users to identify different sections of information.\n\nThe rule function is invoked in two different contexts within the project. First, it is called in the generate_content_for_section function, where it is used to indicate the generation of content for a specific section based on the title extracted from the section dictionary. This helps to clearly delineate the output related to that section.\n\nSecond, it is also called in the process_single_task function, where it is used multiple times to indicate the start of iterations and to separate different outputs, such as the common agent's answer and the critic agent's response. This consistent use of the rule function throughout the codebase contributes to a well-organized and user-friendly console output.\n\n**Note**: When using the rule function, ensure that the title provided is meaningful and relevant to the context in which it is being used, as this will enhance the clarity of the console output."
      ],
      "code_start_line": 16,
      "code_end_line": 20,
      "params": [
        "self",
        "title"
      ],
      "have_return": false,
      "code_content": "    def rule(self, title: str):\n        self.console.rule(\n            f\"{self.default_title_style}{title}\",\n            characters=self.default_line_characters,  # 默认使用 `=` 符号的分隔线\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/generate_content_for_section",
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "log",
      "md_content": [
        "**log**: The function of log is to print a styled log message to the console.\n\n**parameters**: The parameters of this Function.\n· message: str - The log message that will be printed to the console.\n· style: str (optional) - The style to be applied to the log message.\n\n**Code Description**: The log function is a method within the RichPrinter class that is responsible for outputting log messages to the console with optional styling. When invoked, it takes a string message as a mandatory parameter and an optional style parameter that specifies how the message should be formatted when displayed.\n\nThe function utilizes the `self.console.log` method to print the message. This method is part of the Rich library, which allows for enhanced console output, including color and style formatting. The `style` parameter can be used to apply specific visual styles to the message, such as colors or bold formatting, enhancing the readability and visual appeal of the logs.\n\nThe log function is called within other parts of the project, specifically in the `search_and_browse` method of the BaseAgent class and the `process_single_task` function in the main module. In `search_and_browse`, it is used to log the response from a search operation, providing visibility into the internal workings of the search process. In `process_single_task`, it logs the initiation of a conversation with a specific task, allowing developers to trace the flow of execution and understand the context of the operations being performed.\n\nBy integrating the log function into these methods, the RichPrinter class plays a crucial role in maintaining a clear and informative logging system throughout the application. This logging capability is essential for debugging and monitoring the application's behavior, especially when dealing with asynchronous operations and complex workflows.\n\n**Note**: It is important to ensure that the message parameter is a well-formed string, and if a style is provided, it should be a valid style recognized by the Rich library. Proper usage of this function contributes to effective logging practices, aiding in the maintenance and troubleshooting of the application."
      ],
      "code_start_line": 22,
      "code_end_line": 24,
      "params": [
        "self",
        "message",
        "style"
      ],
      "have_return": false,
      "code_content": "    def log(self, message: str, style: str = None):\n        \"\"\"打印带有样式的日志消息\"\"\"\n        self.console.log(message, style=style)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/models.py/ConversationManager/__init__",
        "src/criticsearch/rich_output.py/RichPrinter/print_exception",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponseList/ser_model",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "src/criticsearch/tools/tool_registry.py/ToolRegistry/get_or_create_tool_schema"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "print",
      "md_content": [
        "**print**: The function of print is to output a message to the console with an optional styling.\n\n**parameters**: The parameters of this Function.\n· message: A string that contains the message to be printed to the console.\n· style: An optional string that specifies the style in which the message should be printed.\n\n**Code Description**: The print function is designed to display a message on the console, utilizing the console's print method. It takes two parameters: 'message', which is a required string that represents the content to be printed, and 'style', which is an optional parameter that allows the user to specify a particular style for the output. If no style is provided, the message will be printed in the default format.\n\nThis function is called within the context of other functions in the project, specifically in `generate_content_for_section` and `process_single_task`. In `generate_content_for_section`, the print function is used to output the generated content for a specific section, ensuring that the user is informed of the progress and results of the content generation process. Similarly, in `process_single_task`, the print function is employed to log various messages, including extracted thought processes, queries, and the final answers generated by the common agent. This highlights the function's role in providing feedback and information to the user throughout the execution of tasks.\n\n**Note**: It is important to ensure that the message parameter is always a string, as passing non-string types may lead to unexpected behavior. Additionally, when using the style parameter, users should be aware of the available styles supported by the console to achieve the desired output appearance."
      ],
      "code_start_line": 26,
      "code_end_line": 28,
      "params": [
        "self",
        "message",
        "style"
      ],
      "have_return": false,
      "code_content": "    def print(self, message: str, style: str = None):\n        \"\"\"打印普通的消息\"\"\"\n        self.console.print(message, style=style)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/generate_content_for_section",
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "print_exception",
      "md_content": [
        "**print_exception**: The function of print_exception is to log an error message and print the exception details to the console.\n\n**parameters**: The parameters of this Function.\n· message: str - The message to be logged, indicating the nature of the exception.\n· max_frames: int (optional) - The maximum number of stack frames to display when printing the exception details, defaulting to 5.\n\n**Code Description**: The print_exception function is a method within the RichPrinter class that serves to handle and display exception information in a user-friendly manner. When invoked, it first logs the provided message using the log method, which formats the message in bold red style to draw attention to the error. This is achieved through the call to printer.log(f\"{message}\", style=\"bold red\"). \n\nFollowing the logging of the message, the function utilizes the self.console.print_exception method to print the actual exception details to the console. This method is part of the Rich library, which enhances console output by providing formatted and colored text. The max_frames parameter allows the user to control how many frames of the stack trace are displayed, with a default value of 5, which helps in limiting the amount of information shown while still providing enough context for debugging.\n\nThe print_exception function is called in the critic method of the CriticAgent class when an error occurs during the extraction and validation of YAML content. If the YAML extraction fails, the print_exception function is invoked with a specific message indicating the failure. This integration ensures that any issues encountered during the critique process are logged and displayed, allowing developers to trace errors effectively.\n\nBy providing a clear logging mechanism and formatted output for exceptions, the print_exception function plays a crucial role in maintaining the robustness of the application. It aids in debugging by ensuring that error messages are visible and informative, thereby facilitating easier troubleshooting.\n\n**Note**: It is important to ensure that the message parameter is a well-formed string. The max_frames parameter should be set according to the level of detail required for debugging, keeping in mind that excessive stack trace information may clutter the console output. Proper usage of this function contributes to effective error handling practices within the application."
      ],
      "code_start_line": 30,
      "code_end_line": 34,
      "params": [
        "self",
        "message",
        "max_frames"
      ],
      "have_return": false,
      "code_content": "    def print_exception(self, message: str, max_frames: int = 5):\n        printer.log(f\"{message}\", style=\"bold red\")\n\n        \"\"\"打印异常信息\"\"\"\n        self.console.print_exception(max_frames=max_frames)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/critic_agent.py/CriticAgent/critic",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/models.py/ConversationManager/__init__",
        "src/criticsearch/models.py/ConversationManager/write",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/log"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "save_output_to_file",
      "md_content": [
        "**save_output_to_file**: The function of save_output_to_file is to save the current console output to a specified file.\n\n**parameters**: The parameters of this Function.\n· file_path: A Path object representing the location of the file where the console output will be saved. The default value is \"output.txt\".\n\n**Code Description**:  \nThe `save_output_to_file` function is responsible for saving the current console output to a file. The function takes an optional parameter, `file_path`, which specifies the path where the output will be stored. If not provided, the output is saved to a default file named \"output.txt\". \n\nThe function works as follows:\n1. It opens the file at the specified `file_path` in write-text mode (`\"wt\"`), ensuring the file is created if it does not already exist. The file is opened with UTF-8 encoding to handle text properly.\n2. A `Console` object is created, with the file (`report_file`) passed as the output stream. This step redirects the console's output to the file.\n3. The function then calls the `export_text` method of the `console` object to retrieve the current console output in text format.\n4. The retrieved text is written to the file using the `log` method of the `Console` object.\n5. Finally, a rule (a separator) is added to the output to indicate that the file has been generated, marking the end of the saved content.\n\nThis function ensures that the console's output is properly saved to a file and that the file is marked as complete.\n\n**Note**: \n- The function opens the file in text mode with UTF-8 encoding, so it is important to ensure the file system supports this encoding.\n- The function automatically creates the file if it does not already exist, but overwrites the file if it already exists, without any prompt for user confirmation. \n- The `console.export_text()` method should be correctly populated with the desired console output for accurate results."
      ],
      "code_start_line": 36,
      "code_end_line": 45,
      "params": [
        "self",
        "file_path"
      ],
      "have_return": false,
      "code_content": "    def save_output_to_file(self, file_path: Path = Path(\"output.txt\")):\n        with open(file_path, \"wt\", encoding=\"utf-8\") as report_file:\n            # Redirect Console output to the specified file\n            console_for_export = Console(file=report_file)\n\n            # Export the console's current text output and write it to the file\n            console_for_export.log(self.console.export_text())\n\n            # Add a rule at the end of the output to indicate the file has been generated\n            console_for_export.rule(\"Output file Generated.\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/llm_service.py": [
    {
      "type": "ClassDef",
      "name": "ModelManager",
      "md_content": [
        "**ModelManager**: The function of ModelManager is to manage model configurations and clients, enabling the creation of OpenAI API clients and retrieval of model-specific configurations.\n\n**attributes**:\n- config: Stores the configuration object containing model configurations and other settings.\n- clients: A dictionary that holds initialized client objects indexed by model names.\n\n**Code Description**:  \nThe `ModelManager` class is designed to handle the management of model configurations and the creation of API clients that interact with OpenAI’s services.\n\n- **`__init__(self, config)`**:  \n  The constructor initializes the `ModelManager` with the given `config` parameter, which contains the settings for the models, such as API keys, URLs, and other relevant configurations. It also initializes an empty `clients` dictionary that will store the client instances corresponding to each model.\n\n- **`get_model_config(self, model_name=None)`**:  \n  This method retrieves the configuration for a specific model. If no model name is provided, it defaults to the first model in the configuration. The function raises an error if no models are found or if the specified model name is not available in the configuration. It returns the configuration dictionary for the selected model.\n\n- **`create_client(self, model_name=None)`**:  \n  This method is responsible for creating and returning a client object. It checks if the client for the specified model already exists in the `clients` dictionary. If not, it retrieves the configuration for the model using `get_model_config`, and initializes an OpenAI client (using the `OpenAI` class, which is not shown in the provided code but is assumed to be a predefined class or function) with the relevant settings like the API key, base URL, timeout, and retry parameters. Once created, the client is stored in the `clients` dictionary for reuse.\n\nFrom a functional perspective, the `ModelManager` class is used by the `call_llm` function in the project. The `call_llm` function interacts with `ModelManager` to:\n1. Instantiate a `ModelManager` object using the provided configuration.\n2. Create a client for the specified model (or default to the first model if no model name is provided).\n3. Retrieve the configuration for the selected model.\n4. Use the configuration and client to make API calls to OpenAI’s services and process the responses.\n\nThe `ModelManager` acts as a central utility that abstracts away the complexity of managing multiple models and their configurations. By using this class, the system can easily handle different model configurations and create clients for various models dynamically.\n\n**Note**:\n- The `config` passed into the `ModelManager` should contain a \"models\" key that maps model names to their respective configurations.\n- If a model is not found in the configuration, a `ValueError` will be raised, so proper error handling should be considered when using this class.\n- The OpenAI client is created with a default timeout of 120 seconds and the possibility to customize retry settings, which are important for dealing with API reliability.\n\n**Output Example**:  \nWhen `create_client` is called with a model name like \"gpt-4\", the output will be an instance of an OpenAI client initialized with that model’s configuration, assuming that the configuration contains the appropriate API key and endpoint. The exact appearance of this client is dependent on the implementation of the OpenAI client, but it will allow making requests to OpenAI's API to process prompts and retrieve responses."
      ],
      "code_start_line": 11,
      "code_end_line": 48,
      "params": [],
      "have_return": true,
      "code_content": "class ModelManager:\n    def __init__(self, config):\n        self.config = config\n        self.clients = {}\n\n    def get_model_config(self, model_name=None):\n        models = self.config.get(\"models\", {})\n        if not models:\n            raise ValueError(\"No models found in configuration.\")\n\n        if model_name is None:\n            model_name = next(iter(models.keys()))\n\n        if model_name not in models:\n            raise ValueError(\n                f\"Model '{model_name}' not found in configuration. Available models: {list(models.keys())}\"\n            )\n\n        model_config = models.get(model_name, {})\n        return model_config\n\n    def create_client(self, model_name=None):\n        if model_name is None:\n            model_name = next(iter(self.config.models.keys()))\n\n        if model_name in self.clients:\n            return self.clients[model_name]\n\n        model_config = self.get_model_config(model_name)\n        client = OpenAI(\n            api_key=model_config.get(\"api_key\"),\n            base_url=model_config.get(\"base_url\", \"https://api.openai.com/v1\"),\n            timeout=self.config.get(\"timeout\", 120),\n            max_retries=self.config.get(\"max_retries\"),\n        )\n\n        self.clients[model_name] = client\n        return client\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/llm_service.py/call_llm"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the class with a configuration and an empty client dictionary.\n\n**parameters**: The parameters of this Function.\n· config: A configuration object or data used to initialize the instance.\n\n**Code Description**: \nThe `__init__` function is the constructor method of the class, used to initialize an object of the class when it is instantiated. \n- The `config` parameter is passed into the constructor and assigned to the instance variable `self.config`. This suggests that the configuration data will be available throughout the lifetime of the object for further use. \n- The `clients` instance variable is initialized as an empty dictionary (`{}`). This implies that the object will eventually store client-related data or mappings in this dictionary during its lifecycle.\n\n**Note**: \n- The `config` parameter should be provided when creating an instance of the class; its structure and contents will depend on the requirements of the class and the overall application.\n- The `clients` dictionary is initialized as empty, and its population will likely occur later in the class methods."
      ],
      "code_start_line": 12,
      "code_end_line": 14,
      "params": [
        "self",
        "config"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, config):\n        self.config = config\n        self.clients = {}\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_model_config",
      "md_content": [
        "**get_model_config**: The function of get_model_config is to retrieve the configuration settings for a specific model from the configuration file.\n\n**parameters**:\n· model_name: A string representing the name of the model. If not provided, the first model in the configuration is selected.\n\n**Code Description**:  \nThe `get_model_config` function is responsible for fetching the configuration for a specified model from the configuration dictionary stored in the `self.config` attribute. \n\n1. The function starts by retrieving the \"models\" section of the configuration using `self.config.get(\"models\", {})`. If no models are found, a `ValueError` is raised with the message \"No models found in configuration.\"\n   \n2. If the `model_name` parameter is not provided, the function selects the first available model by using `next(iter(models.keys()))`.\n\n3. The function checks if the `model_name` exists in the models configuration. If it is not found, a `ValueError` is raised indicating the model is not in the configuration, and it provides a list of available models.\n\n4. If the model is found, the function retrieves its configuration from the `models` dictionary and returns it.\n\nThe function is used by other components in the project to ensure that the correct configuration for a model is retrieved. For example, in the `create_client` method, it is used to get the configuration of a specific model when creating an OpenAI client. It is also utilized in the `call_llm` function, where it is used to get the model configuration to set up parameters for making API calls.\n\n**Note**:  \n- The function raises errors if the models section is missing or if the specified model is not found in the configuration.\n- The function defaults to the first model in the configuration if no model name is provided.\n\n**Output Example**:  \nIf the configuration for the model \"gpt-3\" contains the following settings:\n```json\n{\n  \"models\": {\n    \"gpt-3\": {\n      \"api_key\": \"your-api-key\",\n      \"base_url\": \"https://api.openai.com/v1\",\n      \"temperature\": 0.7,\n      \"max_tokens\": 150\n    }\n  }\n}\n```\nCalling `get_model_config(\"gpt-3\")` would return:\n```python\n{\n  \"api_key\": \"your-api-key\",\n  \"base_url\": \"https://api.openai.com/v1\",\n  \"temperature\": 0.7,\n  \"max_tokens\": 150\n}\n```"
      ],
      "code_start_line": 16,
      "code_end_line": 30,
      "params": [
        "self",
        "model_name"
      ],
      "have_return": true,
      "code_content": "    def get_model_config(self, model_name=None):\n        models = self.config.get(\"models\", {})\n        if not models:\n            raise ValueError(\"No models found in configuration.\")\n\n        if model_name is None:\n            model_name = next(iter(models.keys()))\n\n        if model_name not in models:\n            raise ValueError(\n                f\"Model '{model_name}' not found in configuration. Available models: {list(models.keys())}\"\n            )\n\n        model_config = models.get(model_name, {})\n        return model_config\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/llm_service.py/ModelManager/create_client",
        "src/criticsearch/llm_service.py/call_llm"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "create_client",
      "md_content": [
        "**create_client**: The function of create_client is to create and return an OpenAI client instance for a specified model.\n\n**parameters**: The parameters of this Function.\n· model_name: A string representing the name of the model for which the client is to be created. If not provided, the first model in the configuration is selected.\n\n**Code Description**: The `create_client` function is responsible for instantiating and returning an OpenAI client based on the specified model name. The function first checks if a model name has been provided; if not, it defaults to the first model available in the configuration. \n\nThe function then checks if a client for the specified model already exists in the `self.clients` dictionary. If it does, the existing client is returned, avoiding the overhead of creating a new instance. \n\nIf the client does not exist, the function retrieves the model's configuration using the `get_model_config` method. This method fetches the necessary API key, base URL, timeout, and maximum retries for the model from the configuration settings. \n\nAn instance of the OpenAI client is then created using the retrieved configuration parameters. This instance is stored in the `self.clients` dictionary for future use and is returned to the caller.\n\nThe `create_client` function is called by the `call_llm` function, which is responsible for making API calls to the OpenAI service. In `call_llm`, the `create_client` function is invoked to obtain the client needed to interact with the OpenAI API, ensuring that the correct configuration is used for the specified model. This establishes a direct relationship where `call_llm` relies on `create_client` to provide the necessary client instance for executing its operations.\n\n**Note**: It is important to ensure that the model name provided is valid and exists in the configuration. If the model name is not found, the `get_model_config` method will raise an error, which will propagate back to the caller, potentially affecting the execution of the `call_llm` function.\n\n**Output Example**: If the model name \"gpt-3\" is specified and the configuration is correctly set, the function would return an instance of the OpenAI client configured for \"gpt-3\". The returned client would be capable of making API calls to the OpenAI service with the specified parameters."
      ],
      "code_start_line": 32,
      "code_end_line": 48,
      "params": [
        "self",
        "model_name"
      ],
      "have_return": true,
      "code_content": "    def create_client(self, model_name=None):\n        if model_name is None:\n            model_name = next(iter(self.config.models.keys()))\n\n        if model_name in self.clients:\n            return self.clients[model_name]\n\n        model_config = self.get_model_config(model_name)\n        client = OpenAI(\n            api_key=model_config.get(\"api_key\"),\n            base_url=model_config.get(\"base_url\", \"https://api.openai.com/v1\"),\n            timeout=self.config.get(\"timeout\", 120),\n            max_retries=self.config.get(\"max_retries\"),\n        )\n\n        self.clients[model_name] = client\n        return client\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/llm_service.py/call_llm"
      ],
      "reference_who": [
        "src/criticsearch/llm_service.py/ModelManager/get_model_config"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "call_llm",
      "md_content": [
        "## `call_llm` Function Documentation\n\n### Purpose:\nThe `call_llm` function interacts with a language model API to generate responses based on a user-provided prompt. It handles communication with the OpenAI API or a similar service, utilizing a model configuration and client provided through the `ModelManager`.\n\n### Parameters:\n- **model** (`str`):  \n  The name of the model to be used for generating the response (e.g., `\"gpt-4\"`). It is passed to the `ModelManager` to create the appropriate client.\n  \n- **usr_prompt** (`str | Iterable[ChatCompletionMessageParam]`):  \n  The user prompt to send to the model. It can either be a single string message or an iterable of `ChatCompletionMessageParam` objects. The prompt is the basis of the model's response.\n\n- **config** (`dict`):  \n  A configuration dictionary used to initialize the `ModelManager`. It includes settings related to model configurations such as API keys, timeout settings, and retry parameters.\n\n- **tools** (`List | None`, optional):  \n  A list of tools that can be used by the model during the interaction. This is an optional parameter. If not provided, the function defaults to `None`.\n\n### Returns:\n- **ChatCompletionMessage**:  \n  The generated response message from the model. This object contains the model's output based on the provided prompt.\n\n### Functionality:\n1. **Initialize ModelManager**:  \n   A `ModelManager` instance is created using the provided configuration (`config`). This instance is responsible for managing model-specific settings and clients.\n\n2. **Create Model Client**:  \n   The `create_client` method of `ModelManager` is used to create a client for the specified model. If a client for the model does not already exist, it is initialized and stored.\n\n3. **Configure Prompt**:  \n   The `usr_prompt` is processed. If it is a string, it is converted into a message format. If it is already an iterable of `ChatCompletionMessageParam`, it is used as-is.\n\n4. **API Request**:  \n   The function sends the prompt to the OpenAI API (or a compatible service) via the client, using settings such as `temperature` and `max_tokens` from the model's configuration. If tools are provided, they are passed along with the request.\n\n5. **Handle Response**:  \n   The function extracts the response message from the API's result. If the model supports tools, it handles the response accordingly. If not, it reattempts the request without tools.\n\n6. **Error Handling**:  \n   The function raises specific exceptions in case of errors, including:\n   - `APIConnectionError`: If there is an issue connecting to the API.\n   - `ValueError`: If there is an issue with the configuration or model settings.\n   - `BadRequestError`: If the model does not support tools (in which case, a retry without tools is performed).\n\n### Example Usage:\n```python\nresponse = call_llm(\n    model=\"gpt-4\",\n    usr_prompt=\"What is the capital of France?\",\n    config=config_dict\n)\n```\n\n### Notes:\n- The `model` parameter should correspond to a valid model name as specified in the configuration.\n- The `config` dictionary must include all necessary settings for model initialization, including API keys and any model-specific options.\n- The `tools` parameter is optional, and should only be used if supported by the model. If the model does not support tools, the function will handle this by retrying without them.\n- Proper error handling should be implemented to manage exceptions such as connection failures or invalid configurations."
      ],
      "code_start_line": 51,
      "code_end_line": 93,
      "params": [
        "model",
        "usr_prompt",
        "config",
        "tools"
      ],
      "have_return": true,
      "code_content": "def call_llm(\n    model,\n    usr_prompt: str | Iterable[ChatCompletionMessageParam],\n    config,\n    tools: List | None = None,\n) -> ChatCompletionMessage:\n    model_manager = ModelManager(config)\n    client = model_manager.create_client(model)\n\n    # 从 ModelManager 获取配置\n    model_config = model_manager.get_model_config(model)\n    if isinstance(usr_prompt, str):\n        messages = [ChatCompletionUserMessageParam(content=usr_prompt, role=\"user\")]\n    else:\n        messages = usr_prompt\n\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            temperature=model_config.get(\"temperature\", None),\n            max_tokens=model_config.get(\"max_tokens\", None),\n            tools=tools,  # type: ignore\n        )\n\n        response_message = response.choices[0].message\n        return response_message\n\n    except APIConnectionError as e:\n        raise RuntimeError(f\"Failed to connect to OpenAI API: {e}\")\n    except ValueError as e:\n        raise ValueError(f\"Error in configuration or model: {e}\")\n    except BadRequestError:\n        # Some model like gemini-2.0-flash-thinking-exp may not support tools and raise BadRequestError\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            temperature=model_config.get(\"temperature\", None),\n            max_tokens=model_config.get(\"max_tokens\", None),\n        )\n\n        response_message = response.choices[0].message\n        return response_message\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)",
        "tests/test_function_call_between_models.py",
        "tests/test_function_call_between_models.py/test_function_call",
        "tests/test_multiple_same_role.py"
      ],
      "reference_who": [
        "src/criticsearch/llm_service.py/ModelManager",
        "src/criticsearch/llm_service.py/ModelManager/get_model_config",
        "src/criticsearch/llm_service.py/ModelManager/create_client"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/config.py": [],
  "src/criticsearch/utils.py": [
    {
      "type": "FunctionDef",
      "name": "count_tokens",
      "md_content": [
        "**count_tokens**: The function of count_tokens is to calculate the number of tokens in a given text string based on a specified model.\n\n**parameters**:\n· text: The input text that needs to be tokenized.\n· model: The model to be used for tokenization. Default is \"gpt-4o\". This parameter allows the selection of different models that might affect the tokenization method. Common options include \"gpt-3.5-turbo\", \"gpt-4\", and \"text-embedding-ada-002\".\n\n**Code Description**: \nThe function `count_tokens` is designed to calculate the number of tokens in a provided text based on a selected model for tokenization. The process begins by determining the correct encoding method for the specified model using the `tiktoken` library, which is responsible for breaking the text into tokens.\n\n1. The function first attempts to fetch the appropriate encoding for the specified model using `tiktoken.encoding_for_model(model)`.\n2. If the provided model does not exist or is unsupported (raising a `KeyError`), it defaults to using a fallback encoding, `tiktoken.get_encoding(\"cl100k_base\")`.\n3. The text is then encoded into tokens using the encoding method, and the number of tokens is returned by evaluating the length of the tokenized text.\n\nThe function is used in scenarios where it's necessary to measure the token count of a text, particularly when dealing with AI models like OpenAI's GPT family, which have token limits for inputs. This is especially useful in contexts such as text preprocessing or chunking operations, where token limits need to be respected.\n\nIn the project, `count_tokens` is employed within the `extract_sections` function, which recursively processes sections of a dataset to extract content, titles, and their respective paths. The token count for each section's content is calculated and stored in the resulting data structure. The token count is a crucial piece of information because it helps in ensuring that sections do not exceed certain token limits, particularly when preparing inputs for processing by machine learning models, such as in the case of the `sliding_window_pairing` function.\n\nIn the `sliding_window_pairing` function, the `count_tokens` function is critical for ensuring that each \"window\" of text remains within a specified token limit (`max_token_length`). By calculating the token count for each section, the function can group sections together in a way that ensures the total number of tokens in any given window does not exceed the maximum allowable tokens for the model, making it essential for efficient and valid data processing.\n\n**Note**:\n- The `count_tokens` function depends on the `tiktoken` library, which must be available in the environment.\n- If an unsupported model is passed, the function will use a fallback tokenization model, but results may differ slightly depending on the encoding used.\n- The function is optimized to handle different models, and using an inappropriate model might result in less accurate token counts if the fallback encoding is used.\n\n**Output Example**:\nFor example, when calling `count_tokens(\"Hello, world!\")`, the function will return the token count of the string. Assuming \"Hello\" and \"world!\" are treated as separate tokens, the output might be:\n\n```\n4\n```"
      ],
      "code_start_line": 4,
      "code_end_line": 26,
      "params": [
        "text",
        "model"
      ],
      "have_return": true,
      "code_content": "def count_tokens(text: str, model: str = \"gpt-4o\") -> int:\n    \"\"\"\n    Calculate the number of tokens in a text string.\n    \n    Args:\n        text: The text to tokenize\n        model: The model to use for tokenization (default: \"gpt-3.5-turbo\")\n            Some common options: \"gpt-3.5-turbo\", \"gpt-4\", \"text-embedding-ada-002\"\n    \n    Returns:\n        int: The number of tokens in the text\n    \n    Example:\n        >>> count_tokens(\"Hello, world!\")\n        4\n    \"\"\"\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        # Fall back to cl100k_base encoding if model not found\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n    \n    return len(encoding.encode(text))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/sliding_window_pairing/extract_sections",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/sliding_window_pairing"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_queries_from_response",
      "md_content": [
        "## `extract_queries_from_response` Function\n\n### Description\nThe `extract_queries_from_response` function is designed to extract a list of queries from a given response text. The function supports multiple formats for the input text and processes them to return a list of queries. If no valid queries are found, the function returns an empty list.\n\n### Parameters\n- **response_text** (`str`): The input response text containing potential query information. The response text can be in one of the following formats:\n  - `<queries>[query1, query2]</queries>`\n  - `<queries>List[str] = [query1, query2]</queries>`\n  - `<queries> [query1, query2] </queries>`\n\n### Returns\n- **list**: A list of extracted queries. If no queries are found, an empty list is returned.\n\n### Functionality\n1. The function first strips any leading or trailing whitespace from the input `response_text`.\n2. It then attempts to match the text against two possible patterns:\n   - The first pattern matches a format where queries are wrapped in a `List[str] = [...]` structure.\n   - The second pattern matches the standard array-like format `[query1, query2]`.\n3. If a match is found, the queries are extracted and further processed:\n   - The extracted query string is split based on regular expressions, taking into account commas within quotes to avoid splitting queries incorrectly.\n   - Each part is cleaned of any surrounding quotes and extra whitespace.\n4. The cleaned queries are returned as a list.\n5. If no valid queries are extracted, the function returns an empty list.\n\n### Example\n```python\nresponse_text = \"<queries>[\\\"query1\\\", \\\"query2\\\"]</queries>\"\nqueries = extract_queries_from_response(response_text)\nprint(queries)  # Output: ['query1', 'query2']\n```\n\n### Notes\n- The function supports both single-quoted and double-quoted strings for the queries.\n- It uses regular expressions to identify and extract the query list, ensuring flexibility in handling different formatting variations."
      ],
      "code_start_line": 28,
      "code_end_line": 70,
      "params": [
        "response_text"
      ],
      "have_return": true,
      "code_content": "def extract_queries_from_response(response_text: str) -> list:\n    \"\"\"\n    从响应文本中提取查询列表，支持多种格式\n    \n    Args:\n        response_text: 响应文本，支持以下格式：\n            - <\\queries>[query1, query2]<\\queries>\n            - <\\queries>List[str] = [query1, query2]<\\queries>\n            - <\\queries> [query1, query2] <\\queries>\n        \n    Returns:\n        list: 提取的查询列表,如果未找到则返回空列表\n    \"\"\"\n    # 预处理文本\n    response_text = response_text.strip()\n    \n    # 匹配两种可能的格式\n    patterns = [\n        # 匹配 List[str] = [...] 格式\n        r'queries>(?:\\s*List\\[str\\]\\s*=)?\\s*\\[(.*?)\\](?:</queries>|<)',\n        # 匹配普通数组格式\n        r'queries>\\s*\\[(.*?)\\](?:</queries>|<)'\n    ]\n    \n    for pattern in patterns:\n        match = re.search(pattern, response_text, re.DOTALL | re.IGNORECASE)\n        if match:\n            queries_str = match.group(1)\n            # 处理查询列表\n            queries = []\n            # 使用正则表达式分割，考虑引号内的逗号\n            parts = re.findall(r'\"([^\"]*?)\"|\\'([^\\']*?)\\'|([^,]+)', queries_str)\n            for part in parts:\n                # part是一个元组，包含三个捕获组，取非空的那个\n                query = next((p.strip() for p in part if p.strip()), '')\n                if query:\n                    # 清理引号和多余空格\n                    query = query.strip('\"\\'').strip()\n                    if query:\n                        queries.append(query)\n            return queries\n            \n    return []\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_thought_from_response",
      "md_content": [
        "**extract_thought_from_response**: The function of extract_thought_from_response is to extract the thought content from a given response text formatted with specific tags.\n\n**parameters**: The parameters of this Function.\n· response_text: A string containing the response text that includes thought content formatted with `<\\thought>...<\\thought>`.\n\n**Code Description**: The extract_thought_from_response function is designed to parse a string input, specifically looking for content enclosed within the tags `<thought>` and `</thought>`. It utilizes a regular expression pattern to identify and extract the desired content. The function first compiles a regex pattern that matches any text between the `<thought>` and `</thought>` tags. It then applies this pattern to the provided response_text using the re.search method, which allows for matching across multiple lines due to the re.DOTALL flag. If a match is found, the function returns the extracted thought content, stripped of any leading or trailing whitespace. If no match is found, it returns an empty string.\n\nThis function is called within the process_single_task function located in the src/criticsearch/main.py file. During the execution of process_single_task, after generating a response from an agent, extract_thought_from_response is invoked to retrieve the thought process from the agent's response. This extracted thought content is then logged and can be used for further processing, such as appending to conversation data or for debugging purposes. The function plays a crucial role in ensuring that the thought processes of the agent are captured and utilized effectively within the broader task processing workflow.\n\n**Note**: It is important to ensure that the response_text provided to this function is correctly formatted with the appropriate thought tags; otherwise, the function will not be able to extract any content and will return an empty string.\n\n**Output Example**: Given a response_text of \"Here is my thought: <thought>This is the extracted thought content.</thought>\", the function would return \"This is the extracted thought content.\""
      ],
      "code_start_line": 72,
      "code_end_line": 86,
      "params": [
        "response_text"
      ],
      "have_return": true,
      "code_content": "def extract_thought_from_response(response_text: str) -> str:\n    \"\"\"\n    从响应文本中提取thought内容\n    \n    Args:\n        response_text: 包含<\\thought>...<\\thought>格式的响应文本\n        \n    Returns:\n        str: 提取的thought内容,如果未找到则返回空字符串\n    \"\"\"\n    thought_pattern = r'thought>(.*?)<'\n    thought_match = re.search(thought_pattern, response_text, re.DOTALL)\n    if thought_match:\n        return thought_match.group(1).strip()\n    return \"\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_answer_from_response",
      "md_content": [
        "**extract_answer_from_response**: The function of extract_answer_from_response is to extract the content of the answer from a given response text formatted with specific tags.\n\n**parameters**: The parameters of this Function.\n· response_text: A string containing the response text that includes the answer enclosed within `<answer>` tags.\n\n**Code Description**: The extract_answer_from_response function is designed to parse a string input, specifically looking for content that is encapsulated within `<answer>` tags. It utilizes a regular expression pattern to identify and extract the text that appears between these tags. The function employs the `re.search` method from the `re` module, which searches the input string for the specified pattern. If a match is found, the function retrieves the matched content, trims any leading or trailing whitespace, and returns it as a string. If no match is found, the function returns an empty string.\n\nThis function is called within the process_single_task function located in the src/criticsearch/main.py file. In this context, extract_answer_from_response is utilized to extract the answer content from the section content generated by the common agent during the processing of a task. After generating the section content, the function is invoked to retrieve the answer, which is then stored in the conversation data structure along with other relevant information such as thought content and citations. This integration is crucial for maintaining the flow of information and ensuring that the generated reports contain the necessary answers derived from the agent's responses.\n\n**Note**: It is important to ensure that the response_text provided to this function is correctly formatted with the `<answer>` tags; otherwise, the function will return an empty string, indicating that no answer content was found.\n\n**Output Example**: If the input response_text is \"Here is the answer: <answer>This is the extracted answer.</answer>\", the function will return \"This is the extracted answer.\""
      ],
      "code_start_line": 88,
      "code_end_line": 102,
      "params": [
        "response_text"
      ],
      "have_return": true,
      "code_content": "def extract_answer_from_response(response_text: str) -> str:\n    \"\"\"\n    从响应文本中提取answer内容\n    \n    Args:\n        response_text: 包含<\\answer>...<\\answer>格式的响应文本\n        \n    Returns:\n        str: 提取的answer内容,如果未找到则返回空字符串\n    \"\"\"\n    answer_pattern = r'answer>(.*?)<'\n    answer_match = re.search(answer_pattern, response_text, re.DOTALL)\n    if answer_match:\n        return answer_match.group(1).strip()\n    return \"\"\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_citations",
      "md_content": [
        "**extract_citations**: The function of extract_citations is to extract all referenced URLs from a given text and ensure they are unique.\n\n**parameters**: The parameters of this Function.\n· text: A string that contains text formatted with <\\citation>URL<\\citation> tags.\n\n**Code Description**: The extract_citations function is designed to identify and extract URLs that are enclosed within specific citation tags in a provided text. The function takes a single argument, 'text', which is expected to be a string containing one or more citations formatted as <\\citation>URL<\\citation>. \n\nThe function initializes an empty set called 'citations' to store the unique URLs. It then defines a regular expression pattern, `r'citation>(.*?)<'`, which is used to search for matches in the input text. This pattern looks for any substring that starts with 'citation>' and ends with '<', capturing the content in between. \n\nUsing the `re.findall` method, the function searches the input text for all occurrences that match the defined pattern. If any matches are found, they are added to the 'citations' set, which automatically handles duplicates by only retaining unique entries. Finally, the function returns the 'citations' set, which will contain all the extracted URLs. If no citations are found, an empty set is returned.\n\n**Note**: It is important to ensure that the input text is correctly formatted with the specified citation tags for the function to work effectively. The function relies on the presence of these tags to identify and extract URLs.\n\n**Output Example**: An example of the function's return value could be a set containing URLs such as {'http://example.com', 'http://anotherexample.com'} if those URLs were found in the input text. If no URLs are found, the output would be an empty set: set()."
      ],
      "code_start_line": 104,
      "code_end_line": 119,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def extract_citations(text: str) -> set:\n    \"\"\"\n    从文本中提取所有引用的URLs并去重\n    \n    Args:\n        text: 包含<\\citation>URL<\\citation>格式的文本\n        \n    Returns:\n        set: 提取的URL集合,如果未找到则返回空集合\n    \"\"\"\n    citations = set()\n    pattern = r'citation>(.*?)<'\n    matches = re.findall(pattern, text)\n    if matches:\n        citations.update(matches)\n    return citations\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/__init__.py": [],
  "src/criticsearch/tasks_runner.py": [
    {
      "type": "FunctionDef",
      "name": "execute_multiple_tasks",
      "md_content": [
        "**execute_multiple_tasks**: The function of execute_multiple_tasks is to execute a series of tasks iteratively, process them, and log the conversation history.\n\n**parameters**: The parameters of this Function.\n· tasks: list - A list of task strings (questions) that need to be processed.  \n· max_iterations: int - The maximum number of iterations allowed for each task, defaulting to 10.  \n· output_file: Path | str - The file path where the conversation history will be saved in ShareGPT format, defaulting to \"conversation_history_sharegpt.jsonl\".\n\n**Code Description**: The execute_multiple_tasks function is designed to handle the execution of multiple tasks in a systematic manner. It takes a list of tasks and processes each one individually by calling the process_single_task function. This function is responsible for executing a single task, which involves interacting with various agents to generate a comprehensive report based on the provided task and specified maximum iterations.\n\nDuring the execution of each task, the function checks if the setting to save conversation history is enabled. If it is, the function retrieves the conversation data from the BaseAgent's conversation manager and saves it to the specified output file using the write method. This ensures that the conversation history is logged for future reference.\n\nThe execute_multiple_tasks function is called by the start_task_execution function, which serves as the entry point for executing predefined tasks. In this context, start_task_execution initializes a list of tasks and specifies the maximum number of iterations before invoking execute_multiple_tasks to process the tasks.\n\nThe function relies on the proper functioning of the process_single_task function, which manages the execution of each individual task, and the BaseAgent class, which provides the necessary infrastructure for managing conversations and interactions with agents. Additionally, the write method from the ConversationManager class is utilized to handle the saving of conversation data.\n\n**Note**: It is important to ensure that the tasks provided are well-defined and that the output file path is valid. The function's behavior is contingent upon the settings for saving conversation history, and proper error handling should be in place to manage any exceptions that may arise during the execution process."
      ],
      "code_start_line": 8,
      "code_end_line": 34,
      "params": [
        "tasks",
        "max_iterations",
        "output_file"
      ],
      "have_return": false,
      "code_content": "def execute_multiple_tasks(\n    tasks: list,\n    max_iterations: int = 10,\n    output_file: Path | str = \"conversation_history_sharegpt.jsonl\",\n):\n    \"\"\"\n    Function to execute multiple tasks, process them iteratively, and log conversation history.\n\n    Parameters:\n    - tasks (list): List of task strings (questions) to process.\n    - max_iterations (int): Maximum number of iterations for each task.\n    - output_file (Path | str): Path to save the conversation history in ShareGPT format.\n    \"\"\"\n\n    for task in tasks:\n        # Execute a single task\n        process_single_task(task, max_iterations)\n\n        if settings.save_sharegpt:\n            # TODO: When dealing with multiple tasks, we need to save different conversation histories to different files.\n            conversation_data = BaseAgent.conversation_manager.model_dump(\n                context={\"sharegpt\": True}\n            )\n            BaseAgent.conversation_manager.write(\n                data=conversation_data,\n                path=output_file,\n            )\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tasks_runner.py/start_task_execution"
      ],
      "reference_who": [
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/models.py/ConversationManager/write"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "start_task_execution",
      "md_content": [
        "**start_task_execution**: The function of start_task_execution is to serve as the entry point for executing predefined tasks.\n\n**parameters**: The parameters of this Function are not explicitly defined as it does not take any input parameters.\n\n**Code Description**: The start_task_execution function is designed to initiate the execution of a predefined list of tasks. In its current implementation, it contains a single task: \"Write a report about 2024_Syrian_opposition_offensives event\". The function sets a constant MAX_ITERATION to 2, which indicates the maximum number of times each task will be executed. \n\nUpon invocation, the function attempts to execute the tasks by calling the execute_multiple_tasks function, passing the list of tasks and the MAX_ITERATION value as arguments. This function is responsible for handling the execution of multiple tasks iteratively and logging the conversation history associated with each task.\n\nThe start_task_execution function includes error handling to manage interruptions. If a KeyboardInterrupt exception is raised (for example, if the user interrupts the execution with a keyboard command), the function will catch this exception and print a message indicating that the execution was interrupted by the user.\n\nFrom a functional perspective, start_task_execution is called within the main execution context of the application, specifically in the src/criticsearch/__main__.py file. This indicates that it serves as a primary function that triggers the task execution process when the application is run. \n\nThe relationship between start_task_execution and execute_multiple_tasks is crucial, as the former initializes the task execution process, while the latter carries out the actual processing of the tasks. The effectiveness of start_task_execution relies on the proper implementation of execute_multiple_tasks, which manages the iterative execution and logging of tasks.\n\n**Note**: It is important to ensure that the tasks defined within the start_task_execution function are relevant and well-structured. Additionally, users should be aware that the function does not accept parameters, and the execution can be interrupted by user input. Proper error handling is in place to manage such interruptions gracefully."
      ],
      "code_start_line": 37,
      "code_end_line": 49,
      "params": [],
      "have_return": false,
      "code_content": "def start_task_execution():\n    \"\"\"\n    Entry point to start executing predefined tasks.\n    \"\"\"\n    tasks = [\n        \"Write a report about 2024_Syrian_opposition_offensives event\",\n    ]\n    MAX_ITERATION = 2\n    try:\n        # Execute multiple tasks with the specified number of iterations\n        execute_multiple_tasks(tasks, MAX_ITERATION)\n    except KeyboardInterrupt:\n        print(\"Execution interrupted by the user.\")\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/__main__.py"
      ],
      "reference_who": [
        "src/criticsearch/tasks_runner.py/execute_multiple_tasks"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "src/criticsearch/critic_agent.py": [
    {
      "type": "ClassDef",
      "name": "CriticAgent",
      "md_content": [
        "**CriticAgent**: The function of CriticAgent is to generate critiques based on the responses provided by an intelligent agent.\n\n**attributes**: The attributes of this Class.\n· original_task: A string that holds the original task or question received from the user.  \n· critic_prompt: A string that contains the template for generating critiques, loaded from a specified template file.\n\n**Code Description**: The CriticAgent class extends the BaseAgent class, inheriting its foundational capabilities while specializing in the evaluation of responses generated by other agents. Upon initialization, the CriticAgent sets up its original task as an empty string and loads a critique template from a file named \"critic_agent.txt\". \n\nThe primary method of the CriticAgent is `critic`, which is responsible for generating critiques. This method first gathers the necessary data for critique generation by calling `get_data_for_critic`, which compiles the original task and the agent's answer into a dictionary. The `critic` method then renders the critique prompt using this data and interacts with a common chat interface to obtain a response from the model. \n\nThe response is expected to be in YAML format, which is validated and extracted using the `extract_and_validate_yaml` method. If the response contains invalid YAML, an error message is printed, and the method returns None. \n\nAdditionally, the CriticAgent has a method called `receive_agent_answer`, which allows it to store the answer provided by the agent for later critique. The `get_data_for_critic` method is a helper function that prepares the data needed for the critique by returning a dictionary containing the original task and the agent's answer.\n\nThe CriticAgent is utilized within the main function of the project, where it receives the task and the agent's answer, generates a critique, and evaluates whether to continue the interaction based on the critique's content. This interaction is part of a loop that allows for iterative refinement of the agent's responses based on feedback from the CriticAgent.\n\n**Note**: It is important to ensure that the template file \"critic_agent.txt\" is correctly formatted and accessible, as the CriticAgent relies on this template to generate critiques. Additionally, proper handling of YAML responses is crucial to avoid runtime errors during critique validation.\n\n**Output Example**: A possible appearance of the code's return value when generating a critique might look like this:\n```yaml\nCritique:\n  feedback: \"The answer provided lacks depth and does not address the user's question adequately.\"\n  suggestions:\n    - \"Include more detailed examples.\"\n    - \"Clarify the main points.\"\n  Stop: false\n```",
        "**CriticAgent**: The function of CriticAgent is to generate critiques of responses based on user input and agent answers.\n\n**attributes**: The attributes of this Class.\n· original_task: A string that holds the original task or question received from the user.  \n· critic_prompt: A string that contains the loaded template for generating critiques, sourced from a file named \"critic_agent.txt\".  \n· agent_answer: A string that stores the answer provided by the agent, which will be critiqued.\n\n**Code Description**: The CriticAgent class extends the BaseAgent class, inheriting its functionalities while focusing specifically on the critique generation aspect of the agent's operations. Upon initialization, the CriticAgent sets up its environment by calling the constructor of the BaseAgent class through `super().__init__()`. It initializes the `original_task` attribute to an empty string and loads the critique template from the specified file, which will be used to format the critique responses.\n\nThe main functionality of the CriticAgent is encapsulated in the `critic` method. This method is responsible for generating critiques based on the agent's answer to the original task. It first retrieves the necessary data for critique generation by calling the `get_data_for_critic` method, which compiles the `original_task` and `agent_answer` into a dictionary. The method then renders the critique prompt using the `render_template` method, passing in the data retrieved. The rendered prompt is sent to the language model via the `common_chat` method, which handles the interaction with the model.\n\nThe response from the model is expected to be in YAML format, which is validated and extracted using the `extract_and_validate_yaml` method. If the response contains valid YAML, it is returned; otherwise, an error message is printed, and the method returns None.\n\nThe CriticAgent also includes the `receive_agent_answer` method, which allows it to store the agent's answer for later critique. The `get_data_for_critic` method is a helper function that prepares the data needed for generating critiques by returning a dictionary containing the `original_task` and `agent_answer`.\n\nThe CriticAgent is utilized within the broader context of the project, specifically in the `process_single_task` function and the main execution flow of the application. It receives the original task and the agent's answer, critiques the response, and provides feedback that can be used to refine the agent's output. This interaction is crucial for improving the quality of the responses generated by the agent, as it allows for iterative refinement based on critiques.\n\n**Note**: It is important to ensure that the template file \"critic_agent.txt\" is correctly formatted and accessible, as the CriticAgent relies on this template for generating critiques. Additionally, the agent's answer must be provided before calling the `critic` method to ensure that the critique is based on the most recent response.\n\n**Output Example**: A possible appearance of the code's return value when generating a critique might look like this:\n```yaml\ncritique:\n  feedback: \"The response lacks depth and does not address the user's question adequately.\"\n  suggestions:\n    - \"Provide more detailed explanations.\"\n    - \"Include relevant examples to support the claims.\"\n```"
      ],
      "code_start_line": 7,
      "code_end_line": 34,
      "params": [],
      "have_return": true,
      "code_content": "class CriticAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.original_task = \"\"\n        self.critic_prompt = self.load_template(\"critic_agent.txt\")\n\n    def critic(self):\n        \"\"\"\n        生成评论。\n        \"\"\"\n        data = self.get_data_for_critic()\n\n        rendered_prompt = self.render_template(self.critic_prompt, data)\n        model_response = self.common_chat(usr_prompt=rendered_prompt, role=\"critic\")\n\n        try:\n            formatted_yaml = self.extract_and_validate_yaml(model_response)\n            return formatted_yaml\n\n        except yaml.YAMLError:\n            printer.print_exception(f\"Invalid YAML content.\")\n            return None\n\n    def receive_agent_answer(self, agent_answer):\n        self.agent_answer = agent_answer\n\n    def get_data_for_critic(self):\n        return {\"user_question\": self.original_task, \"agent_answer\": self.agent_answer}\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the `CriticAgent` class by calling the parent class's constructor and setting up the initial values for the object's attributes.\n\n**parameters**: The __init__ method does not take any additional parameters besides `self`, which refers to the instance of the class being created.\n\n**Code Description**: \nThe `__init__` method is the constructor for the `CriticAgent` class. It first invokes the constructor of its parent class, `BaseAgent`, using the `super().__init__()` call. This ensures that any initialization logic defined in the parent class is executed, allowing the `CriticAgent` class to inherit and initialize any attributes or methods defined in `BaseAgent`.\n\nAfter calling the parent constructor, the method initializes two attributes specific to the `CriticAgent` class:\n1. `self.original_task`: This attribute is set to an empty string. It is likely intended to hold information about the original task associated with the `CriticAgent`, although its exact usage is not defined within this method.\n   \n2. `self.critic_prompt`: This attribute is set by calling the `load_template` method with the argument `\"critic_agent.txt\"`. The `load_template` method, which is inherited from the `BaseAgent` class, loads the content of a template file named \"critic_agent.txt\" from a specified directory (likely defined in the parent class). This template content is then assigned to the `critic_prompt` attribute, which would presumably be used later in the `CriticAgent` for processing tasks related to the critic agent's functionality.\n\nThe `__init__` method ensures that when an instance of `CriticAgent` is created, the necessary setup for its attributes is completed, and the class is ready for further operations.\n\n**Note**: The `load_template` method is responsible for handling the file reading operation. If the file `critic_agent.txt` is missing or cannot be found in the specified prompts directory, it will raise a `FileNotFoundError`, ensuring that users are informed of any missing template files."
      ],
      "code_start_line": 8,
      "code_end_line": 11,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super().__init__()\n        self.original_task = \"\"\n        self.critic_prompt = self.load_template(\"critic_agent.txt\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "critic",
      "md_content": [
        "**critic**: The function of critic is to generate a review based on user input and agent responses.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The critic function is a method within the CriticAgent class that is responsible for generating a review or critique based on the data it retrieves and processes. The function operates as follows:\n\n1. **Data Retrieval**: It begins by calling the `get_data_for_critic` method, which returns a dictionary containing the user's question and the agent's answer. This data is essential for generating a contextually relevant critique.\n\n2. **Template Rendering**: The function then utilizes the `render_template` method to create a prompt for the model. This method takes the `critic_prompt` (a predefined template) and the data retrieved in the previous step, formatting it into a string that can be understood by the model.\n\n3. **Model Interaction**: The rendered prompt is sent to the `common_chat` method, which facilitates a conversation with the model. This method processes the user prompt and returns a response from the model, which is expected to contain the critique or review.\n\n4. **YAML Extraction and Validation**: After receiving the model's response, the function attempts to extract and validate any YAML content using the `extract_and_validate_yaml` method. This method checks for valid YAML formatting and returns the parsed content if successful.\n\n5. **Error Handling**: If the YAML extraction fails (for instance, if the content is not valid YAML), the function catches the `yaml.YAMLError` exception, prints an error message indicating the issue, and returns `None`.\n\nThe critic function is invoked within the `main` function of the project, specifically after the common agent has provided an answer to the user's question. The response from the critic function is then used to evaluate the agent's answer, potentially influencing subsequent iterations of the conversation. This highlights the function's role in providing feedback and improving the overall interaction quality between the user and the agent.\n\n**Note**: It is crucial to ensure that the `critic_prompt` is properly defined and that the data returned by `get_data_for_critic` is valid. If the model response does not contain valid YAML, the function will return `None`, which may affect the flow of the application.\n\n**Output Example**: A possible return value from the critic function could be a YAML formatted string such as:\n\n```yaml\nfeedback: \"The agent's answer is comprehensive but lacks specific examples.\"\nsuggestions:\n  - \"Include more detailed explanations.\"\n  - \"Provide references to support claims.\"\n```",
        "**critic**: The function of critic is to generate a critique based on the user's question and the agent's answer.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The critic function is a method within the CriticAgent class that is responsible for generating a critique of the agent's response to a user's question. It begins by retrieving relevant data through the get_data_for_critic method, which returns a dictionary containing the user's original question and the agent's answer. This data is then used to create a rendered prompt by calling the render_template method, which formats the critique prompt using the specified template and the retrieved data.\n\nNext, the function sends the rendered prompt to a conversational model using the common_chat method. This method facilitates communication with the model, allowing it to process the prompt and generate a response. The response from the model is expected to contain YAML content, which is then extracted and validated using the extract_and_validate_yaml method. This method ensures that the YAML content is correctly formatted and can be parsed into a usable structure.\n\nIf the extraction and validation of the YAML content are successful, the function returns the formatted YAML. However, if there is an error during this process, such as invalid YAML content, the function catches the exception and logs an error message using the print_exception method from the RichPrinter class. In this case, the function returns None, indicating that the critique could not be generated due to an issue with the model's response.\n\nThe critic function is called within the main function of the project, where it is used to evaluate the agent's answer after it has been generated. The critique provided by the critic function can influence subsequent actions, such as whether to stop the iteration process or to perform additional searches based on the feedback received.\n\n**Note**: It is essential that the model's response contains valid YAML content wrapped in the appropriate delimiters for the extract_and_validate_yaml method to function correctly. If the content is not valid, the function will return None, which may disrupt the expected flow of the application.\n\n**Output Example**: A possible return value from the critic function could be a YAML formatted string such as:\n\n```yaml\nfeedback: \"The agent's answer is comprehensive but lacks specific examples.\"\nsuggestions:\n  - \"Include more detailed explanations.\"\n  - \"Provide references to support claims.\"\n```"
      ],
      "code_start_line": 13,
      "code_end_line": 28,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def critic(self):\n        \"\"\"\n        生成评论。\n        \"\"\"\n        data = self.get_data_for_critic()\n\n        rendered_prompt = self.render_template(self.critic_prompt, data)\n        model_response = self.common_chat(usr_prompt=rendered_prompt, role=\"critic\")\n\n        try:\n            formatted_yaml = self.extract_and_validate_yaml(model_response)\n            return formatted_yaml\n\n        except yaml.YAMLError:\n            printer.print_exception(f\"Invalid YAML content.\")\n            return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/extract_and_validate_yaml",
        "src/criticsearch/rich_output.py/RichPrinter/print_exception",
        "src/criticsearch/critic_agent.py/CriticAgent/get_data_for_critic"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "receive_agent_answer",
      "md_content": [
        "**receive_agent_answer**: The function of receive_agent_answer is to store the answer provided by an agent for further processing.\n\n**parameters**: The parameters of this Function.\n· agent_answer: This parameter represents the answer received from the agent, which will be stored in the instance variable.\n\n**Code Description**: The receive_agent_answer function is a method of the CriticAgent class. Its primary role is to accept an answer from an agent and assign it to the instance variable self.agent_answer. This function is crucial in the context of the CriticAgent's operations, as it allows the agent to receive and retain feedback or responses from other agents involved in the conversation or task execution.\n\nIn the broader context of the project, this function is called within the main function located in src/criticsearch/main.py. During the execution of the main function, after the common agent generates an answer to a user question, the CriticAgent is instantiated. The receive_agent_answer method is then invoked with the common agent's answer as an argument. This step is essential for the CriticAgent to evaluate the quality of the answer provided by the common agent. The stored answer can later be used for further analysis or feedback generation, which is part of the CriticAgent's role in the overall system.\n\n**Note**: It is important to ensure that the agent_answer parameter passed to this function is valid and represents a meaningful response from the agent to avoid any issues in subsequent evaluations or processing.",
        "**receive_agent_answer**: The function of receive_agent_answer is to store the answer provided by an agent for further processing.\n\n**parameters**: The parameters of this Function.\n· agent_answer: This parameter represents the answer received from the agent, which will be stored in the instance variable for later use.\n\n**Code Description**: The receive_agent_answer function is a method defined within the CriticAgent class. Its primary role is to accept an answer from an agent and assign it to an instance variable named agent_answer. This allows the CriticAgent to retain the agent's response for subsequent evaluations or actions. \n\nThis function is invoked within the process_single_task function found in the src/criticsearch/main.py file. In this context, after the common_agent generates an answer to a given task, the CriticAgent receives this answer through the receive_agent_answer method. This step is crucial as it enables the CriticAgent to assess the quality of the answer provided by the common_agent, facilitating a feedback loop where the CriticAgent can evaluate and potentially improve the response based on its own criteria.\n\nThe relationship between receive_agent_answer and its caller, process_single_task, is integral to the overall functionality of the system. The CriticAgent acts as a reviewer of the answers generated by the common_agent, and the receive_agent_answer method serves as the mechanism through which the CriticAgent acquires the necessary information to perform its evaluation.\n\n**Note**: It is important to ensure that the agent_answer passed to this function is valid and relevant to the task at hand, as this will directly impact the effectiveness of the CriticAgent's evaluation process."
      ],
      "code_start_line": 30,
      "code_end_line": 31,
      "params": [
        "self",
        "agent_answer"
      ],
      "have_return": false,
      "code_content": "    def receive_agent_answer(self, agent_answer):\n        self.agent_answer = agent_answer\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/main_old_paralell.py/main",
        "src/criticsearch/main_old_version.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_data_for_critic",
      "md_content": [
        "**get_data_for_critic**: The function of get_data_for_critic is to return a dictionary containing the user's question and the agent's answer.\n\n**parameters**: This function does not accept any parameters.\n\n**Code Description**:  \nThe `get_data_for_critic` function is a method of a class, which retrieves specific data related to the user's original task and the agent's response. It returns a dictionary with two key-value pairs: the first key is `\"user_question\"`, which corresponds to the `original_task` attribute of the class, and the second key is `\"agent_answer\"`, which corresponds to the `agent_answer` attribute of the class. This method is designed to package these two pieces of information into a format suitable for further processing or analysis.\n\nThis function is invoked by the `critic` method within the same class. The `critic` method uses the data returned by `get_data_for_critic` to render a prompt for a model, which is then used to generate a response. The data returned by `get_data_for_critic` is passed to a template-rendering function (`render_template`), where it is incorporated into a template prompt. The rendered prompt is subsequently sent to a model via the `common_chat` method, and the result is further processed. The `get_data_for_critic` function thus plays a key role in providing necessary input data for the comment generation process within the `critic` method.\n\n**Note**: This method relies on the class having attributes `original_task` and `agent_answer` set properly, as these are used to populate the returned dictionary. If either of these attributes is not initialized or is set to `None`, the function will return a dictionary with missing or invalid values.\n\n**Output Example**:  \nAn example output from `get_data_for_critic` could look as follows:\n\n```json\n{\n  \"user_question\": \"What are the benefits of using AI in healthcare?\",\n  \"agent_answer\": \"AI in healthcare can help with diagnostics, treatment plans, and personalized medicine.\"\n}\n```"
      ],
      "code_start_line": 33,
      "code_end_line": 34,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_data_for_critic(self):\n        return {\"user_question\": self.original_task, \"agent_answer\": self.agent_answer}\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/critic_agent.py/CriticAgent/critic"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tools/models.py": [
    {
      "type": "FunctionDef",
      "name": "get_list_type_annotation",
      "md_content": [
        "**get_list_type_annotation**: The function of get_list_type_annotation is to determine the type of elements in a list for constructing the \"items\" field in a JSON Schema.\n\n**parameters**: The parameters of this Function.\n· param_type: A type annotation representing the parameter to inspect. It should typically be a generic list type like `List[str]` or `List[int]`.\n\n**Code Description**: The `get_list_type_annotation` function is designed to extract the type of elements contained within a list. It is primarily used to help generate JSON Schema definitions, specifically for the \"items\" field, which describes the type of elements within an array. \n\nThe function first checks if the provided `param_type` is a list (using the `get_origin` function to check for a generic list type). If `param_type` is indeed a list, the function proceeds to extract the type of the elements within that list by calling `get_args(param_type)`. The first argument returned by `get_args` represents the element type, and if it is a type (i.e., `isinstance(args[0], type)`), the function returns a dictionary with the element's type name (e.g., `\"string\"` or `\"int\"`) for use in a JSON Schema definition.\n\nIf the `param_type` is not a list or the element type cannot be determined, the function defaults to returning a dictionary with the value `\"type\": \"string\"`, signifying that the list elements are assumed to be of string type.\n\nThis function is called in the `create_schema_from_function` method to help construct the schema of function parameters when the function contains a parameter of list type. Specifically, within `create_schema_from_function`, when a parameter is detected to be a list, the `get_list_type_annotation` function is invoked to determine the type of the items in that list, which is then added to the parameter schema as the `\"items\"` field.\n\n**Note**: It is important that the parameter passed to `get_list_type_annotation` be a valid generic list type (such as `List[str]` or `List[int]`). If the type is not a list or cannot be determined, the function will return a default type of `\"string\"`.\n\n**Output Example**: \nFor a `param_type` of `List[str]`, the function would return:\n```json\n{\n  \"type\": \"str\"\n}\n```\n\nFor a `param_type` of `List[int]`, the function would return:\n```json\n{\n  \"type\": \"int\"\n}\n```\n\nFor a `param_type` of an unsupported or non-list type, the function would return:\n```json\n{\n  \"type\": \"string\"\n}\n```"
      ],
      "code_start_line": 8,
      "code_end_line": 20,
      "params": [
        "param_type"
      ],
      "have_return": true,
      "code_content": "def get_list_type_annotation(param_type):\n    \"\"\"\n    获取列表中元素的类型，用于构造 JSON Schema 的 items 字段。\n    支持 List[str]、List[int] 等类型。\n    \"\"\"\n    # 检查是否为泛型 List 类型\n    if get_origin(param_type) is list or get_origin(param_type) is list:\n        # 获取列表的元素类型\n        args = get_args(param_type)\n        if args and isinstance(args[0], type):\n            return {\"type\": args[0].__name__}\n    # 默认返回字符串类型（未明确指定类型时）\n    return {\"type\": \"string\"}\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/models.py/Tool/create_schema_from_function"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "serialize_type",
      "md_content": [
        "**serialize_type**: The function of serialize_type is to map a given string representing a data type to its corresponding serialized name.\n\n**parameters**: The parameters of this Function.\n· value: str - A string representing a data type to be serialized.\n\n**Code Description**:  \nThe `serialize_type` function takes in a string value representing a data type (e.g., \"str\", \"int\", etc.). The function performs the following steps:\n1. A dictionary named `type_mapping` is defined. This dictionary contains key-value pairs where the key is a string representing a Python data type, and the value is the corresponding serialized name in a more generalized format.\n   - `\"str\"` maps to `\"string\"`\n   - `\"int\"` maps to `\"integer\"`\n   - `\"float\"` maps to `\"number\"`\n   - `\"bool\"` maps to `\"boolean\"`\n   - `\"list\"` maps to `\"array\"`\n   - `\"dict\"` maps to `\"object\"`\n   - `\"None\"` maps to `\"null\"`\n   \n2. The input value is converted to lowercase to ensure that the function is case-insensitive.\n\n3. The function attempts to find the serialized name by checking if the lowercase version of the `value` exists in the `type_mapping` dictionary.\n\n4. If a match is found, the corresponding serialized value is returned.\n\n5. If no match is found, the function defaults to returning `\"null\"`, which is the fallback value defined in the dictionary.\n\n**Note**: \n- The function is case-insensitive due to the use of the `lower()` method on the input string.\n- It assumes that the input is a valid Python type name. If the input string does not match any of the predefined keys, it returns `\"null\"`.\n- The function can be used for serializing Python data types to a format suitable for data serialization or communication in various systems.\n\n**Output Example**:\n- For an input value of `\"str\"`, the function will return `\"string\"`.\n- For an input value of `\"int\"`, the function will return `\"integer\"`.\n- For an input value of `\"boolean\"`, the function will return `\"null\"` since the value does not match any of the keys in the `type_mapping` dictionary."
      ],
      "code_start_line": 23,
      "code_end_line": 33,
      "params": [
        "value"
      ],
      "have_return": true,
      "code_content": "def serialize_type(value: str) -> str:\n    type_mapping = {\n        \"str\": \"string\",\n        \"int\": \"integer\",\n        \"float\": \"number\",\n        \"bool\": \"boolean\",\n        \"list\": \"array\",\n        \"dict\": \"object\",\n        \"None\": \"null\",\n    }\n    return type_mapping.get(value.lower(), \"null\")\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "Item",
      "md_content": [
        "**Item**: The function of Item is to represent a list item with a specific type.\n\n**attributes**: The attributes of this Class.\n· type: str - The type of the list item, which is a required field.\n\n**Code Description**: The Item class is a subclass of BaseModel, which indicates that it is likely part of a data modeling framework that includes validation and serialization capabilities. The primary attribute of this class is `type`, which is a string that specifies the type of the list item. This attribute is defined using the Field function, which enforces that it is a required field (denoted by the ellipsis `...`). \n\nThe class also includes a method called `serialize_type`, which is decorated with `@field_serializer`. This method is responsible for customizing the serialization of the `type` attribute. When the `type` attribute is serialized, this method is invoked, allowing for any specific transformations or formatting to be applied to the value before it is returned. The actual transformation is handled by the `serialize_type` function, which is assumed to be defined elsewhere in the project.\n\nThe Item class is utilized within the ParameterProperty class, where it is defined as an optional attribute named `items`. This indicates that a ParameterProperty can have a list of items, each of which is represented by an instance of the Item class. The relationship between Item and ParameterProperty is significant, as it allows for the encapsulation of item types within the broader context of parameter properties, enhancing the structure and organization of the data model.\n\n**Note**: When using the Item class, ensure that the `type` attribute is always provided, as it is a required field. Additionally, be aware that the serialization behavior of the `type` attribute can be customized through the `serialize_type` method.\n\n**Output Example**: An instance of the Item class might return a serialized representation like this:\n```json\n{\n  \"type\": \"string\"\n}\n``` \nThis output indicates that the type of the list item is a string, demonstrating how the class encapsulates and manages item type information."
      ],
      "code_start_line": 36,
      "code_end_line": 41,
      "params": [],
      "have_return": true,
      "code_content": "class Item(BaseModel):\n    type: str = Field(..., description=\"The type of the list item\")\n\n    @field_serializer(\"type\")\n    def serialize_type(self, value: str, _info) -> str:\n        return serialize_type(value)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/models.py/ParameterProperty"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "serialize_type",
      "md_content": [
        "**serialize_type**: The function of serialize_type is to convert a given string value into a serialized format.\n\n**parameters**: The parameters of this Function.\n· parameter1: value (str) - The string input that needs to be serialized.\n· parameter2: _info - An additional parameter that may contain contextual information, though it is not utilized within the function.\n\n**Code Description**: The serialize_type function takes a string input, referred to as 'value', and passes it to another function named serialize_type. This indicates that the function is likely designed to handle serialization tasks, converting the input string into a specific serialized format. The second parameter, _info, is included in the function signature but is not used within the function body, suggesting that it may be intended for future use or for compatibility with a broader interface. The function returns the result of the serialization process, which is expected to be a string.\n\n**Note**: It is important to ensure that the input 'value' is a valid string that can be serialized. The behavior of the function will depend on the implementation of the serialize_type function that it calls. Additionally, since the _info parameter is not utilized, developers should be aware that it may not affect the serialization process.\n\n**Output Example**: If the input value is \"example\", the function might return a serialized representation such as \"serialized_example\"."
      ],
      "code_start_line": 40,
      "code_end_line": 41,
      "params": [
        "self",
        "value",
        "_info"
      ],
      "have_return": true,
      "code_content": "    def serialize_type(self, value: str, _info) -> str:\n        return serialize_type(value)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "ParameterProperty",
      "md_content": [
        "**ParameterProperty**: The function of ParameterProperty is to define the properties of a parameter, including its data type, description, and optional items.\n\n**attributes**: The attributes of this Class.\n· type: str - The data type of the parameter, which is a required field.  \n· description: Optional[str] - A description of the parameter, which is an optional field.  \n· items: Optional[Item] - An optional attribute that represents a list of items associated with the parameter.\n\n**Code Description**: The ParameterProperty class is a subclass of BaseModel, indicating that it is part of a data modeling framework that provides validation and serialization capabilities. The primary attribute of this class is `type`, which is a string that specifies the data type of the parameter. This attribute is defined using the Field function, which enforces that it is a required field (denoted by the ellipsis `...`). \n\nThe `description` attribute is an optional string that provides additional information about the parameter, enhancing the understanding of its purpose and usage. The `items` attribute is also optional and is defined as an instance of the Item class, which allows for the encapsulation of item types within the context of parameter properties. This relationship is significant as it enables ParameterProperty to manage a collection of items, each represented by the Item class, thereby enhancing the structure and organization of the data model.\n\nThe ParameterProperty class is utilized within the Parameters class, where it is defined as a value in a dictionary that maps parameter names to their respective properties. This indicates that each parameter in the Parameters class can have its own set of properties defined by the ParameterProperty class. The Parameters class also includes attributes such as `required`, which specifies a list of required parameter names, and `additionalProperties`, which indicates whether additional properties are allowed.\n\nThe integration of ParameterProperty within the Parameters class allows for a comprehensive representation of parameters, including their types, descriptions, and associated items. This structured approach facilitates the management of complex parameter configurations in applications.\n\n**Note**: When using the ParameterProperty class, ensure that the `type` attribute is always provided, as it is a required field. The `description` and `items` attributes can be included as needed to provide further context and detail about the parameter.\n\n**Output Example**: An instance of the ParameterProperty class might return a serialized representation like this:\n```json\n{\n  \"type\": \"string\",\n  \"description\": \"A parameter that accepts string values.\",\n  \"items\": {\n    \"type\": \"string\"\n  }\n}\n```\nThis output indicates that the parameter is of type string, includes a description, and has associated items, demonstrating how the class encapsulates and manages parameter property information."
      ],
      "code_start_line": 44,
      "code_end_line": 53,
      "params": [],
      "have_return": true,
      "code_content": "class ParameterProperty(BaseModel):\n    type: str = Field(..., description=\"The data type of the parameter.\")\n    description: Optional[str] = Field(\n        None, description=\"A description of the parameter.\"\n    )\n    items: Optional[Item] = None\n\n    @field_serializer(\"type\")\n    def serialize_type(self, value: str, _info) -> str:\n        return serialize_type(value)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/models.py/Parameters"
      ],
      "reference_who": [
        "src/criticsearch/tools/models.py/Item"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "serialize_type",
      "md_content": [
        "**serialize_type**: The function of serialize_type is to convert a given string value into a serialized format.\n\n**parameters**: The parameters of this Function.\n· parameter1: value (str) - The string input that needs to be serialized.\n· parameter2: _info - Additional information that may be used during serialization, though it is not utilized in the current implementation.\n\n**Code Description**: The serialize_type function is designed to take a string input, referred to as 'value', and pass it to another function named serialize_type for processing. The function does not perform any operations on the input itself; instead, it directly calls the serialize_type function, which is presumably defined elsewhere in the codebase. The purpose of this function is to ensure that the input string is transformed into a serialized format, which is often necessary for data storage or transmission. The second parameter, _info, is included in the function signature but is not used within the function body, indicating that it may be intended for future use or for compatibility with a specific interface.\n\n**Note**: It is important to ensure that the input string is valid and that the serialize_type function being called is properly defined and accessible within the scope of this function. Additionally, the behavior of the function may depend on the implementation of the serialize_type function it calls.\n\n**Output Example**: If the input value is \"example\", and the serialize_type function processes it to return a serialized version, the output might look like \"serialized_example\"."
      ],
      "code_start_line": 52,
      "code_end_line": 53,
      "params": [
        "self",
        "value",
        "_info"
      ],
      "have_return": true,
      "code_content": "    def serialize_type(self, value: str, _info) -> str:\n        return serialize_type(value)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "Parameters",
      "md_content": [
        "**Parameters**: The function of Parameters is to define a schema for function parameters, including their properties and requirements.\n\n**attributes**: The attributes of this Class.\n· type: str - The type of the parameter object, which defaults to \"object\".  \n· properties: Dict[str, ParameterProperty] - A dictionary where keys are parameter names and values are their properties, which is a required field.  \n· required: List[str] - A list of required parameter names, which is also a required field.  \n· additionalProperties: bool - A boolean indicating whether additional properties are allowed, defaulting to False.\n\n**Code Description**: The Parameters class is a subclass of BaseModel, which indicates that it is part of a data modeling framework that provides validation and serialization capabilities. This class is designed to encapsulate the structure and requirements of parameters used in functions.\n\nThe `type` attribute specifies the nature of the parameter object and is set to \"object\" by default. This attribute is essential as it establishes the context for the parameters being defined.\n\nThe `properties` attribute is a dictionary that maps parameter names (as strings) to their respective properties, which are defined by the ParameterProperty class. This relationship allows for a detailed specification of each parameter's characteristics, such as its data type and description.\n\nThe `required` attribute is a list that enumerates the names of parameters that must be provided when the function is called. This ensures that any function utilizing this schema adheres to its defined requirements, promoting robustness and reducing the likelihood of errors.\n\nThe `additionalProperties` attribute indicates whether parameters not explicitly defined in the `properties` dictionary are permitted. By default, this is set to False, enforcing a strict schema that only allows the specified parameters.\n\nThe Parameters class is utilized within the Function class, where it is defined as the `parameters` attribute. This integration signifies that every function can have a well-defined set of parameters, enhancing the clarity and maintainability of the code. Additionally, the Parameters class is referenced in the `create_schema_from_function` method, which constructs a Tool schema from a target function. This method extracts function metadata, including parameter information, and organizes it into the Parameters structure, thereby facilitating the creation of a comprehensive schema for the function.\n\n**Note**: When using the Parameters class, ensure that the `properties` and `required` attributes are always provided, as they are essential for defining the parameter schema. The `additionalProperties` attribute can be adjusted based on the desired flexibility of the parameter definitions."
      ],
      "code_start_line": 56,
      "code_end_line": 65,
      "params": [],
      "have_return": false,
      "code_content": "class Parameters(BaseModel):\n    type: str = Field(\"object\", description=\"The type of the parameter object.\")\n    properties: Dict[str, ParameterProperty] = Field(\n        ...,\n        description=\"A dictionary where keys are parameter names and values are their properties.\",\n    )\n    required: List[str] = Field(..., description=\"A list of required parameter names.\")\n    additionalProperties: bool = Field(\n        False, description=\"Whether additional properties are allowed.\"\n    )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/models.py/Function",
        "src/criticsearch/tools/models.py/Tool/create_schema_from_function"
      ],
      "reference_who": [
        "src/criticsearch/tools/models.py/ParameterProperty"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "Function",
      "md_content": [
        "**Function**: The function of Function is to define a structured representation of a function, including its name, description, and parameters.\n\n**attributes**: The attributes of this Class.\n· name: str - The name of the function, which is a required field.  \n· description: str - A description of what the function does, which is also a required field.  \n· parameters: Parameters - An instance of the Parameters class that defines the schema for the function's parameters, which is a required field.  \n\n**Code Description**: The Function class is a subclass of BaseModel, indicating its role within a data modeling framework that provides validation and serialization capabilities. This class is designed to encapsulate the essential details of a function, including its name, description, and a structured schema for its parameters.\n\nThe `name` attribute is a string that holds the name of the function. This is crucial for identifying the function within the broader context of the application or system.\n\nThe `description` attribute is a string that provides a detailed explanation of the function's purpose and behavior. This attribute is important for documentation and clarity, allowing developers to understand the function's role without needing to inspect its implementation.\n\nThe `parameters` attribute is an instance of the Parameters class, which defines the schema for the function's parameters. This integration ensures that every function represented by the Function class has a well-defined set of parameters, promoting clarity and maintainability in the code. The Parameters class specifies the types, requirements, and additional properties of the parameters, thereby enforcing a structured approach to function definitions.\n\nThe Function class is utilized within the Tool class, where it is defined as the `function` attribute. This relationship signifies that each Tool instance can encapsulate a function definition, allowing for the creation of tools that are based on specific functions. The Tool class also includes a class method, `create_schema_from_function`, which constructs a Tool schema from a target function. This method extracts metadata from the target function, including its name, description, and parameters, and organizes this information into the Function and Parameters structures. This process enhances the clarity and usability of the function definitions within the application.\n\n**Note**: When using the Function class, ensure that all attributes (name, description, and parameters) are provided, as they are essential for defining a complete function representation. The integration with the Parameters class is critical for maintaining a structured approach to function parameter definitions."
      ],
      "code_start_line": 68,
      "code_end_line": 75,
      "params": [],
      "have_return": false,
      "code_content": "class Function(BaseModel):\n    name: str = Field(..., description=\"The name of the function.\")\n    description: str = Field(\n        ..., description=\"A description of what the function does.\"\n    )\n    parameters: Parameters = Field(\n        ..., description=\"The parameters schema for the function.\"\n    )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/models.py/Tool",
        "src/criticsearch/tools/models.py/Tool/create_schema_from_function"
      ],
      "reference_who": [
        "src/criticsearch/tools/models.py/Parameters"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "Tool",
      "md_content": [
        "**Tool**: The function of Tool is to create a structured representation of a function, encapsulating its metadata and schema.\n\n**attributes**: The attributes of this Class.\n· type: str - The type of the tool, typically set to 'function'.  \n· function: Function - An instance of the Function class that defines the function's name, description, and parameters.\n\n**Code Description**: The Tool class is a subclass of BaseModel, designed to encapsulate the details of a function within a structured schema. It includes two primary attributes: `type`, which indicates the nature of the tool (defaulting to 'function'), and `function`, which is an instance of the Function class. The Function class itself represents a structured definition of a function, including its name, description, and a schema for its parameters.\n\nA key feature of the Tool class is the class method `create_schema_from_function`. This method takes a target function as an argument and extracts essential metadata, such as the function's name and documentation string. It parses the documentation to generate sections that include a description and a list of parameters. The method utilizes the inspect module to retrieve the function's signature, allowing it to identify required parameters and their types.\n\nThe extracted information is then organized into a Function instance, which is returned as part of the Tool instance. This process facilitates the creation of a comprehensive schema that can be used to represent the function in a structured manner.\n\nThe Tool class is utilized within the ToolRegistry's `get_or_create_tool_schema` method. This method retrieves or creates tool schemas for specified functions. If a function's schema is not already registered, it invokes `Tool.create_schema_from_function` to generate the schema and add it to the registry. This integration ensures that function definitions are consistently represented and easily accessible within the application.\n\n**Note**: When using the Tool class, ensure that the `function` attribute is properly defined as an instance of the Function class, as this is critical for maintaining a structured representation of the function's metadata.\n\nA possible appearance of the code's return value when invoking `Tool.create_schema_from_function` might look like this:\n```json\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"example_function\",\n        \"description\": \"This function serves as an example.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"param1\": {\n                    \"type\": \"int\",\n                    \"description\": \"The first parameter.\"\n                },\n                \"param2\": {\n                    \"type\": \"str\",\n                    \"description\": \"The second parameter.\"\n                }\n            },\n            \"required\": [\"param1\"],\n            \"additionalProperties\": false\n        }\n    }\n}\n```"
      ],
      "code_start_line": 78,
      "code_end_line": 147,
      "params": [],
      "have_return": true,
      "code_content": "class Tool(BaseModel):\n    type: str = Field(\n        \"function\", description=\"The type of the tool, typically 'function'.\"\n    )\n    function: Function = Field(..., description=\"The function definition for the tool.\")\n\n    @classmethod\n    def create_schema_from_function(cls, target_function):\n        \"\"\"Create a Tool schema from a target function.\"\"\"\n\n        # 提取函数名称和文档字符串\n        func_name = target_function.__name__\n        func_doc = inspect.getdoc(target_function) or \"No description provided.\"\n\n        # 解析文档字符串，生成 sections\n        docstring = Docstring(func_doc)\n        # NOTE: Only support  Google-style right now.\n        sections = docstring.parse(\"google\")\n\n        # 提取描述信息\n        description = \"\"\n        parameters = []\n\n        for section in sections:\n            if section.kind == DocstringSectionKind.text:\n                description = section.value.strip()\n            elif section.kind == DocstringSectionKind.parameters:\n                parameters = section.value\n\n        # 提取参数信息\n        signature = inspect.signature(target_function)\n        required = []\n        properties = {}\n\n        for param_name, param in signature.parameters.items():\n            param_type = param.annotation if param.annotation != inspect._empty else Any\n            param_default = param.default if param.default != inspect._empty else ...\n\n            # 从解析的参数部分提取描述\n            param_description = None\n            for param_info in parameters:\n                if param_info.name == param_name:  # 使用属性访问\n                    param_description = param_info.description\n                    break\n\n            if param_default is ...:\n                required.append(param_name)\n\n            properties[param_name] = {\n                \"type\": param_type.__name__,\n                \"description\": param_description or f\"The {param_name} parameter.\",\n            }\n\n            if get_origin(param_type) is list:\n                properties[param_name][\"items\"] = get_list_type_annotation(param_type)\n\n        # Build the final Function and Tool schema\n        function_schema = Function(\n            name=func_name,\n            description=description,\n            parameters=Parameters(\n                type=\"object\",\n                properties=properties,\n                required=required,\n                additionalProperties=False,\n            ),\n        )\n        return cls(type=\"function\", function=function_schema).model_dump(\n            exclude_none=True\n        )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/tool_registry.py",
        "src/criticsearch/tools/tool_registry.py/ToolRegistry/get_or_create_tool_schema"
      ],
      "reference_who": [
        "src/criticsearch/tools/models.py/Function"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "create_schema_from_function",
      "md_content": [
        "### `create_schema_from_function`\n\n#### Overview\n\nThe `create_schema_from_function` method is a class method designed to generate a schema for a tool based on a target function. This schema includes the function's name, description, and parameters, providing a structured format for the tool. The method processes the function's docstring to extract relevant information and constructs a JSON-like schema, which can be used for further automation or documentation purposes.\n\n#### Parameters\n- **cls** (`type`): The class that is calling the method. This is implicitly passed when the method is invoked.\n- **target_function** (`function`): The function from which the schema will be created. The method inspects the function's name, docstring, and parameters to build the schema.\n\n#### Method Description\n\n1. **Extracting Function Metadata**: \n   - The method starts by extracting the function's name and docstring using the `__name__` attribute and `inspect.getdoc()`, respectively.\n   - If the docstring is empty, a default message of \"No description provided.\" is used.\n\n2. **Parsing the Docstring**: \n   - The docstring is parsed using the `Docstring` class, which is assumed to support Google-style docstrings.\n   - Sections of the docstring are identified and classified into text and parameters sections.\n\n3. **Processing the Parameters**: \n   - The method iterates through the parsed sections to extract the description of the function and its parameters.\n   - It then inspects the function's signature using the `inspect.signature()` method to identify the types, default values, and required status of each parameter.\n   - Parameters are classified into required and optional based on whether they have default values or not.\n\n4. **Building the Schema**: \n   - A `Function` schema is constructed using the extracted information. The function's parameters are detailed in the schema with their types and descriptions. If any parameter is of list type, the `get_list_type_annotation` function is used to determine the type of elements in the list.\n   - The `Parameters` class is utilized to define the structure of the parameters, including their types, descriptions, required status, and whether additional properties are allowed.\n\n5. **Returning the Schema**: \n   - Finally, the schema is returned as a serialized model, excluding any `None` values.\n\n#### Return Value\nThe method returns a serialized model of the function schema, including:\n- **name**: The name of the function.\n- **description**: The description extracted from the function's docstring.\n- **parameters**: A structured schema of the function's parameters, which includes their names, types, descriptions, and required status.\n\n#### Example\n\nAssume a function `example_function` with the following signature:\n\n```python\ndef example_function(arg1: int, arg2: List[str] = []):\n    \"\"\"\n    Example function to demonstrate schema creation.\n\n    Args:\n        arg1 (int): The first argument.\n        arg2 (List[str], optional): The second argument, which is a list of strings.\n    \"\"\"\n    pass\n```\n\nThe `create_schema_from_function` method would generate a schema with the following structure:\n\n```json\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"example_function\",\n    \"description\": \"Example function to demonstrate schema creation.\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"arg1\": {\n          \"type\": \"int\",\n          \"description\": \"The first argument.\"\n        },\n        \"arg2\": {\n          \"type\": \"list\",\n          \"items\": {\n            \"type\": \"str\"\n          },\n          \"description\": \"The second argument, which is a list of strings.\"\n        }\n      },\n      \"required\": [\"arg1\"],\n      \"additionalProperties\": false\n    }\n  }\n}\n```\n\n#### Notes\n- The method currently supports Google-style docstrings and expects parameters to be described in a specific format within the docstring.\n- It relies on the `get_list_type_annotation` function to determine the type of elements in list-type parameters.\n- The `Parameters` class is used to define the structure of the parameters, ensuring a consistent schema for function input validation."
      ],
      "code_start_line": 85,
      "code_end_line": 147,
      "params": [
        "cls",
        "target_function"
      ],
      "have_return": true,
      "code_content": "    def create_schema_from_function(cls, target_function):\n        \"\"\"Create a Tool schema from a target function.\"\"\"\n\n        # 提取函数名称和文档字符串\n        func_name = target_function.__name__\n        func_doc = inspect.getdoc(target_function) or \"No description provided.\"\n\n        # 解析文档字符串，生成 sections\n        docstring = Docstring(func_doc)\n        # NOTE: Only support  Google-style right now.\n        sections = docstring.parse(\"google\")\n\n        # 提取描述信息\n        description = \"\"\n        parameters = []\n\n        for section in sections:\n            if section.kind == DocstringSectionKind.text:\n                description = section.value.strip()\n            elif section.kind == DocstringSectionKind.parameters:\n                parameters = section.value\n\n        # 提取参数信息\n        signature = inspect.signature(target_function)\n        required = []\n        properties = {}\n\n        for param_name, param in signature.parameters.items():\n            param_type = param.annotation if param.annotation != inspect._empty else Any\n            param_default = param.default if param.default != inspect._empty else ...\n\n            # 从解析的参数部分提取描述\n            param_description = None\n            for param_info in parameters:\n                if param_info.name == param_name:  # 使用属性访问\n                    param_description = param_info.description\n                    break\n\n            if param_default is ...:\n                required.append(param_name)\n\n            properties[param_name] = {\n                \"type\": param_type.__name__,\n                \"description\": param_description or f\"The {param_name} parameter.\",\n            }\n\n            if get_origin(param_type) is list:\n                properties[param_name][\"items\"] = get_list_type_annotation(param_type)\n\n        # Build the final Function and Tool schema\n        function_schema = Function(\n            name=func_name,\n            description=description,\n            parameters=Parameters(\n                type=\"object\",\n                properties=properties,\n                required=required,\n                additionalProperties=False,\n            ),\n        )\n        return cls(type=\"function\", function=function_schema).model_dump(\n            exclude_none=True\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/tool_registry.py/ToolRegistry/get_or_create_tool_schema"
      ],
      "reference_who": [
        "src/criticsearch/tools/models.py/get_list_type_annotation",
        "src/criticsearch/tools/models.py/Parameters",
        "src/criticsearch/tools/models.py/Function"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_delivery_date",
      "md_content": [
        "**get_delivery_date**: The function of get_delivery_date is to retrieve the delivery date for a customer's order based on the provided order ID and delivery type.\n\n**parameters**: The parameters of this function:\n· order_id (str): A string representing the unique ID of the order for which the delivery date is being requested.\n· delivery_type (str): A string specifying the type of delivery (e.g., \"standard\" or \"express\"). The default value is \"standard\".\n\n**Code Description**: \nThe `get_delivery_date` function is designed to fetch the delivery date associated with a given customer order. It accepts two parameters:\n1. `order_id` (str): This is a required parameter, which uniquely identifies the order whose delivery date is to be determined. \n2. `delivery_type` (str): This is an optional parameter that defines the delivery method for the order. The default value is \"standard\", but other types such as \"express\" could also be provided. The specific functionality regarding how the delivery type impacts the retrieval of the delivery date is not yet implemented, as the function currently has no logic inside it.\n\nAs the function is currently not implemented (indicated by the `pass` statement), no actions are taken within it, and it does not return any values at the moment.\n\n**Note**: \n- This function currently does not contain any operational logic or return values, meaning it needs to be implemented with appropriate business logic to fetch the delivery date based on the order ID and delivery type.\n- The function signature suggests that it will likely interact with a system (e.g., a database or API) to determine the delivery date based on the provided inputs, but this functionality is not present in the current state."
      ],
      "code_start_line": 152,
      "code_end_line": 160,
      "params": [
        "order_id",
        "delivery_type"
      ],
      "have_return": false,
      "code_content": "    def get_delivery_date(order_id: str, delivery_type: str = \"standard\"):\n        \"\"\"\n        Get the delivery date for a customer's order.\n\n        Parameters:\n            order_id (str): The unique ID of the order.\n            delivery_type (str): The type of delivery (e.g., standard or express).\n        \"\"\"\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tools/__init__.py": [],
  "src/criticsearch/tools/tool_registry.py": [
    {
      "type": "ClassDef",
      "name": "ToolRegistry",
      "md_content": [
        "**ToolRegistry**: The function of ToolRegistry is to manage tools by using their function names as keys, allowing easy retrieval and creation of schemas for those tools.\n\n**Attributes**:\n- **_tools**: A dictionary that maps function names (as strings) to their respective tool schemas. It is initialized as an empty dictionary and is used to store the schemas of tools for quick access and reuse.\n\n**Code Description**:  \nThe `ToolRegistry` class is designed to manage the registration and retrieval of tool schemas based on function names. It allows the creation of new schemas for tools if they are not already registered. The primary use case of this class is within systems that work with a variety of tools, where each tool is associated with a function, and its schema needs to be retrieved or generated.\n\n- **`__init__`**: This method initializes an empty registry (`_tools`) for storing tool schemas. The registry is a dictionary where keys are the names of functions (as strings) and values are the schemas related to those functions. The class starts with no schemas stored.\n\n- **`get_or_create_tool_schema`**: This method accepts one or more function references (`target_functions`) as arguments. For each function provided, it checks whether the schema for that function already exists in the `_tools` dictionary. If a schema does not exist for the function, the method creates it using a class method `Tool.create_schema_from_function` and then stores it in the `_tools` dictionary. After the schema is retrieved or created, it is added to a list and returned. This method ensures that schemas are not repeatedly created for the same functions, promoting efficiency and reuse.\n\n    - **Arguments**:\n      - `*target_functions`: One or more functions whose schemas need to be retrieved or created.\n    - **Returns**: A list of dictionaries representing the schemas of the provided functions.\n\nIn the context of the larger project, the `ToolRegistry` class is used in the `BaseAgent` class (found in `src/criticsearch/base_agent.py`). Specifically, instances of `ToolRegistry` are used to manage the schemas for different tools within the agent, such as `search_aggregator` and `content_scraper`. When initializing these tools, `BaseAgent` calls `get_or_create_tool_schema` to retrieve or create the schemas associated with their functions. These schemas are essential for the tools to be correctly used in subsequent operations, such as when the agent interacts with external services or processes data.\n\nThe `ToolRegistry` class plays a crucial role in ensuring that the agent can consistently access the correct schema for its tools, avoiding the need to repeatedly recreate schemas and thus improving efficiency.\n\n**Note**:  \n- The `Tool.create_schema_from_function` method (referenced in `get_or_create_tool_schema`) is expected to be responsible for creating a schema from a function. This method should handle the specifics of schema creation and may include validation or other processing steps.\n- The `_tools` dictionary is managed entirely within the `ToolRegistry` class. Users of this class do not need to directly modify this attribute.\n- This class assumes that function names are unique and can be used as reliable keys for storing schemas.\n\n**Output Example**:  \nWhen calling `get_or_create_tool_schema` with the `search` function of `search_aggregator` and the `scrape` function of `content_scraper`, a possible return value could look like this:\n\n```python\n[\n    {\n        \"function_name\": \"search\",\n        \"schema_details\": { ... }  # Detailed schema information for the 'search' function\n    },\n    {\n        \"function_name\": \"scrape\",\n        \"schema_details\": { ... }  # Detailed schema information for the 'scrape' function\n    }\n]\n```",
        "**ToolRegistry**: The function of ToolRegistry is to manage tools using their function names as keys, allowing for the retrieval or creation of schemas for these tools.\n\n**attributes**: The attributes of this Class.\n· _tools: A dictionary mapping function names (as strings) to their respective schemas (as dictionaries).\n\n**Code Description**: The ToolRegistry class serves as a centralized registry for managing tool schemas, which are essential for the operation of various tools within the application. Upon initialization, it creates an empty dictionary, _tools, to store the schemas associated with different functions. \n\nThe primary method of this class, get_or_create_tool_schema, accepts one or more functions as arguments. For each function, it checks if a schema already exists in the _tools dictionary. If a schema does not exist, it invokes the Tool.create_schema_from_function method to generate a new schema based on the provided function and stores it in the _tools dictionary. This method also logs the creation of new schemas for tracking purposes.\n\nThe ToolRegistry class is utilized by the BaseAgent class, which acts as a foundational component for intelligent agents in the project. The BaseAgent creates an instance of ToolRegistry to manage the schemas for various tools it employs, such as the search aggregator and content scraper. By leveraging the ToolRegistry, the BaseAgent can efficiently retrieve and utilize the necessary schemas for its operations, ensuring that the tools are correctly configured and ready for use.\n\n**Note**: It is crucial to ensure that the ToolRegistry is populated with the required schemas for the tools being used, as the functionality of the BaseAgent and its ability to perform tasks depend on these schemas being available.\n\n**Output Example**: A possible appearance of the code's return value when retrieving schemas for a function might look like this:\n```json\n{\n  \"schemas\": [\n    {\n      \"function_name\": \"search\",\n      \"schema\": {\n        \"parameters\": {\n          \"query\": \"string\",\n          \"results\": \"list\"\n        },\n        \"description\": \"Schema for the search tool.\"\n      }\n    },\n    {\n      \"function_name\": \"scrape\",\n      \"schema\": {\n        \"parameters\": {\n          \"urls\": \"list\",\n          \"content\": \"string\"\n        },\n        \"description\": \"Schema for the content scraper tool.\"\n      }\n    }\n  ]\n}\n```"
      ],
      "code_start_line": 7,
      "code_end_line": 54,
      "params": [],
      "have_return": true,
      "code_content": "class ToolRegistry:\n    \"\"\"\n    A registry for managing tools using their function names as keys.\n\n    This class provides functionality to retrieve or create schemas for tools\n    based on provided functions, storing them for reuse and easy access.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize an empty registry for storing tool schemas.\n\n        Attributes:\n            _tools (Dict[str, dict]): A dictionary mapping function names\n                                      to their respective schemas.\n        \"\"\"\n        self._tools: Dict[str, dict] = {}\n\n    def get_or_create_tool_schema(self, *target_functions: Callable) -> List[Dict]:\n        \"\"\"\n        Retrieve or create tool schemas for the given functions.\n\n        If a function's schema is not already registered, it will be created\n        using `Tool.create_schema_from_function` and added to the registry.\n\n        Args:\n            *target_functions (Callable): One or more functions for which\n                                          schemas are to be retrieved or created.\n\n        Returns:\n            List[Dict]: A list of schemas corresponding to the provided functions.\n        \"\"\"\n        schemas = []\n        for target_function in target_functions:\n            func_name = target_function.__name__\n\n            # Create schema if not already registered\n            if func_name not in self._tools:\n                self._tools[func_name] = Tool.create_schema_from_function(\n                    target_function\n                )\n                printer.log(\n                    f\"Created tool schema for: {func_name}, schema: {self._tools[func_name]}\"\n                )\n\n            schemas.append(self._tools[func_name])\n\n        return schemas\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py",
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/tools/__init__.py"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an empty registry for storing tool schemas.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The __init__ function is a constructor method that is called when an instance of the ToolRegistry class is created. This function initializes an instance variable named _tools, which is a dictionary. The purpose of this dictionary is to map function names (as strings) to their respective schemas (as dictionaries). By initializing _tools as an empty dictionary, the function sets up a clean state for the ToolRegistry instance, allowing it to store tool schemas that can be added later. This design ensures that the registry starts without any pre-existing data, providing a fresh environment for subsequent operations.\n\n**Note**: It is important to understand that this function does not take any parameters and does not return any value. It solely serves the purpose of setting up the initial state of the ToolRegistry instance. Users should ensure that the _tools dictionary is populated with valid tool schemas through other methods provided in the ToolRegistry class after the instance is initialized."
      ],
      "code_start_line": 15,
      "code_end_line": 23,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        \"\"\"\n        Initialize an empty registry for storing tool schemas.\n\n        Attributes:\n            _tools (Dict[str, dict]): A dictionary mapping function names\n                                      to their respective schemas.\n        \"\"\"\n        self._tools: Dict[str, dict] = {}\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_or_create_tool_schema",
      "md_content": [
        "**get_or_create_tool_schema**: The function of get_or_create_tool_schema is to retrieve or create tool schemas for specified functions.\n\n**parameters**: The parameters of this Function.\n· target_functions: One or more functions for which schemas are to be retrieved or created.\n\n**Code Description**: The get_or_create_tool_schema method is a member of the ToolRegistry class, responsible for managing the schemas associated with various tools (functions) within the application. This method accepts one or more callable functions as arguments and checks if a schema for each function is already registered in the internal registry (self._tools). \n\nIf a function's schema is not found, the method invokes the Tool class's create_schema_from_function method to generate a new schema based on the function's metadata, including its name, description, and parameters. This newly created schema is then stored in the registry for future reference. The method logs the creation of the schema for debugging purposes.\n\nThe method returns a list of schemas corresponding to the provided functions, ensuring that all requested schemas are either retrieved from the registry or newly created. This functionality is crucial for maintaining a structured representation of functions within the application, allowing for consistent access to their metadata.\n\nThe get_or_create_tool_schema method is called within the __init__ method of the BaseAgent class. During the initialization of a BaseAgent instance, it retrieves or creates schemas for the search aggregator and content scraper tools. These schemas are then stored in the conversation manager's available tools, facilitating their use in subsequent interactions.\n\n**Note**: When using this method, ensure that the functions passed as arguments are callable and properly defined, as the method relies on their metadata to generate the schemas.\n\n**Output Example**: A possible appearance of the code's return value when invoking get_or_create_tool_schema might look like this:\n```json\n[\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"search\",\n            \"description\": \"Performs a search operation.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"str\",\n                        \"description\": \"The search query.\"\n                    },\n                    \"limit\": {\n                        \"type\": \"int\",\n                        \"description\": \"The maximum number of results to return.\"\n                    }\n                },\n                \"required\": [\"query\"],\n                \"additionalProperties\": false\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"scrape\",\n            \"description\": \"Scrapes content from a given URL.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"url\": {\n                        \"type\": \"str\",\n                        \"description\": \"The URL to scrape.\"\n                    }\n                },\n                \"required\": [\"url\"],\n                \"additionalProperties\": false\n            }\n        }\n    }\n]\n```",
        "## Method: `get_or_create_tool_schema`\n\n### Description:\nThe `get_or_create_tool_schema` method retrieves or creates tool schemas for the given functions. If the schema for a function is not already registered, it will be created using the `Tool.create_schema_from_function` method and added to the registry. This ensures that the necessary tool schemas are available for the provided functions.\n\n### Parameters:\n- `*target_functions` (Callable): One or more functions for which schemas are to be retrieved or created. The parameter accepts a variable number of function arguments.\n\n### Returns:\n- `List[Dict]`: A list of schemas corresponding to the provided functions. Each schema is returned as a dictionary.\n\n### Behavior:\n1. The method iterates through each provided function in `target_functions`.\n2. For each function, it checks if its schema already exists in the registry (using the function's name).\n3. If the schema is not registered:\n   - The schema is created by calling `Tool.create_schema_from_function` with the target function.\n   - The schema is added to the registry.\n   - A log message is printed indicating the creation of the schema.\n4. Finally, the method returns a list of schemas corresponding to the provided functions, whether they were newly created or retrieved from the registry.\n\n### Example Usage:\n```python\ntool_registry = ToolRegistry()\nschemas = tool_registry.get_or_create_tool_schema(function1, function2)\n```\n\n### Notes:\n- The method leverages the `Tool.create_schema_from_function` to generate the schema when necessary.\n- Logging of schema creation is handled by the `printer.log` function, providing feedback on the schema creation process."
      ],
      "code_start_line": 25,
      "code_end_line": 54,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_or_create_tool_schema(self, *target_functions: Callable) -> List[Dict]:\n        \"\"\"\n        Retrieve or create tool schemas for the given functions.\n\n        If a function's schema is not already registered, it will be created\n        using `Tool.create_schema_from_function` and added to the registry.\n\n        Args:\n            *target_functions (Callable): One or more functions for which\n                                          schemas are to be retrieved or created.\n\n        Returns:\n            List[Dict]: A list of schemas corresponding to the provided functions.\n        \"\"\"\n        schemas = []\n        for target_function in target_functions:\n            func_name = target_function.__name__\n\n            # Create schema if not already registered\n            if func_name not in self._tools:\n                self._tools[func_name] = Tool.create_schema_from_function(\n                    target_function\n                )\n                printer.log(\n                    f\"Created tool schema for: {func_name}, schema: {self._tools[func_name]}\"\n                )\n\n            schemas.append(self._tools[func_name])\n\n        return schemas\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/__init__"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/tools/models.py/Tool",
        "src/criticsearch/tools/models.py/Tool/create_schema_from_function"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/tools/image_analyzer.py": [
    {
      "type": "ClassDef",
      "name": "ImageAnalyzer",
      "md_content": [
        "**ImageAnalyzer**: The function of ImageAnalyzer is to analyze an image using a specified vision model.\n\n**attributes**:\n· model: Specifies the vision model used for image analysis, defaulting to \"gpt-4o-mini\".\n\n**Code Description**:  \nThe `ImageAnalyzer` class is designed to facilitate image analysis by processing image data through a specified model. The class contains the following key components:\n\n1. **`__init__(self, model=\"gpt-4o-mini\")`**: This is the initializer method for the `ImageAnalyzer` class. It accepts one optional parameter `model`, which determines the vision model used for image analysis. The default value for this parameter is set to \"gpt-4o-mini\". This attribute is stored within the class as `self.model`.\n\n2. **`analyze_image(self, image_data: str) -> Dict`**: This is the main method responsible for analyzing an image. It accepts `image_data`, which can either be a URL, a file path, or base64-encoded image data. The method follows these steps:\n    - If `image_data` is a string:\n        - If it starts with \"http\", the method assumes the string represents a URL. A request is sent to the URL to retrieve the image, which is then processed by opening it using the `PIL.Image.open()` method after the image content is fetched.\n        - If the string is not a URL, it is treated as a file path and the image is opened directly from the specified location.\n    - The image is then converted to base64 format using the `base64` module, enabling the image to be handled as a string representation. This is done by saving the image into a `BytesIO` buffer, which is then base64-encoded.\n    - If `image_data` is already base64-encoded, the method directly assigns it to `image_base64`.\n    - A dictionary is returned containing a description of the analysis, the model used, and the format of the image data (which is \"base64\").\n\n3. The `analyze_image` method handles exceptions that might occur during the image processing. If an error occurs at any stage, the method returns a dictionary with the error message.\n\n**Note**: \n- The method expects the input image data to either be a file path, URL, or base64-encoded string. It does not handle non-image data or invalid formats.\n- The image analysis itself is not fully implemented in the code; the return statement provides a placeholder response indicating that the analysis was completed.\n- The choice of model for image analysis is configurable but defaults to \"gpt-4o-mini\". However, the analysis logic based on the specified model is not included in this snippet.\n\n**Output Example**:  \nA possible return value from the `analyze_image` method might look like this:\n\n```json\n{\n  \"description\": \"Image analysis completed\",\n  \"model_used\": \"gpt-4o-mini\",\n  \"image_format\": \"base64\"\n}\n```"
      ],
      "code_start_line": 9,
      "code_end_line": 37,
      "params": [],
      "have_return": true,
      "code_content": "class ImageAnalyzer:\n    def __init__(self, model=\"gpt-4o-mini\"):\n        self.model = model\n\n    def analyze_image(self, image_data: str) -> Dict:\n        \"\"\"Analyze an image using the specified vision model.\"\"\"\n        try:\n            # Convert image data to base64 if it's a URL or file path\n            if isinstance(image_data, str):\n                if image_data.startswith(\"http\"):\n                    response = requests.get(image_data)\n                    image = Image.open(BytesIO(response.content))\n                else:\n                    image = Image.open(image_data)\n\n                buffered = BytesIO()\n                image.save(buffered, format=\"PNG\")\n                image_base64 = base64.b64encode(buffered.getvalue()).decode()\n            else:\n                image_base64 = image_data\n\n            # TODO this is a stand in for later use\n            return {\n                \"description\": \"Image analysis completed\",\n                \"model_used\": self.model,\n                \"image_format\": \"base64\",\n            }\n        except Exception as e:\n            return {\"error\": str(e)}\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the object with a specific model configuration.\n\n**parameters**:\n· model: The model to be used, which is set to a default value of \"gpt-4o-mini\".\n\n**Code Description**: \nThe `__init__` method is a constructor that initializes an object of the class. It takes a single parameter, `model`, which defines the model to be used by the object. If no argument is provided during the object creation, it defaults to \"gpt-4o-mini\". The value passed for `model` is then stored in the instance variable `self.model`, which will allow the object to refer to this model later in its usage.\n\nThis method ensures that each object of the class starts with a specific configuration, defined by the `model` parameter. The default value \"gpt-4o-mini\" can be overridden when creating an object if a different model is needed.\n\n**Note**: The `__init__` method does not perform any complex operations or return any value. It only sets up the initial state of the object by assigning the provided `model` value to the instance."
      ],
      "code_start_line": 10,
      "code_end_line": 11,
      "params": [
        "self",
        "model"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, model=\"gpt-4o-mini\"):\n        self.model = model\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "analyze_image",
      "md_content": [
        "**analyze_image**: The function of analyze_image is to analyze an image using the specified vision model.\n\n**parameters**: The parameters of this Function.\n· image_data: A string representing the image data, which can be a URL or a file path to an image.\n\n**Code Description**: The analyze_image function is designed to process an image for analysis. It accepts a single parameter, image_data, which can either be a URL pointing to an image or a local file path. The function first checks if the provided image_data is a string. If it is a URL (indicated by the string starting with \"http\"), the function makes an HTTP GET request to retrieve the image. The image is then opened using the PIL library's Image module. If the image_data is a local file path, the function directly opens the image from that path.\n\nOnce the image is successfully opened, the function converts the image into a PNG format and encodes it into a base64 string. This base64 representation is useful for transmitting image data over the web or embedding it in JSON responses. If the image_data is already in base64 format, it is used directly without further processing.\n\nThe function is currently set up to return a dictionary containing a description of the analysis completion, the model used for analysis (which is an attribute of the class), and the format of the image (base64). In the event of an error during the image processing, the function captures the exception and returns a dictionary with an error message.\n\n**Note**: It is important to ensure that the image_data provided is either a valid URL or a correct file path. The function does not perform extensive validation on the input and assumes that the provided data is in the correct format. Additionally, the actual image analysis logic is not implemented yet, as indicated by the placeholder comment in the code.\n\n**Output Example**: \n{\n    \"description\": \"Image analysis completed\",\n    \"model_used\": \"VisionModelXYZ\",\n    \"image_format\": \"base64\"\n}"
      ],
      "code_start_line": 13,
      "code_end_line": 37,
      "params": [
        "self",
        "image_data"
      ],
      "have_return": true,
      "code_content": "    def analyze_image(self, image_data: str) -> Dict:\n        \"\"\"Analyze an image using the specified vision model.\"\"\"\n        try:\n            # Convert image data to base64 if it's a URL or file path\n            if isinstance(image_data, str):\n                if image_data.startswith(\"http\"):\n                    response = requests.get(image_data)\n                    image = Image.open(BytesIO(response.content))\n                else:\n                    image = Image.open(image_data)\n\n                buffered = BytesIO()\n                image.save(buffered, format=\"PNG\")\n                image_base64 = base64.b64encode(buffered.getvalue()).decode()\n            else:\n                image_base64 = image_data\n\n            # TODO this is a stand in for later use\n            return {\n                \"description\": \"Image analysis completed\",\n                \"model_used\": self.model,\n                \"image_format\": \"base64\",\n            }\n        except Exception as e:\n            return {\"error\": str(e)}\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tools/content_scraper/fallback_web_scraper.py": [
    {
      "type": "ClassDef",
      "name": "FallbackWebScraper",
      "md_content": [
        "**FallbackWebScraper**: The function of FallbackWebScraper is to scrape content from a list of webpages asynchronously, serving as a fallback mechanism when the Tavily API fails.\n\n**attributes**: The attributes of this Class.\n· urls: List[str] - A list of URLs to scrape content from.\n\n**Code Description**: The FallbackWebScraper class contains a static method named `scrape`, which is designed to handle the asynchronous scraping of web content from a provided list of URLs. This method is particularly useful in scenarios where the primary content extraction method, the Tavily API, encounters issues or fails to return valid results. \n\nThe `scrape` method begins by defining a set of HTTP headers to mimic a standard web browser request, enhancing the likelihood of successful content retrieval. It then defines an inner asynchronous function `fetch_url`, which takes a single URL as an argument. This function is responsible for making the actual HTTP GET request using the `httpx` library, which supports asynchronous operations and HTTP/2.\n\nWithin `fetch_url`, the method attempts to fetch the content of the specified URL. If the response status code is not 200 (indicating a successful request), it returns a `ScrapedData` object containing the URL and an error message detailing the HTTP status code and reason. If the request is successful, the HTML content is parsed using BeautifulSoup, where unwanted elements such as scripts and styles are removed to focus on the main content.\n\nThe main content is extracted by searching for common HTML structures like `<main>`, `<article>`, or `<div>` elements that typically contain the primary content of a webpage. If no main content is found, a default message indicating \"No content available\" is returned. Otherwise, the text from paragraph tags is concatenated and cleaned up to form the final content string. The method then returns a `ScrapedData` object containing the URL, the page title, and the extracted content.\n\nThe `scrape` method utilizes `asyncio.gather` to concurrently fetch content from all provided URLs, improving efficiency and reducing overall scraping time.\n\nThis class is called by the `scrape` method of the `ContentScraper` class, which first attempts to extract content using the Tavily API. If the Tavily API fails, the `FallbackWebScraper.scrape` method is invoked to handle the fallback scraping process. This relationship ensures that the FallbackWebScraper serves as a reliable alternative for content extraction when the primary method is unavailable.\n\n**Note**: When using this code, ensure that the URLs provided are valid and accessible. Additionally, be aware of the legal and ethical considerations surrounding web scraping, including compliance with the target website's terms of service.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n[\n    ScrapedData(url=\"http://example.com\", title=\"Example Title\", content=\"This is the main content of the page.\"),\n    ScrapedData(url=\"http://anotherexample.com\", title=\"Another Example\", content=\"No content available\")\n]\n```"
      ],
      "code_start_line": 11,
      "code_end_line": 67,
      "params": [],
      "have_return": true,
      "code_content": "class FallbackWebScraper:\n    @staticmethod\n    async def scrape(urls: List[str]) -> List[ScrapedData]:\n        \"\"\"\n        Scrapes content from a list of webpages asynchronously.\n        If Tavily API fails, fall back to scraping.\n        \"\"\"\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n        }\n\n        async def fetch_url(url: str) -> ScrapedData:\n            try:\n                async with httpx.AsyncClient(\n                    http2=True, follow_redirects=True\n                ) as client:\n                    response = await client.get(url, headers=headers, timeout=10)\n\n                    if response.status_code != 200:\n                        return ScrapedData(\n                            url=url,\n                            error=f\"HTTP {response.status_code}: {response.reason_phrase}\",\n                        )\n\n                    html = response.text\n                    soup = BeautifulSoup(html, \"html.parser\")\n\n                    # Remove unwanted elements\n                    for script in soup([\"script\", \"style\", \"meta\", \"noscript\"]):\n                        script.decompose()\n\n                    # Extract content\n                    main_content = (\n                        soup.find(\"main\")\n                        or soup.find(\"article\")\n                        or soup.find(\"div\", class_=re.compile(r\"content|main|article\"))\n                    )\n                    content = (\n                        \"No content available\"\n                        if not main_content\n                        else \"\\n\".join(\n                            [\n                                re.sub(r\"\\s+\", \" \", p.get_text(strip=True))\n                                for p in main_content.find_all(\"p\")\n                            ]\n                        )\n                    )\n\n                    return ScrapedData(\n                        url=url,\n                        title=soup.title.string if soup.title else \"Untitled\",\n                        content=content,\n                    )\n            except Exception as e:\n                return ScrapedData(url=url, error=f\"Error: {str(e)}\")\n\n        return await asyncio.gather(*(fetch_url(url) for url in urls))\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/content_scraper/__init__.py",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "scrape",
      "md_content": [
        "**scrape**: The function of scrape is to asynchronously scrape content from a list of webpages, utilizing a fallback mechanism if the Tavily API fails.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs from which the content will be scraped.\n\n**Code Description**: The scrape function is designed to handle the asynchronous scraping of multiple webpages. It takes a list of URLs as input and returns a list of ScrapedData objects, which encapsulate the results of the scraping operation for each URL. The function begins by defining a headers dictionary that includes a User-Agent string to mimic a typical web browser request, which helps avoid potential blocks from the target websites.\n\nWithin the scrape function, an inner asynchronous function named fetch_url is defined. This function is responsible for making the actual HTTP requests to each URL. It utilizes the httpx library's AsyncClient to send GET requests with the specified headers and a timeout of 10 seconds. If the response status code is not 200 (indicating a successful request), the function constructs a ScrapedData object containing the URL and an error message detailing the HTTP status code and reason.\n\nIf the request is successful, the function processes the HTML content of the response using BeautifulSoup. It removes unwanted elements such as scripts, styles, and meta tags to focus on the main content of the page. The function then attempts to locate the main content area by searching for common HTML tags like <main>, <article>, or <div> with class names that suggest they contain the primary content. If no main content is found, it defaults to a message indicating that no content is available.\n\nThe content is extracted from the identified main content area by joining the text of all <p> tags, ensuring that excessive whitespace is removed. A ScrapedData object is then created with the URL, the title of the page (if available), and the extracted content.\n\nThe scrape function ultimately calls asyncio.gather to execute the fetch_url function concurrently for all URLs in the input list. This allows for efficient scraping of multiple pages at once, significantly reducing the time required compared to synchronous requests.\n\nFrom a functional perspective, the scrape function is integral to the FallbackWebScraper module, providing a mechanism to retrieve webpage content when other methods, such as the Tavily API, are unavailable. The results are structured in ScrapedData objects, which standardize the output format, making it easier for other components of the system to handle both successful and failed scraping attempts.\n\n**Note**: It is important to handle the ScrapedData objects carefully, particularly by checking the error attribute to determine if the scraping was successful. The content and title attributes may be None if the scraping process encounters issues or if the content is not available on the page.\n\n**Output Example**: \n[\n    ScrapedData(url=\"http://example.com\", title=\"Example Domain\", content=\"This domain is for use in illustrative examples in documents.\", error=None),\n    ScrapedData(url=\"http://nonexistent.com\", title=None, content=None, error=\"HTTP 404: Not Found\")\n]"
      ],
      "code_start_line": 13,
      "code_end_line": 67,
      "params": [
        "urls"
      ],
      "have_return": true,
      "code_content": "    async def scrape(urls: List[str]) -> List[ScrapedData]:\n        \"\"\"\n        Scrapes content from a list of webpages asynchronously.\n        If Tavily API fails, fall back to scraping.\n        \"\"\"\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n        }\n\n        async def fetch_url(url: str) -> ScrapedData:\n            try:\n                async with httpx.AsyncClient(\n                    http2=True, follow_redirects=True\n                ) as client:\n                    response = await client.get(url, headers=headers, timeout=10)\n\n                    if response.status_code != 200:\n                        return ScrapedData(\n                            url=url,\n                            error=f\"HTTP {response.status_code}: {response.reason_phrase}\",\n                        )\n\n                    html = response.text\n                    soup = BeautifulSoup(html, \"html.parser\")\n\n                    # Remove unwanted elements\n                    for script in soup([\"script\", \"style\", \"meta\", \"noscript\"]):\n                        script.decompose()\n\n                    # Extract content\n                    main_content = (\n                        soup.find(\"main\")\n                        or soup.find(\"article\")\n                        or soup.find(\"div\", class_=re.compile(r\"content|main|article\"))\n                    )\n                    content = (\n                        \"No content available\"\n                        if not main_content\n                        else \"\\n\".join(\n                            [\n                                re.sub(r\"\\s+\", \" \", p.get_text(strip=True))\n                                for p in main_content.find_all(\"p\")\n                            ]\n                        )\n                    )\n\n                    return ScrapedData(\n                        url=url,\n                        title=soup.title.string if soup.title else \"Untitled\",\n                        content=content,\n                    )\n            except Exception as e:\n                return ScrapedData(url=url, error=f\"Error: {str(e)}\")\n\n        return await asyncio.gather(*(fetch_url(url) for url in urls))\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/content_scraper/models.py/ScrapedData"
      ],
      "special_reference_type": [
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "fetch_url",
      "md_content": [
        "**fetch_url**: The function of fetch_url is to asynchronously retrieve and scrape content from a specified URL, returning structured data about the page, including its title and main content, or an error message if the retrieval fails.\n\n**parameters**: The parameters of this Function.\n· url: str - The URL from which the content is to be scraped.\n\n**Code Description**: The fetch_url function is designed to perform an asynchronous HTTP GET request to a specified URL using the httpx library. It initiates an asynchronous context with an HTTP client that supports HTTP/2 and follows redirects. The function attempts to retrieve the content of the page within a timeout period of 10 seconds.\n\nUpon receiving a response, the function checks the HTTP status code. If the status code is not 200 (indicating a successful request), it constructs and returns a ScrapedData object containing the URL and an error message that specifies the HTTP status code and reason phrase.\n\nIf the response is successful, the function proceeds to parse the HTML content using BeautifulSoup. It removes unwanted elements such as scripts, styles, meta tags, and noscript tags to clean the document. The function then attempts to extract the main content of the page by searching for specific HTML tags, such as <main>, <article>, or <div> elements with class names that include \"content\", \"main\", or \"article\".\n\nThe extracted content is processed to ensure that it is clean and well-formatted, specifically by stripping unnecessary whitespace from paragraph elements. If no main content is found, a default message \"No content available\" is returned.\n\nFinally, the function returns a ScrapedData object populated with the URL, the title of the page (if available), and the extracted content. If any exceptions occur during the process, the function captures the exception and returns a ScrapedData object with the URL and an error message indicating the nature of the error.\n\nThis function is integral to the web scraping process as it provides a structured way to handle both successful and failed scraping attempts. The results are encapsulated in ScrapedData objects, which can be further processed or aggregated by other components of the system, such as the FallbackWebScraper.\n\n**Note**: It is important to handle the returned ScrapedData object with care, particularly by checking the error attribute to determine if the scraping was successful. The title and content attributes may be optional and could be None if the information is not available.\n\n**Output Example**: \nA possible return value from the fetch_url function could look like this:\n```\nScrapedData(\n    url=\"https://example.com\",\n    title=\"Example Domain\",\n    content=\"This domain is for use in illustrative examples in documents.\"\n)\n```\nOr in the case of an error:\n```\nScrapedData(\n    url=\"https://example.com\",\n    error=\"HTTP 404: Not Found\"\n)\n```"
      ],
      "code_start_line": 22,
      "code_end_line": 65,
      "params": [
        "url"
      ],
      "have_return": true,
      "code_content": "        async def fetch_url(url: str) -> ScrapedData:\n            try:\n                async with httpx.AsyncClient(\n                    http2=True, follow_redirects=True\n                ) as client:\n                    response = await client.get(url, headers=headers, timeout=10)\n\n                    if response.status_code != 200:\n                        return ScrapedData(\n                            url=url,\n                            error=f\"HTTP {response.status_code}: {response.reason_phrase}\",\n                        )\n\n                    html = response.text\n                    soup = BeautifulSoup(html, \"html.parser\")\n\n                    # Remove unwanted elements\n                    for script in soup([\"script\", \"style\", \"meta\", \"noscript\"]):\n                        script.decompose()\n\n                    # Extract content\n                    main_content = (\n                        soup.find(\"main\")\n                        or soup.find(\"article\")\n                        or soup.find(\"div\", class_=re.compile(r\"content|main|article\"))\n                    )\n                    content = (\n                        \"No content available\"\n                        if not main_content\n                        else \"\\n\".join(\n                            [\n                                re.sub(r\"\\s+\", \" \", p.get_text(strip=True))\n                                for p in main_content.find_all(\"p\")\n                            ]\n                        )\n                    )\n\n                    return ScrapedData(\n                        url=url,\n                        title=soup.title.string if soup.title else \"Untitled\",\n                        content=content,\n                    )\n            except Exception as e:\n                return ScrapedData(url=url, error=f\"Error: {str(e)}\")\n",
      "name_column": 18,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/content_scraper/models.py/ScrapedData"
      ],
      "special_reference_type": [
        true
      ]
    }
  ],
  "src/criticsearch/tools/content_scraper/models.py": [
    {
      "type": "ClassDef",
      "name": "ScrapedData",
      "md_content": [
        "**ScrapedData**: The function of ScrapedData is to represent a data object that holds information scraped from a given URL, including content, title, and potential errors.\n\n**attributes**: The attributes of this Class.\n· url: str - The URL from which the data is scraped.\n· title: Optional[str] - The title of the scraped page, which can be None if unavailable.\n· content: Optional[str] - The main content of the scraped page, which can be None if not found or unavailable.\n· error: Optional[str] - An error message, if there was an issue during scraping.\n\n**Code Description**: The ScrapedData class is a data container that is designed to store and structure the information extracted from web scraping operations. It inherits from `BaseModel`, which likely provides data validation and serialization functionality. The class includes four attributes:\n\n1. **url** (str): This is the URL from which content is scraped, making it an essential identifier for the scraped data.\n2. **title** (Optional[str]): This attribute holds the title of the webpage or content if available, providing a reference to the type or subject of the page. It is optional and can be None if the title is not present.\n3. **content** (Optional[str]): The main content that was extracted from the webpage is stored in this field. It is optional, as some pages might not have meaningful content or may not be accessible.\n4. **error** (Optional[str]): This attribute is used to store any error message that might have occurred during the scraping process. If an error was encountered, this field will contain the error message; otherwise, it will be None.\n\nThe ScrapedData class plays a crucial role in both successful and failed scraping scenarios. When content is scraped successfully, the `url` and `content` attributes are populated. If there are any issues during scraping, such as an HTTP error or a failure to retrieve meaningful content, the `error` attribute will contain details about what went wrong. This class is used by the `scrape` functions found in the `FallbackWebScraper` and `ContentScraper` modules.\n\nFrom a functional perspective, ScrapedData objects are instantiated during the scraping process to store the results of individual scraping attempts. If the scraping process is successful, the object is populated with data like the content of the page and its title. If scraping fails, the object includes an error message to provide insight into the issue. These instances are then collected in lists, such as `ScrapedDataList`, which is used to return the aggregated results.\n\nThe `scrape` function in `ContentScraper` first tries to extract content using the Tavily API and constructs a ScrapedData object for each URL it processes. In case of failure, it uses `FallbackWebScraper.scrape`, which also returns ScrapedData objects. These instances are used throughout the system to ensure that scraping results are handled consistently, even when some URLs result in errors.\n\n**Note**: The `ScrapedData` class should always be handled with care, especially when dealing with failed scrapes. The presence of the `error` attribute should be checked to ensure that the scraping process was successful. Additionally, it is important to understand that content and title are optional attributes and may be missing in certain cases."
      ],
      "code_start_line": 6,
      "code_end_line": 10,
      "params": [],
      "have_return": false,
      "code_content": "class ScrapedData(BaseModel):\n    url: str\n    title: Optional[str] = None\n    content: Optional[str] = None\n    error: Optional[str] = None\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/content_scraper/__init__.py",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape",
        "src/criticsearch/tools/content_scraper/fallback_web_scraper.py",
        "src/criticsearch/tools/content_scraper/fallback_web_scraper.py/FallbackWebScraper/scrape",
        "src/criticsearch/tools/content_scraper/fallback_web_scraper.py/FallbackWebScraper/scrape/fetch_url",
        "src/criticsearch/tools/content_scraper/models.py/ScrapedDataList"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "ScrapedDataList",
      "md_content": [
        "**ScrapedDataList**: The function of ScrapedDataList is to represent a collection of ScrapedData objects, holding the results of web scraping, including both successful and failed attempts, with serialization capabilities to output the data in a readable format.\n\n**attributes**: The attributes of this Class.\n· data: List[ScrapedData] - A list of ScrapedData objects, representing the individual scraping results for each URL processed. It contains both successful and failed results.\n· max_content_length: int - The maximum allowed length for individual scraped content. If the content exceeds this length, it will be truncated.\n· max_output_length: int - The maximum length of the serialized result that can be returned. If the output exceeds this length, it will be truncated.\n\n**Code Description**: \nThe ScrapedDataList class is designed to manage a collection of ScrapedData objects, which store the results of scraping attempts for multiple URLs. It inherits from BaseModel, providing data validation and serialization capabilities. The class contains three key attributes:\n\n1. **data** (List[ScrapedData]): This attribute holds a list of ScrapedData instances, which represent the individual results of the scraping process. Each ScrapedData object can contain information like the scraped URL, title, content, and any errors encountered during scraping. This list forms the core of the class, containing all the data gathered during scraping.\n\n2. **max_content_length** (int): This attribute defines the maximum allowable length for the content of each scraped data object. If any scraped content exceeds this length, it will be truncated and appended with the string \"[TOO LONG, END]\" to indicate that the content has been cut off. The default value is set to 10,000 characters.\n\n3. **max_output_length** (int): This attribute controls the maximum length of the entire serialized output. If the concatenated result of all scraped data exceeds this length, the output will be truncated with the message \"[OUTPUT TOO LONG, TRUNCATED]\" to avoid excessive output. The default value is set to 100,000 characters.\n\nThe class includes the method `ser_model`, which performs the serialization of the ScrapedDataList object into a human-readable string format. The function iterates over each ScrapedData object in the `data` attribute and processes it as follows:\n\n- If the ScrapedData object contains an error, the method appends an error message to the result, specifying the URL and the associated error.\n- If no error is found, the content is checked against the `max_content_length`. If the content exceeds this length, it is truncated to the specified limit, with the string \"[TOO LONG, END]\" appended to signal the truncation.\n- The method then constructs a string for each ScrapedData object, including the URL, title, and content.\n- All these strings are concatenated with a separator (\"---\") between each entry.\n\nFinally, the complete serialized string is checked against the `max_output_length`. If the overall string exceeds the defined length, it will be truncated with the message \"[OUTPUT TOO LONG, TRUNCATED]\".\n\nFrom a functional perspective, ScrapedDataList is typically used to aggregate multiple ScrapedData objects that represent the results of a scraping operation. For example, the `scrape` function in the `ContentScraper` module calls this class to collect the results of web scraping and returns a serialized version of the results as a string.\n\nThe ScrapedDataList class provides an efficient way to manage and output scraped data, ensuring that even large datasets are handled appropriately by applying length restrictions at both the individual content level and the overall output level. This ensures that the serialized results remain within acceptable limits for processing or logging.\n\n**Note**: When using ScrapedDataList, ensure that the content length and output length are managed effectively, especially when handling large sets of data, to avoid data truncation. Additionally, be aware that the `error` field in ScrapedData objects may be present, and it should be checked to handle failed scraping attempts appropriately.\n\n**Output Example**:\nHere is an example of a serialized output that might be returned by the `ser_model` method:\n\n```\nURL: https://example.com/page1\nTitle: Example Page 1\nContent:\nThis is the content of the first page. It contains relevant information about the topic.\n\n---\nURL: https://example.com/page2\nTitle: Example Page 2\nContent:\nThis page has too much content. It is truncated here: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. [TOO LONG, END]\n\n[OUTPUT TOO LONG, TRUNCATED]\n```"
      ],
      "code_start_line": 13,
      "code_end_line": 46,
      "params": [],
      "have_return": true,
      "code_content": "class ScrapedDataList(BaseModel):\n    data: List[ScrapedData] = Field(default_factory=list)\n    max_content_length: int = 10000  # Max length for individual content\n    max_output_length: int = 100000  # Max length for the entire serialized result\n\n    @model_serializer\n    def ser_model(self) -> str:\n        # List to store concatenated strings\n        result = []\n\n        for data in self.data:\n            if data.error:\n                result.append(f\"Error for URL {data.url}: {data.error}\\n\")\n                continue  # Skip further processing if there's an error\n\n            assert data.content is not None\n\n            # Truncate content to ensure it does not exceed max_content_length\n            if len(data.content) > self.max_content_length:\n                data.content = (\n                    data.content[: self.max_content_length] + \"[TOO LONG, END]\"\n                )\n\n            result.append(\n                f\"URL: {data.url}\\nTitle: {data.title}\\nContent:\\n{data.content}\\n\"\n            )\n\n        output = \"\\n---\\n\".join(result)\n\n        # Apply final length truncation to the overall result\n        if len(output) > self.max_output_length:\n            output = output[: self.max_output_length] + \"\\n[OUTPUT TOO LONG, TRUNCATED]\"\n\n        return output\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/content_scraper/__init__.py",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape"
      ],
      "reference_who": [
        "src/criticsearch/tools/content_scraper/models.py/ScrapedData"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "ser_model",
      "md_content": [
        "**ser_model**: The function of ser_model is to generate a formatted string representation of scraped data, including error handling and content truncation.\n\n**parameters**: The parameters of this Function.\n· self: An instance of the class that contains the scraped data and configuration settings.\n\n**Code Description**: The ser_model function processes a collection of scraped data stored in the instance variable `self.data`. It initializes an empty list called `result` to accumulate formatted strings for each data entry. The function iterates over each item in `self.data`, checking for any errors associated with the data. If an error is found, it appends an error message to the `result` list and skips further processing for that entry.\n\nFor valid entries, the function asserts that the content is not None. It then checks if the length of the content exceeds a predefined maximum length (`self.max_content_length`). If it does, the content is truncated, and a suffix indicating truncation is added. The function constructs a formatted string that includes the URL, title, and content of the data entry, which is then appended to the `result` list.\n\nAfter processing all entries, the function joins the strings in the `result` list with a separator (\"---\") to create a single output string. It then checks if the total length of this output exceeds another predefined maximum length (`self.max_output_length`). If it does, the output is truncated, and a message indicating truncation is appended.\n\nFinally, the function returns the formatted output string, which contains the processed information of all the scraped data entries.\n\n**Note**: It is important to ensure that `self.data` is properly populated with valid data objects before calling this function. Additionally, the maximum lengths for content and output should be set appropriately to avoid unintended truncation.\n\n**Output Example**: \n```\nURL: http://example.com/page1\nTitle: Example Page 1\nContent:\nThis is the content of the first example page.\n\n---\nURL: http://example.com/page2\nTitle: Example Page 2\nContent:\nError for URL http://example.com/page2: Page not found\n\n---\nURL: http://example.com/page3\nTitle: Example Page 3\nContent:\nThis is the content of the third example page, which is quite informative and exceeds the maximum length set for content. [TOO LONG, END]\n```"
      ],
      "code_start_line": 19,
      "code_end_line": 46,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def ser_model(self) -> str:\n        # List to store concatenated strings\n        result = []\n\n        for data in self.data:\n            if data.error:\n                result.append(f\"Error for URL {data.url}: {data.error}\\n\")\n                continue  # Skip further processing if there's an error\n\n            assert data.content is not None\n\n            # Truncate content to ensure it does not exceed max_content_length\n            if len(data.content) > self.max_content_length:\n                data.content = (\n                    data.content[: self.max_content_length] + \"[TOO LONG, END]\"\n                )\n\n            result.append(\n                f\"URL: {data.url}\\nTitle: {data.title}\\nContent:\\n{data.content}\\n\"\n            )\n\n        output = \"\\n---\\n\".join(result)\n\n        # Apply final length truncation to the overall result\n        if len(output) > self.max_output_length:\n            output = output[: self.max_output_length] + \"\\n[OUTPUT TOO LONG, TRUNCATED]\"\n\n        return output\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tools/content_scraper/__init__.py": [
    {
      "type": "ClassDef",
      "name": "ContentScraper",
      "md_content": [
        "**ContentScraper**: The function of ContentScraper is to scrape content from the provided URLs by using an external API (Tavily) with a fallback mechanism for custom scraping.\n\n**attributes**: \n- urls: List[str] - A list of URLs from which content is to be scraped.\n\n**Code Description**: \nThe `ContentScraper` class provides a static method, `scrape`, that is used to extract content from a list of URLs. The method follows these main steps:\n\n1. **API Key and Initialization**: The method starts by retrieving an API key from the project settings (`settings.tavily.api_key`) and initializes an instance of `TavilyExtract` with the API key.\n   \n2. **Tavily Extraction**: The method attempts to scrape content from the provided URLs using the `TavilyExtract` instance by calling `extract_content` on the `tavily` object. This method returns results that are either successful or failed.\n\n3. **Error Handling**: If the Tavily extraction results contain an error (indicated by the presence of the \"error\" key), the method logs the error and proceeds to fall back to a custom web scraping solution by invoking `FallbackWebScraper.scrape`. This is done to ensure that content scraping continues even if the Tavily service fails.\n\n4. **Processing Successful Results**: If the Tavily extraction is successful, the results (contained in the `results` key) are iterated through. For each result, the URL and raw content are extracted and stored as `ScrapedData` objects, which are then added to a list of successful results.\n\n5. **Handling Failed Results**: If any URLs are marked as failed in the Tavily results (`failed_results`), the method logs a warning and retries scraping those URLs using the custom fallback scraper.\n\n6. **Final Results**: After processing both successful and failed results, the method merges these into a final list and returns them encapsulated within a `ScrapedDataList` object, which is serialized using the `model_dump` method before being returned.\n\nThis approach ensures a robust content scraping process with error handling and fallback mechanisms for cases when the primary extraction method (Tavily) fails.\n\nThe `scrape` method is used within the `BaseAgent` class (located in `src/criticsearch/base_agent.py`), which initializes an instance of `ContentScraper` and adds it to the list of available tools for the conversation manager. This allows the agent to call the `scrape` method when it requires content extraction from a set of URLs.\n\n**Note**: \n- The `scrape` method is asynchronous, meaning it must be awaited when called.\n- The method handles both successful and failed scraping attempts by using a combination of the Tavily API and a custom fallback scraper.\n- Ensure the API key for Tavily is correctly set in the project settings for successful integration.\n\n**Output Example**: \nThe `scrape` method will return a serialized `ScrapedDataList` object that contains a list of `ScrapedData` objects. Each `ScrapedData` object includes the URL and the raw content extracted from the source. Here's a mock example:\n\n```json\n{\n  \"data\": [\n    {\n      \"url\": \"https://example.com/page1\",\n      \"content\": \"This is the raw content from page 1.\"\n    },\n    {\n      \"url\": \"https://example.com/page2\",\n      \"content\": \"This is the raw content from page 2.\"\n    }\n  ]\n}\n```",
        "**ContentScraper**: The function of ContentScraper is to scrape content from provided URLs using a combination of API-based extraction and fallback web scraping.\n\n**attributes**:  \n· urls: List of strings containing the URLs to be scraped.  \n\n**Code Description**:  \nThe `ContentScraper` class contains a single static method `scrape`, which is designed to scrape content from a list of URLs. This method handles the process of scraping through an external service (Tavily API) and, in case of failure, falls back to web scraping using another method. The `scrape` method performs the following key operations:\n\n1. **Tavily API Extraction**:  \n   The method begins by extracting an API key from the settings configuration. It initializes an instance of `TavilyExtract` with this key. The `TavilyExtract` class is responsible for interacting with the Tavily API, which is used to attempt content extraction from the provided URLs. The `scrape` method asynchronously calls `tavily.extract_content(urls)` to perform the extraction.\n\n2. **Error Handling and Fallback**:  \n   If the Tavily API response contains an error (as indicated by the presence of a \"detail\" key in the response), the method logs an error message and proceeds with a fallback scraping approach. The fallback scraping is performed by the `FallbackWebScraper`, which scrapes content directly from the web.\n\n3. **Processing Successful Results**:  \n   In cases where Tavily extraction is successful, the results are processed. The method iterates over the successful results returned by Tavily. For each result, the URL and raw content are extracted, and a `ScrapedData` object is created to store these values. These successful results are then stored in a list.\n\n4. **Handling Failed Results**:  \n   If some URLs fail in the Tavily extraction process, the method logs this issue and proceeds to scrape those URLs using the fallback web scraper. The failed results are processed in a similar way to the successful ones, resulting in another list of `ScrapedData`.\n\n5. **Merging Results**:  \n   After handling both successful and failed results, the method merges these two sets of results into a final list and wraps it in a `ScrapedDataList` object. This final result is then returned by the method after invoking `model_dump()` to serialize the data for further processing or storage.\n\nThis class is called within the context of the `BaseAgent` class, where an instance of `ContentScraper` is created and used to scrape content as part of the agent's overall functionality. Specifically, `BaseAgent` utilizes the `scrape` method to extract relevant content from URLs during its execution flow. It relies on the `ContentScraper` for content extraction, which is an essential part of the agent’s toolset, enabling the agent to gather and process web data.\n\n**Note**:  \nWhen using the `ContentScraper` class, ensure that the settings are correctly configured, particularly the API key for Tavily, and that the fallback web scraper is available. The success of the content scraping process is dependent on both the Tavily API's performance and the fallback web scraper's ability to handle failed cases. The `scrape` method is asynchronous and should be called within an asynchronous context.\n\n**Output Example**:  \nA possible return value from the `scrape` method could be a serialized list of `ScrapedData` objects, each containing the URL and the corresponding scraped content. The serialized structure may look like:\n\n```json\n{\n  \"data\": [\n    {\n      \"url\": \"https://example.com/article1\",\n      \"content\": \"This is the scraped content from article 1.\"\n    },\n    {\n      \"url\": \"https://example.com/article2\",\n      \"content\": \"This is the scraped content from article 2.\"\n    }\n  ]\n}\n```"
      ],
      "code_start_line": 10,
      "code_end_line": 67,
      "params": [],
      "have_return": true,
      "code_content": "class ContentScraper:\n    @staticmethod\n    async def scrape(urls: List[str]) -> ScrapedDataList:\n        \"\"\"\n        Scrapes content using the provided URLs.\n\n        Args:\n            urls (List[str]): A list of URLs to scrape content from.\n        \"\"\"\n        api_key = settings.tavily.api_key\n        tavily = TavilyExtract(api_key)\n\n        # Try to extract using Tavily\n        tavily_results = await tavily.extract_content(urls)\n        final_results = []\n\n        # Check for errors or failed results in the Tavily response\n        if \"detail\" in tavily_results:\n            error_message = tavily_results[\"detail\"].get(\"error\", \"Unknown error\")\n\n            printer.log(\n                f\"Tavily API extraction failed: {error_message}, falling back to web scraping.\",\n                style=\"bold red\",\n            )\n\n            final_results = await FallbackWebScraper.scrape(urls)\n        else:\n            # Process successful results from Tavily\n            successful_results = []\n\n            results = tavily_results.get(\"results\", [])\n            for result in results:\n                # Extract the necessary data from the Tavily API response\n                url = result.get(\"url\")\n                raw_content = result.get(\"raw_content\")\n\n                successful_results.append(\n                    ScrapedData(\n                        url=url,\n                        content=raw_content,\n                    )\n                )\n\n            failed_results = tavily_results.get(\"failed_results\", [])\n\n            # If Tavily has failed results, log them and proceed with fallback\n            if failed_results:\n                printer.log(\n                    f\"Some URLs failed in Tavily extraction. Using fallback web scraping for these URLs.\",\n                    style=\"bold yellow\",\n                )\n                failed_urls = [result.get(\"url\") for result in failed_results]\n                failed_results = await FallbackWebScraper.scrape(failed_urls)\n\n            # Merge both successful and failed results into ScrapedDataList\n            final_results = successful_results + failed_results\n\n        return ScrapedDataList(data=final_results).model_dump()\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py",
        "src/criticsearch/base_agent.py/BaseAgent/__init__",
        "src/criticsearch/tools/__init__.py"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "scrape",
      "md_content": [
        "**scrape**: The function of scrape is to asynchronously scrape content from a list of provided URLs, utilizing the Tavily API for extraction and falling back to a custom web scraping method if necessary.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs to scrape content from.\n\n**Code Description**: The `scrape` function is an asynchronous method designed to extract content from a list of URLs. It first initializes an instance of the `TavilyExtract` class using an API key retrieved from the settings. This instance is responsible for interacting with the Tavily API to perform the content extraction.\n\nThe function begins by calling the `extract_content` method of the `TavilyExtract` instance, passing the list of URLs. This method sends an asynchronous POST request to the Tavily API, which returns a JSON response containing the results of the extraction. If the response includes an error, the function logs the error message and proceeds to invoke the `FallbackWebScraper.scrape` method, which serves as a backup scraping mechanism.\n\nIn the case of successful extraction, the function processes the results returned by the Tavily API. It iterates through the successful results, extracting the URL and raw content from each result. For each successful extraction, it creates a `ScrapedData` object, which encapsulates the URL and the corresponding content.\n\nAdditionally, if there are any failed results in the Tavily API response, the function logs a warning and attempts to scrape the failed URLs using the `FallbackWebScraper.scrape` method. The results from both successful and fallback scraping are merged into a final list of `ScrapedData` objects.\n\nFinally, the function returns a `ScrapedDataList` object, which contains all the scraped data, including both successful and failed attempts. This object is serialized using the `model_dump` method, providing a structured output of the scraping results.\n\nThe `scrape` function is called within the `BaseAgent` class, specifically in the `search_and_browse` method. This method orchestrates the overall search and scraping process, first performing a search using the `SearchAggregator` and then invoking the `scrape` function to gather additional content from the URLs identified during the search. This integration ensures that the scraping functionality is seamlessly incorporated into the broader search and retrieval workflow of the application.\n\n**Note**: When using this function, ensure that the URLs provided are valid and accessible. Additionally, be aware of the legal and ethical considerations surrounding web scraping, including compliance with the target website's terms of service.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n[\n    ScrapedData(url=\"http://example.com\", title=\"Example Title\", content=\"This is the main content of the page.\"),\n    ScrapedData(url=\"http://anotherexample.com\", title=\"Another Example\", content=\"No content available\")\n]\n```",
        "**scrape**: The function of scrape is to asynchronously scrape content from a list of provided URLs, utilizing the Tavily API for extraction and falling back to a web scraping method if necessary.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs to scrape content from.\n\n**Code Description**: The `scrape` function is an asynchronous method designed to extract content from a list of URLs. It begins by initializing an instance of the `TavilyExtract` class using an API key sourced from the application settings. This instance is responsible for interacting with the Tavily API to retrieve content.\n\nThe function then calls the `extract_content` method of the `TavilyExtract` instance, passing the provided list of URLs. This method sends a POST request to the Tavily API, which returns a structured response containing either the extracted content or error details.\n\nUpon receiving the results from the Tavily API, the function checks for any errors indicated in the response. If an error is present, it logs the error message using the `log` method from the `RichPrinter` class, indicating that the Tavily API extraction has failed. In this case, the function falls back to the `FallbackWebScraper` class, invoking its `scrape` method to attempt content extraction from the original list of URLs.\n\nIf the Tavily API returns successful results, the function processes these results by iterating through the list of extracted data. For each successful extraction, it creates a `ScrapedData` object, which encapsulates the URL and the raw content retrieved. The function also checks for any failed results returned by the Tavily API. If there are failed results, it logs a warning message and attempts to scrape content from the failed URLs using the `FallbackWebScraper`.\n\nFinally, the function aggregates both successful and failed results into a `ScrapedDataList` object, which is returned after being serialized. This structured approach ensures that the function handles both successful and failed scraping attempts effectively, providing a comprehensive output that includes all relevant data.\n\nThe `scrape` function is called within the `web_scrape_results` method of the `BaseAgent` class. This method orchestrates the overall scraping process by rendering a prompt and interacting with the model. It collects the URLs from the tool calls and invokes the `scrape` function to retrieve content, ensuring that the agent can effectively extract information from web sources.\n\n**Note**: When using this function, ensure that the URLs provided are valid and accessible. Additionally, be aware of the legal and ethical considerations surrounding web scraping, including compliance with the target website's terms of service.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n{\n    \"data\": [\n        {\"url\": \"http://example.com\", \"title\": \"Example Title\", \"content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"title\": \"Another Example\", \"content\": \"No content available\"}\n    ]\n}\n```"
      ],
      "code_start_line": 12,
      "code_end_line": 67,
      "params": [
        "urls"
      ],
      "have_return": true,
      "code_content": "    async def scrape(urls: List[str]) -> ScrapedDataList:\n        \"\"\"\n        Scrapes content using the provided URLs.\n\n        Args:\n            urls (List[str]): A list of URLs to scrape content from.\n        \"\"\"\n        api_key = settings.tavily.api_key\n        tavily = TavilyExtract(api_key)\n\n        # Try to extract using Tavily\n        tavily_results = await tavily.extract_content(urls)\n        final_results = []\n\n        # Check for errors or failed results in the Tavily response\n        if \"detail\" in tavily_results:\n            error_message = tavily_results[\"detail\"].get(\"error\", \"Unknown error\")\n\n            printer.log(\n                f\"Tavily API extraction failed: {error_message}, falling back to web scraping.\",\n                style=\"bold red\",\n            )\n\n            final_results = await FallbackWebScraper.scrape(urls)\n        else:\n            # Process successful results from Tavily\n            successful_results = []\n\n            results = tavily_results.get(\"results\", [])\n            for result in results:\n                # Extract the necessary data from the Tavily API response\n                url = result.get(\"url\")\n                raw_content = result.get(\"raw_content\")\n\n                successful_results.append(\n                    ScrapedData(\n                        url=url,\n                        content=raw_content,\n                    )\n                )\n\n            failed_results = tavily_results.get(\"failed_results\", [])\n\n            # If Tavily has failed results, log them and proceed with fallback\n            if failed_results:\n                printer.log(\n                    f\"Some URLs failed in Tavily extraction. Using fallback web scraping for these URLs.\",\n                    style=\"bold yellow\",\n                )\n                failed_urls = [result.get(\"url\") for result in failed_results]\n                failed_results = await FallbackWebScraper.scrape(failed_urls)\n\n            # Merge both successful and failed results into ScrapedDataList\n            final_results = successful_results + failed_results\n\n        return ScrapedDataList(data=final_results).model_dump()\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/__init__",
        "src/criticsearch/base_agent.py/BaseAgent/web_scrape_results"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/tools/content_scraper/fallback_web_scraper.py/FallbackWebScraper",
        "src/criticsearch/tools/content_scraper/models.py/ScrapedData",
        "src/criticsearch/tools/content_scraper/models.py/ScrapedDataList",
        "src/criticsearch/tools/content_scraper/tavily_extract.py/TavilyExtract",
        "src/criticsearch/tools/content_scraper/tavily_extract.py/TavilyExtract/extract_content"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        true,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/tools/content_scraper/tavily_extract.py": [
    {
      "type": "ClassDef",
      "name": "TavilyExtract",
      "md_content": [
        "**TavilyExtract**: The function of TavilyExtract is to interact with the Tavily API to extract content from a list of provided URLs.\n\n**attributes**: The attributes of this Class.\n· base_url: The base URL for the Tavily API endpoint used for content extraction.\n· _api_key: The API key used for authentication with the Tavily API.\n· headers: The headers required for making requests to the Tavily API, including content type and authorization.\n\n**Code Description**: The TavilyExtract class is designed to facilitate the extraction of content from specified URLs using the Tavily API. Upon initialization, it requires an API key, which is stored as a private attribute (_api_key). The class constructs the necessary headers for API requests, ensuring that the content type is set to JSON and that the API key is included in the authorization header.\n\nThe primary method of the class, `extract_content`, takes a list of URLs as input. It constructs a JSON payload containing these URLs and makes an asynchronous POST request to the Tavily API using the httpx library. The method handles potential HTTP errors by raising exceptions for any 4xx or 5xx responses, and it captures request errors as well. If the request is successful, the method parses the JSON response and returns the data. In case of errors, it returns a dictionary containing the error message.\n\nThis class is utilized within the `scrape` function of the ContentScraper module. The `scrape` function first initializes an instance of TavilyExtract with the API key retrieved from the settings. It then calls the `extract_content` method, passing the list of URLs to be scraped. The results from the Tavily API are checked for errors; if any errors are present, the function logs the error and falls back to a custom web scraping method. If the extraction is successful, it processes the results, extracting the necessary data and merging successful and failed results into a final output.\n\n**Note**: Ensure that the API key is valid and has the necessary permissions to access the Tavily API. The URLs provided should be accessible and valid to avoid unnecessary errors during the extraction process.\n\n**Output Example**: A possible appearance of the code's return value could be:\n{\n  \"results\": [\n    {\n      \"url\": \"https://example.com\",\n      \"raw_content\": \"<html>...</html>\"\n    }\n  ],\n  \"failed_results\": [\n    {\n      \"url\": \"https://failed-url.com\"\n    }\n  ]\n}",
        "**TavilyExtract**: The function of TavilyExtract is to facilitate the extraction of content from a list of URLs using the Tavily API.\n\n**attributes**: The attributes of this Class.\n· api_key: str - The API key used for authenticating requests to the Tavily API.  \n· base_url: str - The base URL for the Tavily API endpoint used for content extraction.  \n· headers: dict - The headers required for making requests to the Tavily API, including content type and authorization.\n\n**Code Description**: The TavilyExtract class is designed to interact with the Tavily API for the purpose of extracting content from specified URLs. Upon initialization, the class requires an API key, which is stored as a private attribute. The base URL for the Tavily API is set to \"https://api.tavily.com/extract\", and the necessary headers for making requests are constructed, including the content type and authorization using the provided API key.\n\nThe primary method of this class is `extract_content`, which accepts a list of URLs as its parameter. This method is asynchronous and utilizes the httpx library to send a POST request to the Tavily API. The request payload consists of the list of URLs, which is sent in JSON format. Upon receiving a response, the method attempts to parse the JSON data returned by the API. If the request is successful, the parsed data is returned. In the event of a request error, the method captures the exception and returns a structured error message indicating the nature of the issue.\n\nThe TavilyExtract class is utilized within the `scrape` function found in the ContentScraper module. In this context, an instance of TavilyExtract is created using an API key sourced from the application settings. The `scrape` function calls the `extract_content` method of the TavilyExtract instance to retrieve content from the provided URLs. If the Tavily API returns an error, the function logs the error and resorts to a fallback web scraping method to ensure that content extraction continues. This integration allows for a seamless workflow where the TavilyExtract class serves as the primary means of content extraction, while also providing a backup mechanism in case of failure.\n\n**Note**: When using this class, ensure that the API key is valid and that the URLs provided are accessible. Additionally, be mindful of the legal and ethical considerations surrounding web scraping, including compliance with the target website's terms of service.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n{\n    \"results\": [\n        {\"url\": \"http://example.com\", \"raw_content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"raw_content\": \"No content available\"}\n    ],\n    \"failed_results\": []\n}\n```"
      ],
      "code_start_line": 6,
      "code_end_line": 28,
      "params": [],
      "have_return": true,
      "code_content": "class TavilyExtract:\n    def __init__(self, api_key: str):\n        self.base_url = \"https://api.tavily.com/extract\"\n        self._api_key = api_key\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self._api_key}\",\n        }\n\n    async def extract_content(self, urls: List[str]):\n        payload = {\"urls\": urls}\n\n        async with httpx.AsyncClient(http2=True) as client:\n            try:\n                response = await client.post(\n                    self.base_url, headers=self.headers, json=payload\n                )\n                # Parse the response data and return the whole response object\n                data = response.json()\n                return data\n\n            except httpx.RequestError as e:\n                return {\"detail\": {\"error\": f\"Request error occurred: {str(e)}\"}}\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/content_scraper/__init__.py",
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the TavilyExtract object by setting up the base URL for API requests and configuring the necessary headers for authentication.\n\n**parameters**: The parameters of this function.\n· api_key: str - This is the API key required for authenticating requests to the Tavily API.\n\n**Code Description**: \nThe `__init__` function is the constructor method of the `TavilyExtract` class. It is responsible for setting up the essential attributes needed to interact with the Tavily API. Upon initialization:\n\n1. The `base_url` attribute is assigned a fixed value, `\"https://api.tavily.com/extract\"`. This URL is the endpoint used for making API requests.\n  \n2. The `_api_key` attribute is initialized with the provided `api_key` parameter, which is a string containing the user's personal API key. This key is used to authenticate requests made to the Tavily API.\n\n3. The `headers` attribute is a dictionary containing two key-value pairs:\n   - `\"Content-Type\": \"application/json\"`: This specifies that the data being sent in requests will be formatted as JSON.\n   - `\"Authorization\": f\"Bearer {self._api_key}\"`: This header is used for bearer token authentication, where the API key is included in the Authorization header to verify the identity of the requester.\n\nThis constructor ensures that an instance of the `TavilyExtract` class is correctly set up with the necessary API URL and authentication details for subsequent API interactions.\n\n**Note**: \n- The `api_key` parameter is mandatory for initializing the `TavilyExtract` class and must be kept secure to prevent unauthorized access.\n- The `base_url` and `headers` are automatically configured and cannot be modified directly through the constructor. Any changes to these values would require altering the code itself."
      ],
      "code_start_line": 7,
      "code_end_line": 13,
      "params": [
        "self",
        "api_key"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, api_key: str):\n        self.base_url = \"https://api.tavily.com/extract\"\n        self._api_key = api_key\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self._api_key}\",\n        }\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_content",
      "md_content": [
        "**extract_content**: The function of extract_content is to asynchronously request and retrieve content data from an external service based on a list of provided URLs.\n\n**parameters**:\n· urls: List[str]  \n  - A list of URLs for which content needs to be extracted.\n\n**Code Description**:  \nThe `extract_content` function is an asynchronous method that facilitates the extraction of content from a specified external service. The method is designed to take a list of URLs as input and then make an HTTP POST request to a remote API to retrieve the content associated with these URLs.\n\n1. **Payload Creation**:  \n   The function first constructs a payload dictionary containing the `urls` parameter passed to the function. This payload is intended to be sent as the body of the POST request.\n\n2. **Making the API Call**:  \n   Using the `httpx.AsyncClient` with HTTP/2 support enabled, the function sends an asynchronous POST request to the service’s base URL (stored in `self.base_url`). The headers (stored in `self.headers`) and the payload (containing the URLs) are included in the request. This is performed within an asynchronous context to ensure non-blocking behavior during the request.\n\n3. **Handling Responses**:  \n   Upon receiving the response, the function checks whether the request was successful by calling `response.raise_for_status()`. If the status code indicates an error (4xx/5xx), an exception is raised and caught.\n\n4. **Error Handling**:  \n   If an HTTP error occurs (e.g., 404 or 500), the function will return a dictionary with the key `error` and a message indicating that an HTTP error occurred. Similarly, if there’s an issue with the request itself (e.g., network issues), a `RequestError` is caught, and an error message is returned.\n\n5. **Returning Data**:  \n   If the request is successful, the function parses the response as JSON and returns the resulting data. This JSON data is typically structured to contain content related to the requested URLs.\n\nIn terms of its usage, the `extract_content` method is invoked within the `scrape` function found in the `src/criticsearch/tools/content_scraper/__init__.py` file. The `scrape` function calls `extract_content` to attempt content extraction using the Tavily API. If successful, it processes the returned results into a list of `ScrapedData` objects. If the extraction fails or encounters errors, it falls back to a custom web scraping mechanism via `FallbackWebScraper`. This integration ensures the robustness of the scraping process, allowing for alternative methods when the primary API fails.\n\n**Note**:  \n- The function is asynchronous and requires an `await` keyword when calling it.  \n- The `httpx` library should be installed and configured correctly to ensure successful HTTP requests.  \n- Proper error handling is implemented to catch both HTTP-specific and general request errors, returning meaningful error messages for each type of failure.\n\n**Output Example**:  \nA possible response when the request is successful could look like the following:\n\n```json\n{\n  \"results\": [\n    {\n      \"url\": \"http://example.com/page1\",\n      \"raw_content\": \"This is the content of the first page.\"\n    },\n    {\n      \"url\": \"http://example.com/page2\",\n      \"raw_content\": \"This is the content of the second page.\"\n    }\n  ],\n  \"failed_results\": []\n}\n```\n\nIf there is an error, the returned dictionary may look like this:\n\n```json\n{\n  \"error\": \"HTTP error occurred: 404 Client Error: Not Found for url: http://example.com/page1\"\n}\n```",
        "**extract_content**: The function of extract_content is to asynchronously extract content from a list of provided URLs using the Tavily API.\n\n**parameters**: The parameters of this Function.\n· urls: List[str] - A list of URLs to scrape content from.\n\n**Code Description**: The `extract_content` function is an asynchronous method that takes a list of URLs as input and sends a POST request to the Tavily API to extract content from those URLs. The function begins by constructing a payload that includes the provided URLs. It then utilizes the `httpx.AsyncClient` to create an asynchronous HTTP client capable of handling HTTP/2 requests.\n\nWithin a try-except block, the function attempts to send the POST request to the API endpoint specified by `self.base_url`, including necessary headers and the JSON payload. Upon receiving a response, the function parses the JSON data from the response and returns it. If a request error occurs during this process, the function catches the exception and returns a structured error message indicating the nature of the request error.\n\nThis function is called by the `scrape` function within the `ContentScraper` class. The `scrape` function orchestrates the content extraction process by first invoking `extract_content` to retrieve data from the Tavily API. If the API call is successful, the `scrape` function processes the results, creating `ScrapedData` objects for each successful extraction. In cases where the Tavily API returns an error or fails to extract content from certain URLs, the `scrape` function logs the error and may fall back to an alternative web scraping method.\n\nThe integration of `extract_content` within the `scrape` function highlights its role as a critical component in the content extraction workflow, enabling seamless interaction with the Tavily API to facilitate data retrieval from multiple URLs.\n\n**Note**: When using this function, ensure that the URLs provided are valid and accessible. Additionally, be aware of the legal and ethical considerations surrounding web scraping, including compliance with the target website's terms of service.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```\n{\n    \"results\": [\n        {\"url\": \"http://example.com\", \"raw_content\": \"This is the main content of the page.\"},\n        {\"url\": \"http://anotherexample.com\", \"raw_content\": \"No content available\"}\n    ],\n    \"failed_results\": []\n}\n```"
      ],
      "code_start_line": 15,
      "code_end_line": 28,
      "params": [
        "self",
        "urls"
      ],
      "have_return": true,
      "code_content": "    async def extract_content(self, urls: List[str]):\n        payload = {\"urls\": urls}\n\n        async with httpx.AsyncClient(http2=True) as client:\n            try:\n                response = await client.post(\n                    self.base_url, headers=self.headers, json=payload\n                )\n                # Parse the response data and return the whole response object\n                data = response.json()\n                return data\n\n            except httpx.RequestError as e:\n                return {\"detail\": {\"error\": f\"Request error occurred: {str(e)}\"}}\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/content_scraper/__init__.py/ContentScraper/scrape"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tools/search_adapter/base_search_client.py": [
    {
      "type": "ClassDef",
      "name": "BaseSearchClient",
      "md_content": [
        "**BaseSearchClient**: The function of BaseSearchClient is to define the structure and interface for search clients. \n\n**attributes**: \n- There are no attributes in this class.\n\n**Code Description**: \nThe `BaseSearchClient` class is an abstract base class, designed to be inherited by other search client classes that implement specific search API integrations. It contains a single abstract method, `search`, which must be implemented by any subclass. The method signature specifies that `search` should be an asynchronous method that accepts a `query` string and potentially other keyword arguments (`kwargs`) to allow flexibility. The return type for `search` is expected to be any type, which provides a broad scope for returning various formats of search responses depending on the specific implementation in the subclasses.\n\nThis class does not implement any functionality itself; it serves as a contract for all subclasses that handle search functionality. Subclasses such as `BingClient`, `DuckDuckGoClient`, and `TavilyClient` inherit from `BaseSearchClient` and implement the `search` method to interact with specific search engines. These subclasses are responsible for providing the actual logic for sending search requests, handling responses, and structuring the search results.\n\nThe `BaseSearchClient` class is crucial in this context as it standardizes the interface for search functionality, allowing the rest of the system to interact with any specific search engine implementation in a uniform way, without needing to worry about the underlying details of the API being used. Any object or module in the system that relies on search results can use `BaseSearchClient` as the reference for interacting with any subclass that implements the `search` method.\n\n**Note**: \n- The `BaseSearchClient` class is not intended to be instantiated directly. It is meant to be subclassed, and the `search` method should be implemented in those subclasses to provide the actual search functionality.\n- This class enforces the asynchronous nature of search operations, ensuring that any subclass providing the `search` method must handle asynchronous behavior."
      ],
      "code_start_line": 5,
      "code_end_line": 8,
      "params": [],
      "have_return": false,
      "code_content": "class BaseSearchClient(ABC):\n    @abstractmethod\n    async def search(self, query: str, **kwargs: Any) -> Any:\n        pass\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/bing_client.py",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient",
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py",
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient",
        "src/criticsearch/tools/search_adapter/tavily_client.py",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [
        "**search**: The function of search is to asynchronously process a search query and return the result.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the search query to be executed.\n· kwargs: A variable number of additional keyword arguments, which may provide further customization or configuration for the search operation.\n\n**Code Description**: \nThe `search` function is defined as an asynchronous method, which implies that it will likely handle operations that may take time to complete, such as making network requests or querying a database. The function signature takes in two parameters: `query` and `kwargs`. \n- The `query` parameter is a required string that holds the search term or phrase the function will process.\n- The `kwargs` parameter allows for the inclusion of additional keyword arguments, which could be used for supplementary parameters such as pagination, filters, or sorting criteria in the search operation.\n\nHowever, the function body itself is currently not implemented, which means it lacks the specific logic to execute the search and return results. The `pass` statement indicates that this function is a placeholder or is intended to be overridden in a subclass or future implementation. The function is defined to return a result of type `Any`, indicating that the return type is flexible and could be adjusted based on the actual implementation.\n\n**Note**: \n- The `search` function is expected to be part of an asynchronous workflow, so it should be awaited when called in an asynchronous context.\n- The behavior and utility of the `kwargs` parameter are not defined here and would depend on the actual implementation of the method."
      ],
      "code_start_line": 7,
      "code_end_line": 8,
      "params": [
        "self",
        "query"
      ],
      "have_return": false,
      "code_content": "    async def search(self, query: str, **kwargs: Any) -> Any:\n        pass\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tools/search_adapter/search_aggregator.py": [
    {
      "type": "ClassDef",
      "name": "SearchAggregator",
      "md_content": [
        "**SearchAggregator**: The function of SearchAggregator is to aggregate and execute search queries across multiple search engines while handling various error conditions and API limitations.\n\n**attributes**:\n- clients: A dictionary holding instances of different search engine clients, such as TavilyClient and BingClient, which are used to execute search queries.\n- available_clients: A set that tracks the search engines that are currently available for use, excluding any that are marked as unavailable due to errors or limitations.\n\n**Code Description**:  \nThe `SearchAggregator` class is responsible for managing multiple search engines and executing queries across them. Upon initialization, it checks for API keys for both Tavily and Bing search engines from the settings. If valid keys are found, it initializes corresponding clients for these engines and stores them in the `clients` attribute. The `available_clients` set is then populated with the keys (names) of the initialized clients. \n\nThe class includes the following key methods:\n\n- `mark_engine_unavailable(engine: str)`: This method marks a specified search engine as unavailable by removing it from the `available_clients` set. This is used when an error or limitation (such as an invalid API key, retry limit reached, or usage limit exceeded) occurs with a particular search engine.\n  \n- `_search_single_query(query: str, engines: List[str])`: This asynchronous method attempts to search a given query across the provided list of search engines. For each engine, it checks if the engine is available and then tries to perform the search. If the search is successful, the result is returned. If an exception occurs, the engine is marked as unavailable, and the method moves on to the next engine in the list. Various exceptions are handled, including `RetryError`, `InvalidAPIKeyError`, and `UsageLimitExceededError`.\n\n- `search(query: List[str])`: This is the primary method that allows the user to perform searches using a list of queries. It first checks if there are any available search engines. If none are available, it raises an exception. It then creates asynchronous tasks for each query and executes them concurrently. The method returns the aggregated search results as a list of responses.\n\nThe `SearchAggregator` class is used in other components of the project to facilitate search functionality. For instance, in `BaseAgent`, it is instantiated and its search method is called to perform searches with specific queries. This enables the broader application to conduct searches in a robust and fault-tolerant manner by leveraging multiple search engines. Additionally, in the main entry point (`src/criticsearch/tools/search_adapter/__main__.py/main`), an asynchronous search is triggered, and the results are printed.\n\n**Note**:  \n- Ensure that valid API keys for the search engines (Tavily and Bing) are provided in the settings, as the functionality depends on these keys to initialize the respective clients.\n- The `search` method handles multiple queries simultaneously, making it efficient in handling concurrent searches.\n- If all search engines are marked unavailable, the method returns a `SearchResponse` indicating the failure to execute the search, along with an appropriate error message.\n  \n**Output Example**:\nA possible return value of the `search` method might look like this:\n\n```json\n{\n  \"responses\": [\n    {\n      \"query\": \"Who is Leo Messi?\",\n      \"results\": [\n        {\n          \"title\": \"Lionel Messi - Wikipedia\",\n          \"url\": \"https://en.wikipedia.org/wiki/Lionel_Messi\",\n          \"snippet\": \"Lionel Andrés Messi is an Argentine professional footballer widely regarded as one of the greatest players of all time.\"\n        }\n      ]\n    }\n  ]\n}\n```",
        "**SearchAggregator**: The function of SearchAggregator is to manage and execute search queries across multiple search engines.\n\n**attributes**: The attributes of this Class.\n· clients: A dictionary that holds instances of search engine clients, specifically TavilyClient and BingClient, keyed by their respective engine names.  \n· available_clients: A set that contains the names of currently available search engines that can be used for querying.\n\n**Code Description**: The SearchAggregator class is designed to facilitate searching through multiple search engines by managing their respective clients. Upon initialization, the class checks for the presence of API keys for Tavily and Bing. If the keys are available, it creates instances of TavilyClient and BingClient and stores them in the clients dictionary. The available_clients set is initialized to keep track of which search engines are currently available for use.\n\nThe class provides two main functionalities: marking a search engine as unavailable and performing searches. The `mark_engine_unavailable` method allows the user to remove a search engine from the available_clients set if it encounters issues during a search operation. This is particularly useful for handling errors such as invalid API keys or usage limits being exceeded.\n\nThe `_search_single_query` method is an asynchronous function that attempts to execute a search query against the specified engines. It iterates through the provided list of engines, checking if they are available. If an engine is available, it calls its search method and logs the result. If an error occurs, it logs the error and marks the engine as unavailable. If all specified engines fail, it returns a SearchResponse indicating that no available search engines could handle the query.\n\nThe `search` method allows users to perform searches using a list of queries. It gathers the available engines and creates tasks for concurrent execution of searches for each query. The results are awaited and returned as a SearchResponseList, which encapsulates the responses from the search engines.\n\nThe SearchAggregator class is utilized in the BaseAgent class, where an instance of SearchAggregator is created to manage search queries. This integration allows the BaseAgent to leverage the capabilities of multiple search engines, enhancing its ability to gather information. Additionally, the SearchAggregator is instantiated in the process_single_task function, where it is used to execute search queries based on user tasks.\n\n**Note**: When using the SearchAggregator, ensure that the necessary API keys for the search engines are configured correctly in the settings. Additionally, be aware that if all search engines become unavailable, the search operation will fail, and an appropriate error message will be returned.\n\n**Output Example**: A possible appearance of the code's return value after executing a search query could look like this:\n```json\n{\n  \"responses\": [\n    {\n      \"query\": \"Who is Leo Messi?\",\n      \"error_message\": \"Search failed: No available search engines for this query.\"\n    }\n  ]\n}\n```"
      ],
      "code_start_line": 14,
      "code_end_line": 97,
      "params": [],
      "have_return": true,
      "code_content": "class SearchAggregator:\n    def __init__(self):\n        self.clients: Dict[str, TavilyClient | BingClient] = {}\n\n        # 如果 Tavily 的 API key 存在，初始化客户端\n        tavily_api_key = settings.tavily.api_key\n        if tavily_api_key:\n            self.clients[\"tavily\"] = TavilyClient(tavily_api_key)\n\n        # 如果 Bing 的 API key 存在，初始化客户端\n        bing_api_key = settings.search_engine.bing.api_key\n        if bing_api_key:\n            self.clients[\"bing\"] = BingClient(bing_api_key)\n\n        self.available_clients = set(self.clients.keys())\n\n    def mark_engine_unavailable(self, engine: str):\n        \"\"\"\n        Mark a specific search engine as unavailable.\n\n        Args:\n            engine (str): The name of the engine to mark as unavailable.\n        \"\"\"\n        if engine in self.available_clients:\n            self.available_clients.remove(engine)\n\n    async def _search_single_query(\n        self, query: str, engines: List[str]\n    ) -> SearchResponse:\n        for engine in engines:\n            if engine in self.available_clients:\n                try:\n                    # Call the asynchronous search method for the specified engine\n                    result = await self.clients[engine].search(query)\n                    printer.log(f\"{result.model_dump()}\")\n                    return result\n                except RetryError:\n                    printer.log(\n                        f\"Engine '{engine}' for query: {query} failed after multiple retries. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except InvalidAPIKeyError:\n                    printer.log(\n                        f\"Engine '{engine}' for query: {query} failed because of wrong api key. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except UsageLimitExceededError:\n                    printer.log(\n                        f\"Engine '{engine}' for query: {query} failed because of usage limit exceeded. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except Exception:\n                    printer.print_exception(\n                        f\"Engine '{engine}' encountered error. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n\n        printer.log(\"All specified search engines are unavailable.\", style=\"bold red\")\n        return SearchResponse(\n            query=query,\n            error_message=\"Search failed: No available search engines for this query.\",\n        )\n\n    async def search(self, query: List[str]) -> str:\n        \"\"\"\n        Performs a search using the provided query.\n        Supports various search techniques using special syntax.\n\n        Args:\n            query (List[str]): A list of search queries.\n        \"\"\"\n        # Get the list of currently available search engines\n        engines = list(self.available_clients)\n        if not engines:\n            raise ValueError(\"No available engines to perform the search.\")\n\n        # Create tasks for concurrent search\n        tasks = [self._search_single_query(q, engines) for q in query]\n\n        # Execute all tasks and gather the responses\n        responses = await gather(*tasks)\n\n        # Return the search responses as a dictionary\n        return SearchResponseList(responses=responses).model_dump()  # type: ignore\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py",
        "src/criticsearch/base_agent.py/BaseAgent/__init__",
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/tools/__init__.py",
        "src/criticsearch/tools/search_adapter/__init__.py",
        "src/criticsearch/tools/search_adapter/__main__.py",
        "src/criticsearch/tools/search_adapter/__main__.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the SearchAggregator class, setting up the necessary search clients based on available API keys.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The __init__ method is the constructor for the SearchAggregator class. It initializes an empty dictionary named `clients`, which is intended to hold instances of search clients that can be used for executing search queries. The method first checks for the presence of an API key for the Tavily search service by accessing `settings.tavily.api_key`. If a valid API key is found, an instance of the TavilyClient is created and added to the `clients` dictionary with the key \"tavily\". \n\nSimilarly, the method checks for the Bing search service API key using `settings.search_engine.bing.api_key`. If this API key is available, an instance of the BingClient is created and added to the `clients` dictionary with the key \"bing\". \n\nAfter initializing the clients, the method creates a set called `available_clients`, which contains the keys of the clients that have been successfully instantiated. This allows the SearchAggregator to keep track of which search clients are available for use.\n\nThe SearchAggregator class is designed to manage multiple search clients, enabling it to perform searches across different services. By initializing the clients in the constructor, the class ensures that it can leverage the functionalities of both TavilyClient and BingClient, provided that the necessary API keys are available. This design promotes flexibility and extensibility, allowing for easy integration of additional search clients in the future.\n\n**Note**: It is important to ensure that the API keys for both Tavily and Bing are correctly configured in the settings. If the keys are missing or invalid, the corresponding clients will not be initialized, which may affect the functionality of the SearchAggregator. Proper error handling should be implemented when using the SearchAggregator to manage search requests."
      ],
      "code_start_line": 15,
      "code_end_line": 28,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.clients: Dict[str, TavilyClient | BingClient] = {}\n\n        # 如果 Tavily 的 API key 存在，初始化客户端\n        tavily_api_key = settings.tavily.api_key\n        if tavily_api_key:\n            self.clients[\"tavily\"] = TavilyClient(tavily_api_key)\n\n        # 如果 Bing 的 API key 存在，初始化客户端\n        bing_api_key = settings.search_engine.bing.api_key\n        if bing_api_key:\n            self.clients[\"bing\"] = BingClient(bing_api_key)\n\n        self.available_clients = set(self.clients.keys())\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "mark_engine_unavailable",
      "md_content": [
        "**mark_engine_unavailable**: The function of mark_engine_unavailable is to mark a specific search engine as unavailable.\n\n**parameters**:\n· engine: str - The name of the engine to mark as unavailable.\n\n**Code Description**:  \nThe `mark_engine_unavailable` method is part of a class and is responsible for updating the status of a specified search engine by marking it as unavailable. This is achieved by removing the engine's name from the `available_clients` collection. The parameter `engine` is expected to be a string, which represents the name of the search engine that needs to be marked as unavailable.\n\nIn terms of its usage, this function is typically called within error-handling blocks. Specifically, it is invoked whenever an exception is raised during a search operation, indicating that a particular engine is no longer functional for the query being processed. The method ensures that once an engine fails due to errors like retries, invalid API keys, or usage limits, it is effectively removed from the list of available search engines, preventing any further attempts to use that engine in future queries.\n\n**Note**:  \n- The `mark_engine_unavailable` function operates by directly modifying the `available_clients` list. Therefore, it is essential that `available_clients` is a mutable collection that supports the `remove` operation, like a list or set.\n- The engine's unavailability is handled automatically through exception handling in related functions (like `_search_single_query`), ensuring that engines that fail during the search process are excluded from subsequent attempts. \n- It is important to note that this function does not handle the re-inclusion of the engine once it has been marked as unavailable; this would need to be managed separately if required."
      ],
      "code_start_line": 30,
      "code_end_line": 38,
      "params": [
        "self",
        "engine"
      ],
      "have_return": false,
      "code_content": "    def mark_engine_unavailable(self, engine: str):\n        \"\"\"\n        Mark a specific search engine as unavailable.\n\n        Args:\n            engine (str): The name of the engine to mark as unavailable.\n        \"\"\"\n        if engine in self.available_clients:\n            self.available_clients.remove(engine)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_search_single_query",
      "md_content": [
        "**_search_single_query**: The function of _search_single_query is to perform an asynchronous search query using specified search engines and return the results encapsulated in a SearchResponse object.\n\n**parameters**: The parameters of this Function.\n· query: str - A string representing the user's search query. This parameter cannot be empty.\n· engines: List[str] - A list of search engine names that are available for performing the search.\n\n**Code Description**: The `_search_single_query` function is an asynchronous method designed to execute a search query against a list of specified search engines. It iterates through each engine in the provided `engines` list and checks if the engine is available in the `available_clients` collection. If the engine is available, it attempts to call the asynchronous `search` method of the corresponding client.\n\nThe function handles various exceptions that may arise during the search process:\n- If a `RetryError` occurs, it logs a warning indicating that the engine has failed after multiple retries and marks the engine as unavailable using the `mark_engine_unavailable` method.\n- If an `InvalidAPIKeyError` is raised, it logs an error message indicating that the search engine's API key is invalid and marks the engine as unavailable.\n- If a `UsageLimitExceededError` is encountered, it logs a warning about the usage limit being exceeded and marks the engine as unavailable.\n- For any other exceptions, it logs the exception details and marks the engine as unavailable.\n\nIf all specified search engines are unavailable, the function logs an error message and returns a `SearchResponse` object containing the original query and an error message indicating that no available search engines could fulfill the request.\n\nThis function is called by the `search` method of the `SearchAggregator` class. The `search` method gathers a list of currently available search engines and creates tasks for concurrent execution of `_search_single_query` for each query in the provided list. The results from these tasks are then collected and returned as a structured response.\n\n**Note**: It is essential to ensure that the engines passed to this function are valid and that the `available_clients` collection is properly maintained to reflect the current state of search engines. The function does not handle the re-inclusion of engines marked as unavailable; this must be managed separately if required.\n\n**Output Example**: A possible return value of the `_search_single_query` function could be a `SearchResponse` object structured as follows:\n```python\nSearchResponse(\n    query=\"latest technology news\",\n    results=[\n        SearchResult(title=\"Tech Innovations\", url=\"https://example.com/tech-innovations\", content=\"Explore the latest in technology.\"),\n        SearchResult(title=\"Gadget Reviews\", url=\"https://example.com/gadget-reviews\", content=\"Read reviews on the newest gadgets.\")\n    ],\n    error_message=None\n)\n```",
        "**_search_single_query**: The function of _search_single_query is to execute a search query against multiple search engines and return the results encapsulated in a SearchResponse object.\n\n**parameters**: The parameters of this Function.\n· query: str - A string representing the search query to be executed.  \n· engines: List[str] - A list of search engine names that are available for executing the query.\n\n**Code Description**: The _search_single_query function is an asynchronous method designed to perform a search operation using a specified query across multiple search engines. It iterates through the provided list of engines, checking if each engine is available for use. If an engine is found to be available, the function attempts to execute the search by calling the search method of the corresponding client for that engine.\n\nUpon calling the search method, the function awaits the result. If the search is successful, it logs the result using the printer.log method and returns the result encapsulated in a SearchResponse object. The logging provides visibility into the search results, which can be useful for debugging and monitoring purposes.\n\nIn the event of an error during the search operation, the function handles several specific exceptions:\n- RetryError: Indicates that the search engine failed after multiple retry attempts. The engine is marked as unavailable using the mark_engine_unavailable method.\n- InvalidAPIKeyError: Raised when the API key used for the search is invalid. The engine is also marked as unavailable in this case.\n- UsageLimitExceededError: This exception is raised when the search engine has exceeded its usage limits. The engine is marked as unavailable as well.\n- General Exception: Any other unexpected errors are caught, and the function logs the exception details using the printer.print_exception method, marking the engine as unavailable.\n\nIf all specified search engines are found to be unavailable after iterating through the list, the function logs a message indicating that no available search engines could be used for the query. It then returns a SearchResponse object containing the original query and an error message stating that the search failed due to the unavailability of search engines.\n\nThe _search_single_query function is called by the search method within the SearchAggregator class. The search method is responsible for executing multiple search queries concurrently by creating tasks for each query and utilizing the asyncio.gather function to run them. This design allows for efficient handling of multiple queries, leveraging the asynchronous capabilities of the underlying framework.\n\n**Note**: It is important to ensure that the engines parameter contains valid search engine names that are currently available. The function relies on proper exception handling to manage errors effectively, ensuring that any issues encountered during the search process are logged and handled gracefully.\n\n**Output Example**: A possible return value of the _search_single_query function could be structured as follows:\n```python\nSearchResponse(\n    query=\"latest technology news\",\n    results=[\n        SearchResult(title=\"Tech Innovations\", url=\"https://example.com/tech-innovations\", content=\"Explore the latest in technology.\"),\n        SearchResult(title=\"Gadget Reviews\", url=\"https://example.com/gadget-reviews\", content=\"Read reviews on the newest gadgets.\")\n    ],\n    error_message=None\n)\n``` \nThis output illustrates a successful search response containing the original query, a list of search results, and no error messages."
      ],
      "code_start_line": 40,
      "code_end_line": 75,
      "params": [
        "self",
        "query",
        "engines"
      ],
      "have_return": true,
      "code_content": "    async def _search_single_query(\n        self, query: str, engines: List[str]\n    ) -> SearchResponse:\n        for engine in engines:\n            if engine in self.available_clients:\n                try:\n                    # Call the asynchronous search method for the specified engine\n                    result = await self.clients[engine].search(query)\n                    printer.log(f\"{result.model_dump()}\")\n                    return result\n                except RetryError:\n                    printer.log(\n                        f\"Engine '{engine}' for query: {query} failed after multiple retries. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except InvalidAPIKeyError:\n                    printer.log(\n                        f\"Engine '{engine}' for query: {query} failed because of wrong api key. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except UsageLimitExceededError:\n                    printer.log(\n                        f\"Engine '{engine}' for query: {query} failed because of usage limit exceeded. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n                except Exception:\n                    printer.print_exception(\n                        f\"Engine '{engine}' encountered error. Marking as unavailable.\"\n                    )\n                    self.mark_engine_unavailable(engine)\n\n        printer.log(\"All specified search engines are unavailable.\", style=\"bold red\")\n        return SearchResponse(\n            query=query,\n            error_message=\"Search failed: No available search engines for this query.\",\n        )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/log",
        "src/criticsearch/rich_output.py/RichPrinter/print_exception",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/mark_engine_unavailable",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient/search",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponse",
        "src/criticsearch/tools/search_adapter/exceptions.py/UsageLimitExceededError",
        "src/criticsearch/tools/search_adapter/exceptions.py/InvalidAPIKeyError",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [
        "**search**: The function of search is to perform a search using the provided query and return the results as a serialized response.\n\n**parameters**: The parameters of this Function.\n· query: List[str] - A list of search queries that the user wants to search for.\n\n**Code Description**: The `search` method is an asynchronous function designed to execute multiple search queries concurrently using available search engines. It takes a list of search queries as input and returns a serialized string representation of the search results.\n\nThe method begins by retrieving the list of currently available search engines from the `available_clients` attribute. If there are no available engines, it raises a `ValueError`, indicating that the search cannot be performed. This ensures that the function does not attempt to execute a search when there are no resources to handle it.\n\nNext, the method creates a list of tasks, where each task corresponds to a single search query. This is done by calling the `_search_single_query` method for each query in the provided list, passing the list of available engines to it. The `_search_single_query` method is responsible for executing the search against each engine and returning the results encapsulated in a `SearchResponse` object.\n\nOnce the tasks are created, the method uses the `gather` function from the `asyncio` library to execute all the search tasks concurrently. This allows for efficient handling of multiple queries, as it does not block the execution while waiting for each individual search to complete.\n\nAfter all tasks are executed, the method collects the responses and constructs an instance of `SearchResponseList`, which is designed to hold and serialize the search results. The `model_dump()` method of `SearchResponseList` is called to generate a formatted string representation of the results, ensuring that any duplicate content across the responses is removed.\n\nThe `search` method is called by various components in the project, including the `search_and_browse` method in the `BaseAgent` class and the `main` function in the `__main__.py` module. In `search_and_browse`, it is invoked to handle user prompts for search queries, while in `main`, it demonstrates a simple use case of performing a search for a specific query.\n\n**Note**: It is important to ensure that the search engines are properly configured and available before calling this method. The method handles the serialization of results, ensuring that duplicates are filtered out, which enhances the relevance of the returned data.\n\n**Output Example**: A possible return value of the `search` function could be a serialized string representing the search results, structured as follows:\n```\n{\n  \"results\": [\n    {\n      \"title\": \"Introduction to Python\",\n      \"url\": \"https://example.com/python\",\n      \"content\": \"Python is a high-level programming language.\"\n    },\n    {\n      \"title\": \"Python Tutorials\",\n      \"url\": \"https://example.com/tutorials\",\n      \"content\": \"Learn Python programming with these tutorials.\"\n    }\n  ],\n  \"summary\": {\n    \"total_results\": 5,\n    \"unique_results\": 3,\n    \"duplicates_removed\": 2\n  }\n}\n``` \nThis output illustrates the results of the search queries, including a summary of the total results and the number of unique results returned.",
        "**search**: The function of search is to perform a search using the provided query, supporting various search techniques through special syntax.\n\n**parameters**: The parameters of this Function.\n· query: List[str] - A list of search queries to be executed.\n\n**Code Description**: The search function is an asynchronous method within the SearchAggregator class that facilitates searching across multiple search engines based on the provided list of queries. The function begins by retrieving the currently available search engines from the instance's available_clients attribute. If no engines are available, it raises a ValueError, indicating that the search cannot proceed.\n\nTo execute the search, the function creates a list of tasks, where each task corresponds to a single search query. These tasks are generated by calling the _search_single_query method, which is responsible for executing a search against the available engines. The search_single_query method is called for each query in the provided list, allowing for concurrent execution of searches.\n\nOnce the tasks are created, the function uses the asyncio.gather method to run all tasks concurrently and await their results. This approach enhances efficiency by allowing multiple searches to be processed simultaneously rather than sequentially.\n\nAfter gathering the responses from all search tasks, the function constructs a SearchResponseList object, which encapsulates the results of the searches. This object is then serialized and returned as a string representation, providing a structured output of the search results.\n\nThe search method is called by various components within the project, including the search_and_browse method in the BaseAgent class. This method utilizes the search function to perform searches based on user prompts and integrates the results into a broader workflow that may involve web scraping and content generation.\n\n**Note**: It is essential to ensure that the query parameter contains valid search terms and that the available_clients attribute is populated with active search engines. Proper error handling is implemented to manage cases where no engines are available, ensuring that the function fails gracefully.\n\n**Output Example**: A possible return value of the search function could be structured as follows:\n```json\n{\n  \"responses\": [\n    {\n      \"query\": \"latest technology news\",\n      \"results\": [\n        {\n          \"title\": \"Tech Innovations\",\n          \"url\": \"https://example.com/tech-innovations\",\n          \"content\": \"Explore the latest in technology.\"\n        },\n        {\n          \"title\": \"Gadget Reviews\",\n          \"url\": \"https://example.com/gadget-reviews\",\n          \"content\": \"Read reviews on the newest gadgets.\"\n        }\n      ],\n      \"error_message\": null\n    }\n  ]\n}\n``` \nThis output illustrates a successful search response containing the original query, a list of search results, and no error messages."
      ],
      "code_start_line": 77,
      "code_end_line": 97,
      "params": [
        "self",
        "query"
      ],
      "have_return": true,
      "code_content": "    async def search(self, query: List[str]) -> str:\n        \"\"\"\n        Performs a search using the provided query.\n        Supports various search techniques using special syntax.\n\n        Args:\n            query (List[str]): A list of search queries.\n        \"\"\"\n        # Get the list of currently available search engines\n        engines = list(self.available_clients)\n        if not engines:\n            raise ValueError(\"No available engines to perform the search.\")\n\n        # Create tasks for concurrent search\n        tasks = [self._search_single_query(q, engines) for q in query]\n\n        # Execute all tasks and gather the responses\n        responses = await gather(*tasks)\n\n        # Return the search responses as a dictionary\n        return SearchResponseList(responses=responses).model_dump()  # type: ignore\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/base_agent.py/BaseAgent/__init__",
        "src/criticsearch/base_agent.py/BaseAgent/search_and_browse",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/tools/search_adapter/__main__.py/main"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponseList"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "src/criticsearch/tools/search_adapter/bing_client.py": [
    {
      "type": "ClassDef",
      "name": "BingClient",
      "md_content": [
        "**BingClient**: The function of BingClient is to serve as a client for the Bing Search API, facilitating search queries and handling responses.\n\n**attributes**: The attributes of this Class.\n· base_url: str - This attribute stores the base URL for the Bing Search API endpoint.\n· _api_key: str - This attribute holds the API key required for authenticating requests to the Bing Search API.\n· _client_name: str - This attribute defines the name of the client, which is \"BingClient\".\n\n**Code Description**: The `BingClient` class is an implementation of the `BaseSearchClient`, specifically designed to interact with the Bing Search API. It initializes with an API key, which is essential for authenticating requests to the Bing service. The class defines a method `search`, which is an asynchronous function that takes a search query as input and returns a structured response containing search results.\n\nUpon instantiation, the `BingClient` sets the `base_url` to the Bing Search API endpoint and stores the provided API key. The `search` method is decorated with a retry mechanism that allows it to attempt the request up to five times in case of a `RatelimitException`, which indicates that the rate limit for API requests has been exceeded. The retry logic employs exponential backoff combined with random jitter to manage the timing of retries effectively.\n\nIn the `search` method, an HTTP GET request is made to the Bing API using the `httpx.AsyncClient`. The request includes headers for authentication and parameters that specify the search query and additional options such as the number of results to return, safe search settings, and response filters. The method processes the API response, extracting relevant information from the JSON payload, and constructs a list of `SearchResult` objects that encapsulate the title, URL, and snippet of each search result.\n\nThe `BingClient` is utilized within the `SearchAggregator` class, which manages multiple search clients. When the `SearchAggregator` is initialized, it checks for the presence of the Bing API key in the settings. If the key is available, it creates an instance of `BingClient` and adds it to its collection of clients. This design allows the `SearchAggregator` to leverage the Bing search functionality alongside other search clients, providing a unified interface for executing search queries across different services.\n\n**Note**: It is crucial to handle exceptions such as `RatelimitException` and `InvalidAPIKeyError` when using the `BingClient` to ensure robust error management and user experience. The `search` method should be called asynchronously to comply with its design.\n\nA possible appearance of the code's return value from the `search` method could look like this:\n```json\n{\n    \"query\": \"fishing\",\n    \"results\": [\n        {\n            \"title\": \"Fishing Tips and Techniques\",\n            \"url\": \"https://www.example.com/fishing-tips\",\n            \"content\": \"Learn the best fishing tips and techniques for a successful day on the water.\"\n        },\n        {\n            \"title\": \"Top Fishing Spots\",\n            \"url\": \"https://www.example.com/top-fishing-spots\",\n            \"content\": \"Discover the top fishing spots in your area for a great fishing experience.\"\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 16,
      "code_end_line": 92,
      "params": [],
      "have_return": true,
      "code_content": "class BingClient(BaseSearchClient):\n    \"\"\"\n    Bing Search API client.\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        self.base_url = \"https://api.bing.microsoft.com/v7.0/search\"\n        self._api_key = api_key\n\n        self._client_name = \"BingClient\"\n\n    @retry(\n        stop=stop_after_attempt(5),  # 重试最多5次\n        wait=wait_exponential(multiplier=1, min=4, max=30)\n        + wait_random(min=1, max=5),  # 指数退避 + 随机抖动\n        retry=retry_if_exception_type(RatelimitException),\n    )\n    async def search(\n        self,\n        query: str,\n    ) -> SearchResponse:\n        headers = {\"Ocp-Apim-Subscription-Key\": self._api_key}\n\n        params = {\n            \"q\": query,  # 用户的搜索查询词。不能为空。\n            # 查询词可以包含 Bing 高级操作符，例如使用 site: 操作符限定结果来源于特定域名。\n            # 示例：q=\"fishing+site:fishing.contoso.com\"。\n            # 注意：即使使用了 site: 操作符，结果可能仍会包含其他站点的内容，具体取决于相关结果的数量。\n            \"count\": 2,  # 返回的搜索结果数量。默认值为 10，最大值为 50。\n            # 可以与 offset 参数结合使用来分页结果。\n            # 示例：如果每页显示 10 个搜索结果，第一页设置 count=10 和 offset=0，\n            # 第二页设置 offset=10，以此类推。分页时可能存在部分结果重叠。\n            \"safeSearch\": \"strict\",  # 过滤成人内容的设置。\n            # 可选值：\n            # - \"Off\": 返回包含成人文本和图片但不包括成人视频的内容。\n            # - \"Moderate\": 返回包含成人文本但不包括成人图片或视频的内容。\n            # - \"Strict\": 不返回任何成人文本、图片或视频内容。\n            \"responseFilter\": \"Webpages\",  # 用逗号分隔的答案类型，指定要在响应中包含的内容。\n            # 如果未指定此参数，则响应包含所有相关的数据类型。\n            # 可选值包括：\n            # - Computation, Entities, Images, News, Places, RelatedSearches,\n            #   SpellSuggestions, TimeZone, Translations, Videos, Webpages。\n            # 示例：使用 &responseFilter=-images 排除图片结果。\n            # 注意：若想获得单一答案类型，应优先使用特定的 API 端点。\n        }\n\n        async with httpx.AsyncClient(timeout=10) as client:\n            response = await client.get(self.base_url, headers=headers, params=params)\n\n        if response.status_code == 200:\n            json_response = response.json()\n\n            # 解析 Bing 的响应\n            web_pages = json_response.get(\"webPages\", {})\n            items = web_pages.get(\"value\", [])\n\n            results = []\n            for item in items:\n                results.append(\n                    SearchResult(\n                        title=item.get(\"name\", \"\"),\n                        url=item.get(\"url\", \"\"),\n                        content=item.get(\"snippet\", \"\"),\n                    )\n                )\n\n            return SearchResponse(query=query, results=results)\n        # 处理 HTTP 状态码\n        if response.status_code == 429:\n            raise RatelimitException()\n        elif response.status_code == 401:\n            raise InvalidAPIKeyError()\n        else:\n            return SearchResponse(\n                query=query,\n                error_message=f\"Unexpected status code: {response.status_code}. Response: {response.text}\",\n            )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/__init__"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/base_search_client.py/BaseSearchClient",
        "src/criticsearch/tools/search_adapter/exceptions.py/RatelimitException"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the BingClient object with necessary configuration values.\n\n**parameters**: The parameters of this Function.\n· api_key: A string representing the API key required to authenticate the client with Bing's search API.\n\n**Code Description**:  \nThe `__init__` method is a constructor function for the `BingClient` class. It is called when an instance of the class is created. This method performs the following actions:\n\n1. It assigns a fixed string URL (`\"https://api.bing.microsoft.com/v7.0/search\"`) to the `base_url` attribute, which is the endpoint for Bing's search API. This URL will be used in API requests to interact with Bing's search service.\n  \n2. It stores the provided `api_key` parameter in a private attribute `_api_key`. This API key is essential for authenticating requests to Bing's API. The key is passed to the class upon instantiation, and it is stored for later use in making authorized API calls.\n\n3. It initializes the `self._client_name` attribute with the string value `\"BingClient\"`. This can be used to identify the client or for logging purposes to track requests made by this client.\n\nThe constructor ensures that an instance of `BingClient` is ready for making authenticated API requests to the Bing search service.\n\n**Note**: \n- The `api_key` parameter must be correctly provided when creating an instance of the `BingClient`, as it is required for API authentication.\n- The `base_url` is fixed and does not change during the lifetime of the object, which makes it reusable for all API requests initiated from the same instance."
      ],
      "code_start_line": 21,
      "code_end_line": 25,
      "params": [
        "self",
        "api_key"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, api_key: str):\n        self.base_url = \"https://api.bing.microsoft.com/v7.0/search\"\n        self._api_key = api_key\n\n        self._client_name = \"BingClient\"\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [
        "**search**: The function of search is to perform an asynchronous search query using the Bing search API and return the results encapsulated in a SearchResponse object.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the user's search query. This parameter cannot be empty.\n\n**Code Description**: The `search` function is an asynchronous method that interacts with the Bing search API to retrieve search results based on a specified query. It begins by setting the necessary headers, including the API key required for authentication. The function constructs a parameters dictionary that includes the search query, the number of results to return, safe search settings, and response filters.\n\nThe function utilizes the `httpx.AsyncClient` to send an asynchronous GET request to the Bing API endpoint. The response is awaited, and upon receiving a successful status code (200), the function processes the JSON response. It extracts the relevant search results from the response, specifically focusing on the \"webPages\" section. Each result is then instantiated as a `SearchResult` object, which includes the title, URL, and content snippet of the search result.\n\nIf the response indicates a rate limit error (status code 429), the function raises a `RatelimitException`. If the API key is invalid (status code 401), it raises an `InvalidAPIKeyError`. For any other unexpected status codes, the function returns a `SearchResponse` object that includes the query and an error message detailing the unexpected status.\n\nThis function is called by the `_search_single_query` method in the `SearchAggregator` class. The `_search_single_query` method iterates through a list of available search engines and attempts to execute the `search` function for each engine. If the search is successful, it logs the result and returns it. In case of exceptions such as `RetryError`, `InvalidAPIKeyError`, or `UsageLimitExceededError`, the method handles these by marking the engine as unavailable and logging the appropriate error messages.\n\n**Note**: It is essential to ensure that the query parameter is not empty when calling this function, as it will lead to an error. Additionally, proper handling of the exceptions raised by this function is crucial for maintaining the robustness of the application, especially in scenarios where API limits or invalid keys may affect search operations.\n\n**Output Example**: A possible return value of the `search` function could be a `SearchResponse` object structured as follows:\n```python\nSearchResponse(\n    query=\"fishing\",\n    results=[\n        SearchResult(title=\"Fishing Tips\", url=\"https://example.com/fishing-tips\", content=\"Learn the best tips for fishing.\"),\n        SearchResult(title=\"Fishing Gear\", url=\"https://example.com/fishing-gear\", content=\"Find the best gear for your fishing adventures.\")\n    ],\n    error_message=None\n)\n```"
      ],
      "code_start_line": 33,
      "code_end_line": 92,
      "params": [
        "self",
        "query"
      ],
      "have_return": true,
      "code_content": "    async def search(\n        self,\n        query: str,\n    ) -> SearchResponse:\n        headers = {\"Ocp-Apim-Subscription-Key\": self._api_key}\n\n        params = {\n            \"q\": query,  # 用户的搜索查询词。不能为空。\n            # 查询词可以包含 Bing 高级操作符，例如使用 site: 操作符限定结果来源于特定域名。\n            # 示例：q=\"fishing+site:fishing.contoso.com\"。\n            # 注意：即使使用了 site: 操作符，结果可能仍会包含其他站点的内容，具体取决于相关结果的数量。\n            \"count\": 2,  # 返回的搜索结果数量。默认值为 10，最大值为 50。\n            # 可以与 offset 参数结合使用来分页结果。\n            # 示例：如果每页显示 10 个搜索结果，第一页设置 count=10 和 offset=0，\n            # 第二页设置 offset=10，以此类推。分页时可能存在部分结果重叠。\n            \"safeSearch\": \"strict\",  # 过滤成人内容的设置。\n            # 可选值：\n            # - \"Off\": 返回包含成人文本和图片但不包括成人视频的内容。\n            # - \"Moderate\": 返回包含成人文本但不包括成人图片或视频的内容。\n            # - \"Strict\": 不返回任何成人文本、图片或视频内容。\n            \"responseFilter\": \"Webpages\",  # 用逗号分隔的答案类型，指定要在响应中包含的内容。\n            # 如果未指定此参数，则响应包含所有相关的数据类型。\n            # 可选值包括：\n            # - Computation, Entities, Images, News, Places, RelatedSearches,\n            #   SpellSuggestions, TimeZone, Translations, Videos, Webpages。\n            # 示例：使用 &responseFilter=-images 排除图片结果。\n            # 注意：若想获得单一答案类型，应优先使用特定的 API 端点。\n        }\n\n        async with httpx.AsyncClient(timeout=10) as client:\n            response = await client.get(self.base_url, headers=headers, params=params)\n\n        if response.status_code == 200:\n            json_response = response.json()\n\n            # 解析 Bing 的响应\n            web_pages = json_response.get(\"webPages\", {})\n            items = web_pages.get(\"value\", [])\n\n            results = []\n            for item in items:\n                results.append(\n                    SearchResult(\n                        title=item.get(\"name\", \"\"),\n                        url=item.get(\"url\", \"\"),\n                        content=item.get(\"snippet\", \"\"),\n                    )\n                )\n\n            return SearchResponse(query=query, results=results)\n        # 处理 HTTP 状态码\n        if response.status_code == 429:\n            raise RatelimitException()\n        elif response.status_code == 401:\n            raise InvalidAPIKeyError()\n        else:\n            return SearchResponse(\n                query=query,\n                error_message=f\"Unexpected status code: {response.status_code}. Response: {response.text}\",\n            )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/models.py/SearchResult",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponse",
        "src/criticsearch/tools/search_adapter/exceptions.py/InvalidAPIKeyError",
        "src/criticsearch/tools/search_adapter/exceptions.py/RatelimitException"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/tools/search_adapter/models.py": [
    {
      "type": "ClassDef",
      "name": "SearchResult",
      "md_content": [
        "**SearchResult**: The function of SearchResult is to represent an individual search result, encapsulating details such as the title, URL, and content.\n\n**attributes**: The attributes of this Class.\n· title: A string representing the title of the search result.  \n· url: A string representing the URL of the search result.  \n· content: A string containing a brief description or snippet of the content of the search result.  \n\n**Code Description**:  \nThe `SearchResult` class is a data model that stores information about a single search result retrieved from a search engine API. It inherits from `BaseModel`, suggesting that it is likely used with Pydantic or a similar library for data validation and serialization. \n\n- `title`: This attribute holds the title of the search result. It is represented as a string and typically corresponds to the main heading or name of the webpage or resource returned in the search results.\n  \n- `url`: This attribute contains the URL string that directs to the search result. It is essential for linking directly to the content described in the `title`.\n\n- `content`: This is a string attribute that contains a brief snippet or preview of the content found at the given URL. This snippet is often a portion of text extracted from the webpage to give users a preview of what the page is about.\n\nThe `SearchResult` class is frequently used in the context of search operations, where it is populated with relevant data for each individual search result. This can be seen in the `search` methods of both `BingClient` and `DuckDuckGoClient`. In these classes, after performing a search query, a list of `SearchResult` instances is created, each representing an individual result in the response from the respective search engine API.\n\nIn the `BingClient.search` method, for example, the response from the Bing API is parsed, and each item in the list of results is used to instantiate a `SearchResult` object with the corresponding title, URL, and content snippet. The resulting list of `SearchResult` instances is then included in a `SearchResponse` object, which encapsulates the entire search result for further use.\n\nSimilarly, in the `DuckDuckGoClient.search` method, the search results are processed, and a list of `SearchResult` objects is created, each holding the title, URL, and content for the results returned by DuckDuckGo.\n\n**Note**:  \n- The attributes `title`, `url`, and `content` are expected to be provided when creating an instance of `SearchResult`. Missing or malformed values may lead to errors, especially in systems that rely on proper data validation.\n- The `SearchResult` class is often used as part of a larger response object, such as `SearchResponse`, which groups together multiple `SearchResult` instances along with metadata like the search query and potential error messages.\n- It is important that each `SearchResult` contains relevant and accurate data, as it directly reflects the results returned by external search APIs, which may vary in structure."
      ],
      "code_start_line": 9,
      "code_end_line": 12,
      "params": [],
      "have_return": false,
      "code_content": "class SearchResult(BaseModel):\n    title: str\n    url: str\n    content: str\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/bing_client.py",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient/search",
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py",
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient/search",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponse"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "SearchResponse",
      "md_content": [
        "## Class: `SearchResponse`\n\n### Overview\nThe `SearchResponse` class is a data model used to represent the results of a search query. It encapsulates the query itself, the list of search results, and any potential error messages that may have occurred during the search process. This class is commonly used to structure the response from a search API client, such as a Bing or DuckDuckGo search client.\n\n### Attributes\n- **query** (`str`): The search query string that was submitted by the user.\n- **results** (`List[SearchResult]`): A list of `SearchResult` objects representing the individual search results. If no results are found, this list will be empty.\n- **error_message** (`Optional[str]`): An optional string that contains an error message, if applicable. If an error occurred during the search, this attribute will contain a descriptive error message.\n\n### Method\n\n#### `ser_model() -> str`\nThe `ser_model` method is used to serialize the `SearchResponse` object into a human-readable string. It formats the response based on the availability of results and error messages.\n\n- **Returns**: A formatted string representing the search response, including the query, any error message, and the details of the search results.\n  \n  **Behavior**:\n  - If an `error_message` is provided, the method will include it in the formatted response.\n  - If no results are found (i.e., `results` is an empty list), the method will indicate that no results were found.\n  - If results are available, the method will format the response with the titles, URLs, and content of the search results.\n\n### Usage Example\n```python\nsearch_response = SearchResponse(query=\"Python programming\", results=[...], error_message=None)\nprint(search_response.ser_model())\n```\n\n### Notes\n- The `SearchResult` class is used to structure each individual search result, containing attributes like `title`, `url`, and `content`. These details are displayed when the `ser_model` method is called, allowing for easy inspection of the search results.\n- The `error_message` is optional, and if there are no errors, the response will display the search results or indicate that no results were found.\n\nThis class is useful in the context of search client responses, enabling structured representation and easy presentation of the query and search outcomes.",
        "**SearchResponse**: The function of SearchResponse is to encapsulate the results of a search query, including the query string, a list of search results, and any error messages encountered during the search process.\n\n**attributes**: The attributes of this Class.\n· query: A string representing the user's search query.  \n· results: A list of `SearchResult` objects that represent the individual search results returned from the query.  \n· error_message: An optional string that contains an error message if an error occurred during the search.\n\n**Code Description**: The `SearchResponse` class is a data model that serves to structure the response from search queries. It inherits from `BaseModel`, indicating that it is likely part of a data validation and serialization framework, such as Pydantic. \n\nThe primary attributes of the `SearchResponse` class include:\n- `query`: This attribute holds the search query string that was submitted by the user. It is essential for understanding the context of the results returned.\n- `results`: This attribute is a list of `SearchResult` instances, each representing an individual result from the search query. The `SearchResult` class encapsulates details such as the title, URL, and content snippet of each search result.\n- `error_message`: This optional attribute is used to convey any error messages that may arise during the search process. If no errors occur, this attribute will be `None`.\n\nThe `SearchResponse` class includes a method named `ser_model`, which is responsible for serializing the response into a formatted string. This method constructs a human-readable representation of the search results. It checks for the presence of an error message and formats the output accordingly. If there are no results, it indicates that no results were found. If results are present, it enumerates through each `SearchResult`, appending its details to the formatted string.\n\nThe `SearchResponse` class is utilized by various search client implementations, such as `BingClient` and `DuckDuckGoClient`. In these implementations, after executing a search query, the results are collected and instantiated as `SearchResult` objects. These objects are then aggregated into a `SearchResponse` object, which is returned to the caller. This structured response allows for consistent handling of search results across different search engines.\n\nFor example, in the `BingClient.search` method, after successfully retrieving search results from the Bing API, a list of `SearchResult` instances is created. This list, along with the original query and any potential error messages, is used to construct a `SearchResponse` object that is returned to the user.\n\n**Note**: \n- The `query` attribute must be a valid string representing the user's search input. \n- The `results` attribute will contain a list of `SearchResult` objects, which must be populated with valid data to ensure meaningful output.\n- The `error_message` attribute is optional and should be used to provide feedback in case of errors during the search process.\n- Proper handling of the `SearchResponse` object is crucial for applications that rely on search functionality, as it encapsulates both successful results and error states.\n\n**Output Example**: A possible return value of the `SearchResponse` object could be structured as follows:\n```python\nSearchResponse(\n    query=\"latest technology news\",\n    results=[\n        SearchResult(title=\"Tech Innovations\", url=\"https://example.com/tech-innovations\", content=\"Explore the latest in technology.\"),\n        SearchResult(title=\"Gadget Reviews\", url=\"https://example.com/gadget-reviews\", content=\"Read reviews on the newest gadgets.\")\n    ],\n    error_message=None\n)\n``` \n\nThis output illustrates a successful search response containing the original query, a list of search results, and no error messages."
      ],
      "code_start_line": 15,
      "code_end_line": 37,
      "params": [],
      "have_return": true,
      "code_content": "class SearchResponse(BaseModel):\n    query: str\n    results: List[SearchResult] = Field(default_factory=list)\n    error_message: Optional[str] = None\n\n    @model_serializer\n    def ser_model(self) -> str:\n        if self.error_message:\n            formatted_response = (\n                f\"Query: {self.query}\\nError: {self.error_message}\\n\" + \"-\" * 50\n            )\n        elif self.results == []:\n            formatted_response = (\n                f\"Query: {self.query}\\nError: No results found.\" + \"-\" * 50\n            )\n        else:\n            formatted_response = f\"Query: {self.query}\\nSearch Results:\\n\" + \"-\" * 50\n            for i, res in enumerate(self.results, 1):\n                formatted_response += (\n                    f\"\\n[{i}]:\\nTITLE: {res.title}\\nURL: {res.url}\\nCONTENT: {res.content}\\n\"\n                    + \"-\" * 50\n                )\n        return formatted_response\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/bing_client.py",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient/search",
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py",
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient/search",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponseList",
        "src/criticsearch/tools/search_adapter/search_aggregator.py",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "src/criticsearch/tools/search_adapter/tavily_client.py",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/models.py/SearchResult"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "ser_model",
      "md_content": [
        "**ser_model**: The function of ser_model is to generate a formatted string representation of the search response based on the query, error messages, and results.\n\n**parameters**: The parameters of this Function.\n· self: An instance of the class that contains the attributes query, error_message, and results.\n\n**Code Description**: The ser_model function constructs a formatted response string that summarizes the outcome of a search operation. It first checks if there is an error message present in the instance. If an error message exists, it formats the response to include the query and the error message, followed by a line of dashes for separation. If there are no results (i.e., the results list is empty), it similarly formats the response to indicate that no results were found. In the case where there are valid search results, the function constructs a response that includes the query and iterates through the results list. For each result, it appends the index, title, URL, and content of the result to the formatted response, again followed by a line of dashes for clarity. Finally, the function returns the complete formatted response string.\n\n**Note**: It is important to ensure that the attributes query, error_message, and results are properly initialized in the class before calling this function. The results should be a list of objects that contain title, url, and content attributes for the function to work correctly.\n\n**Output Example**: \n```\nQuery: \"How to use Python for data analysis?\"\nError: No results found.--------------------------------------------------\n```\nor \n```\nQuery: \"How to use Python for data analysis?\"\nSearch Results:\n--------------------------------------------------\n[1]:\nTITLE: \"Data Analysis with Python\"\nURL: \"https://example.com/data-analysis-python\"\nCONTENT: \"This article provides an overview of data analysis techniques using Python.\"\n--------------------------------------------------\n[2]:\nTITLE: \"Python for Data Science\"\nURL: \"https://example.com/python-data-science\"\nCONTENT: \"Learn how Python is used in data science and analytics.\"\n--------------------------------------------------\n```",
        "**ser_model**: The function of ser_model is to generate a formatted string representation of the search response, including the query, any errors, and the results.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**:  \nThe `ser_model` function generates a formatted string that presents the search results or error messages associated with a search query. It checks for the presence of an error message or results in the following order:\n\n1. If there is an error message, the function creates a formatted response containing the query and the error message, followed by a separator line.\n2. If the `results` list is empty, it generates a formatted response indicating that no results were found, including the query and an appropriate error message, followed by a separator line.\n3. If there are search results, the function formats the response with the query and a header indicating search results. It then iterates over the results, adding details such as the title, URL, and content of each result, followed by a separator line after each entry.\n\nThe final formatted string is returned to the caller. This function is useful for presenting a search summary in a readable format, including any potential errors or results from a search operation.\n\n**Note**:  \n- This function does not take any input parameters, as it operates on the attributes of the object it is a part of. \n- The function relies on the `query`, `error_message`, and `results` attributes of the object to generate the formatted response. It assumes that `results` is a list of objects that have `title`, `url`, and `content` attributes.\n\n**Output Example**:  \nHere’s an example of the returned string based on different scenarios:\n\n1. **Error with a message:**\n\n```\nQuery: example query\nError: Something went wrong.\n--------------------------------------------------\n```\n\n2. **No results found:**\n\n```\nQuery: example query\nError: No results found.\n--------------------------------------------------\n```\n\n3. **With search results:**\n\n```\nQuery: example query\nSearch Results:\n--------------------------------------------------\n[1]:\nTITLE: Example Title 1\nURL: http://example.com/1\nCONTENT: Example content for result 1.\n--------------------------------------------------\n[2]:\nTITLE: Example Title 2\nURL: http://example.com/2\nCONTENT: Example content for result 2.\n--------------------------------------------------\n```"
      ],
      "code_start_line": 21,
      "code_end_line": 37,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def ser_model(self) -> str:\n        if self.error_message:\n            formatted_response = (\n                f\"Query: {self.query}\\nError: {self.error_message}\\n\" + \"-\" * 50\n            )\n        elif self.results == []:\n            formatted_response = (\n                f\"Query: {self.query}\\nError: No results found.\" + \"-\" * 50\n            )\n        else:\n            formatted_response = f\"Query: {self.query}\\nSearch Results:\\n\" + \"-\" * 50\n            for i, res in enumerate(self.results, 1):\n                formatted_response += (\n                    f\"\\n[{i}]:\\nTITLE: {res.title}\\nURL: {res.url}\\nCONTENT: {res.content}\\n\"\n                    + \"-\" * 50\n                )\n        return formatted_response\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "SearchResponseList",
      "md_content": [
        "**SearchResponseList**: The function of SearchResponseList is to represent a list of search response objects and provide functionality to serialize them, ensuring that duplicate content across queries is removed.\n\n**attributes**: The attributes of this Class.\n· responses: A list of `SearchResponse` objects that represent the responses from multiple search queries.\n\n**Code Description**: \nThe `SearchResponseList` class is used to hold a collection of `SearchResponse` objects, each representing an individual search query's response. It provides a method, `ser_model()`, which serializes these responses into a formatted string while ensuring the removal of duplicate content across the search results.\n\n- The class inherits from `BaseModel`, indicating that it is part of a data model system, likely designed for serialization or structured data handling.\n- The primary attribute, `responses`, is a list that holds instances of `SearchResponse`. This attribute holds the individual responses from search queries and is initialized as an empty list by default.\n  \nThe key functionality of this class is in the `ser_model()` method. This method iterates over the `SearchResponse` objects in the `responses` list and performs several actions:\n1. It maintains a set of `global_seen_contents` to ensure that duplicate search results are removed across queries.\n2. For each `SearchResponse`, if there is an error message (i.e., `error_message` is not `None`), the method will log the error and skip processing that response.\n3. For each result in a valid `SearchResponse`, it checks if the content of the result has been seen before across all responses. If the content is unique, it is added to a list of unique results, and this result is serialized.\n4. After processing, the method updates the `SearchResponse` object to reflect only unique results and keeps track of the total number of results and the unique results count. It then generates a string representation of the serialized results.\n5. The method logs the number of duplicates removed and returns the final serialized string.\n\nThe `SearchResponseList` class is primarily used in the context of aggregating and serializing multiple search responses in a way that filters out redundant results. It plays a crucial role in handling multiple search queries, especially when working with multiple search engines or sources. \n\nFrom a functional perspective, this class is invoked in the `search` method of the `SearchAggregator` class. The `search` method performs concurrent searches for a list of queries, collects the responses, and returns them as an instance of `SearchResponseList`. This allows for streamlined processing and serialization of the search results, ensuring that the final output only includes unique search results from the queries.\n\n**Note**: \n- The `ser_model()` method only includes `SearchResponse` objects that do not contain error messages in its serialization.\n- It ensures that any duplicate content across multiple responses is filtered out based on the content of the search results, which helps in returning cleaner and more relevant data.\n- This class uses logging to provide feedback on the serialization process, including information on skipped responses due to errors and the number of duplicates removed.\n\n**Output Example**:\nAn example output of the `ser_model()` method might look like this:\n\n```\nQuery: \"Python programming\"\nSearch Results:\n--------------------------------------------------\n[1]:\nTITLE: Introduction to Python\nURL: https://example.com/python\nCONTENT: Python is a high-level programming language.\n--------------------------------------------------\n[2]:\nTITLE: Python Tutorials\nURL: https://example.com/tutorials\nCONTENT: Learn Python programming with these tutorials.\n--------------------------------------------------\nSerialization completed. Total results: 5, Unique results: 3, Duplicates removed: 2.\n``` \n\nThis output shows the results of the search query, including the serialized format of unique results, and a summary of the serialization process.",
        "### Class: `SearchResponseList`\n\n#### Overview:\nThe `SearchResponseList` class is designed to represent a collection of `SearchResponse` objects. It provides functionality to serialize the list of search responses, ensuring unique content across all search queries by removing duplicates.\n\n#### Attributes:\n- **responses** (`List[SearchResponse]`): A list containing `SearchResponse` objects. Each `SearchResponse` encapsulates the results of a single search query, including the query string, a list of search results, and any error messages encountered during the search process. The list is initialized with an empty list by default.\n\n#### Methods:\n- **ser_model()** -> `str`:\n  The `ser_model` method serializes the list of `SearchResponse` objects into a string format. This method ensures that any duplicate content across the search results is removed. The serialization process proceeds as follows:\n  1. The method iterates over each `SearchResponse` in the `responses` list.\n  2. If the `SearchResponse` contains an error message, it logs the error and skips serialization for that particular response.\n  3. For each valid `SearchResponse`, the method checks for unique search results by comparing the `content` of each result to a global set of seen contents.\n  4. The results are filtered to retain only unique entries, and the count of unique results is updated.\n  5. After filtering, the method appends the serialized content of the `SearchResponse` to the result string.\n  6. A summary log is generated, indicating the total number of results, the number of unique results, and the number of duplicates removed.\n  \n  The method returns a formatted string containing the serialized results of the search responses.\n\n#### Example Usage:\n```python\nsearch_response_list = SearchResponseList(responses=[response1, response2])\nserialized_results = search_response_list.ser_model()\n```\n\n#### Notes:\n- The `responses` attribute must be populated with instances of the `SearchResponse` class.\n- The method `ser_model` performs de-duplication across all responses and presents the results in a human-readable format.\n- If any `SearchResponse` contains an error message, it will be skipped during serialization, ensuring that the final output only contains valid data."
      ],
      "code_start_line": 40,
      "code_end_line": 86,
      "params": [],
      "have_return": true,
      "code_content": "class SearchResponseList(BaseModel):\n    responses: List[SearchResponse] = Field(default_factory=list)\n\n    @model_serializer\n    def ser_model(self) -> str:\n        \"\"\"\n        Serialize the list of SearchResponse objects into a dictionary,\n        ensuring unique content across queries.\n\n        Returns:\n            Dict[str, str]: A dictionary where the key is the query,\n                            and the value is a formatted string representation of the search response.\n        \"\"\"\n        global_seen_contents = set()  # 全局去重逻辑\n        total_results = 0\n        unique_results_count = 0\n        result_str = \"\"\n\n        for response in self.responses:\n            if response.error_message:\n                printer.log(\n                    f\"Skipping serialize query '{response.query}' due to error: {response.error_message}\"\n                )\n                continue  # 跳过有 error_message 的响应\n\n            unique_results = []\n            for res in response.results:\n                total_results += 1\n                if res.content not in global_seen_contents:\n                    global_seen_contents.add(res.content)\n                    unique_results.append(res)\n\n            # 将去重后的结果更新到当前 response\n            response.results = unique_results\n            unique_results_count += len(unique_results)\n            result_str += response.model_dump()  # type: ignore\n\n        # 打印提示信息\n        duplicates_removed = total_results - unique_results_count\n        printer.log(\n            f\"Serialization completed. Total results: {total_results}, \"\n            f\"Unique results: {unique_results_count}, \"\n            f\"Duplicates removed: {duplicates_removed}.\",\n            style=\"bold green\",\n        )\n\n        return result_str\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/models.py/SearchResponse"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "ser_model",
      "md_content": [
        "**ser_model**: The function of ser_model is to serialize a list of SearchResponse objects into a string, ensuring uniqueness in the content across different queries.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**:  \nThe `ser_model` method is responsible for serializing the list of `SearchResponse` objects contained within the instance. It processes each `SearchResponse` object to ensure that the search results are unique by comparing their content against a global set of previously seen content.\n\n1. **Global Uniqueness Tracking**:  \n   A set named `global_seen_contents` is used to store the content of the results encountered so far, ensuring that each result is unique across all queries processed by this method.\n\n2. **Response Processing**:  \n   The method loops over each `SearchResponse` object in `self.responses`. For each `response`, if it contains an error message, it is skipped, and the serialization continues with the next response.\n\n3. **Result Deduplication**:  \n   For each `response`, the method iterates through its `results` and checks whether the `content` of each result has already been encountered (using the `global_seen_contents` set). If a result’s content is unique (i.e., not found in the set), it is added to the list `unique_results` and the content is added to the set to prevent future duplicates. The total number of results and unique results are tracked during this process.\n\n4. **Updating the Response**:  \n   After deduplication, the `results` attribute of the `response` is updated with the `unique_results`. The count of unique results is accumulated in `unique_results_count`.\n\n5. **Serialization**:  \n   The `model_dump()` method is called on each `response` to generate its serialized string representation, which is appended to `result_str`. This `result_str` will contain the serialized data of all responses, with duplicates removed.\n\n6. **Logging**:  \n   After processing all responses, the method logs a summary message, indicating the total number of results processed, the number of unique results, and the number of duplicates removed.\n\n7. **Return Value**:  \n   Finally, the method returns the `result_str`, which contains the serialized data of the unique results.\n\n**Note**: \n- The method ensures that only unique search results are included in the serialized output, removing any duplicates based on content.\n- The method relies on the `model_dump()` method of the `SearchResponse` object to generate its string representation, which may vary depending on the implementation of that method.\n- If any `SearchResponse` contains an error message, it will be skipped entirely, and no results from that response will be included in the final serialized output.\n\n**Output Example**:  \nAssuming `self.responses` contains two `SearchResponse` objects with some duplicate results, the returned `result_str` might look like this:\n\n```\n\"SearchResponse(query='query1', results=[{'content': 'unique content 1'}, {'content': 'unique content 2'}])SearchResponse(query='query2', results=[{'content': 'unique content 3'}, {'content': 'unique content 1'}])\"\n```\n\nIn this example, 'unique content 1' is only included once, even though it appeared in multiple responses.",
        "**ser_model**: The function of ser_model is to serialize a list of SearchResponse objects into a formatted string, ensuring unique content across different queries.\n\n**parameters**: The parameters of this Function.\n- None\n\n**Code Description**:  \nThe `ser_model` method is designed to process a list of `SearchResponse` objects and serialize the relevant data into a string representation. The method ensures that content within the search responses is unique across all queries by removing duplicate results based on their content.\n\n1. **Global Deduplication Logic**:  \n   The function initializes a set called `global_seen_contents` to store content that has already been encountered. This set helps track which results have been seen across all queries, ensuring that duplicates are eliminated.\n\n2. **Processing Search Responses**:  \n   The method then iterates over each `response` in the `self.responses` list. If a response contains an `error_message`, it is skipped, and a log message is generated using the `printer.log` method to notify that the response was ignored due to an error. This ensures that only valid responses are processed.\n\n3. **Result Deduplication**:  \n   For each valid response, the method further iterates over the `results` in the response. It checks whether the `content` of each result has already been encountered by comparing it against the `global_seen_contents` set. If a result's content is unique, it is added to the list `unique_results`, and its content is stored in the set to prevent future duplicates. The method keeps track of both the total number of results and the number of unique results.\n\n4. **Updating the Response**:  \n   Once duplicates are removed, the `results` of the current `response` are updated to include only the unique results. The count of unique results is then updated.\n\n5. **Result Serialization**:  \n   After processing all responses, the `model_dump` method is called on each `response` to obtain a serialized string representation of the response, which is concatenated into the `result_str` variable.\n\n6. **Logging the Summary**:  \n   After completing the serialization process, a log message is printed using the `printer.log` method to summarize the operation. The message includes the total number of results, the number of unique results, and the number of duplicates removed.\n\n7. **Return Value**:  \n   The method returns the concatenated string (`result_str`) representing all serialized and deduplicated search responses.\n\n**Note**:  \n- The `model_dump` method is assumed to serialize each response object into a string format, and the exact format depends on its implementation.\n- The `printer.log` function is used for logging, which formats the messages in a styled manner to enhance readability in the console output.\n- It is important to ensure that all responses in `self.responses` are well-formed and contain the expected attributes, especially `error_message` and `results`. This method does not handle any unexpected data formats or missing attributes.\n\n**Output Example**:  \nAssuming a scenario where there are two search responses with some duplicate content, the output might look like:\n\n```\nSerialization completed. Total results: 10, Unique results: 7, Duplicates removed: 3.\n<Serialized String of the Responses>\n```\n\nThe string returned would contain the serialized representations of the unique search responses, with any duplicates removed based on the content comparison."
      ],
      "code_start_line": 44,
      "code_end_line": 86,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def ser_model(self) -> str:\n        \"\"\"\n        Serialize the list of SearchResponse objects into a dictionary,\n        ensuring unique content across queries.\n\n        Returns:\n            Dict[str, str]: A dictionary where the key is the query,\n                            and the value is a formatted string representation of the search response.\n        \"\"\"\n        global_seen_contents = set()  # 全局去重逻辑\n        total_results = 0\n        unique_results_count = 0\n        result_str = \"\"\n\n        for response in self.responses:\n            if response.error_message:\n                printer.log(\n                    f\"Skipping serialize query '{response.query}' due to error: {response.error_message}\"\n                )\n                continue  # 跳过有 error_message 的响应\n\n            unique_results = []\n            for res in response.results:\n                total_results += 1\n                if res.content not in global_seen_contents:\n                    global_seen_contents.add(res.content)\n                    unique_results.append(res)\n\n            # 将去重后的结果更新到当前 response\n            response.results = unique_results\n            unique_results_count += len(unique_results)\n            result_str += response.model_dump()  # type: ignore\n\n        # 打印提示信息\n        duplicates_removed = total_results - unique_results_count\n        printer.log(\n            f\"Serialization completed. Total results: {total_results}, \"\n            f\"Unique results: {unique_results_count}, \"\n            f\"Duplicates removed: {duplicates_removed}.\",\n            style=\"bold green\",\n        )\n\n        return result_str\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/log"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "src/criticsearch/tools/search_adapter/exceptions.py": [
    {
      "type": "ClassDef",
      "name": "SearchClientError",
      "md_content": [
        "**SearchClientError**: The function of SearchClientError is to define a base exception class for errors related to the search client.\n\n**attributes**:\n· message: str - This attribute stores the error message that is associated with the exception.\n\n**Code Description**:  \nThe `SearchClientError` class is a custom exception that extends Python's built-in `Exception` class. It is used as a base class for specific error types related to the search client functionality. The class constructor (`__init__`) accepts a message parameter, which defaults to \"An error occurred in the search client.\" This message is passed to the parent `Exception` class using `super().__init__(message)`, enabling the `SearchClientError` instance to store and propagate the message to whoever handles the exception.\n\nThe class is intended to be subclassed by other specific error classes that need to indicate particular search client-related errors, such as exceeded usage limits, bad requests, or invalid API keys. In the project, we can see that multiple exceptions inherit from `SearchClientError`, such as `UsageLimitExceededError`, `BadRequestError`, `InvalidAPIKeyError`, `RatelimitException`, and `TimeoutException`. Each of these subclasses customizes the default error message to reflect a more specific error condition, but they all share the same base behavior provided by `SearchClientError`. \n\nThe subclasses inherit the core functionality of the `SearchClientError` class, while modifying the error message for clarity and relevance to the particular situation. These subclasses ensure that the search client exceptions are appropriately handled in different cases, providing clear and specific error messages to the developers working with the search client.\n\n**Note**:  \n- The `SearchClientError` class serves as a foundational class for defining more granular exception types.\n- It is important that any custom error related to the search client extend from `SearchClientError` to maintain consistency across the project.\n- The default message can be overridden when raising exceptions, but the base class guarantees that a message is always available, which helps in debugging and troubleshooting."
      ],
      "code_start_line": 4,
      "code_end_line": 6,
      "params": [],
      "have_return": false,
      "code_content": "class SearchClientError(Exception):\n    def __init__(self, message: str = \"An error occurred in the search client.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/exceptions.py/UsageLimitExceededError",
        "src/criticsearch/tools/search_adapter/exceptions.py/BadRequestError",
        "src/criticsearch/tools/search_adapter/exceptions.py/InvalidAPIKeyError",
        "src/criticsearch/tools/search_adapter/exceptions.py/RatelimitException",
        "src/criticsearch/tools/search_adapter/exceptions.py/TimeoutException"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the SearchClientError class with a specified error message.\n\n**parameters**: The parameters of this Function.\n· message: A string that represents the error message to be displayed. It defaults to \"An error occurred in the search client.\"\n\n**Code Description**: This __init__ function is a constructor for the SearchClientError class, which is likely a custom exception class used to handle errors related to a search client in the application. The function takes one optional parameter, `message`, which allows the user to specify a custom error message. If no message is provided when an instance of the class is created, the default message \"An error occurred in the search client.\" will be used. The constructor calls the `__init__` method of its superclass (presumably a built-in exception class) using `super().__init__(message)`, which initializes the base class with the provided message. This ensures that the error message is properly set up for the exception handling mechanism in Python.\n\n**Note**: It is important to provide a meaningful error message when raising this exception to facilitate debugging and error tracking. Users of this class should be aware that the default message may not always convey the specific issue encountered, so customizing the message is recommended when applicable."
      ],
      "code_start_line": 5,
      "code_end_line": 6,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"An error occurred in the search client.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "UsageLimitExceededError",
      "md_content": [
        "**UsageLimitExceededError**: The function of UsageLimitExceededError is to indicate that the usage limit for the search client has been exceeded.\n\n**attributes**:\n· message: str - This attribute stores the error message that is associated with the exception.\n\n**Code Description**: The `UsageLimitExceededError` class is a custom exception that inherits from the `SearchClientError` base class. It is specifically designed to handle scenarios where the usage limit of the search client has been surpassed. The constructor of the `UsageLimitExceededError` class accepts a single parameter, `message`, which defaults to \"Usage limit exceeded.\" This message provides a clear indication of the error condition when the exception is raised.\n\nWhen an instance of `UsageLimitExceededError` is created, it calls the constructor of its parent class, `SearchClientError`, using `super().__init__(message)`. This ensures that the error message is properly initialized and can be accessed by any exception handling mechanisms that catch this specific error.\n\nIn the context of the project, the `UsageLimitExceededError` is utilized within the `search` method of the `TavilyClient` class. When a search request is made and the server responds with a status code of 429, indicating that the usage limit has been exceeded, the `UsageLimitExceededError` is raised. This allows the error to propagate up the call stack, where it can be handled appropriately, such as logging the error and marking the search engine as unavailable.\n\nAdditionally, the `UsageLimitExceededError` is caught in the `_search_single_query` method of the `SearchAggregator` class. When this exception is encountered, it logs a warning message indicating that the engine has failed due to the usage limit being exceeded and subsequently marks the engine as unavailable. This structured approach to error handling ensures that the application can gracefully manage situations where the search client cannot fulfill requests due to usage constraints.\n\n**Note**: \n- The `UsageLimitExceededError` class is intended to be used specifically for signaling that the search client's usage limit has been reached.\n- It is essential to handle this exception in the calling code to maintain robust error management and provide meaningful feedback to users or developers regarding the state of the search client."
      ],
      "code_start_line": 9,
      "code_end_line": 11,
      "params": [],
      "have_return": false,
      "code_content": "class UsageLimitExceededError(SearchClientError):\n    def __init__(self, message: str = \"Usage limit exceeded.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "src/criticsearch/tools/search_adapter/tavily_client.py",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the UsageLimitExceededError class with a specific error message.\n\n**parameters**: The parameters of this Function.\n· message: A string that represents the error message to be displayed when the exception is raised. It defaults to \"Usage limit exceeded.\"\n\n**Code Description**: The __init__ method is a constructor for the UsageLimitExceededError class, which is a custom exception that inherits from the built-in Exception class. When an instance of UsageLimitExceededError is created, this method is called to set up the instance. The method takes one optional parameter, message, which allows the user to specify a custom error message. If no message is provided, it defaults to \"Usage limit exceeded.\" The constructor then calls the constructor of the parent class (Exception) using super().__init__(message), which initializes the base class with the provided message. This ensures that the error message is properly stored and can be accessed when the exception is raised.\n\n**Note**: It is important to provide a meaningful message when raising this exception to ensure that the context of the error is clear to the user. If the default message is used, it may not provide sufficient information about the specific situation that caused the exception."
      ],
      "code_start_line": 10,
      "code_end_line": 11,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Usage limit exceeded.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "BadRequestError",
      "md_content": [
        "**BadRequestError**: The function of BadRequestError is to represent an exception that occurs when a bad request is made to the search client.\n\n**attributes**:\n· message: str - This attribute stores the error message that is associated with the exception. The default message is \"Bad request.\"\n\n**Code Description**:  \nThe `BadRequestError` class is a custom exception that extends the `SearchClientError` base exception class. This class is designed to handle errors that arise specifically from a bad request made to the search client. It inherits the core functionality of `SearchClientError`, which means it can propagate an error message and be caught by any exception handling mechanism designed to manage `SearchClientError` exceptions.\n\nThe constructor of the `BadRequestError` class accepts a message parameter, which has a default value of \"Bad request.\" If no message is provided, this default message will be used when raising the exception. The message is passed to the parent class `SearchClientError` using `super().__init__(message)`, ensuring that the error message is properly stored and can be accessed by the exception handler.\n\nAs a subclass of `SearchClientError`, the `BadRequestError` inherits the attributes and behaviors of its parent, but it customizes the default message to indicate that the error specifically pertains to a bad request. This helps in distinguishing it from other types of errors in the search client, such as those related to invalid API keys, rate limits, or usage limits.\n\nIn terms of functionality, the `BadRequestError` is useful for handling scenarios where the client makes a request that the search service cannot process due to an issue with the request itself (such as invalid parameters or incorrect syntax). The error message provides clarity about the nature of the issue, assisting developers in debugging and resolving the problem.\n\n**Note**:  \n- The `BadRequestError` class is intended to be raised when a bad request is made to the search client, providing a clear and specific error message.\n- This class inherits from `SearchClientError`, so it benefits from the structure and behavior defined in the base class.\n- The default message for this exception can be overridden when raising the exception, but the base class ensures that an appropriate message is always available.\n"
      ],
      "code_start_line": 14,
      "code_end_line": 16,
      "params": [],
      "have_return": false,
      "code_content": "class BadRequestError(SearchClientError):\n    def __init__(self, message: str = \"Bad request.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the BadRequestError class with a specified error message.\n\n**parameters**: The parameters of this Function.\n· message: A string that represents the error message to be associated with the BadRequestError instance. It defaults to \"Bad request.\" if no message is provided.\n\n**Code Description**: The __init__ function is a constructor for the BadRequestError class, which is likely a custom exception class used to indicate that a client has made a bad request to a server or an API. This function takes one optional parameter, message, which allows the user to specify a custom error message. If the user does not provide a message, the default value \"Bad request.\" is used. The function then calls the constructor of its superclass (presumably Exception or a subclass thereof) using super().__init__(message), which initializes the base class with the provided message. This ensures that the error message is properly set up for the exception handling mechanism in Python.\n\n**Note**: It is important to provide meaningful error messages when raising exceptions to facilitate debugging and improve the user experience. Users of this class should be aware that the message parameter is optional, but providing a specific message can help clarify the nature of the error encountered."
      ],
      "code_start_line": 15,
      "code_end_line": 16,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Bad request.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "InvalidAPIKeyError",
      "md_content": [
        "**InvalidAPIKeyError**: The function of InvalidAPIKeyError is to define a specific exception that signals an invalid API key error.\n\n**attributes**:\n· message: str - This attribute stores the error message associated with the exception.\n\n**Code Description**:  \nThe `InvalidAPIKeyError` class is a custom exception that extends from the `SearchClientError` base exception class. It is specifically designed to handle errors related to invalid API keys when interacting with a search engine client. This class inherits the functionality of the `SearchClientError` class, ensuring that it shares the same message handling mechanism, but it customizes the error message to indicate that the error is due to an invalid API key.\n\nThe constructor of the `InvalidAPIKeyError` class accepts an optional `message` parameter that defaults to \"Invalid API key.\" If no specific message is provided when the exception is raised, this default message is used. The constructor calls the parent class's (`SearchClientError`) constructor using `super().__init__(message)`, allowing the error message to be passed to the parent class, which ensures consistent exception handling and message propagation throughout the system.\n\nThis class is primarily used within the project to signal when an API key used for making requests to a search engine is invalid. It is invoked in the `search` methods of different client classes like `BingClient` and `TavilyClient`. For example, in the case of a 401 HTTP status code (Unauthorized), the `InvalidAPIKeyError` is raised to indicate that the API key is incorrect. Once raised, it can be caught and handled appropriately by the surrounding code, such as marking the search engine as unavailable, as seen in the `SearchAggregator` class.\n\nIn the project, this exception serves as a clear and specific indicator of issues related to invalid API keys, providing a more precise understanding of the problem than more generic error messages.\n\n**Note**:  \n- The `InvalidAPIKeyError` class is a subclass of `SearchClientError`, ensuring it follows the same error-handling structure as other search client exceptions.\n- This exception should be used whenever an invalid API key is encountered in the system, providing consistency in error reporting and debugging.\n- The default message can be customized by providing a different `message` value when raising the exception, but the base message is helpful for general use cases."
      ],
      "code_start_line": 19,
      "code_end_line": 21,
      "params": [],
      "have_return": false,
      "code_content": "class InvalidAPIKeyError(SearchClientError):\n    def __init__(self, message: str = \"Invalid API key.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/bing_client.py",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient/search",
        "src/criticsearch/tools/search_adapter/search_aggregator.py",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query",
        "src/criticsearch/tools/search_adapter/tavily_client.py",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the InvalidAPIKeyError class, setting a custom error message.\n\n**parameters**:\n· message: str (default value: \"Invalid API key.\") - A string message that is used to describe the error.\n\n**Code Description**:  \nThe __init__ method in this class is responsible for initializing an instance of the InvalidAPIKeyError exception. It inherits from the built-in Exception class, and its purpose is to provide a custom error message when an invalid API key is encountered. \n\nWhen an instance of InvalidAPIKeyError is created, the method first checks if a custom message has been provided by the caller. If no message is given, it defaults to \"Invalid API key.\" This message is then passed to the parent class' constructor using `super().__init__(message)`, which ensures that the Exception class is properly initialized with the message string. This allows the InvalidAPIKeyError to carry the custom or default error message when raised.\n\n**Note**:  \n- The message parameter is optional. If not provided, the default message \"Invalid API key.\" will be used.\n- The super() function is used to call the parent class constructor (Exception) to ensure that the error message is properly handled by the base Exception class."
      ],
      "code_start_line": 20,
      "code_end_line": 21,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Invalid API key.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "RatelimitException",
      "md_content": [
        "**RatelimitException**: The function of RatelimitException is to indicate that the rate limit for API requests has been exceeded.\n\n**attributes**:\n· message: str - This attribute stores the error message that is associated with the exception.\n\n**Code Description**: The `RatelimitException` class is a custom exception that inherits from the `SearchClientError` base class. It is specifically designed to handle scenarios where the rate limit for API requests has been exceeded. The constructor of `RatelimitException` accepts a message parameter, which defaults to \"Rate limit exceeded.\" This message is passed to the parent class `SearchClientError` using `super().__init__(message)`, ensuring that the exception carries a relevant error message when raised.\n\nIn the context of the project, `RatelimitException` is utilized within the `BingClient` and `TavilyClient` classes, which are responsible for interacting with their respective search APIs. When a request to these APIs results in a 429 HTTP status code, which signifies that the rate limit has been reached, the `RatelimitException` is raised. This allows the application to handle the situation appropriately, such as by implementing retry logic or notifying the user of the issue.\n\nThe `RatelimitException` serves as a clear indication to developers that the application has encountered a rate limiting issue, allowing for better error handling and user experience. It is part of a broader hierarchy of exceptions that extend from `SearchClientError`, which includes other specific error types like `UsageLimitExceededError` and `InvalidAPIKeyError`. Each of these exceptions provides a way to manage different error conditions that may arise while interacting with search client functionalities.\n\n**Note**: \n- It is essential to handle `RatelimitException` in the application logic to ensure that the user is informed of the rate limit issue and that appropriate measures are taken, such as retrying the request after a delay.\n- The default message can be overridden if a more specific message is required when raising the exception, but the base functionality ensures that a message is always available for debugging purposes."
      ],
      "code_start_line": 24,
      "code_end_line": 26,
      "params": [],
      "have_return": false,
      "code_content": "class RatelimitException(SearchClientError):\n    def __init__(self, message: str = \"Rate limit exceeded.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/bing_client.py",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient",
        "src/criticsearch/tools/search_adapter/bing_client.py/BingClient/search",
        "src/criticsearch/tools/search_adapter/tavily_client.py",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient",
        "src/criticsearch/tools/search_adapter/tavily_client.py/TavilyClient/search"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize a RatelimitException instance with a specific error message.\n\n**parameters**: The parameters of this Function.\n· message: A string that represents the error message to be displayed when the exception is raised. It defaults to \"Rate limit exceeded.\"\n\n**Code Description**: The __init__ method is a constructor for the RatelimitException class, which is likely a custom exception used to indicate that a rate limit has been exceeded in an application. When an instance of RatelimitException is created, it calls the constructor of its superclass (presumably Exception) using the super() function, passing the message parameter to it. This ensures that the base exception class is properly initialized with the provided message. If no message is specified when the exception is raised, it defaults to \"Rate limit exceeded,\" providing a clear indication of the error condition.\n\n**Note**: It is important to provide a meaningful message when raising this exception to ensure that the context of the error is clear to the developers or users handling the exception."
      ],
      "code_start_line": 25,
      "code_end_line": 26,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Rate limit exceeded.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "TimeoutException",
      "md_content": [
        "**TimeoutException**: The function of TimeoutException is to represent an error that occurs when a timeout condition is encountered in the search client operations.\n\n**attributes**: The attributes of this Class.\n· message: str - This attribute stores the error message that is associated with the exception, defaulting to \"Timeout occurred.\"\n\n**Code Description**: The `TimeoutException` class is a custom exception that extends the `SearchClientError` class. It is specifically designed to handle timeout errors that may arise during the execution of search client operations. When an instance of `TimeoutException` is created, it invokes the constructor of its parent class, `SearchClientError`, passing a default message that indicates a timeout has occurred. This allows the exception to carry a meaningful message that can be used for debugging and error handling.\n\nThe `TimeoutException` class inherits all the properties and methods of the `SearchClientError` class, which serves as a base for various search client-related exceptions. By extending `SearchClientError`, `TimeoutException` ensures that it maintains a consistent structure and behavior with other exceptions in the search client domain, such as `UsageLimitExceededError`, `BadRequestError`, and others. This design promotes a clear hierarchy of exceptions, making it easier for developers to handle specific error cases effectively.\n\nIn practical terms, when a timeout occurs during a search operation, the `TimeoutException` can be raised to signal this specific issue. Developers can catch this exception in their code to implement appropriate error handling strategies, such as retrying the operation, logging the error, or notifying users of the timeout condition.\n\n**Note**: \n- It is essential to use `TimeoutException` in scenarios where a timeout is a relevant error condition, ensuring that the error handling is precise and informative.\n- The default message can be customized when raising the exception, but it is advisable to maintain clarity regarding the nature of the timeout error for effective debugging and user communication."
      ],
      "code_start_line": 29,
      "code_end_line": 31,
      "params": [],
      "have_return": false,
      "code_content": "class TimeoutException(SearchClientError):\n    def __init__(self, message: str = \"Timeout occurred.\"):\n        super().__init__(message)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/exceptions.py/SearchClientError"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the TimeoutException class with a specific error message.\n\n**parameters**: The parameters of this Function.\n· message: A string that represents the error message to be displayed when the exception is raised. It defaults to \"Timeout occurred.\" if no message is provided.\n\n**Code Description**: The __init__ function is a constructor for the TimeoutException class, which is likely a custom exception used to indicate that a timeout has occurred in a process or operation. This function takes one optional parameter, `message`, which allows the user to specify a custom error message. If the user does not provide a message, the default message \"Timeout occurred.\" will be used. The constructor calls the superclass's __init__ method using `super().__init__(message)`, which ensures that the base class (likely Exception or a subclass thereof) is properly initialized with the provided message. This allows the TimeoutException to inherit all the properties and methods of the base exception class, enabling it to be used effectively in exception handling.\n\n**Note**: It is important to provide a meaningful message when raising this exception to ensure that the context of the timeout is clear to the developers or users handling the exception."
      ],
      "code_start_line": 30,
      "code_end_line": 31,
      "params": [
        "self",
        "message"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, message: str = \"Timeout occurred.\"):\n        super().__init__(message)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/tools/search_adapter/__main__.py": [
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "## Function Documentation: `main`\n\n### Overview:\nThe `main` function serves as the entry point for executing a search query using the `SearchAggregator` class. It is an asynchronous function that initializes the search aggregator, performs a search, and prints the results.\n\n### Function Signature:\n```python\nasync def main()\n```\n\n### Description:\nThe `main` function is responsible for initiating the search process by utilizing the `SearchAggregator` class to handle search queries. It performs the following steps:\n\n1. **Initialization**: The function first creates an instance of the `SearchAggregator` class, which is used to manage and execute searches across multiple available search engines.\n\n2. **Search Execution**: It calls the `search` method of the `SearchAggregator` instance, passing a list of queries. In this case, the query is a single string: `\"Who is Leo Messi?\"`.\n\n3. **Results Handling**: After executing the search, the function awaits the response from the `search` method. The results are then printed to the console.\n\n### Purpose:\nThe function provides an example of how to interact with the `SearchAggregator` class to perform searches. It demonstrates the process of query submission and result handling within an asynchronous context.\n\n### Parameters:\nThis function does not accept any parameters.\n\n### Execution Flow:\n1. An instance of the `SearchAggregator` is created.\n2. The `search` method of the `SearchAggregator` is called with a predefined query.\n3. The search results are printed to the console.\n\n### Example Output:\n```json\n{\n  \"responses\": [\n    {\n      \"query\": \"Who is Leo Messi?\",\n      \"results\": [\n        {\n          \"title\": \"Lionel Messi - Wikipedia\",\n          \"url\": \"https://en.wikipedia.org/wiki/Lionel_Messi\",\n          \"snippet\": \"Lionel Andrés Messi is an Argentine professional footballer widely regarded as one of the greatest players of all time.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Notes:\n- The function demonstrates basic usage of the `SearchAggregator` class, performing an asynchronous search and printing the response.\n- The search query used in this example is predefined, but in a real-world scenario, queries could be dynamic, and multiple queries could be passed to the `search` method concurrently.\n"
      ],
      "code_start_line": 6,
      "code_end_line": 12,
      "params": [],
      "have_return": false,
      "code_content": "    async def main():\n        search_aggregator = SearchAggregator()\n\n        # 调用异步搜索方法\n        response = await search_aggregator.search(query=[\"Who is Leo Messi?\"])\n\n        print(response)\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/search"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "src/criticsearch/tools/search_adapter/duckduckgo_client.py": [
    {
      "type": "ClassDef",
      "name": "DuckDuckGoClient",
      "md_content": [
        "**DuckDuckGoClient**: The function of DuckDuckGoClient is to implement a search client that interacts with the DuckDuckGo search engine, providing search results based on specified query parameters.\n\n**attributes**: \n- None.\n\n**Code Description**: \nThe `DuckDuckGoClient` class is a subclass of the `BaseSearchClient` class. It is specifically designed to perform search operations via DuckDuckGo's search engine. This class implements the asynchronous `search` method, which retrieves search results by sending a query to DuckDuckGo's API, processes the results, and returns a structured response.\n\nThe class contains the following key components:\n\n1. **_convert_days_to_timelimit**: This helper method converts a specified number of days into DuckDuckGo's internal time limit format. It accepts an integer `days` as input and returns a string corresponding to one of the following time periods:\n   - `\"d\"` for the last 24 hours\n   - `\"w\"` for the last week\n   - `\"m\"` for the last month\n   - `\"y\"` for the last year\n   The method is essential for formatting the time-related parameter when calling the DuckDuckGo search API.\n\n2. **search**: This asynchronous method is responsible for sending a search query to DuckDuckGo's API. It accepts the following parameters:\n   - `query`: A string representing the search query.\n   - `days`: An optional integer that filters results to a specific time frame. It defaults to 7 (last week).\n   - `max_results`: An optional integer specifying the maximum number of results to return. It defaults to 10.\n   - `region`: A string that defines the search region. It can be `\"us-en\"` for the United States or `\"cn-zh\"` for China, with a default value of `\"us-en\"`.\n   \n   The method utilizes the `_convert_days_to_timelimit` helper to determine the time frame for the search, constructs a request to DuckDuckGo, and processes the response into a list of `SearchResult` objects. Each `SearchResult` contains the title, URL, and content of a search result. Finally, the method returns a `SearchResponse` object that contains the query and the processed search results, limited to the specified `max_results`.\n\n3. **retry decorator**: The `search` method is wrapped with a retry decorator that handles retries in case of failures. The retry logic is configured to stop after 5 attempts (`stop_after_attempt(5)`) and apply an exponential backoff strategy with random jitter (`wait_exponential` and `wait_random`). The decorator specifically targets the `RatelimitException`, which is likely raised when the DuckDuckGo API is being rate-limited.\n\nIn terms of functionality, the `DuckDuckGoClient` integrates with the `AsyncDDGS` (asynchronous DuckDuckGo search) client, which is responsible for sending the query and receiving raw search results. These results are then transformed into a structured format suitable for further use in the system. \n\n**Note**: \n- The `search` method is asynchronous, so it must be awaited when called.\n- The method supports time-based filtering, allowing users to retrieve results from the last 24 hours, week, month, or year.\n- The retry logic ensures robustness in case of rate limiting or temporary failures in the API interaction.\n\n**Output Example**:\nA possible return value from the `search` method might look like this:\n\n```json\n{\n  \"query\": \"example search query\",\n  \"results\": [\n    {\n      \"title\": \"Example Result 1\",\n      \"url\": \"https://www.example1.com\",\n      \"content\": \"This is the content of the first search result.\"\n    },\n    {\n      \"title\": \"Example Result 2\",\n      \"url\": \"https://www.example2.com\",\n      \"content\": \"This is the content of the second search result.\"\n    }\n  ]\n}\n```"
      ],
      "code_start_line": 18,
      "code_end_line": 74,
      "params": [],
      "have_return": true,
      "code_content": "class DuckDuckGoClient(BaseSearchClient):\n    def _convert_days_to_timelimit(self, days: int) -> str:\n        \"\"\"\n        Convert days to DuckDuckGo's timelimit format.\n\n        Args:\n            days (int): Number of days to filter results.\n\n        Returns:\n            str: A string representing DuckDuckGo's timelimit format ('d', 'w', 'm', 'y').\n        \"\"\"\n        if days <= 1:\n            return \"d\"  # Last 24 hours\n        elif days <= 7:\n            return \"w\"  # Last week\n        elif days <= 30:\n            return \"m\"  # Last month\n        else:\n            return \"y\"  # Last year\n\n    @retry(\n        stop=stop_after_attempt(5),  # 重试最多5次\n        wait=wait_exponential(multiplier=1, min=4, max=30)\n        + wait_random(min=1, max=5),  # 指数退避 + 随机抖动\n        retry=retry_if_exception_type(RatelimitException),\n    )\n    async def search(\n        self,\n        query: str,\n        days: int = 7,\n        max_results: int = 10,\n        region: Literal[\"us-en\", \"cn-zh\"] = \"us-en\",\n    ) -> SearchResponse:\n        timelimit = self._convert_days_to_timelimit(days)\n\n        logger.debug(f\"Using 'duckduckgo-search-client' for query '{query}'.\")\n\n        raw_results = await AsyncDDGS(timeout=10).atext(\n            query,\n            region=region,\n            safesearch=\"on\",\n            timelimit=timelimit,\n            max_results=max_results,\n        )\n\n        results = [\n            SearchResult(\n                title=result[\"title\"],\n                url=result[\"href\"],\n                content=result[\"body\"],\n            )\n            for result in raw_results\n        ]\n        return SearchResponse(\n            query=query,\n            results=results[:max_results],\n        )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/base_search_client.py/BaseSearchClient"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_convert_days_to_timelimit",
      "md_content": [
        "**_convert_days_to_timelimit**: The function of _convert_days_to_timelimit is to convert a given number of days into a timelimit format used by DuckDuckGo's search client.\n\n**parameters**:\n· days: An integer representing the number of days to filter results.\n\n**Code Description**:  \nThe function _convert_days_to_timelimit takes an integer value for `days` and returns a corresponding string representing DuckDuckGo's timelimit format. This format is used to filter search results based on the specified timeframe.\n\nThe function works as follows:\n1. If the `days` value is less than or equal to 1, the function returns `\"d\"`, which stands for results from the last 24 hours.\n2. If the `days` value is greater than 1 but less than or equal to 7, the function returns `\"w\"`, representing results from the last week.\n3. If the `days` value is greater than 7 but less than or equal to 30, the function returns `\"m\"`, indicating results from the last month.\n4. If the `days` value exceeds 30, the function returns `\"y\"`, indicating results from the last year.\n\nThis function is crucial for the `search` method within the `DuckDuckGoClient` class. Specifically, it is called to determine the appropriate timelimit format when initiating a search request. The `search` method passes the `days` argument to _convert_days_to_timelimit, which then returns the timelimit string to be used in the search query. The timelimit helps filter search results based on recency, allowing the user to retrieve results relevant to a specific timeframe.\n\n**Note**: The `days` parameter should be an integer, and the function will return one of the following string values:\n- `\"d\"` for the last 24 hours,\n- `\"w\"` for the last week,\n- `\"m\"` for the last month,\n- `\"y\"` for the last year.\n\n**Output Example**:\nFor `days = 3`, the output would be `\"w\"`, representing results from the last week.  \nFor `days = 40`, the output would be `\"y\"`, representing results from the last year."
      ],
      "code_start_line": 19,
      "code_end_line": 36,
      "params": [
        "self",
        "days"
      ],
      "have_return": true,
      "code_content": "    def _convert_days_to_timelimit(self, days: int) -> str:\n        \"\"\"\n        Convert days to DuckDuckGo's timelimit format.\n\n        Args:\n            days (int): Number of days to filter results.\n\n        Returns:\n            str: A string representing DuckDuckGo's timelimit format ('d', 'w', 'm', 'y').\n        \"\"\"\n        if days <= 1:\n            return \"d\"  # Last 24 hours\n        elif days <= 7:\n            return \"w\"  # Last week\n        elif days <= 30:\n            return \"m\"  # Last month\n        else:\n            return \"y\"  # Last year\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient/search"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [
        "**search**: The function of search is to perform an asynchronous search query using the DuckDuckGo search client and return the results in a structured format.\n\n**parameters**: The parameters of this Function.\n· query: A string representing the search term to be queried.  \n· days: An integer representing the number of days to filter results, defaulting to 7.  \n· max_results: An integer indicating the maximum number of search results to return, defaulting to 10.  \n· region: A string literal that specifies the region for the search results, defaulting to \"us-en\".  \n\n**Code Description**: The `search` function is an asynchronous method designed to query the DuckDuckGo search engine. It accepts a search term (`query`) and several optional parameters that allow users to customize the search results based on recency and quantity.\n\n1. The method begins by converting the `days` parameter into a timelimit format using the `_convert_days_to_timelimit` method. This conversion is crucial as it determines how recent the search results should be, based on the specified number of days.\n\n2. A debug log statement is executed to indicate the initiation of a search query, providing visibility into the operation being performed.\n\n3. The function then calls the `atext` method of the `AsyncDDGS` class, which is part of the DuckDuckGo search client. This method is awaited, meaning that the function will pause execution until the search results are retrieved. The parameters passed to `atext` include the search `query`, the `region`, a safesearch setting, the `timelimit`, and the maximum number of results to return.\n\n4. Once the raw results are obtained, the function processes these results into a list of `SearchResult` objects. Each `SearchResult` is instantiated with the title, URL, and content extracted from the raw results.\n\n5. Finally, the function returns a `SearchResponse` object that encapsulates the original query and the list of search results, limited to the specified `max_results`.\n\nThe `search` function is integral to the `DuckDuckGoClient` class, facilitating user queries and structuring the response in a way that is easy to consume by other components of the application. It relies on the `_convert_days_to_timelimit` method to filter results based on recency and utilizes the `SearchResult` and `SearchResponse` classes to format the output.\n\n**Note**: \n- The `query` parameter is mandatory and should be a valid search term. \n- The `days`, `max_results`, and `region` parameters are optional and have default values, allowing for flexible usage.\n- The function is asynchronous, which means it should be awaited when called to ensure proper execution flow.\n\n**Output Example**: \nA possible return value of the `search` function could look like this:\n```json\n{\n  \"query\": \"Python programming\",\n  \"results\": [\n    {\n      \"title\": \"Learn Python - Full Course for Beginners\",\n      \"url\": \"https://www.example.com/learn-python\",\n      \"content\": \"This comprehensive course covers Python basics and advanced topics.\"\n    },\n    {\n      \"title\": \"Python Programming Language\",\n      \"url\": \"https://www.example.com/python\",\n      \"content\": \"Python is a popular programming language known for its simplicity.\"\n    }\n  ]\n}\n```"
      ],
      "code_start_line": 44,
      "code_end_line": 74,
      "params": [
        "self",
        "query",
        "days",
        "max_results",
        "region"
      ],
      "have_return": true,
      "code_content": "    async def search(\n        self,\n        query: str,\n        days: int = 7,\n        max_results: int = 10,\n        region: Literal[\"us-en\", \"cn-zh\"] = \"us-en\",\n    ) -> SearchResponse:\n        timelimit = self._convert_days_to_timelimit(days)\n\n        logger.debug(f\"Using 'duckduckgo-search-client' for query '{query}'.\")\n\n        raw_results = await AsyncDDGS(timeout=10).atext(\n            query,\n            region=region,\n            safesearch=\"on\",\n            timelimit=timelimit,\n            max_results=max_results,\n        )\n\n        results = [\n            SearchResult(\n                title=result[\"title\"],\n                url=result[\"href\"],\n                content=result[\"body\"],\n            )\n            for result in raw_results\n        ]\n        return SearchResponse(\n            query=query,\n            results=results[:max_results],\n        )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/models.py/SearchResult",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponse",
        "src/criticsearch/tools/search_adapter/duckduckgo_client.py/DuckDuckGoClient/_convert_days_to_timelimit"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/tools/search_adapter/tavily_client.py": [
    {
      "type": "ClassDef",
      "name": "TavilyClient",
      "md_content": [
        "**TavilyClient**: The function of TavilyClient is to serve as an API client for interacting with the Tavily search service.\n\n**attributes**: The attributes of this Class.\n· base_url: str - This attribute stores the base URL for the Tavily API.\n· _api_key: str - This attribute holds the API key required for authenticating requests to the Tavily API.\n· headers: dict - This attribute contains the headers that will be sent with each API request, specifically setting the content type to JSON.\n\n**Code Description**: The TavilyClient class inherits from the BaseSearchClient class, establishing itself as a specific implementation for interacting with the Tavily API. Upon initialization, it sets the base URL for the Tavily API and stores the provided API key for use in subsequent requests. The class is designed to facilitate asynchronous search operations through the `search` method, which allows users to query the Tavily API with various parameters.\n\nThe `search` method is decorated with a retry mechanism, which attempts to resend the request up to five times in case of a rate limit exception (RatelimitException). This method accepts several parameters: `query`, `search_depth`, `topic`, `days`, and `max_results`, allowing for flexible search configurations. The method constructs a JSON payload containing these parameters along with the API key and sends an asynchronous POST request to the Tavily API's search endpoint.\n\nUpon receiving a response, the method checks the status code to determine the outcome of the request. A successful response (HTTP status code 200) results in the parsing of the JSON response into a SearchResponse object. If the response indicates a rate limit has been exceeded (HTTP status code 429), it raises a UsageLimitExceededError or RatelimitException based on the details provided in the response. An unauthorized access attempt (HTTP status code 401) raises an InvalidAPIKeyError. For any other unexpected status codes, the method returns a SearchResponse object containing an error message.\n\nThe TavilyClient is utilized within the SearchAggregator class, where it is instantiated if a valid Tavily API key is available. This integration allows the SearchAggregator to manage multiple search clients, including TavilyClient and BingClient, enabling it to perform searches across different services seamlessly.\n\n**Note**: It is essential to ensure that the API key provided to the TavilyClient is valid and that the application handles exceptions appropriately, particularly those related to rate limits and unauthorized access. Proper error handling will enhance the robustness of the application when interacting with the Tavily API.\n\nA possible appearance of the code's return value from the `search` method could be:\n```json\n{\n    \"query\": \"example search\",\n    \"results\": [\n        {\n            \"title\": \"Example Result 1\",\n            \"link\": \"https://example.com/result1\",\n            \"snippet\": \"This is a snippet of the first result.\"\n        },\n        {\n            \"title\": \"Example Result 2\",\n            \"link\": \"https://example.com/result2\",\n            \"snippet\": \"This is a snippet of the second result.\"\n        }\n    ],\n    \"error_message\": null\n}\n```",
        "**TavilyClient**: The function of TavilyClient is to serve as an API client for interacting with the Tavily search service.\n\n**attributes**: The attributes of this Class.\n· base_url: str - This attribute stores the base URL for the Tavily API, which is \"https://api.tavily.com\".\n· _api_key: str - This attribute holds the API key required for authenticating requests to the Tavily API.\n· headers: dict - This attribute contains the HTTP headers to be sent with API requests, specifically setting the \"Content-Type\" to \"application/json\".\n\n**Code Description**: The TavilyClient class inherits from BaseSearchClient and is designed to facilitate asynchronous search operations through the Tavily API. Upon initialization, the class requires an API key, which is stored in the _api_key attribute. The base URL for the API is predefined, and the headers for the requests are set to indicate that the content type is JSON.\n\nThe primary functionality of the TavilyClient is encapsulated in the asynchronous search method. This method accepts several parameters: a query string, search depth, topic, the number of days to look back, and the maximum number of results to return. The method constructs a JSON payload with these parameters, including the API key, and sends an asynchronous POST request to the Tavily API's search endpoint.\n\nThe search method is equipped with retry logic, allowing it to attempt the request up to five times in the event of a rate limit exception (indicated by a 429 status code). It employs an exponential backoff strategy combined with random jitter to manage retries effectively. If the API responds with a 200 status code, the method processes the response and returns a validated SearchResponse object. If a 429 status code is encountered, it checks for specific error details and raises a UsageLimitExceededError if applicable. For a 401 status code, it raises an InvalidAPIKeyError, indicating that the provided API key is invalid. Any other unexpected status codes result in the method returning a SearchResponse object that includes an error message detailing the issue.\n\nThe TavilyClient is utilized within the SearchAggregator class, which initializes instances of various search clients based on available API keys. If a valid Tavily API key is found in the settings, an instance of TavilyClient is created and added to the clients dictionary. This design allows the SearchAggregator to manage multiple search clients, including TavilyClient, enabling it to perform searches across different services seamlessly.\n\n**Note**: It is essential to ensure that the API key for Tavily is correctly configured in the settings. If the key is missing or invalid, the TavilyClient will not be instantiated, which may limit the functionality of the SearchAggregator. Proper error handling should be implemented when using the TavilyClient to manage search requests effectively.\n\nA possible appearance of the code's return value from the search method could be:\n```json\n{\n    \"query\": \"example search\",\n    \"results\": [\n        {\n            \"title\": \"Example Result 1\",\n            \"url\": \"https://example.com/result1\",\n            \"snippet\": \"This is a snippet of the first result.\"\n        },\n        {\n            \"title\": \"Example Result 2\",\n            \"url\": \"https://example.com/result2\",\n            \"snippet\": \"This is a snippet of the second result.\"\n        }\n    ],\n    \"error_message\": null\n}\n```"
      ],
      "code_start_line": 20,
      "code_end_line": 88,
      "params": [],
      "have_return": true,
      "code_content": "class TavilyClient(BaseSearchClient):\n    \"\"\"\n    Tavily API client class.\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        self.base_url = \"https://api.tavily.com\"\n        self._api_key = api_key\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n        }\n\n    @retry(\n        stop=stop_after_attempt(5),  # 重试最多5次\n        wait=wait_exponential(multiplier=1, min=4, max=30)\n        + wait_random(min=1, max=5),  # 指数退避 + 随机抖动\n        retry=retry_if_exception_type(RatelimitException),\n    )\n    async def search(\n        self,\n        query: str,\n        search_depth: Literal[\"basic\", \"advanced\"] = \"basic\",\n        topic: Literal[\"general\", \"news\"] = \"general\",\n        days: int = 7,\n        max_results: int = getattr(settings, \"max_results\", 10) or 10,\n    ) -> SearchResponse:\n        \"\"\"\n        异步搜索方法\n        \"\"\"\n\n        # 发起异步请求\n        data = {\n            \"query\": query,\n            \"search_depth\": search_depth,\n            \"topic\": topic,\n            \"days\": days,\n            \"max_results\": max_results,\n            \"api_key\": self._api_key,\n        }\n\n        async with httpx.AsyncClient(timeout=30, http2=True) as client:\n            response = await client.post(\n                self.base_url + \"/search\", json=data, headers=self.headers\n            )\n\n        if response.status_code == 200:\n            return SearchResponse.model_validate(response.json())\n        elif response.status_code == 429:\n            try:\n                detail = response.json().get(\"detail\", {}).get(\"error\")\n                if detail:\n                    raise UsageLimitExceededError(detail)  # 抛出后直接传播，不被捕获\n            except UsageLimitExceededError:\n                raise  # 直接传播 UsageLimitExceededError，避免被后续捕获\n            except Exception:\n                # 捕获其他异常并记录日志\n                printer.print_exception(\n                    f\"Failed to process 429 response. Response: {response.text}\"\n                )\n                raise RatelimitException()  # 抛出通用限流异常\n\n            raise RatelimitException()\n        elif response.status_code == 401:\n            raise InvalidAPIKeyError()\n        else:\n            return SearchResponse(\n                query=query,\n                error_message=f\"Unexpected status code: {response.status_code}. Response: {response.text}\",\n            )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py",
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/__init__"
      ],
      "reference_who": [
        "src/criticsearch/tools/search_adapter/base_search_client.py/BaseSearchClient",
        "src/criticsearch/tools/search_adapter/exceptions.py/RatelimitException"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the TavilyClient object with the required API key and configure default settings for the instance.\n\n**parameters**: The parameters of this Function.\n- api_key: A string representing the API key used for authentication with the Tavily API.\n\n**Code Description**: The `__init__` method is the constructor of the `TavilyClient` class. It initializes an instance of the class by setting up default values and configurations necessary for interacting with the Tavily API.\n  \n1. **self.base_url**: This attribute is set to the string \"https://api.tavily.com\", which represents the base URL of the Tavily API. It is used as the root URL for all API requests made by this client.\n  \n2. **self._api_key**: This attribute is initialized with the value of the `api_key` parameter. It stores the API key provided when the instance of the client is created. This key will likely be used in API requests for authentication purposes.\n\n3. **self.headers**: A dictionary is created with the key `\"Content-Type\"` set to `\"application/json\"`. This header is typically included in HTTP requests to specify that the body of the request is in JSON format. It ensures that the client interacts with the API in a manner that is compatible with its expected input format.\n\nThe method does not return any value. It simply prepares the object with the necessary attributes to interact with the Tavily API.\n\n**Note**: \n- The `api_key` is essential for authentication with the Tavily API and must be provided when creating an instance of the `TavilyClient`.\n- The `base_url` and headers are set to default values, which can be further customized or extended by other methods in the `TavilyClient` class for specific API requests."
      ],
      "code_start_line": 25,
      "code_end_line": 30,
      "params": [
        "self",
        "api_key"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, api_key: str):\n        self.base_url = \"https://api.tavily.com\"\n        self._api_key = api_key\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n        }\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "search",
      "md_content": [
        "## Function: `search`\n\n### Overview\nThe `search` function is an asynchronous method designed to perform a search query using the Tavily engine. It allows users to specify search parameters, such as the query string, search depth, topic, date range, and maximum number of results. The function interacts with an API endpoint and returns a structured response encapsulated in the `SearchResponse` class. In case of errors such as rate limiting or invalid API keys, appropriate exceptions are raised.\n\n### Parameters\n- **query** (`str`): The search query string to be used in the search request. This parameter is required.\n- **search_depth** (`Literal[\"basic\", \"advanced\"]`): The depth of the search. The default value is `\"basic\"`, and the available options are:\n  - `\"basic\"`: A standard search depth.\n  - `\"advanced\"`: A more detailed search.\n  \n- **topic** (`Literal[\"general\", \"news\"]`): The topic of the search query. The default value is `\"general\"`, and the options available are:\n  - `\"general\"`: General search results.\n  - `\"news\"`: Results specifically related to news topics.\n  \n- **days** (`int`): The time frame in days for filtering results. The default value is `7`, meaning results from the past 7 days will be included.\n  \n- **max_results** (`int`): The maximum number of search results to return. The default value is `10`, and it can be adjusted as needed.\n\n### Returns\n- **SearchResponse**: The function returns an instance of the `SearchResponse` class, which contains the results of the search query. The `SearchResponse` includes the query string, a list of search results, and any error messages encountered during the process.\n\n### Exceptions\nThe function raises the following exceptions in case of errors:\n- **UsageLimitExceededError**: Raised when the API usage limit is exceeded. This is specifically handled when a `429` status code is returned.\n- **RatelimitException**: Raised when a rate-limiting issue occurs, typically in the event of frequent API requests or when an exception other than `UsageLimitExceededError` is encountered due to rate-limiting issues.\n- **InvalidAPIKeyError**: Raised when the provided API key is invalid, typically corresponding to a `401` status code.\n\n### Behavior\n1. The function sends an asynchronous POST request to the Tavily search API with the specified parameters.\n2. If the request is successful (status code `200`), the function returns a `SearchResponse` object containing the search results.\n3. If the API response returns a `429` status code (rate limiting), the function checks if the response contains a usage limit error. If found, it raises a `UsageLimitExceededError`. If any other error occurs while handling the response, a `RatelimitException` is raised.\n4. If the API response returns a `401` status code (authentication error), the function raises an `InvalidAPIKeyError`.\n5. If the status code is not `200`, `429`, or `401`, the function returns a `SearchResponse` with an error message indicating an unexpected status code.\n\n### Usage Example\n```python\ntavily_client = TavilyClient(api_key=\"your_api_key\")\nresponse = await tavily_client.search(query=\"Python programming\", search_depth=\"advanced\", topic=\"general\", days=7, max_results=5)\nprint(response.ser_model())\n```\n\n### Notes\n- The `SearchResponse` class is used to structure the response from the search API, which includes the query, results, and any error messages.\n- The `UsageLimitExceededError` is raised when the search client has exceeded the API usage limits.",
        "**search**: The function of search is to perform an asynchronous search query based on user-defined parameters and return the results encapsulated in a SearchResponse object.\n\n**parameters**: The parameters of this Function.\n· query: str - A string representing the user's search query. This parameter cannot be empty and is essential for executing the search.  \n· search_depth: Literal[\"basic\", \"advanced\"] - This optional parameter specifies the depth of the search, with a default value of \"basic\".  \n· topic: Literal[\"general\", \"news\"] - This optional parameter defines the topic of the search, defaulting to \"general\".  \n· days: int - This optional parameter indicates the number of days to look back for results, defaulting to 7 days.  \n· max_results: int - This optional parameter sets the maximum number of results to return, defaulting to the value specified in the settings or 10 if not defined.\n\n**Code Description**: The search function is an asynchronous method designed to facilitate search queries against a specified API. It constructs a data payload containing the search parameters, including the query string, search depth, topic, days, maximum results, and the API key. Using the httpx library, it initiates an asynchronous POST request to the search endpoint of the API.\n\nUpon receiving a response, the function checks the status code:\n- If the status code is 200, it validates and returns the response data as a SearchResponse object.\n- If the status code is 429, indicating that the usage limit has been exceeded, it attempts to extract the error detail from the response. If a specific error message is present, it raises a UsageLimitExceededError. If no detail is found, it logs the exception and raises a RatelimitException.\n- If the status code is 401, it raises an InvalidAPIKeyError, indicating that the provided API key is invalid.\n- For any other status codes, it constructs a SearchResponse object containing the original query and an error message indicating the unexpected status code.\n\nThe search function is called by the _search_single_query method within the SearchAggregator class. This method manages the execution of search queries across multiple search engines. It iterates through a list of available engines, invoking the search method for each engine that is operational. The results from these searches are collected and returned as a structured response. The search function is integral to the overall search functionality, providing a consistent interface for querying search engines and handling various error conditions.\n\n**Note**: It is crucial to ensure that the query parameter is a valid and non-empty string. The optional parameters should be set according to the desired search behavior. Proper error handling is implemented for various HTTP status codes, and developers should be aware of the implications of exceeding usage limits or providing invalid API keys.\n\n**Output Example**: A possible return value of the search function could be structured as follows:\n```python\nSearchResponse(\n    query=\"latest technology news\",\n    results=[\n        SearchResult(title=\"Tech Innovations\", url=\"https://example.com/tech-innovations\", content=\"Explore the latest in technology.\"),\n        SearchResult(title=\"Gadget Reviews\", url=\"https://example.com/gadget-reviews\", content=\"Read reviews on the newest gadgets.\")\n    ],\n    error_message=None\n)\n```\nThis output illustrates a successful search response containing the original query, a list of search results, and no error messages."
      ],
      "code_start_line": 38,
      "code_end_line": 88,
      "params": [
        "self",
        "query",
        "search_depth",
        "topic",
        "days",
        "max_results"
      ],
      "have_return": true,
      "code_content": "    async def search(\n        self,\n        query: str,\n        search_depth: Literal[\"basic\", \"advanced\"] = \"basic\",\n        topic: Literal[\"general\", \"news\"] = \"general\",\n        days: int = 7,\n        max_results: int = getattr(settings, \"max_results\", 10) or 10,\n    ) -> SearchResponse:\n        \"\"\"\n        异步搜索方法\n        \"\"\"\n\n        # 发起异步请求\n        data = {\n            \"query\": query,\n            \"search_depth\": search_depth,\n            \"topic\": topic,\n            \"days\": days,\n            \"max_results\": max_results,\n            \"api_key\": self._api_key,\n        }\n\n        async with httpx.AsyncClient(timeout=30, http2=True) as client:\n            response = await client.post(\n                self.base_url + \"/search\", json=data, headers=self.headers\n            )\n\n        if response.status_code == 200:\n            return SearchResponse.model_validate(response.json())\n        elif response.status_code == 429:\n            try:\n                detail = response.json().get(\"detail\", {}).get(\"error\")\n                if detail:\n                    raise UsageLimitExceededError(detail)  # 抛出后直接传播，不被捕获\n            except UsageLimitExceededError:\n                raise  # 直接传播 UsageLimitExceededError，避免被后续捕获\n            except Exception:\n                # 捕获其他异常并记录日志\n                printer.print_exception(\n                    f\"Failed to process 429 response. Response: {response.text}\"\n                )\n                raise RatelimitException()  # 抛出通用限流异常\n\n            raise RatelimitException()\n        elif response.status_code == 401:\n            raise InvalidAPIKeyError()\n        else:\n            return SearchResponse(\n                query=query,\n                error_message=f\"Unexpected status code: {response.status_code}. Response: {response.text}\",\n            )\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/tools/search_adapter/search_aggregator.py/SearchAggregator/_search_single_query"
      ],
      "reference_who": [
        "src/criticsearch/rich_output.py/RichPrinter/print_exception",
        "src/criticsearch/tools/search_adapter/models.py/SearchResponse",
        "src/criticsearch/tools/search_adapter/exceptions.py/UsageLimitExceededError",
        "src/criticsearch/tools/search_adapter/exceptions.py/InvalidAPIKeyError",
        "src/criticsearch/tools/search_adapter/exceptions.py/RatelimitException"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "src/criticsearch/tools/search_adapter/__init__.py": [],
  "src/criticsearch/reportbench/report_benchmark.py": [
    {
      "type": "ClassDef",
      "name": "ReportBenchmark",
      "md_content": [
        "**ReportBenchmark**: The function of ReportBenchmark is to generate report evaluations by building ground truths and performing fact extraction and FactualQA evaluations.\n\n**attributes**: The attributes of this Class.\n· json_path: The path to the JSON input file containing the report data.\n· agent: An instance of BaseAgent used for interacting with the model.\n· breadth_gt: The ground truth for report breadth extracted from the JSON input.\n· article_content: The content of the article extracted from the Markdown file.\n· sections: The sections of the article extracted from the Markdown content.\n· section_content_pairs: Pairs of section titles and their corresponding content.\n· user_query: The user-defined query or a generated query based on the breadth ground truth.\n· cache_dir: The directory path for storing cached benchmark results.\n\n**Code Description**: The ReportBenchmark class is designed to facilitate the evaluation of reports by generating ground truths and extracting relevant facts from the provided report data. Upon initialization, it takes a JSON input path and an optional user query. It extracts the necessary information from the JSON file, including the breadth ground truth, article content, and sections, which are essential for generating comprehensive reports.\n\nThe class includes several key methods:\n- **_get_cache_key**: Generates a unique cache key based on the JSON path and user query, which is used to store and retrieve cached results.\n- **_load_from_cache**: Loads previously cached results if available, allowing for efficient reuse of data.\n- **_save_to_cache**: Saves the results to the cache for future reference.\n- **sliding_window_pairing**: Creates a sliding window of section content, merging sections while respecting a specified token limit. This method ensures that the content is organized and manageable for processing.\n- **run_fact_extraction**: Executes fact extraction on each section of the report using the BaseAgent's common_chat method. It handles retries in case of failures, ensuring robustness in the extraction process.\n- **run_factualqa**: Conducts a FactualQA evaluation using the user query and the ground truths, returning the evaluation results.\n- **process_window_content**: Processes the content of a single window, retrying if the result is empty.\n- **generate_benchmark_item**: Generates benchmark items with caching support, combining the results of the sliding window pairing and fact extraction.\n- **process_section**: A helper method that encapsulates the logic for processing a section of the report, similar to the run_fact_extraction method.\n- **parse_tagged_data_to_table**: Parses the extracted data into a structured format, specifically a table, which can be useful for further analysis.\n- **verify_extraction_meaningful**: A placeholder method intended to check the meaningfulness of the fact extraction results.\n\nThe ReportBenchmark class is called within the process_single_task function in the main.py file. It initializes an instance of ReportBenchmark with a JSON file path and generates benchmark items, which are then used to guide the conversation and content generation process. This integration highlights the class's role in facilitating the overall report generation workflow.\n\n**Note**: When using the ReportBenchmark class, ensure that the input JSON file is correctly formatted and contains the necessary data for extraction. The caching mechanism can significantly improve performance by avoiding redundant computations.\n\n**Output Example**: A possible return value from the generate_benchmark_item method could be a list of dictionaries, each containing the path of the section, the merged content of the section, and the extracted facts, structured as follows:\n```json\n[\n    {\n        \"path\": \"Section 1 -> Subsection 1.1\",\n        \"merged_section_window_content\": \"Content of Subsection 1.1...\",\n        \"extracted_facts\": [\n            {\"question\": \"What is the main topic?\", \"format\": \"text\", \"answer\": \"The main topic is...\"},\n            {\"question\": \"What are the key points?\", \"format\": \"list\", \"answer\": \"1. Point one\\n2. Point two\"}\n        ]\n    },\n    ...\n]\n```"
      ],
      "code_start_line": 30,
      "code_end_line": 382,
      "params": [],
      "have_return": true,
      "code_content": "class ReportBenchmark:\n    \"\"\"\n    A benchmarking class for generating report evaluations.\n    Builds ground truths for report breadth & depth using two modules,\n    and calls prompts (fact_extraction, outline_generation) via BaseAgent's common_chat.\n    Also includes a method for FactualQA evaluation using a model (e.g., GPT-4o).\n    \"\"\"\n\n    def __init__(self, json_input_path, user_query=None):\n        self.json_path = json_input_path\n        self.agent = BaseAgent()\n        self.breadth_gt = extractDirectoryTree(\n            self.json_path\n        )  # Extract breadth ground truth，得到一个json结构的广度树\n        self.article_content = extractMarkdownContent(self.json_path)\n        self.sections = extract_markdown_sections(self.article_content)\n        self.section_content_pairs = extractSectionContentPairs(json_input_path)\n        self.user_query = (\n            f\"Generate a comprehensive long report about {self.breadth_gt.get('title', '')}\"\n            if user_query is None\n            else user_query\n        )\n        # 添加缓存相关的属性\n        self.cache_dir = Path(\"cache/benchmark_results\")\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n    def _get_cache_key(self):\n        \"\"\"生成缓存文件的唯一标识\"\"\"\n        # 使用输入文件路径和查询作为缓存key的基础\n        content = f\"{self.json_path}_{self.user_query}\"\n        return hashlib.md5(content.encode()).hexdigest()\n\n    def _load_from_cache(self):\n        \"\"\"从缓存加载结果\"\"\"\n        cache_file = self.cache_dir / f\"{self._get_cache_key()}.json\"\n        if cache_file.exists():\n            with open(cache_file, 'r', encoding='utf-8') as f:\n                return json.load(f)\n        return None\n\n    def _save_to_cache(self, results):\n        \"\"\"保存结果到缓存\"\"\"\n        cache_file = self.cache_dir / f\"{self._get_cache_key()}.json\"\n        with open(cache_file, 'w', encoding='utf-8') as f:\n            json.dump(results, f, ensure_ascii=False, indent=2)\n\n    def sliding_window_pairing(self, max_token_length=2000):\n        \"\"\"\n        创建 section 内容的滑动窗口，尽可能在 token 限制内合并更多的 section。\n        \n        Args:\n            max_token_length: 每个窗口的最大 token 数量\n            \n        Returns:\n            List[Dict]: 一个包含合并窗口的列表，每个窗口包含最高级标题、合并内容、路径等信息\n        \"\"\"\n        # 首先提取所有 section 的标题和内容\n        sections = []\n        \n        def extract_sections(data, path=[], depth=0):\n            \"\"\"递归提取所有 section 的标题、内容、深度和路径\"\"\"\n            if isinstance(data, dict):\n                # 如果是直接的字典对象\n                title = data.get('title', '')\n                content = data.get('content', '')\n                \n                # 获取正确的层级深度\n                current_depth = depth + 1  # 默认深度加1\n                # 如果数据中有明确的id，我们可以从id解析深度\n                section_id = data.get('id', '')\n                if section_id and '.' in section_id:\n                    # 例如 \"4.3.5\" 表示这是第4个主标题下的第3个子标题下的第5个小节\n                    # 那么深度应该是3（4是1级，4.3是2级，4.3.5是3级）\n                    current_depth = len(section_id.split('.'))\n                \n                if title and content:\n                    current_path = path + [{\"title\": title, \"depth\": current_depth}]\n                    sections.append({\n                        \"title\": title,\n                        \"content\": content,\n                        \"depth\": current_depth,\n                        \"path\": current_path.copy(),\n                        \"tokens\": count_tokens(content)\n                    })\n                \n                # 检查是否有 children\n                if 'children' in data and data['children']:\n                    current_path = path + [{\"title\": title, \"depth\": current_depth}]\n                    # 递归处理 children，子节点的深度加1\n                    for child in data['children']:\n                        extract_sections(child, current_path, current_depth)\n        \n        # 从 section_content_pairs 中提取所有 section\n        extract_sections(self.section_content_pairs)\n        \n        # 按深度和标题路径排序，确保父节点在子节点之前\n        # 使用路径长度和路径中的标题字符串进行排序\n        def path_sort_key(section):\n            path_len = len(section['path'])\n            # 将路径转换为可比较的字符串序列\n            path_titles = [p[\"title\"] for p in section['path']]\n            return (path_len, tuple(path_titles))\n            \n        sections.sort(key=path_sort_key)\n        \n        # 创建滑动窗口\n        windows = []\n        i = 0\n        while i < len(sections):\n            current_section = sections[i]\n            highest_title = current_section['title']\n            highest_depth = current_section['depth']\n            current_path = current_section['path']\n            \n            # 使用原始深度来设置标题级别\n            merged_content = f\"{'#' * current_section['depth']} {highest_title}\\n{current_section['content']}\"\n            window_tokens = current_section['tokens'] + count_tokens(f\"{'#' * current_section['depth']} {highest_title}\\n\")\n            window_path = [current_section['path'][-1]]\n            \n            # 尝试添加后续的 section，如果它们是当前 section 的子节点或平级节点\n            j = i + 1\n            while j < len(sections) and window_tokens < max_token_length:\n                next_section = sections[j]\n                next_path = next_section['path']\n                \n                # 检查路径关系时使用标题字符串进行比较\n                current_path_titles = [p[\"title\"] for p in current_path]\n                next_path_titles = [p[\"title\"] for p in next_path]\n                \n                # 检查是否为子节点或平级节点\n                is_subsection_or_sibling = False\n                \n                # 如果下一个 section 是当前 section 的子节点\n                if len(next_path_titles) > len(current_path_titles) and all(next_path_titles[k] == current_path_titles[k] for k in range(len(current_path_titles))):\n                    is_subsection_or_sibling = True\n                \n                # 如果下一个 section 是当前 section 的平级节点\n                elif len(next_path_titles) == len(current_path_titles) and all(next_path_titles[k] == current_path_titles[k] for k in range(len(current_path_titles) - 1)):\n                    is_subsection_or_sibling = True\n                \n                if is_subsection_or_sibling:\n                    # 使用原始深度来设置标题级别，不再计算相对深度\n                    section_header = '#' * next_section['depth'] + ' ' + next_section['title'] + '\\n'\n                    additional_tokens = count_tokens(section_header + next_section['content'])\n                    \n                    if window_tokens + additional_tokens <= max_token_length:\n                        # 可以添加到当前窗口\n                        merged_content += f\"\\n{section_header}{next_section['content']}\"\n                        window_tokens += additional_tokens\n                        window_path.append({\"title\": next_section['title'], \"depth\": next_section['depth']})\n                        j += 1\n                    else:\n                        # 超出 token 限制，不能再添加\n                        break\n                else:\n                    # 不是子节点或平级节点，跳过\n                    break\n            \n            # 创建窗口对象\n            # 修改这里，将路径文本包含标题的层级信息\n            def format_path_with_depth(path_nodes):\n                formatted_titles = []\n                for node in path_nodes:\n                    depth = node[\"depth\"]\n                    title = node[\"title\"]\n                    formatted_title = '#' * depth + ' ' + title\n                    formatted_titles.append(formatted_title)\n                return ' -> '.join(formatted_titles)\n            \n            path_text = format_path_with_depth(window_path)\n            window = {\n                \"highest_title\": highest_title,\n                \"merged_section_window_content\": merged_content,\n                \"section_window_path\": window_path,\n                \"section_window_path_text\": path_text,\n                \"section_window_tokens\": window_tokens\n            }\n            \n            windows.append(window)\n            \n            # 下一个窗口的起始位置\n            i = j if j > i + 1 else i + 1\n        \n        return windows\n\n    def run_fact_extraction(self):\n        \"\"\"\n        使用 self.sections 中的每个 markdown 文本调用 fact_extraction，\n        并行执行，返回一个包含各 section 响应的列表。\n        如果对某个 section 10 次尝试后都失败，则发出警告并跳过该 section。\n        \"\"\"\n\n        def process_section(section_text):\n            @retry(stop=stop_after_attempt(10), wait=wait_fixed(1), reraise=True)\n            def attempt():\n                template_str = self.agent.load_template(\"fact_extraction.txt\")\n                data = {\n                    \"wiki_text\": section_text,\n                    \"UserQuery\": self.user_query,\n                }\n                prompt = self.agent.render_template(template_str, data)\n                response = self.agent.common_chat(usr_prompt=prompt)\n                if not isinstance(response, list):\n                    if not response.strip():\n                        raise Exception(\"Empty response received from common_chat\")\n                    try:\n                        candidate = json.loads(response)\n                        print(candidate)\n                        if isinstance(candidate, list):\n                            return candidate\n                    except Exception as e:\n                        raise Exception(\"Section response conversion failed\") from e\n                    raise Exception(\"Section response is not a list\")\n                return response\n\n            try:\n                return attempt()\n            except Exception as e:\n                print(\n                    f\"Warning: Failed to process section after 10 attempts. Skipping this section. Error: {e}\"\n                )\n                return None\n\n        with ThreadPoolExecutor() as executor:\n            raw_results = list(executor.map(process_section, self.sections))\n        # Filter out any sections that failed after 10 attempts\n        results = [result for result in raw_results if result is not None]\n        return results  # 返回 List[List[str]]，每个元素是一个 section 的 fact extraction 结果\n\n    def run_factualqa(self):\n        # Load and render a template \"factual_qa.txt\" for FactualQA evaluation.\n        # Pass Query, BreadthGT (converted to JSON string) and DepthGT.\n        template_str = self.agent.load_template(\"factual_qa.txt\")\n        data = {\n            \"Query\": self.user_query,  # Updated from self.query to self.user_query\n            \"BreadthGT\": json.dumps(self.breadth_gt),\n            \"DepthGT\": self.depth_gt,\n        }\n        prompt = self.agent.render_template(template_str, data)\n        response = self.agent.common_chat(usr_prompt=prompt)\n        return response\n\n    def process_window_content(self, content, max_retries=10):\n        \"\"\"处理单个窗口内容,如果结果为空则重试\"\"\"\n        for attempt in range(max_retries):\n            try:\n                result = self.process_section(content)\n                if result:\n                    parsed_data = self.parse_tagged_data_to_table(result)\n                    if parsed_data:  # 如果解析出的数据不为空\n                        return parsed_data\n                print(f\"Attempt {attempt + 1}: Empty result, retrying...\")\n            except Exception as e:\n                print(f\"Attempt {attempt + 1} failed with error: {e}\")\n        return []  # 如果所有尝试都失败,返回空列表\n\n    def generate_benchmark_item(self, use_cache=True, max_window_tokens=300):\n        \"\"\"添加缓存支持的基准测试项生成方法\"\"\"\n        if use_cache:\n            cached_results = self._load_from_cache()\n            if cached_results is not None:\n                print(\"Loading results from cache...\")\n                return cached_results\n        \n        # 原有的生成逻辑\n        results = []\n        windows = self.sliding_window_pairing(max_token_length=max_window_tokens)\n        \n        # 准备抽取任务的输入\n        window_contents = []\n        window_paths = []\n        for window in windows:\n            window_contents.append(window[\"merged_section_window_content\"])\n            window_paths.append(window[\"section_window_path_text\"])\n        \n        # 组合抽取结果和路径信息\n        final_results = []\n        for path, content in zip(window_paths, window_contents):\n            parsed_data = self.process_window_content(content)\n            if parsed_data:\n                final_results.append({\n                    \"path\": path,\n                    \"merged_section_window_content\": content,\n                    \"extracted_facts\": parsed_data\n                })\n        \n        # 保存结果到缓存\n        if use_cache:\n            self._save_to_cache(final_results)\n            \n        return final_results\n\n    def process_section(self, section_text):\n        \"\"\"将原来run_fact_extraction中的process_section逻辑移到单独的方法\"\"\"\n        @retry(stop=stop_after_attempt(10), wait=wait_fixed(1), reraise=True)\n        def attempt():\n            template_str = self.agent.load_template(\"fact_extraction.txt\")\n            data = {\n                \"wiki_text\": section_text,\n                \"UserQuery\": self.user_query,\n            }\n            prompt = self.agent.render_template(template_str, data)\n            response = self.agent.common_chat(usr_prompt=prompt)\n            if not isinstance(response, list):\n                if not response.strip():\n                    raise Exception(\"Empty response received from common_chat\")\n                try:\n                    candidate = json.loads(response)\n                    print(candidate)\n                    if isinstance(candidate, list):\n                        return candidate\n                except Exception as e:\n                    raise Exception(\"Section response conversion failed\") from e\n                raise Exception(\"Section response is not a list\")\n            return response\n\n        try:\n            return attempt()\n        except Exception as e:\n            print(\n                f\"Warning: Failed to process section after 10 attempts. Skipping this section. Error: {e}\"\n            )\n            return None\n\n    def parse_tagged_data_to_table(self, entries, csv_path=None):\n        parsed_data = []\n        for entry in entries:\n            # Extract question\n            question_match = re.search(r\"</question>(.*?)</question>\", entry)\n            question = question_match.group(1).strip() if question_match else \"\"\n\n            # Extract format description\n            format_match = re.search(\n                r\"</constrained_format>(.*?)</constrained_format>\", entry\n            )\n            format_desc = format_match.group(1).strip() if format_match else \"\"\n\n            # Extract answer\n            answer_match = re.search(r\"</answer>(.*?)</answer>\", entry)\n            answer = answer_match.group(1).strip() if answer_match else \"\"\n\n            # 验证answer中是否包含\\boxed{...}格式的内容\n            boxed_match = re.search(r\"\\\\boxed{([^}]+)}\", answer)\n            if boxed_match:  # 只有当匹配到boxed内容时才添加到结果中\n                parsed_data.append(\n                    {\"question\": question, \"format\": format_desc, \"answer\": answer}\n                )\n\n        return parsed_data\n\n    def verify_extraction_meaningful(self):\n        # Check if the fact extraction result is meaningful enough and correct.\n        pass\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/process_single_task",
        "src/criticsearch/reportbench/report_evaluation.py",
        "src/criticsearch/reportbench/report_evaluation.py/ReportEvaluation/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the ReportBenchmark class, setting up necessary attributes and processing input data.\n\n**parameters**: The parameters of this Function.\n· json_input_path: A string representing the path to the input JSON file that contains the data to be processed.\n· user_query: An optional string that allows the user to specify a custom query for generating a report.\n\n**Code Description**: The __init__ method of the ReportBenchmark class is responsible for initializing an instance of the class with specific attributes and processing the input data provided through the json_input_path parameter. \n\nUpon invocation, the method begins by storing the provided json_input_path in the instance variable self.json_path. It then creates an instance of the BaseAgent class, which serves as a foundational component for managing interactions and functionalities related to intelligent agents. The BaseAgent instance is assigned to the self.agent attribute.\n\nNext, the method calls the extractDirectoryTree function, passing the json_input_path to extract a breadth ground truth representation of the data. This function reads the JSON file, filters its contents, and constructs a hierarchical tree structure, which is crucial for understanding the relationships within the data. The resulting tree structure is stored in the self.breadth_gt attribute.\n\nFollowing this, the method utilizes the extractMarkdownContent function to read the JSON file and convert its content into a markdown format. The generated markdown content is stored in the self.article_content attribute. The extract_markdown_sections function is then called to split the markdown content into distinct sections based on header lines, and the resulting sections are stored in the self.sections attribute.\n\nAdditionally, the method calls the extractSectionContentPairs function to extract structured section-content pairs from the JSON file. This structured data is stored in the self.section_content_pairs attribute, which is essential for organizing the report content.\n\nThe user_query attribute is set based on the provided user_query parameter. If no custom query is specified, a default query is generated using the title from the breadth ground truth. This query is intended to guide the report generation process.\n\nLastly, the method initializes a cache directory for storing benchmark results. The cache directory is created if it does not already exist, ensuring that the application has a designated location for caching results.\n\nOverall, the __init__ method establishes the foundational setup for the ReportBenchmark class, ensuring that all necessary data is processed and stored for subsequent operations related to report generation.\n\n**Note**: It is important to ensure that the input JSON file adheres to the expected structure, as the various extraction functions rely on specific keys and formats to process the data correctly. Any deviations from this structure may lead to errors or unexpected results during execution."
      ],
      "code_start_line": 38,
      "code_end_line": 54,
      "params": [
        "self",
        "json_input_path",
        "user_query"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, json_input_path, user_query=None):\n        self.json_path = json_input_path\n        self.agent = BaseAgent()\n        self.breadth_gt = extractDirectoryTree(\n            self.json_path\n        )  # Extract breadth ground truth，得到一个json结构的广度树\n        self.article_content = extractMarkdownContent(self.json_path)\n        self.sections = extract_markdown_sections(self.article_content)\n        self.section_content_pairs = extractSectionContentPairs(json_input_path)\n        self.user_query = (\n            f\"Generate a comprehensive long report about {self.breadth_gt.get('title', '')}\"\n            if user_query is None\n            else user_query\n        )\n        # 添加缓存相关的属性\n        self.cache_dir = Path(\"cache/benchmark_results\")\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent",
        "src/criticsearch/reportbench/extract_ground_truth.py/extractDirectoryTree",
        "src/criticsearch/reportbench/extract_ground_truth.py/extractMarkdownContent",
        "src/criticsearch/reportbench/extract_ground_truth.py/extractSectionContentPairs",
        "src/criticsearch/reportbench/extract_ground_truth.py/extract_markdown_sections"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_get_cache_key",
      "md_content": [
        "**_get_cache_key**: The function of _get_cache_key is to generate a unique identifier for a cache file based on the input file path and user query.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The _get_cache_key function constructs a unique cache key by concatenating the instance's json_path and user_query attributes. This concatenated string is then encoded and hashed using the MD5 algorithm to produce a hexadecimal digest, which serves as a unique identifier for the cache file. The resulting cache key is crucial for ensuring that cached results are correctly associated with specific input parameters, thereby preventing conflicts and ensuring data integrity.\n\nThis function is called by two other methods within the ReportBenchmark class: _load_from_cache and _save_to_cache. \n\n- In _load_from_cache, the cache key generated by _get_cache_key is used to construct the filename of the cache file from which results are loaded. If the cache file exists, the method reads the JSON data from the file and returns it. If the file does not exist, it returns None, indicating that there are no cached results available.\n\n- In _save_to_cache, the cache key is similarly used to create the filename for storing results. The method writes the provided results to a JSON file named after the cache key, ensuring that the results can be retrieved later using the same key.\n\nThe relationship between _get_cache_key and its callers is essential for the caching mechanism implemented in the ReportBenchmark class. By generating a consistent cache key based on the input parameters, the class can efficiently save and retrieve results, optimizing performance and reducing redundant computations.\n\n**Note**: It is important to ensure that both json_path and user_query are properly set before calling _get_cache_key, as they directly influence the uniqueness of the generated cache key.\n\n**Output Example**: A possible return value of the _get_cache_key function could be a string like \"d41d8cd98f00b204e9800998ecf8427e\", which represents the MD5 hash of the concatenated json_path and user_query."
      ],
      "code_start_line": 56,
      "code_end_line": 60,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _get_cache_key(self):\n        \"\"\"生成缓存文件的唯一标识\"\"\"\n        # 使用输入文件路径和查询作为缓存key的基础\n        content = f\"{self.json_path}_{self.user_query}\"\n        return hashlib.md5(content.encode()).hexdigest()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_load_from_cache",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_save_to_cache"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_load_from_cache",
      "md_content": [
        "**_load_from_cache**: The function of _load_from_cache is to load the results from a cached file if available.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**:  \nThe _load_from_cache method is designed to retrieve previously stored data from a cache, if it exists. It constructs the path to the cache file using the cache directory (accessible via `self.cache_dir`) and a cache key generated by the _get_cache_key method. This cache key is used to create the filename of the cache file in JSON format.\n\n1. **Cache File Path**: The function first constructs the path to the cache file by calling `self._get_cache_key()` to generate a unique key, then appends this key to the cache directory path. The final file path is represented as `cache_file = self.cache_dir / f\"{self._get_cache_key()}.json\"`.\n   \n2. **Checking Cache Existence**: The function checks if the cache file exists using the `exists()` method. If the file exists, the cache data is read.\n\n3. **Reading the Cache**: If the cache file is found, the function opens the file in read mode (`'r'`) with UTF-8 encoding and loads the contents of the file using the `json.load()` function. The loaded JSON data is then returned.\n\n4. **Return None if No Cache**: If the cache file does not exist, the method returns `None`, indicating that no cached data is available for loading.\n\nThe method is used by other functions in the `ReportBenchmark` class, such as `generate_benchmark_item`. In `generate_benchmark_item`, the method is called with the `use_cache` argument set to `True`. If caching is enabled and valid cached results are found, they are returned immediately, bypassing the need to regenerate the results. If no cache is found, the function proceeds to generate new results and optionally saves them to the cache.\n\n**Note**: This method relies on the successful generation of a cache key by the _get_cache_key method. It is crucial that the cache directory (`self.cache_dir`) is properly initialized and that a valid cache key is generated for this function to operate correctly.\n\n**Output Example**: A possible return value of the _load_from_cache function could be a Python object, such as a list of dictionaries containing cached results, or `None` if no cached data exists. For instance:\n\n```json\n[\n    {\n        \"path\": \"/path/to/section\",\n        \"merged_section_window_content\": \"Example content\",\n        \"extracted_facts\": {\"fact1\": \"value1\", \"fact2\": \"value2\"}\n    }\n]\n```"
      ],
      "code_start_line": 62,
      "code_end_line": 68,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _load_from_cache(self):\n        \"\"\"从缓存加载结果\"\"\"\n        cache_file = self.cache_dir / f\"{self._get_cache_key()}.json\"\n        if cache_file.exists():\n            with open(cache_file, 'r', encoding='utf-8') as f:\n                return json.load(f)\n        return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_benchmark_item"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_get_cache_key"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_save_to_cache",
      "md_content": [
        "**__save_to_cache**: The function of _save_to_cache is to save the results to a cache file.\n\n**parameters**: The parameters of this Function.\n· results: The data that needs to be stored in the cache file.\n\n**Code Description**: The _save_to_cache function is responsible for saving the results of a computation or process to a cache file. It constructs a file path using the cache directory defined in the instance (`self.cache_dir`), appending a filename generated by the _get_cache_key method. This method ensures that the file is uniquely identified based on the current context. The file is named with the cache key followed by the \".json\" extension.\n\nOnce the cache file path is determined, the function opens the file in write mode with UTF-8 encoding and uses the `json.dump` function to serialize the `results` parameter into a JSON format. The `ensure_ascii=False` option is set to handle non-ASCII characters properly, and `indent=2` is used for pretty-printing the JSON data to make it human-readable.\n\nThis method is called by other parts of the class, particularly in scenarios where data needs to be saved for future use or to avoid redundant computations. One of the main functions calling _save_to_cache is `generate_benchmark_item`. In `generate_benchmark_item`, after generating the benchmark results, the method checks whether caching is enabled. If it is, the results are passed to _save_to_cache for storage.\n\n**Note**: It is important that the cache directory (`self.cache_dir`) is properly defined and accessible before calling this method. Additionally, the cache file name depends on the correct functioning of the _get_cache_key method, which generates the unique identifier for the file. Therefore, the cache key must be correctly generated for the caching process to work as intended."
      ],
      "code_start_line": 70,
      "code_end_line": 74,
      "params": [
        "self",
        "results"
      ],
      "have_return": false,
      "code_content": "    def _save_to_cache(self, results):\n        \"\"\"保存结果到缓存\"\"\"\n        cache_file = self.cache_dir / f\"{self._get_cache_key()}.json\"\n        with open(cache_file, 'w', encoding='utf-8') as f:\n            json.dump(results, f, ensure_ascii=False, indent=2)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_benchmark_item"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_get_cache_key"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "sliding_window_pairing",
      "md_content": [
        "## `sliding_window_pairing` Function Documentation\n\n### Purpose\nThe `sliding_window_pairing` function is designed to create sliding windows of section content, merging as many sections as possible within a specified token limit. This function ensures that sections are grouped in a way that maximizes content within the constraints of a token limit, which is essential for processing and inputting into models that have token limitations.\n\n### Arguments\n\n- **max_token_length (int, optional)**: \n  - The maximum number of tokens allowed in each window. The default value is 2000 tokens. This argument determines the size of each content window created by the function. Sections will be combined until the token limit is reached.\n\n### Returns\n\n- **List[Dict]**: \n  - A list of dictionaries, where each dictionary represents a merged window of sections. Each window contains:\n    - **highest_title** (str): The title of the highest-level section within the window.\n    - **merged_section_window_content** (str): The combined content of the sections within the window.\n    - **section_window_path** (list): A list of dictionaries representing the hierarchical path of titles and their respective depths within the window.\n    - **section_window_path_text** (str): A string representation of the section path, formatted with depth information.\n    - **section_window_tokens** (int): The total number of tokens in the window, calculated based on the combined content of the sections.\n\n### Functionality\n\n1. **Section Extraction**: \n   The function starts by recursively extracting the titles and content of all sections from the provided data structure (`self.section_content_pairs`). For each section, it records the title, content, depth, path, and token count.\n\n2. **Section Sorting**: \n   The sections are sorted by their depth and title path to ensure that parent sections appear before their children. This sorting helps in maintaining a logical structure when creating the sliding windows.\n\n3. **Sliding Window Creation**: \n   The function then iterates through the sorted sections, creating windows of sections that fit within the specified token limit. It merges sections if the total token count of the merged content does not exceed the `max_token_length`.\n\n4. **Path Formatting**: \n   Each window includes a formatted path that displays the hierarchical structure of the sections within that window. The path is represented as a sequence of titles, with each title preceded by a number of hash (`#`) symbols indicating its depth level.\n\n5. **Token Counting**: \n   The function uses the `count_tokens` function to calculate the number of tokens in each section's content and the combined content of each window. This is crucial for ensuring that the windows respect the token limit.\n\n6. **Window Finalization**: \n   For each window, the function compiles the content and metadata (title, path, and token count) into a dictionary and adds it to the list of windows. The function continues to the next window, ensuring that the token limit is adhered to at all times.\n\n### Example Use Case\n\nThis function is particularly useful in scenarios where content needs to be preprocessed into manageable chunks for model input, such as when dealing with natural language processing models that have token limits (e.g., GPT-3 or GPT-4). The sliding windows allow for efficient content grouping, ensuring that the model receives input without exceeding token constraints.\n\n### Dependencies\n\n- **count_tokens**: The function relies on `count_tokens` to calculate the number of tokens in a section's content. The tokenization is model-specific and ensures that the content remains within acceptable limits for the processing model.\n\n---\n\nThis function provides an efficient method for handling and processing large sets of hierarchical content, especially when working with token-limited systems. By ensuring that content is grouped and tokenized appropriately, it helps optimize the preparation of content for further processing or analysis."
      ],
      "code_start_line": 76,
      "code_end_line": 213,
      "params": [
        "self",
        "max_token_length"
      ],
      "have_return": true,
      "code_content": "    def sliding_window_pairing(self, max_token_length=2000):\n        \"\"\"\n        创建 section 内容的滑动窗口，尽可能在 token 限制内合并更多的 section。\n        \n        Args:\n            max_token_length: 每个窗口的最大 token 数量\n            \n        Returns:\n            List[Dict]: 一个包含合并窗口的列表，每个窗口包含最高级标题、合并内容、路径等信息\n        \"\"\"\n        # 首先提取所有 section 的标题和内容\n        sections = []\n        \n        def extract_sections(data, path=[], depth=0):\n            \"\"\"递归提取所有 section 的标题、内容、深度和路径\"\"\"\n            if isinstance(data, dict):\n                # 如果是直接的字典对象\n                title = data.get('title', '')\n                content = data.get('content', '')\n                \n                # 获取正确的层级深度\n                current_depth = depth + 1  # 默认深度加1\n                # 如果数据中有明确的id，我们可以从id解析深度\n                section_id = data.get('id', '')\n                if section_id and '.' in section_id:\n                    # 例如 \"4.3.5\" 表示这是第4个主标题下的第3个子标题下的第5个小节\n                    # 那么深度应该是3（4是1级，4.3是2级，4.3.5是3级）\n                    current_depth = len(section_id.split('.'))\n                \n                if title and content:\n                    current_path = path + [{\"title\": title, \"depth\": current_depth}]\n                    sections.append({\n                        \"title\": title,\n                        \"content\": content,\n                        \"depth\": current_depth,\n                        \"path\": current_path.copy(),\n                        \"tokens\": count_tokens(content)\n                    })\n                \n                # 检查是否有 children\n                if 'children' in data and data['children']:\n                    current_path = path + [{\"title\": title, \"depth\": current_depth}]\n                    # 递归处理 children，子节点的深度加1\n                    for child in data['children']:\n                        extract_sections(child, current_path, current_depth)\n        \n        # 从 section_content_pairs 中提取所有 section\n        extract_sections(self.section_content_pairs)\n        \n        # 按深度和标题路径排序，确保父节点在子节点之前\n        # 使用路径长度和路径中的标题字符串进行排序\n        def path_sort_key(section):\n            path_len = len(section['path'])\n            # 将路径转换为可比较的字符串序列\n            path_titles = [p[\"title\"] for p in section['path']]\n            return (path_len, tuple(path_titles))\n            \n        sections.sort(key=path_sort_key)\n        \n        # 创建滑动窗口\n        windows = []\n        i = 0\n        while i < len(sections):\n            current_section = sections[i]\n            highest_title = current_section['title']\n            highest_depth = current_section['depth']\n            current_path = current_section['path']\n            \n            # 使用原始深度来设置标题级别\n            merged_content = f\"{'#' * current_section['depth']} {highest_title}\\n{current_section['content']}\"\n            window_tokens = current_section['tokens'] + count_tokens(f\"{'#' * current_section['depth']} {highest_title}\\n\")\n            window_path = [current_section['path'][-1]]\n            \n            # 尝试添加后续的 section，如果它们是当前 section 的子节点或平级节点\n            j = i + 1\n            while j < len(sections) and window_tokens < max_token_length:\n                next_section = sections[j]\n                next_path = next_section['path']\n                \n                # 检查路径关系时使用标题字符串进行比较\n                current_path_titles = [p[\"title\"] for p in current_path]\n                next_path_titles = [p[\"title\"] for p in next_path]\n                \n                # 检查是否为子节点或平级节点\n                is_subsection_or_sibling = False\n                \n                # 如果下一个 section 是当前 section 的子节点\n                if len(next_path_titles) > len(current_path_titles) and all(next_path_titles[k] == current_path_titles[k] for k in range(len(current_path_titles))):\n                    is_subsection_or_sibling = True\n                \n                # 如果下一个 section 是当前 section 的平级节点\n                elif len(next_path_titles) == len(current_path_titles) and all(next_path_titles[k] == current_path_titles[k] for k in range(len(current_path_titles) - 1)):\n                    is_subsection_or_sibling = True\n                \n                if is_subsection_or_sibling:\n                    # 使用原始深度来设置标题级别，不再计算相对深度\n                    section_header = '#' * next_section['depth'] + ' ' + next_section['title'] + '\\n'\n                    additional_tokens = count_tokens(section_header + next_section['content'])\n                    \n                    if window_tokens + additional_tokens <= max_token_length:\n                        # 可以添加到当前窗口\n                        merged_content += f\"\\n{section_header}{next_section['content']}\"\n                        window_tokens += additional_tokens\n                        window_path.append({\"title\": next_section['title'], \"depth\": next_section['depth']})\n                        j += 1\n                    else:\n                        # 超出 token 限制，不能再添加\n                        break\n                else:\n                    # 不是子节点或平级节点，跳过\n                    break\n            \n            # 创建窗口对象\n            # 修改这里，将路径文本包含标题的层级信息\n            def format_path_with_depth(path_nodes):\n                formatted_titles = []\n                for node in path_nodes:\n                    depth = node[\"depth\"]\n                    title = node[\"title\"]\n                    formatted_title = '#' * depth + ' ' + title\n                    formatted_titles.append(formatted_title)\n                return ' -> '.join(formatted_titles)\n            \n            path_text = format_path_with_depth(window_path)\n            window = {\n                \"highest_title\": highest_title,\n                \"merged_section_window_content\": merged_content,\n                \"section_window_path\": window_path,\n                \"section_window_path_text\": path_text,\n                \"section_window_tokens\": window_tokens\n            }\n            \n            windows.append(window)\n            \n            # 下一个窗口的起始位置\n            i = j if j > i + 1 else i + 1\n        \n        return windows\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_benchmark_item"
      ],
      "reference_who": [
        "src/criticsearch/utils.py/count_tokens"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "extract_sections",
      "md_content": [
        "**extract_sections**: The function of extract_sections is to recursively extract the titles, content, depth, and path of all sections in a given hierarchical data structure.\n\n**parameters**:\n· data: The input data which can be a dictionary containing the details of a section, including title, content, id, and potentially nested child sections.\n· path: A list used to track the hierarchical path of sections (default is an empty list).\n· depth: An integer representing the current depth of recursion within the section hierarchy (default is 0).\n\n**Code Description**: \nThe `extract_sections` function is designed to recursively traverse a hierarchical data structure, typically representing sections of a document or report, extracting specific details about each section such as its title, content, depth, and hierarchical path. It processes the data in a depth-first manner, identifying and handling nested sections (children) as it proceeds.\n\n1. **Base Case (Dictionary Check)**: The function first checks if the input `data` is a dictionary. This is necessary because each section is expected to be represented as a dictionary, containing specific keys such as `'title'`, `'content'`, and `'id'`. If the data is not a dictionary, the function does not process it, and the recursion moves back.\n\n2. **Extracting Section Information**: For each valid section dictionary, the function extracts the title and content of the section. If the title and content are present, the function then calculates the depth of the section in the hierarchy. The depth is incremented by 1 for each recursive call, which represents moving one level deeper into the section structure. Additionally, if the section has an `id`, the depth can be recalculated based on the format of the `id`. For example, an `id` like \"4.3.5\" indicates the section is at depth 3, since \"4\" is level 1, \"4.3\" is level 2, and \"4.3.5\" is level 3.\n\n3. **Storing Section Data**: If both the title and content of the section are found, the function stores the section's information in the `sections` list. Each entry in this list includes the title, content, calculated depth, the path of the section (a list of parent sections leading up to this one), and the number of tokens in the content. The `count_tokens` function is called to determine the token count of the section's content, which can be used for various purposes, such as ensuring that sections do not exceed token limits when processed by AI models.\n\n4. **Processing Children Sections**: After handling the current section, the function checks if the section contains any child sections under the `'children'` key. If there are children, the function recursively calls `extract_sections` for each child, passing along the updated path and depth. This allows the function to extract and process nested sections, ensuring that all levels of the hierarchy are traversed and their details are captured.\n\n5. **Path and Depth**: The `path` parameter is used to keep track of the section’s place in the hierarchy. As the function recurses, it appends the current section's title and depth to the path. This helps in maintaining a record of where in the document structure the current section resides. The depth provides insight into how deep the current section is in the hierarchy.\n\n**Note**:\n- The `extract_sections` function relies on the presence of certain keys (`'title'`, `'content'`, `'id'`, and `'children'`) in the section data. If these keys are missing or structured differently, the function may not behave as expected.\n- The function is recursive and can handle deeply nested structures. However, it assumes that the input `data` is a dictionary and that the sections are consistently formatted with the expected fields.\n- The function uses the `count_tokens` function to calculate the number of tokens in each section's content. The `count_tokens` function itself relies on the `tiktoken` library, and the results may vary depending on the model used for tokenization.\n- The `sections` list, which stores the extracted data, is assumed to be defined outside of this function. It holds the processed sections for further use or analysis."
      ],
      "code_start_line": 89,
      "code_end_line": 120,
      "params": [
        "data",
        "path",
        "depth"
      ],
      "have_return": false,
      "code_content": "        def extract_sections(data, path=[], depth=0):\n            \"\"\"递归提取所有 section 的标题、内容、深度和路径\"\"\"\n            if isinstance(data, dict):\n                # 如果是直接的字典对象\n                title = data.get('title', '')\n                content = data.get('content', '')\n                \n                # 获取正确的层级深度\n                current_depth = depth + 1  # 默认深度加1\n                # 如果数据中有明确的id，我们可以从id解析深度\n                section_id = data.get('id', '')\n                if section_id and '.' in section_id:\n                    # 例如 \"4.3.5\" 表示这是第4个主标题下的第3个子标题下的第5个小节\n                    # 那么深度应该是3（4是1级，4.3是2级，4.3.5是3级）\n                    current_depth = len(section_id.split('.'))\n                \n                if title and content:\n                    current_path = path + [{\"title\": title, \"depth\": current_depth}]\n                    sections.append({\n                        \"title\": title,\n                        \"content\": content,\n                        \"depth\": current_depth,\n                        \"path\": current_path.copy(),\n                        \"tokens\": count_tokens(content)\n                    })\n                \n                # 检查是否有 children\n                if 'children' in data and data['children']:\n                    current_path = path + [{\"title\": title, \"depth\": current_depth}]\n                    # 递归处理 children，子节点的深度加1\n                    for child in data['children']:\n                        extract_sections(child, current_path, current_depth)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/utils.py/count_tokens"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "path_sort_key",
      "md_content": [
        "**path_sort_key**: The function of path_sort_key is to generate a sorting key based on the length and titles of a given path in a section.\n\n**parameters**: The parameters of this Function.\n· section: A dictionary containing a key 'path', which is a list of dictionaries, each having a 'title' key.\n\n**Code Description**: The path_sort_key function takes a single parameter, section, which is expected to be a dictionary. This dictionary must contain a key named 'path', which should be a list of dictionaries. Each dictionary in this list must have a 'title' key. The function first calculates the length of the path by determining the number of elements in the section['path'] list. It then constructs a list of titles by extracting the 'title' from each dictionary in the path list. Finally, the function returns a tuple consisting of the path length and a tuple of the titles. This tuple can be used as a sorting key, allowing for comparison based on the length of the path and the lexicographical order of the titles.\n\n**Note**: It is important to ensure that the 'path' key exists in the section dictionary and that it contains a list of dictionaries with 'title' keys. If these conditions are not met, the function may raise an error.\n\n**Output Example**: An example return value of the function could be (3, ('Title A', 'Title B', 'Title C')) if the section['path'] contained three dictionaries with those titles."
      ],
      "code_start_line": 127,
      "code_end_line": 131,
      "params": [
        "section"
      ],
      "have_return": true,
      "code_content": "        def path_sort_key(section):\n            path_len = len(section['path'])\n            # 将路径转换为可比较的字符串序列\n            path_titles = [p[\"title\"] for p in section['path']]\n            return (path_len, tuple(path_titles))\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "format_path_with_depth",
      "md_content": [
        "**format_path_with_depth**: The function of format_path_with_depth is to format a list of path nodes into a string representation that visually indicates the depth of each node.\n\n**parameters**: The parameters of this Function.\n· path_nodes: A list of dictionaries, where each dictionary represents a node containing a \"depth\" key indicating the node's depth level and a \"title\" key representing the node's title.\n\n**Code Description**: The format_path_with_depth function takes a list of path nodes as input. Each node in the list is expected to be a dictionary with two specific keys: \"depth\" and \"title\". The function initializes an empty list called formatted_titles to store the formatted titles of each node. It then iterates over each node in the path_nodes list. For each node, it retrieves the depth and title values. The depth value is used to determine the number of '#' characters to prepend to the title, effectively creating a visual representation of the node's depth in a hierarchical structure. The formatted title is constructed by concatenating the '#' characters, a space, and the title. This formatted title is then appended to the formatted_titles list. After processing all nodes, the function joins the formatted titles using ' -> ' as a separator and returns the resulting string. This output provides a clear and structured representation of the nodes, indicating their relative depths.\n\n**Note**: It is important to ensure that each node in the path_nodes list contains both the \"depth\" and \"title\" keys to avoid KeyError exceptions during execution. The depth value should be a non-negative integer to maintain the intended formatting.\n\n**Output Example**: For an input of path_nodes = [{'depth': 1, 'title': 'Node A'}, {'depth': 2, 'title': 'Node B'}, {'depth': 1, 'title': 'Node C'}], the function would return the string: \"# Node A -> ## Node B -> # Node C\"."
      ],
      "code_start_line": 190,
      "code_end_line": 197,
      "params": [
        "path_nodes"
      ],
      "have_return": true,
      "code_content": "            def format_path_with_depth(path_nodes):\n                formatted_titles = []\n                for node in path_nodes:\n                    depth = node[\"depth\"]\n                    title = node[\"title\"]\n                    formatted_title = '#' * depth + ' ' + title\n                    formatted_titles.append(formatted_title)\n                return ' -> '.join(formatted_titles)\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "run_fact_extraction",
      "md_content": [
        "**run_fact_extraction**: The function of run_fact_extraction is to extract facts from markdown text sections using a parallel processing approach, handling retries for failed attempts.\n\n**parameters**: The parameters of this Function.\n· self: An instance of the class that contains the sections to be processed and the user query.\n\n**Code Description**: The run_fact_extraction function is designed to process each markdown text section stored in self.sections by invoking the fact extraction process. It utilizes a nested function, process_section, which is responsible for handling the extraction for each individual section. The process_section function employs a retry mechanism, allowing up to 10 attempts to successfully extract facts from a given section. \n\nWithin process_section, the attempt function is defined and decorated with the @retry decorator, which manages the retry logic. The attempt function first loads a template for fact extraction, constructs a prompt using the section text and user query, and then calls the common_chat method of the agent to get a response. The response is expected to be a JSON string that can be converted into a list. If the response is empty or cannot be parsed into a list, an exception is raised. If the extraction fails after 10 attempts, a warning is printed, and the section is skipped.\n\nThe main function uses a ThreadPoolExecutor to execute process_section concurrently for all sections in self.sections. The results are collected, and any sections that failed to extract facts after the maximum attempts are filtered out. The function ultimately returns a list of lists, where each inner list contains the fact extraction results for a corresponding section.\n\n**Note**: It is important to ensure that the sections provided in self.sections are valid markdown texts and that the agent is properly configured to handle the fact extraction process. Users should be aware that if a section fails to process after 10 attempts, it will be skipped without any results.\n\n**Output Example**: An example of the return value could be:\n[\n    [\"Fact 1 from section 1\", \"Fact 2 from section 1\"],\n    [\"Fact 1 from section 2\", \"Fact 2 from section 2\", \"Fact 3 from section 2\"],\n    ...\n] \nThis output represents a list where each element corresponds to the extracted facts from each section, with each inner list containing the facts derived from that specific section."
      ],
      "code_start_line": 215,
      "code_end_line": 257,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def run_fact_extraction(self):\n        \"\"\"\n        使用 self.sections 中的每个 markdown 文本调用 fact_extraction，\n        并行执行，返回一个包含各 section 响应的列表。\n        如果对某个 section 10 次尝试后都失败，则发出警告并跳过该 section。\n        \"\"\"\n\n        def process_section(section_text):\n            @retry(stop=stop_after_attempt(10), wait=wait_fixed(1), reraise=True)\n            def attempt():\n                template_str = self.agent.load_template(\"fact_extraction.txt\")\n                data = {\n                    \"wiki_text\": section_text,\n                    \"UserQuery\": self.user_query,\n                }\n                prompt = self.agent.render_template(template_str, data)\n                response = self.agent.common_chat(usr_prompt=prompt)\n                if not isinstance(response, list):\n                    if not response.strip():\n                        raise Exception(\"Empty response received from common_chat\")\n                    try:\n                        candidate = json.loads(response)\n                        print(candidate)\n                        if isinstance(candidate, list):\n                            return candidate\n                    except Exception as e:\n                        raise Exception(\"Section response conversion failed\") from e\n                    raise Exception(\"Section response is not a list\")\n                return response\n\n            try:\n                return attempt()\n            except Exception as e:\n                print(\n                    f\"Warning: Failed to process section after 10 attempts. Skipping this section. Error: {e}\"\n                )\n                return None\n\n        with ThreadPoolExecutor() as executor:\n            raw_results = list(executor.map(process_section, self.sections))\n        # Filter out any sections that failed after 10 attempts\n        results = [result for result in raw_results if result is not None]\n        return results  # 返回 List[List[str]]，每个元素是一个 section 的 fact extraction 结果\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_section",
      "md_content": [
        "**process_section**: The function of process_section is to process a given section of text by attempting to extract relevant facts using a predefined template and the agent's common chat functionality.\n\n**parameters**:  \n· section_text: A string representing the text of the section that needs to be processed.\n\n**Code Description**:  \nThe `process_section` function is responsible for processing a given section of text in order to extract relevant information using a predefined template for fact extraction. \n\n- The function begins by defining an inner function called `attempt`, which is decorated with a retry mechanism. This retry decorator ensures that if the `attempt` function fails, it will automatically retry up to 10 times with a 1-second wait between attempts. The retry behavior is controlled by the `retry` function, which uses the `stop_after_attempt` and `wait_fixed` parameters to set the maximum number of attempts and the fixed waiting time between each attempt. The `reraise=True` argument ensures that exceptions are raised again after retries, allowing for proper error handling.\n  \n- Inside the `attempt` function:\n  - The template for fact extraction is loaded by calling `self.agent.load_template(\"fact_extraction.txt\")`. This template is expected to contain some form of instructions or structure for processing the section text.\n  - The function then prepares the data dictionary with the section text (`section_text`) and a user query (`self.user_query`). This data is passed to the agent’s `render_template` method, which renders the template with the provided data, forming the `prompt`.\n  - The generated `prompt` is then passed to the agent’s `common_chat` method to retrieve a response. This method is responsible for interacting with the underlying system to process the request.\n  \n  - If the response is not a list:\n    - If the response is empty (after stripping whitespace), it raises an exception indicating that an empty response was received from the chat system.\n    - The function attempts to parse the response as a JSON object. If parsing is successful, it checks whether the parsed object is a list. If it is, it returns this list of candidates.\n    - If parsing fails or the parsed object is not a list, the function raises an exception with appropriate error messages.\n  \n- If the `attempt` function succeeds, it returns the response (which is expected to be a list). If an error occurs during the entire process, the main function catches the exception and logs a warning message, indicating the failure after 10 attempts, and then returns `None`.\n\n**Note**:  \n- The function retries the processing of the section up to 10 times. This retry mechanism is particularly useful when dealing with intermittent failures or network issues.\n- The response from the `common_chat` function is expected to be in JSON format, which should be a list. If the format is incorrect or the response is empty, appropriate exceptions are raised to handle such errors.\n- A failure to process the section successfully after 10 attempts is logged as a warning, and the section is skipped, returning `None`.\n\n**Output Example**:  \nThe function will either return a list of extracted facts (e.g., a list of candidate facts) or `None` if it fails to process the section after 10 attempts.\n\nExample of a successful response:\n```json\n[\n    {\"fact\": \"The Eiffel Tower is in Paris.\"},\n    {\"fact\": \"It was completed in 1889.\"}\n]\n```\n\nExample of a failure (when no valid response is returned after retries):\n```text\nWarning: Failed to process section after 10 attempts. Skipping this section. Error: <Error details>\n```"
      ],
      "code_start_line": 222,
      "code_end_line": 251,
      "params": [
        "section_text"
      ],
      "have_return": true,
      "code_content": "        def process_section(section_text):\n            @retry(stop=stop_after_attempt(10), wait=wait_fixed(1), reraise=True)\n            def attempt():\n                template_str = self.agent.load_template(\"fact_extraction.txt\")\n                data = {\n                    \"wiki_text\": section_text,\n                    \"UserQuery\": self.user_query,\n                }\n                prompt = self.agent.render_template(template_str, data)\n                response = self.agent.common_chat(usr_prompt=prompt)\n                if not isinstance(response, list):\n                    if not response.strip():\n                        raise Exception(\"Empty response received from common_chat\")\n                    try:\n                        candidate = json.loads(response)\n                        print(candidate)\n                        if isinstance(candidate, list):\n                            return candidate\n                    except Exception as e:\n                        raise Exception(\"Section response conversion failed\") from e\n                    raise Exception(\"Section response is not a list\")\n                return response\n\n            try:\n                return attempt()\n            except Exception as e:\n                print(\n                    f\"Warning: Failed to process section after 10 attempts. Skipping this section. Error: {e}\"\n                )\n                return None\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "attempt",
      "md_content": [
        "**attempt**: The function of attempt is to execute the process of fact extraction by generating a prompt, sending it to a conversational model, and handling the response.\n\n**parameters**: The parameters of this Function.\n· section_text: The text of the section from which facts are to be extracted. This variable is used to provide context for the prompt generated for the conversational model.\n· self: Refers to the instance of the class that contains the attempt method, providing access to its attributes and methods.\n\n**Code Description**: The attempt function is designed to facilitate the extraction of facts from a specified section of text. It begins by loading a template for fact extraction using the load_template method from the BaseAgent class, which retrieves the content of a template file named \"fact_extraction.txt\". This template serves as a foundation for generating a prompt that will be sent to a conversational model.\n\nNext, the function constructs a data dictionary containing the section_text and the user_query attributes from the instance. This dictionary is then used to render the template into a complete prompt using the render_template method. The rendered prompt is intended to provide the conversational model with the necessary context to generate a relevant response.\n\nThe function then calls the common_chat method, passing the rendered prompt as the usr_prompt parameter. This method is responsible for sending the prompt to the conversational model and receiving the generated response. The response is expected to be a list of candidate facts. \n\nAfter receiving the response, the function checks if the response is a list. If the response is not a list, the function performs several checks: it raises an exception if the response is empty or attempts to parse the response as JSON. If the parsing fails, it raises an exception indicating that the section response conversion failed. If the response is successfully parsed but is not a list, another exception is raised to indicate that the response is not in the expected format.\n\nIn summary, the attempt function integrates several methods from the BaseAgent class to load templates, render prompts, and communicate with a conversational model, ensuring that the fact extraction process is executed smoothly and that appropriate error handling is in place.\n\n**Note**: It is essential to ensure that the section_text variable is properly defined and that the template file \"fact_extraction.txt\" exists in the prompts directory. Failure to do so may result in exceptions being raised during execution. Additionally, the response from the common_chat method must be carefully validated to ensure it meets the expected format.\n\n**Output Example**: A possible return value from the attempt function could be a list of extracted facts such as:\n```\n[\n    {\"fact\": \"The capital of France is Paris.\"},\n    {\"fact\": \"The largest planet in our solar system is Jupiter.\"}\n]\n```"
      ],
      "code_start_line": 224,
      "code_end_line": 243,
      "params": [],
      "have_return": true,
      "code_content": "            def attempt():\n                template_str = self.agent.load_template(\"fact_extraction.txt\")\n                data = {\n                    \"wiki_text\": section_text,\n                    \"UserQuery\": self.user_query,\n                }\n                prompt = self.agent.render_template(template_str, data)\n                response = self.agent.common_chat(usr_prompt=prompt)\n                if not isinstance(response, list):\n                    if not response.strip():\n                        raise Exception(\"Empty response received from common_chat\")\n                    try:\n                        candidate = json.loads(response)\n                        print(candidate)\n                        if isinstance(candidate, list):\n                            return candidate\n                    except Exception as e:\n                        raise Exception(\"Section response conversion failed\") from e\n                    raise Exception(\"Section response is not a list\")\n                return response\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "run_factualqa",
      "md_content": [
        "## Function Documentation: `run_factualqa`\n\n### Purpose:\nThe `run_factualqa` function is responsible for executing the FactualQA evaluation. It loads a template for FactualQA, populates it with relevant data (such as a user query and ground truth values), and then interacts with a chat system to generate a response based on the template.\n\n### Parameters:\nThis function does not accept any parameters.\n\n### Method Overview:\n1. **Template Loading**: The function begins by loading a template file named `\"factual_qa.txt\"` using the `load_template` method from the `agent` object.\n2. **Data Preparation**: It prepares a dictionary containing the following key-value pairs:\n   - `Query`: This is the user query, fetched from the `user_query` attribute.\n   - `BreadthGT`: The breadth ground truth, converted to a JSON string using `json.dumps`.\n   - `DepthGT`: The depth ground truth, directly retrieved from the `depth_gt` attribute.\n3. **Template Rendering**: The template loaded in step 1 is then rendered with the data dictionary using the `render_template` method.\n4. **Chat Interaction**: The rendered template is passed to the `common_chat` method, which processes the prompt and returns a response.\n5. **Return Value**: The function returns the response generated from the chat interaction.\n\n### Key Functions Involved:\n- **`load_template`**: Loads the template file `\"factual_qa.txt\"` from the predefined prompts directory.\n- **`render_template`**: Renders the loaded template by replacing placeholders with values from the data dictionary.\n- **`common_chat`**: Interacts with the chat system using the rendered template and returns the system's response.\n\n### Return:\nThe function returns the response generated from the `common_chat` method, which is the result of processing the prompt for the FactualQA evaluation.\n\n### Example:\n```python\nresponse = run_factualqa()\n```\n\nIn this example, `response` will contain the output from the chat system after the factual QA evaluation. The response will be based on the user query, breadth ground truth, and depth ground truth provided in the function."
      ],
      "code_start_line": 259,
      "code_end_line": 270,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def run_factualqa(self):\n        # Load and render a template \"factual_qa.txt\" for FactualQA evaluation.\n        # Pass Query, BreadthGT (converted to JSON string) and DepthGT.\n        template_str = self.agent.load_template(\"factual_qa.txt\")\n        data = {\n            \"Query\": self.user_query,  # Updated from self.query to self.user_query\n            \"BreadthGT\": json.dumps(self.breadth_gt),\n            \"DepthGT\": self.depth_gt,\n        }\n        prompt = self.agent.render_template(template_str, data)\n        response = self.agent.common_chat(usr_prompt=prompt)\n        return response\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "process_window_content",
      "md_content": [
        "**process_window_content**: The function of process_window_content is to process the content of a single window, retrying if the result is empty.\n\n**parameters**: The parameters of this Function.\n· content: A string representing the content of the window to be processed.\n· max_retries: An integer specifying the maximum number of retry attempts if the result is empty. Default is 10.\n\n**Code Description**: The process_window_content function is designed to handle the processing of a specific content window by invoking the process_section method to extract relevant data. It operates within a retry mechanism, allowing for multiple attempts to process the content in case the initial attempts yield no results. \n\nUpon invocation, the function iterates up to max_retries times. In each iteration, it attempts to process the provided content by calling the process_section method. If process_section returns a non-empty result, the function then calls parse_tagged_data_to_table to convert the extracted data into a structured table format. If parse_tagged_data_to_table successfully returns parsed data, this data is returned as the output of process_window_content.\n\nIf the result from process_section is empty, the function logs a message indicating the attempt number and continues to retry until the maximum number of attempts is reached. If all attempts fail, the function returns an empty list to indicate that no valid data could be extracted from the content.\n\nThis function is called by generate_benchmark_item, which is responsible for generating benchmark items with caching support. Within generate_benchmark_item, process_window_content is called for each window of content that is prepared for extraction. The results from process_window_content are collected and returned as part of the final results, which may also be cached for future use.\n\n**Note**: The function relies on the successful execution of both process_section and parse_tagged_data_to_table. If either of these functions encounters an error or returns an empty result, process_window_content will continue to retry until the specified limit is reached. The retry mechanism is crucial for handling transient issues that may arise during data extraction.\n\n**Output Example**: A possible return value from process_window_content could be a list of parsed data entries, such as:\n```python\n[\n    {\"question\": \"What is the capital of France?\", \"format\": \"Geography\", \"answer\": \"\\\\boxed{Paris}\"},\n    {\"question\": \"What is the largest planet?\", \"format\": \"Astronomy\", \"answer\": \"\\\\boxed{Jupiter}\"}\n]\n```  \nIf no valid data is extracted after all retry attempts, the function will return an empty list:\n```python\n[]\n```"
      ],
      "code_start_line": 272,
      "code_end_line": 284,
      "params": [
        "self",
        "content",
        "max_retries"
      ],
      "have_return": true,
      "code_content": "    def process_window_content(self, content, max_retries=10):\n        \"\"\"处理单个窗口内容,如果结果为空则重试\"\"\"\n        for attempt in range(max_retries):\n            try:\n                result = self.process_section(content)\n                if result:\n                    parsed_data = self.parse_tagged_data_to_table(result)\n                    if parsed_data:  # 如果解析出的数据不为空\n                        return parsed_data\n                print(f\"Attempt {attempt + 1}: Empty result, retrying...\")\n            except Exception as e:\n                print(f\"Attempt {attempt + 1} failed with error: {e}\")\n        return []  # 如果所有尝试都失败,返回空列表\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/generate_benchmark_item"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_section",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/parse_tagged_data_to_table"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "generate_benchmark_item",
      "md_content": [
        "**generate_benchmark_item**: The function of generate_benchmark_item is to generate benchmark items with optional caching support.\n\n**parameters**: The parameters of this Function.\n· use_cache: A boolean indicating whether to use cached results. Default is True.\n· max_window_tokens: An integer specifying the maximum number of tokens allowed in each window. Default is 300.\n\n**Code Description**: The generate_benchmark_item function is designed to create benchmark items by processing content windows derived from a sliding window approach. It first checks if caching is enabled and attempts to load results from a cache using the _load_from_cache method. If valid cached results are found, they are returned immediately, which optimizes performance by avoiding redundant computations.\n\nIf no cached results are available or if caching is disabled, the function proceeds to generate new benchmark items. It utilizes the sliding_window_pairing method to create windows of content, ensuring that the content does not exceed the specified token limit defined by max_window_tokens. Each window contains merged section content and associated metadata, such as the path of the sections.\n\nFor each window, the function prepares the content for extraction by collecting the merged section content and its corresponding path. It then processes this content using the process_window_content method, which incorporates a retry mechanism to handle potential transient failures during data extraction. If the processing yields valid results, these are compiled into a final results list that includes the path, merged content, and extracted facts.\n\nOnce the new benchmark items are generated, if caching is enabled, the results are saved to the cache using the _save_to_cache method. This ensures that future calls to generate_benchmark_item can benefit from the cached data, improving efficiency.\n\nThe generate_benchmark_item function is called by the process_single_task function, which is responsible for initializing the benchmark generation process as part of a larger task management system. This integration allows for seamless generation and retrieval of benchmark items within the context of processing user-defined tasks.\n\n**Note**: It is essential to ensure that the cache directory is properly initialized and that the cache key generation method (_get_cache_key) functions correctly for caching to operate as intended. Additionally, the sliding_window_pairing method must be correctly implemented to ensure that content windows are generated accurately within the specified token limits.\n\n**Output Example**: A possible return value from the generate_benchmark_item function could be a list of dictionaries, each representing a benchmark item, such as:\n\n```json\n[\n    {\n        \"path\": \"/path/to/section\",\n        \"merged_section_window_content\": \"Example content\",\n        \"extracted_facts\": [{\"fact1\": \"value1\"}, {\"fact2\": \"value2\"}]\n    }\n]\n```"
      ],
      "code_start_line": 286,
      "code_end_line": 320,
      "params": [
        "self",
        "use_cache",
        "max_window_tokens"
      ],
      "have_return": true,
      "code_content": "    def generate_benchmark_item(self, use_cache=True, max_window_tokens=300):\n        \"\"\"添加缓存支持的基准测试项生成方法\"\"\"\n        if use_cache:\n            cached_results = self._load_from_cache()\n            if cached_results is not None:\n                print(\"Loading results from cache...\")\n                return cached_results\n        \n        # 原有的生成逻辑\n        results = []\n        windows = self.sliding_window_pairing(max_token_length=max_window_tokens)\n        \n        # 准备抽取任务的输入\n        window_contents = []\n        window_paths = []\n        for window in windows:\n            window_contents.append(window[\"merged_section_window_content\"])\n            window_paths.append(window[\"section_window_path_text\"])\n        \n        # 组合抽取结果和路径信息\n        final_results = []\n        for path, content in zip(window_paths, window_contents):\n            parsed_data = self.process_window_content(content)\n            if parsed_data:\n                final_results.append({\n                    \"path\": path,\n                    \"merged_section_window_content\": content,\n                    \"extracted_facts\": parsed_data\n                })\n        \n        # 保存结果到缓存\n        if use_cache:\n            self._save_to_cache(final_results)\n            \n        return final_results\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_load_from_cache",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/_save_to_cache",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/sliding_window_pairing",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_window_content"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "process_section",
      "md_content": [
        "**process_section**: The function of process_section is to process a given section of text and return a list of candidate data after extracting relevant information.\n\n**parameters**:  \n· section_text: A string containing the text of a specific section to be processed.  \n\n**Code Description**:  \nThe `process_section` function is designed to handle the extraction of relevant data from a provided section of text, utilizing a template-based approach. It attempts to load a predefined template (`fact_extraction.txt`), which is used to generate a prompt for an agent. The function then renders the template with the provided `section_text` and a `user_query` parameter, sending the generated prompt to the agent for processing. The agent’s response is expected to be a list, which the function returns as the result.\n\nThe function is wrapped in a retry mechanism, where it will attempt to process the section up to 10 times, with a 1-second delay between each attempt. If any error occurs during the process (such as an empty or incorrectly formatted response from the agent), the function will raise an exception and retry. If the agent's response is not a list, or if the response cannot be parsed as JSON, the function will throw an exception, triggering a retry. If the retry attempts fail, a warning is logged, and the function returns `None` to indicate the failure to process the section.\n\nThis function is invoked by other components in the system, specifically within the `process_window_content` function. In this case, `process_section` is called to process a single section of content. If the result is non-empty, the data is further parsed into a table format; otherwise, the system retries up to a predefined number of attempts. If all attempts fail, it returns an empty list.\n\n**Note**:  \n- The function relies on external dependencies such as `self.agent`, which is responsible for loading templates, rendering them, and handling communication with an agent system.\n- The function assumes the agent will return a valid JSON response formatted as a list. If the response cannot be parsed or is empty, an exception will be raised.\n- The retry mechanism ensures that temporary issues with the agent or network do not lead to an immediate failure, offering multiple attempts before logging a failure message.\n\n**Output Example**:  \nThe output is expected to be a list of candidate data, which could look like the following:\n```json\n[\n    {\"fact\": \"Fact 1\", \"confidence\": 0.95},\n    {\"fact\": \"Fact 2\", \"confidence\": 0.89}\n]\n```\nIf the response is empty or not properly formatted, the function will return `None`."
      ],
      "code_start_line": 322,
      "code_end_line": 352,
      "params": [
        "self",
        "section_text"
      ],
      "have_return": true,
      "code_content": "    def process_section(self, section_text):\n        \"\"\"将原来run_fact_extraction中的process_section逻辑移到单独的方法\"\"\"\n        @retry(stop=stop_after_attempt(10), wait=wait_fixed(1), reraise=True)\n        def attempt():\n            template_str = self.agent.load_template(\"fact_extraction.txt\")\n            data = {\n                \"wiki_text\": section_text,\n                \"UserQuery\": self.user_query,\n            }\n            prompt = self.agent.render_template(template_str, data)\n            response = self.agent.common_chat(usr_prompt=prompt)\n            if not isinstance(response, list):\n                if not response.strip():\n                    raise Exception(\"Empty response received from common_chat\")\n                try:\n                    candidate = json.loads(response)\n                    print(candidate)\n                    if isinstance(candidate, list):\n                        return candidate\n                except Exception as e:\n                    raise Exception(\"Section response conversion failed\") from e\n                raise Exception(\"Section response is not a list\")\n            return response\n\n        try:\n            return attempt()\n        except Exception as e:\n            print(\n                f\"Warning: Failed to process section after 10 attempts. Skipping this section. Error: {e}\"\n            )\n            return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_window_content"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "attempt",
      "md_content": [
        "### Function: `attempt`\n\n#### Description:\nThe `attempt` function is responsible for generating a response based on the interaction with the agent's prompt system. It retrieves a template, processes the relevant data, and engages with a common chat system to obtain a response. The function ensures that the response is valid and in the correct format (list). If any errors occur during the response handling, they are raised with detailed exceptions.\n\n#### Parameters:\nThe function does not accept any parameters directly. However, it utilizes the following objects:\n- `self.agent`: An instance of a class (likely `BaseAgent`), which provides methods for interacting with templates and the chat system.\n- `section_text`: The text content of the section, which is used as part of the prompt data.\n- `self.user_query`: The query provided by the user, also included in the data for the prompt.\n\n#### Function Flow:\n1. **Load Template**:  \n   The function begins by loading a template string using the method `self.agent.load_template(\"fact_extraction.txt\")`. This template is crucial for rendering the prompt used in the agent's response generation.\n   \n2. **Data Preparation**:  \n   A dictionary, `data`, is created containing the `wiki_text` (which is `section_text`) and the `UserQuery` (which is `self.user_query`). This dictionary is used to populate the template during the rendering process.\n\n3. **Render Template**:  \n   The template string is rendered by calling `self.agent.render_template(template_str, data)`, where `template_str` is the loaded template and `data` is the dictionary of relevant data.\n\n4. **Generate Response**:  \n   After rendering the template, the function invokes the `self.agent.common_chat(usr_prompt=prompt)` method to send the generated prompt to the chat system. The resulting response is stored in the `response` variable.\n\n5. **Response Validation**:  \n   - If the response is not a list, the function performs a series of checks:\n     - **Empty Response Handling**: If the response is an empty string, an exception is raised indicating that the response from the chat system was empty.\n     - **JSON Parsing**: The function attempts to parse the response as JSON using `json.loads(response)`. If the parsing fails, an exception is raised to indicate that the conversion of the response has failed.\n     - **Check for List**: If the parsed response is a list, it is returned as the result of the function.\n     - **Exception Handling**: If the response is neither a valid list nor a valid JSON object, an exception is raised, indicating that the response is not in the expected format.\n   \n6. **Return Response**:  \n   If the response is already in the expected list format, it is returned directly.\n\n#### Exception Handling:\n- **Empty Response**: An exception is raised if the response is an empty string.\n- **JSON Parsing Failure**: If the response cannot be parsed as JSON, an exception is raised with a relevant error message.\n- **Invalid Response Format**: If the response is not a list or valid JSON, an exception is raised to ensure proper error reporting.\n\n#### Example Usage:\n```python\nresult = attempt()  # Invokes the attempt function, processes the prompt, and returns the response.\n```\n\n#### Output:\nThe function returns a list containing the processed response from the chat system, provided that the response is valid and in the correct format.\n\nIf the response is invalid, an exception is raised with detailed information regarding the failure."
      ],
      "code_start_line": 325,
      "code_end_line": 344,
      "params": [],
      "have_return": true,
      "code_content": "        def attempt():\n            template_str = self.agent.load_template(\"fact_extraction.txt\")\n            data = {\n                \"wiki_text\": section_text,\n                \"UserQuery\": self.user_query,\n            }\n            prompt = self.agent.render_template(template_str, data)\n            response = self.agent.common_chat(usr_prompt=prompt)\n            if not isinstance(response, list):\n                if not response.strip():\n                    raise Exception(\"Empty response received from common_chat\")\n                try:\n                    candidate = json.loads(response)\n                    print(candidate)\n                    if isinstance(candidate, list):\n                        return candidate\n                except Exception as e:\n                    raise Exception(\"Section response conversion failed\") from e\n                raise Exception(\"Section response is not a list\")\n            return response\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/load_template",
        "src/criticsearch/base_agent.py/BaseAgent/render_template",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_0(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_1(name_duplicate_version)",
        "src/criticsearch/base_agent.py/BaseAgent/common_chat_2(name_duplicate_version)"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "parse_tagged_data_to_table",
      "md_content": [
        "**parse_tagged_data_to_table**: The function of parse_tagged_data_to_table is to process a list of entries containing tagged data, extract relevant information, and return a filtered collection of parsed data based on specific patterns.\n\n**parameters**:\n· entries: A list of strings representing entries, each containing tagged data.\n· csv_path: An optional parameter that specifies the path to a CSV file where the results could be stored. Default is None.\n\n**Code Description**:  \nThe `parse_tagged_data_to_table` function processes a list of entries where each entry contains tagged data in the form of XML-like tags. The function extracts specific pieces of information from each entry, such as the question, format description, and answer, by searching for the corresponding tags. For each entry, it performs the following steps:\n\n1. It uses regular expressions to extract the text between the `<question>` and `</question>` tags and assigns it to the `question` variable. If no match is found, it defaults to an empty string.\n2. Similarly, it extracts the content between the `<constrained_format>` and `</constrained_format>` tags, storing the result in the `format_desc` variable.\n3. The answer is extracted in the same manner, using the `<answer>` and `</answer>` tags.\n4. Additionally, the function checks if the answer contains a LaTeX-formatted boxed content (indicated by the pattern `\\boxed{...}`). If a match is found, the function adds the corresponding data (question, format description, and answer) to the `parsed_data` list.\n5. Finally, it returns the `parsed_data` list, which contains all the entries that have a boxed answer.\n\nThe `parse_tagged_data_to_table` function is used by the `process_window_content` method, which handles the processing of a single content window and retries the process in case of empty results. When `process_window_content` calls `parse_tagged_data_to_table`, it passes the extracted content (a list of entries) to the function. If the function successfully parses the entries and finds non-empty results, it returns the parsed data for further processing.\n\n**Note**:  \n- The function relies on regular expressions to extract specific patterns from the tagged data. If the expected tags are not properly formatted or missing, the function will return an empty string or skip processing for that entry.\n- The `csv_path` parameter is not currently used within the function, but it can be useful if the caller intends to save the parsed data to a CSV file in the future.\n- The boxed answer (`\\boxed{...}`) is a key criterion for including an entry in the final parsed data. If no boxed content is found, the entry will be excluded.\n\n**Output Example**:  \nFor a list of entries where one of them includes a boxed answer, the return value could look like the following:\n\n```python\n[\n    {\"question\": \"What is 2+2?\", \"format\": \"Basic Arithmetic\", \"answer\": \"4\"},\n    {\"question\": \"What is the square root of 16?\", \"format\": \"Mathematical Expression\", \"answer\": \"\\\\boxed{4}\"}\n]\n```  \n\nThis example shows the format of the returned data, where the second entry includes a boxed answer and thus is included in the parsed results."
      ],
      "code_start_line": 354,
      "code_end_line": 378,
      "params": [
        "self",
        "entries",
        "csv_path"
      ],
      "have_return": true,
      "code_content": "    def parse_tagged_data_to_table(self, entries, csv_path=None):\n        parsed_data = []\n        for entry in entries:\n            # Extract question\n            question_match = re.search(r\"</question>(.*?)</question>\", entry)\n            question = question_match.group(1).strip() if question_match else \"\"\n\n            # Extract format description\n            format_match = re.search(\n                r\"</constrained_format>(.*?)</constrained_format>\", entry\n            )\n            format_desc = format_match.group(1).strip() if format_match else \"\"\n\n            # Extract answer\n            answer_match = re.search(r\"</answer>(.*?)</answer>\", entry)\n            answer = answer_match.group(1).strip() if answer_match else \"\"\n\n            # 验证answer中是否包含\\boxed{...}格式的内容\n            boxed_match = re.search(r\"\\\\boxed{([^}]+)}\", answer)\n            if boxed_match:  # 只有当匹配到boxed内容时才添加到结果中\n                parsed_data.append(\n                    {\"question\": question, \"format\": format_desc, \"answer\": answer}\n                )\n\n        return parsed_data\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/process_window_content"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "verify_extraction_meaningful",
      "md_content": [
        "**verify_extraction_meaningful**: The function of verify_extraction_meaningful is to check if the fact extraction result is meaningful enough and correct.\n\n**parameters**:  \nThis function does not take any parameters.\n\n**Code Description**:  \nThe `verify_extraction_meaningful` function is designed to assess the quality and correctness of fact extraction results. However, the function body is currently empty (`pass` statement), indicating that the actual logic to perform the check has not yet been implemented. Based on the function name, it is intended to verify whether the extracted facts hold enough significance and accuracy, potentially ensuring that the results are useful and reliable for further processing or analysis.  \n\n**Note**:  \nAs it stands, this function serves as a placeholder for the functionality that will eventually assess the meaningfulness of extracted facts. Any future implementation would need to include a method for evaluating the extraction results, potentially comparing them to expected outcomes or verifying their relevance to the context in which they are being used."
      ],
      "code_start_line": 380,
      "code_end_line": 382,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def verify_extraction_meaningful(self):\n        # Check if the fact extraction result is meaningful enough and correct.\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/reportbench/process_search_result.py": [
    {
      "type": "FunctionDef",
      "name": "filter_results",
      "md_content": [
        "**filter_results**: The function of filter_results is to filter out results that do not have raw content in the provided data.\n\n**parameters**: \n· data: A dictionary containing the key \"results\", which is expected to hold a list of result objects.\n\n**Code Description**: The function `filter_results` is designed to process a given dictionary called `data`. Specifically, it filters the list under the key \"results\" by removing any items (results) that do not contain the key \"raw_content\". If a result does not have \"raw_content\" or has it as a falsy value (e.g., None, empty string), it is excluded from the results list. The filtered data is then returned in the same dictionary format. If the \"results\" key does not exist in the input data, the function ensures that an empty list is used instead, effectively preventing errors during processing.\n\nHere is a step-by-step breakdown:\n1. The function first accesses the value associated with the \"results\" key in the input dictionary `data`.\n2. It then applies a list comprehension to iterate over each item (`r`) in this list.\n3. For each item, it checks whether the item contains the key \"raw_content\" and whether it holds a truthy value.\n4. If \"raw_content\" exists and has a truthy value, the item is kept in the list.\n5. Finally, the filtered list replaces the existing \"results\" key in the `data` dictionary.\n6. The updated `data` dictionary is returned.\n\n**Note**: \n- The function does not handle situations where the input `data` does not contain the \"results\" key at all. In this case, it will safely return the original data with an empty list under the \"results\" key.\n- The function assumes that the \"results\" key, when present, always holds a list, which is a standard format for handling multiple results.\n\n**Output Example**: \nGiven the following input data:\n```python\n{\n    \"results\": [\n        {\"raw_content\": \"valid content\"},\n        {\"raw_content\": None},\n        {\"raw_content\": \"another valid content\"}\n    ]\n}\n```\n\nThe output after calling `filter_results` would be:\n```python\n{\n    \"results\": [\n        {\"raw_content\": \"valid content\"},\n        {\"raw_content\": \"another valid content\"}\n    ]\n}\n```"
      ],
      "code_start_line": 7,
      "code_end_line": 9,
      "params": [
        "data"
      ],
      "have_return": true,
      "code_content": "def filter_results(data):\n    data[\"results\"] = [r for r in data.get(\"results\", []) if r.get(\"raw_content\")]\n    return data\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "generate_markdown",
      "md_content": [
        "**generate_markdown**: The function of generate_markdown is to generate a markdown formatted string based on input data containing images and search results.\n\n**parameters**:\n· parameter1: data (dict) - A dictionary containing the data with two possible keys: \"images\" and \"results\". The \"images\" key should contain a list of image objects, and the \"results\" key should contain a list of search result objects.\n\n**Code Description**: \nThe `generate_markdown` function is responsible for producing a markdown-formatted string based on the given input data. The function is structured to create two main sections: one for images and another for search results. It uses the input data, which is expected to be a dictionary, and processes the \"images\" and \"results\" lists within the dictionary to construct markdown content.\n\n1. **Images Section**: \n   The function first initializes an empty list, `lines`, to store the markdown content. It then checks if the input dictionary has an \"images\" key, and retrieves the associated list. For each image in this list, the function adds a description (or \"N/A\" if no description is found) and the URL (or \"N/A\" if no URL is found) to the `lines` list. The images are enumerated starting from 1, with each entry formatted as follows:\n   - `[idx] DESCRIPTION: {description}`\n   - `URL: {url}`\n   \n2. **Search Results Section**: \n   After handling the images, the function proceeds to process the \"results\" key, if present. It iterates over the list of search results, adding each result's title (or \"N/A\"), URL (or \"N/A\"), and raw content (or \"N/A\") to the markdown string. Each result is also enumerated starting from 1, and the entries are formatted as follows:\n   - `[idx]: TITLE: {title}`\n   - `URL: {url}`\n   - `CONTENT: {raw_content}`\n   \n3. **Return Value**: \n   Once both the images and search results sections have been processed, the function joins all lines in the `lines` list with newline characters and returns the final string. This string contains a well-structured markdown representation of the input data.\n\n**Note**:\n- The function assumes that the \"images\" and \"results\" keys in the input data are lists of dictionaries.\n- If the \"description\", \"url\", \"title\", \"raw_content\" are not provided for an item, the function defaults to \"N/A\".\n- The function does not handle any cases where the input data structure deviates from the expected format (i.e., the presence of \"images\" and \"results\" keys).\n- The markdown string generated can be directly used in markdown viewers or other systems that support markdown formatting.\n\n**Output Example**:\n\n```markdown\n# images \n\n[1] DESCRIPTION: Sunset over mountains\nURL: http://example.com/sunset.jpg\n\n[2] DESCRIPTION: N/A\nURL: http://example.com/placeholder.jpg\n\n# Search Result\n\n[1]: TITLE: Exploring the beauty of nature\nURL: http://example.com/nature-article\nCONTENT: This article dives deep into the beauty of nature and its impact on our lives.\n\n[2]: TITLE: N/A\nURL: http://example.com/empty-article\nCONTENT: N/A\n```"
      ],
      "code_start_line": 12,
      "code_end_line": 31,
      "params": [
        "data"
      ],
      "have_return": true,
      "code_content": "def generate_markdown(data):\n    lines = []\n    # Images section\n    lines.append(\"# images \\n\")\n    images = data.get(\"images\", [])\n    for idx, image in enumerate(images, start=1):\n        desc = image.get(\"description\") or \"N/A\"\n        url = image.get(\"url\") or \"N/A\"\n        lines.append(f\"[{idx}] DESCRIPTION: {desc}\")\n        lines.append(f\"URL: {url}\\n\")\n    # Search Results section\n    lines.append(\"# Search Result\\n\")\n    for idx, result in enumerate(data.get(\"results\", []), start=1):\n        title = result.get(\"title\") or \"N/A\"\n        url = result.get(\"url\") or \"N/A\"\n        raw = result.get(\"raw_content\") or \"N/A\"\n        lines.append(f\"[{idx}]:TITLE: {title}\")\n        lines.append(f\"URL: {url}\")\n        lines.append(f\"CONTENT: {raw}\\n\")\n    return \"\\n\".join(lines)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/reportbench/verifier.py": [
    {
      "type": "ClassDef",
      "name": "ReportVerifier",
      "md_content": [
        "# ReportVerifier Class Documentation\n\n## Overview\nThe `ReportVerifier` class is designed to perform factual verification of extracted data based on a provided context. It verifies whether the answers extracted from a report or document match the expected answers to specific questions. The class utilizes the Rouge-L scoring metric for partial matches and supports multi-threaded verification of multiple questions for efficiency.\n\n### Key Features:\n- **Factual Question Verification**: Verifies the correctness of answers to factual questions.\n- **Answer Comparison**: Compares model-generated answers with ground truth using exact matches and Rouge-L score for partial matches.\n- **Parallel Processing**: Uses a thread pool to process multiple questions concurrently, improving efficiency.\n- **Answer Normalization**: Standardizes answers by removing non-alphanumeric characters and converting text to lowercase.\n\n## Constructor: `__init__(self, agent)`\n### Parameters:\n- `agent`: The agent used to interact with the external source (e.g., a model or API) for verifying answers.\n\n### Description:\nThe constructor initializes the `ReportVerifier` with an agent for interacting with external systems and a `RougeScorer` to calculate Rouge-L scores for partial answer matches. \n\n---\n\n## Methods\n\n### 1. `verify_section(self, context: str, extracted_facts: List[Dict]) -> float`\n#### Parameters:\n- `context`: A string representing the context (e.g., document or passage) that is used to verify the extracted facts.\n- `extracted_facts`: A list of dictionaries, where each dictionary contains a question (`\"question\"`), the expected answer (`\"answer\"`), and the format in which the answer should be.\n\n#### Returns:\n- A float representing the final accuracy score for the verification process.\n\n#### Description:\nThis method verifies a list of factual questions using the provided context. Each question is matched against the extracted answers. For each question, it sends the question to an agent for verification, compares the model's answer with the ground truth, and computes the accuracy. The verification process is executed in parallel using a thread pool for efficiency.\n\n- For each extracted fact, the `verify_single_question` helper function is used to verify the question by interacting with the agent.\n- After all questions are verified, the method calculates the accuracy by considering exact matches and Rouge-L scores for partial matches.\n- The final accuracy is a weighted score (70% exact match and 30% Rouge-L score).\n\n---\n\n### 2. `_normalize_text(self, text: str) -> str`\n#### Parameters:\n- `text`: The string to be normalized.\n\n#### Returns:\n- A normalized string containing only alphanumeric characters, converted to lowercase, with spaces removed.\n\n#### Description:\nThis method standardizes the input text by:\n- Removing all non-alphanumeric characters.\n- Converting the text to lowercase.\n- This is particularly useful for comparing answers while ignoring irrelevant formatting differences.\n\n---\n\n### 3. `_check_answer(self, model_answer: str, ground_truth: str) -> tuple\n#### Parameters:\n- `model_answer`: The answer generated by the model or system.\n- `ground_truth`: The expected correct answer.\n\n#### Returns:\n- A tuple `(is_correct, rouge_score)`, where:\n  - `is_correct` is a boolean indicating whether the answer is an exact match.\n  - `rouge_score` is the Rouge-L score for the answer, if the match is partial.\n\n#### Description:\nThis method compares the model’s answer to the ground truth:\n- It extracts the boxed answers from both the model and the ground truth, if present.\n- The answers are normalized (non-alphanumeric characters are removed, and text is converted to lowercase) for comparison.\n- The method checks if the answers are an exact match. If not, it computes the Rouge-L score for partial matches.\n- The method also prints detailed information about the comparison, showing both the original and normalized answers, and indicating whether the match was exact or partial.\n\n---\n\n### 4. `_calculate_score(self, results: List, total: int) -> float`\n#### Parameters:\n- `results`: A list of tuples containing the verification results, where each tuple consists of a boolean indicating if the answer was correct and the Rouge-L score for partial matches.\n- `total`: The total number of questions verified.\n\n#### Returns:\n- A float representing the final weighted accuracy score.\n\n#### Description:\nThis method calculates the final accuracy score:\n- It counts the exact matches and computes the average Rouge-L score for partial matches.\n- The final accuracy is a weighted sum of the exact matches (70%) and the average Rouge-L score (30%).\n- The method prints a summary of the verification results, including the number of exact matches, average Rouge-L score for partial matches, and the final weighted score.\n\n---\n\n## Example Usage\n\n```python\n# Example of initializing the ReportVerifier class and verifying a section\ncommon_agent = BaseAgent()  # Assuming BaseAgent is defined elsewhere\nverifier = ReportVerifier(common_agent)\n\ncontext = \"The context of the document or report.\"\nextracted_facts = [\n    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\", \"format\": \"text\"},\n    {\"question\": \"Who wrote Hamlet?\", \"answer\": \"Shakespeare\", \"format\": \"text\"}\n]\n\naccuracy = verifier.verify_section(context, extracted_facts)\nprint(f\"Final Verification Accuracy: {accuracy:.2%}\")\n```\n\n---\n\n## Notes:\n- The `verify_section` method performs concurrent verification of multiple facts using a thread pool with a maximum of 20 workers.\n- The Rouge-L score is used to assess partial matches between the model's answer and the ground truth. This score measures the overlap of the longest common subsequences between the answers.\n- The method prints detailed logs during the verification process, including the comparison results for each question."
      ],
      "code_start_line": 8,
      "code_end_line": 112,
      "params": [],
      "have_return": true,
      "code_content": "class ReportVerifier:\n    def __init__(self, agent):\n        self.agent = agent\n        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n        \n    def verify_section(self, context: str, extracted_facts: List[Dict]) -> float:\n        print(\"\\n=== Starting Factual QA Verification ===\")\n        print(f\"Total questions to verify: {len(extracted_facts)}\\n\")\n\n        def verify_single_question(fact):\n            qa_data = {\n                \"context\": context,\n                \"user_question\": fact[\"question\"],\n                \"constrained_format\": fact[\"format\"]\n            }\n            \n            print(f\"\\nVerifying Question: {fact['question']}\")\n            print(f\"Format: {fact['format']}\")\n            \n            response = self.agent.chat_with_template(\n                \"factQA_verifier.txt\",\n                qa_data\n            )\n            \n            return self._check_answer(response, fact[\"answer\"])\n            \n        with ThreadPoolExecutor(max_workers=20) as executor:\n            futures = {executor.submit(verify_single_question, fact): fact \n                      for fact in extracted_facts}\n            results = []\n            \n            for future in tqdm(\n                concurrent.futures.as_completed(futures),\n                total=len(extracted_facts),\n                desc=\"Verifying questions\"\n            ):\n                results.append(future.result())\n                \n        accuracy = self._calculate_score(results, len(extracted_facts))\n        return accuracy\n\n    def _normalize_text(self, text: str) -> str:\n        \"\"\"标准化文本,只保留字母数字,转小写并去除空格\n        \n        Args:\n            text: 输入文本\n            \n        Returns:\n            标准化后的文本\n        \"\"\"\n        # 只保留字母和数字\n        text = re.sub(r'[^a-zA-Z0-9]', '', text)\n        # 转换为小写\n        text = text.lower()\n        return text\n\n    def _check_answer(self, model_answer: str, ground_truth: str) -> tuple:\n        pattern = r'\\\\boxed{(.*?)}'\n        model_boxed = re.findall(pattern, model_answer)\n        ground_truth_boxed = re.findall(pattern, ground_truth)\n        \n        is_correct = False\n        rouge_score = 0.0\n        \n        if model_boxed and ground_truth_boxed:\n            # 对答案进行标准化处理\n            model_ans = self._normalize_text(model_boxed[0])\n            ground_truth = self._normalize_text(ground_truth_boxed[0])\n            \n            # 完全匹配检查\n            is_correct = model_ans == ground_truth\n            \n            if not is_correct:\n                scores = self.scorer.score(ground_truth, model_ans)\n                rouge_score = scores['rougeL'].fmeasure\n\n            # 输出时显示原始答案和标准化后的答案\n            print(\"-\" * 50)\n            if is_correct:\n                print(\"✓ Exact Match\")\n            else:\n                print(f\"✗ Partial Match (ROUGE-L: {rouge_score:.2%})\")\n            print(f\"Expected (original): {ground_truth_boxed[0]}\")\n            print(f\"Got (original): {model_boxed[0]}\")\n            print(f\"Expected (normalized): {ground_truth}\")\n            print(f\"Got (normalized): {model_ans}\")\n            print(\"-\" * 50)\n                \n        return is_correct, rouge_score\n\n    def _calculate_score(self, results: List, total: int) -> float:\n        exact_matches = sum(1 for correct, _ in results if correct)\n        rouge_scores = [score for correct, score in results if not correct]\n        avg_rouge = sum(rouge_scores) / total if rouge_scores else 0\n        \n        final_accuracy = 0.7 * (exact_matches / total) + 0.3 * avg_rouge\n        \n        print(\"\\n=== Verification Results Summary ===\")\n        print(f\"Total Questions: {total}\")\n        print(f\"Exact Matches: {exact_matches}/{total} ({exact_matches/total:.2%})\")\n        print(f\"Average ROUGE-L for Partial Matches: {avg_rouge:.2%}\")\n        print(f\"Final Weighted Score: {final_accuracy:.2%}\")\n        print(\"=\" * 40 + \"\\n\")\n        \n        return final_accuracy\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py",
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the ReportVerifier object with a given agent and configure a RougeScorer instance for evaluating text similarity.\n\n**parameters**: The parameters of this Function.\n- agent: An object that is assigned to the `agent` attribute of the ReportVerifier class.\n\n**Code Description**: \nThe `__init__` function is the constructor method for the `ReportVerifier` class. When an instance of `ReportVerifier` is created, the method takes in a parameter `agent` and assigns it to the instance's `agent` attribute. This allows the `ReportVerifier` object to interact with or use the agent throughout its lifecycle.\n\nIn addition to initializing the `agent`, the method also initializes a `rouge_scorer.RougeScorer` object. The `RougeScorer` is configured to evaluate the Rouge-L metric, which is used to assess the quality of text summaries by comparing them with reference texts. The `use_stemmer=True` argument is passed to the `RougeScorer` to ensure that stemming is applied during the comparison process. Stemming reduces words to their base or root form, enhancing the robustness of the text comparison by ignoring minor variations in word forms.\n\n**Note**: The `agent` parameter must be an object that is compatible with the functionality intended in the `ReportVerifier` class. The `RougeScorer` instance will always be configured with the Rouge-L metric and stemming enabled, which is crucial for ensuring consistent evaluation results when performing text similarity comparisons."
      ],
      "code_start_line": 9,
      "code_end_line": 11,
      "params": [
        "self",
        "agent"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, agent):\n        self.agent = agent\n        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "verify_section",
      "md_content": [
        "## Function Documentation: `verify_section`\n\n### Description:\nThe `verify_section` function is responsible for verifying a series of factual questions based on a provided context and extracted facts. It evaluates the accuracy of answers for each question by comparing the response generated by an agent with the expected answer. The function utilizes parallel processing to verify multiple questions simultaneously and calculates an overall accuracy score based on the results.\n\n### Parameters:\n- **context** (str): A string representing the context in which the questions are being asked. This context is used by the agent to generate responses relevant to each question.\n  \n- **extracted_facts** (List[Dict]): A list of dictionaries, where each dictionary represents a question and its associated metadata. Each dictionary must contain the following keys:\n  - `\"question\"` (str): The question to be verified.\n  - `\"format\"` (str): The expected format of the answer.\n  - `\"answer\"` (str): The expected answer to the question.\n\n### Return Value:\n- **float**: The function returns a floating-point number representing the overall accuracy score of the verification process. This score is calculated by evaluating exact matches and partial matches using the ROUGE-L metric.\n\n### Functionality:\n1. **Initialization**: The function begins by printing a summary of the number of questions to be verified.\n  \n2. **Verification Process**:\n   - A nested function `verify_single_question` is defined to verify each question individually. This function creates a `qa_data` dictionary containing the context, user question, and expected answer format.\n   - For each fact in `extracted_facts`, the function invokes the agent's `chat_with_template` method, passing the `qa_data` to generate a response based on the context.\n   - The agent's response is then compared with the expected answer using the `_check_answer` method to determine if the answer is correct.\n   \n3. **Parallel Processing**:\n   - The function utilizes `ThreadPoolExecutor` to concurrently verify multiple questions, optimizing the process and reducing the overall verification time.\n   - A list of future tasks is created, each representing a call to `verify_single_question`. These tasks are executed in parallel, and their results are collected as they complete.\n\n4. **Results Compilation**:\n   - After all questions have been verified, the results (whether each answer was correct or not, and the corresponding ROUGE-L score) are collected.\n\n5. **Accuracy Calculation**:\n   - The function calls the `_calculate_score` method to compute a final accuracy score based on the results of the verification. The score is calculated as a weighted average of exact matches and ROUGE-L scores.\n\n6. **Final Output**:\n   - The accuracy score is returned to indicate the overall correctness of the factual question-answer verification process.\n\n### Example:\n```python\ncontext = \"The context of the document or passage to verify.\"\nextracted_facts = [\n    {\"question\": \"What is the capital of France?\", \"format\": \"text\", \"answer\": \"Paris\"},\n    {\"question\": \"Who wrote '1984'?\", \"format\": \"text\", \"answer\": \"George Orwell\"}\n]\naccuracy = verifier.verify_section(context, extracted_facts)\nprint(f\"Verification Accuracy: {accuracy}\")\n```\n\n### Notes:\n- The function is designed to handle large numbers of questions efficiently by utilizing multithreading.\n- The accuracy score returned provides a combined metric of both exact matches and partial matches, ensuring a comprehensive evaluation of the verification process."
      ],
      "code_start_line": 13,
      "code_end_line": 47,
      "params": [
        "self",
        "context",
        "extracted_facts"
      ],
      "have_return": true,
      "code_content": "    def verify_section(self, context: str, extracted_facts: List[Dict]) -> float:\n        print(\"\\n=== Starting Factual QA Verification ===\")\n        print(f\"Total questions to verify: {len(extracted_facts)}\\n\")\n\n        def verify_single_question(fact):\n            qa_data = {\n                \"context\": context,\n                \"user_question\": fact[\"question\"],\n                \"constrained_format\": fact[\"format\"]\n            }\n            \n            print(f\"\\nVerifying Question: {fact['question']}\")\n            print(f\"Format: {fact['format']}\")\n            \n            response = self.agent.chat_with_template(\n                \"factQA_verifier.txt\",\n                qa_data\n            )\n            \n            return self._check_answer(response, fact[\"answer\"])\n            \n        with ThreadPoolExecutor(max_workers=20) as executor:\n            futures = {executor.submit(verify_single_question, fact): fact \n                      for fact in extracted_facts}\n            results = []\n            \n            for future in tqdm(\n                concurrent.futures.as_completed(futures),\n                total=len(extracted_facts),\n                desc=\"Verifying questions\"\n            ):\n                results.append(future.result())\n                \n        accuracy = self._calculate_score(results, len(extracted_facts))\n        return accuracy\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/main.py/process_single_task"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/_calculate_score"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "verify_single_question",
      "md_content": [
        "**verify_single_question**: The function of verify_single_question is to verify a single question by interacting with a model, comparing its answer to the expected answer, and checking the accuracy.\n\n**parameters**: The parameters of this Function.\n· fact: A dictionary containing information about the question to be verified, including the question itself, the format of the question, and the expected answer.\n\n**Code Description**: The verify_single_question function is responsible for verifying a specific question and its associated answer by interacting with a conversational model. It accepts a single parameter, fact, which is a dictionary containing key information about the question, its format, and the expected answer.\n\nThe function begins by constructing a dictionary, `qa_data`, which stores the relevant details of the question and its format. The dictionary contains the following keys:\n- \"context\": context (assumed to be defined elsewhere in the class or script)\n- \"user_question\": the question to be verified, which is retrieved from the fact dictionary under the key \"question\"\n- \"constrained_format\": the format of the question, which is retrieved from the fact dictionary under the key \"format\"\n\nNext, the function prints the question and its format to the console for logging or tracking purposes.\n\nThe function then interacts with a model by calling the `chat_with_template` method from the `self.agent` object. This method is used to communicate with the model, utilizing a specific template file, \"factQA_verifier.txt\". The `qa_data` dictionary is passed as input to the template, allowing the model to generate a response that is contextually relevant to the provided question and format.\n\nAfter receiving the model's response, the function calls another method, `_check_answer`, to evaluate the model's answer. The response from the model is compared to the expected answer stored in the fact dictionary under the key \"answer\". The `_check_answer` method checks if the model's answer matches the expected answer and provides a score (typically ROUGE-L) if there is a partial match. This result is returned by the `verify_single_question` function.\n\nThe function relies on the `chat_with_template` method from the `BaseAgent` class to facilitate communication with the model, and the `_check_answer` method to determine the accuracy of the response.\n\n**Note**: The `verify_single_question` function assumes that the necessary context is available when it is called, as the \"context\" field is part of the `qa_data` but is not explicitly provided in the function itself. Additionally, the correct formatting of the \"fact\" dictionary is essential for the function to perform its task successfully.\n\n**Output Example**: The output of the function is the result of the `_check_answer` method, which typically returns a tuple indicating whether the model's answer is correct and the ROUGE-L score (if applicable). A possible return value could be:\n```\n(is_correct=True, rouge_score=0.0)\n```"
      ],
      "code_start_line": 17,
      "code_end_line": 32,
      "params": [
        "fact"
      ],
      "have_return": true,
      "code_content": "        def verify_single_question(fact):\n            qa_data = {\n                \"context\": context,\n                \"user_question\": fact[\"question\"],\n                \"constrained_format\": fact[\"format\"]\n            }\n            \n            print(f\"\\nVerifying Question: {fact['question']}\")\n            print(f\"Format: {fact['format']}\")\n            \n            response = self.agent.chat_with_template(\n                \"factQA_verifier.txt\",\n                qa_data\n            )\n            \n            return self._check_answer(response, fact[\"answer\"])\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/base_agent.py/BaseAgent/chat_with_template",
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/_check_answer"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_normalize_text",
      "md_content": [
        "**_normalize_text**: The function of _normalize_text is to standardize the input text by retaining only alphanumeric characters, converting the text to lowercase, and removing any spaces.\n\n**parameters**: The parameters of this Function.\n· text: The input string that needs to be standardized.\n\n**Code Description**: \nThe _normalize_text function is responsible for normalizing an input string by performing the following operations:\n1. It removes any non-alphanumeric characters using a regular expression (`re.sub(r'[^a-zA-Z0-9]', '', text)`), which ensures that only letters (both uppercase and lowercase) and numbers remain in the string.\n2. It converts the resulting string to lowercase using the `lower()` method, ensuring case uniformity.\n3. It then returns the transformed string.\n\nThis function is primarily used within the context of answer verification. For example, the function is called by `_check_answer`, where it plays a key role in standardizing both the model's predicted answer and the ground truth answer. After extracting specific parts of the answers (using a regular expression to find content inside `\\boxed{}`), `_normalize_text` is applied to both the model's and the ground truth answers before performing any further comparisons. This standardization ensures that the comparison is made in a consistent format, eliminating discrepancies due to case differences, spacing, or punctuation.\n\n**Note**: \n- The function only retains alphanumeric characters, so punctuation and spaces are completely discarded. Ensure that the input text is appropriate for this kind of transformation.\n- This function may be useful in scenarios where exact matches are required and formatting"
      ],
      "code_start_line": 49,
      "code_end_line": 62,
      "params": [
        "self",
        "text"
      ],
      "have_return": true,
      "code_content": "    def _normalize_text(self, text: str) -> str:\n        \"\"\"标准化文本,只保留字母数字,转小写并去除空格\n        \n        Args:\n            text: 输入文本\n            \n        Returns:\n            标准化后的文本\n        \"\"\"\n        # 只保留字母和数字\n        text = re.sub(r'[^a-zA-Z0-9]', '', text)\n        # 转换为小写\n        text = text.lower()\n        return text\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/_check_answer"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_answer",
      "md_content": [
        "**_check_answer**: The function of _check_answer is to evaluate the correctness of a model's answer against a ground truth answer by performing normalization and comparison, and to calculate a ROUGE-L score if the answers do not match exactly.\n\n**parameters**: The parameters of this Function.\n· model_answer: A string representing the answer generated by the model that needs to be evaluated.\n· ground_truth: A string representing the correct answer against which the model's answer is compared.\n\n**Code Description**: The _check_answer function is designed to assess the accuracy of a model's answer by comparing it with the ground truth answer. It begins by defining a regular expression pattern to extract content enclosed within `\\boxed{}` from both the model's answer and the ground truth. The function utilizes the `re.findall` method to retrieve these boxed answers.\n\nOnce the boxed answers are extracted, the function initializes two variables: `is_correct`, which is set to False, and `rouge_score`, initialized to 0.0. If both the model's boxed answer and the ground truth boxed answer are present, the function proceeds to normalize these answers using the _normalize_text method. This normalization process standardizes the text by removing non-alphanumeric characters, converting it to lowercase, and eliminating spaces.\n\nAfter normalization, the function checks for an exact match between the normalized model answer and the normalized ground truth. If they match, `is_correct` is set to True. If they do not match, the function calculates the ROUGE-L score using the scorer's score method, which provides a measure of similarity between the two answers based on their longest common subsequence.\n\nThe function then prints a detailed output, indicating whether the answers matched exactly or partially, along with the original and normalized versions of both answers. Finally, it returns a tuple containing the correctness of the model's answer (as a boolean) and the calculated ROUGE-L score.\n\nThis function is called by the verify_single_question function, which is responsible for verifying individual questions by preparing the necessary data and invoking the _check_answer function to perform the evaluation. The verify_single_question function collects the question and its expected answer, interacts with an agent to obtain the model's response, and then utilizes _check_answer to determine the accuracy of that response.\n\n**Note**: It is important to ensure that the model_answer and ground_truth inputs are formatted correctly to contain boxed answers for the function to operate effectively. The function's output will provide insights into the accuracy of the model's response, which is crucial for evaluating the performance of the model in generating answers.\n\n**Output Example**: \n(is_correct=True, rouge_score=0.0)"
      ],
      "code_start_line": 64,
      "code_end_line": 96,
      "params": [
        "self",
        "model_answer",
        "ground_truth"
      ],
      "have_return": true,
      "code_content": "    def _check_answer(self, model_answer: str, ground_truth: str) -> tuple:\n        pattern = r'\\\\boxed{(.*?)}'\n        model_boxed = re.findall(pattern, model_answer)\n        ground_truth_boxed = re.findall(pattern, ground_truth)\n        \n        is_correct = False\n        rouge_score = 0.0\n        \n        if model_boxed and ground_truth_boxed:\n            # 对答案进行标准化处理\n            model_ans = self._normalize_text(model_boxed[0])\n            ground_truth = self._normalize_text(ground_truth_boxed[0])\n            \n            # 完全匹配检查\n            is_correct = model_ans == ground_truth\n            \n            if not is_correct:\n                scores = self.scorer.score(ground_truth, model_ans)\n                rouge_score = scores['rougeL'].fmeasure\n\n            # 输出时显示原始答案和标准化后的答案\n            print(\"-\" * 50)\n            if is_correct:\n                print(\"✓ Exact Match\")\n            else:\n                print(f\"✗ Partial Match (ROUGE-L: {rouge_score:.2%})\")\n            print(f\"Expected (original): {ground_truth_boxed[0]}\")\n            print(f\"Got (original): {model_boxed[0]}\")\n            print(f\"Expected (normalized): {ground_truth}\")\n            print(f\"Got (normalized): {model_ans}\")\n            print(\"-\" * 50)\n                \n        return is_correct, rouge_score\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/verify_section/verify_single_question"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/_normalize_text"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_calculate_score",
      "md_content": [
        "**_calculate_score**: The function of _calculate_score is to compute a final accuracy score based on exact matches and ROUGE-L scores.\n\n**parameters**:\n· results: A list of tuples where each tuple consists of a boolean indicating whether the answer was correct and the ROUGE-L score for incorrect answers.  \n· total: An integer representing the total number of questions being evaluated.\n\n**Code Description**:  \nThe _calculate_score function calculates a weighted accuracy score for a factual question-answer verification task. The function takes in two arguments: `results` and `total`.\n\n1. **exact_matches**: This is calculated using a generator expression that iterates through the `results` list. For each tuple, it checks if the first element (a boolean) is `True` and counts the number of such occurrences. This represents the number of exact matches between the expected answer and the actual answer.\n\n2. **rouge_scores**: A list comprehension is used to extract the second element (ROUGE-L score) from the tuples in the `results` list where the first element is `False`. This is done to track the ROUGE-L scores for answers that were not an exact match. ROUGE-L is a metric used to evaluate the quality of partial matches in text.\n\n3. **avg_rouge**: The average ROUGE-L score is computed by summing up the values in the `rouge_scores` list and dividing by the `total`. If there are no partial matches (i.e., `rouge_scores` is empty), it defaults to 0.\n\n4. **final_accuracy**: This is the weighted average of the exact match rate and the average ROUGE-L score. The final score is calculated as 70% of the exact match ratio and 30% of the average ROUGE-L score. This weighted score provides a combined metric of both exact and partial matches.\n\n5. **Print Statements**: The function outputs a summary of the verification process, including the total number of questions, the number of exact matches, the average ROUGE-L score for partial matches, and the final weighted accuracy score.\n\n6. **Return Value**: The function returns the final accuracy score as a floating-point value.\n\nThe _calculate_score function is called by the `verify_section` function, which is responsible for verifying a set of factual questions. The results from the verification process (a list of tuples containing exact match statuses and ROUGE-L scores) are passed into _calculate_score to determine the final accuracy of the verification process.\n\n**Note**:  \n- The accuracy score returned by _calculate_score is crucial for evaluating the performance of the factual question-answer verification task.  \n- The exact match percentage has a higher weight (70%) compared to the partial match score (30%).  \n- Ensure that the `results` list contains the appropriate structure, i.e., a tuple with a boolean and a score, for this function to work correctly.\n\n**Output Example**:  \nThe following is an example of the output printed by the function:\n```\n=== Verification Results Summary ===\nTotal Questions: 100\nExact Matches: 85/100 (85.00%)\nAverage ROUGE-L for Partial Matches: 25.00%\nFinal Weighted Score: 66.50%\n========================================\n```\nThis indicates that out of 100 questions, 85 exact matches were found, and the average ROUGE-L score for partial matches is 25%. The final weighted score is 66.50%."
      ],
      "code_start_line": 98,
      "code_end_line": 112,
      "params": [
        "self",
        "results",
        "total"
      ],
      "have_return": true,
      "code_content": "    def _calculate_score(self, results: List, total: int) -> float:\n        exact_matches = sum(1 for correct, _ in results if correct)\n        rouge_scores = [score for correct, score in results if not correct]\n        avg_rouge = sum(rouge_scores) / total if rouge_scores else 0\n        \n        final_accuracy = 0.7 * (exact_matches / total) + 0.3 * avg_rouge\n        \n        print(\"\\n=== Verification Results Summary ===\")\n        print(f\"Total Questions: {total}\")\n        print(f\"Exact Matches: {exact_matches}/{total} ({exact_matches/total:.2%})\")\n        print(f\"Average ROUGE-L for Partial Matches: {avg_rouge:.2%}\")\n        print(f\"Final Weighted Score: {final_accuracy:.2%}\")\n        print(\"=\" * 40 + \"\\n\")\n        \n        return final_accuracy\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/verifier.py/ReportVerifier/verify_section"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/reportbench/tree_comparison.py": [
    {
      "type": "FunctionDef",
      "name": "parse_tree",
      "md_content": [
        "**parse_tree**: The function of parse_tree is to recursively parse a tree-like data structure and organize its elements by their depth level.\n\n**parameters**: The parameters of this Function.\n- node: A dictionary representing a node in the tree, which may contain a 'title' and optionally 'children', a list of child nodes.\n- current_depth: An integer representing the current depth level in the tree being parsed. It defaults to 0.\n- levels: A defaultdict of lists used to store the titles of nodes grouped by their depth levels. It defaults to None.\n\n**Code Description**: The `parse_tree` function processes a tree-like structure represented by a nested dictionary. It starts at a given node (the root of the tree) and recursively explores all its descendants, accumulating the titles of nodes at each depth level.\n\n- If the `levels` parameter is not provided, it initializes it as a `defaultdict` to store lists, which will hold the titles of nodes at different depths.\n- If the node has a 'title', it adds that title to the list corresponding to its current depth (`current_depth`).\n- If the node has a 'children' key, the function recursively calls itself on each child node, increasing the depth by 1.\n- The function eventually returns the `levels` dictionary, which maps each depth level to a list of titles of nodes at that level.\n\nThis function is utilized by the `tree_similarity` function to process and compare two tree-like structures (`std_tree` and `student_tree`). It is called on both the standard tree and the student tree to build depth-based structures of node titles. These structures are then used to calculate the semantic similarity between nodes at corresponding depths, as well as to compute penalties based on the number of nodes at each level. \n\n**Note**: This function assumes that nodes are represented as dictionaries, each potentially containing a 'title' and 'children' key. It is important that the input tree structures follow this format for the function to operate correctly. \n\n**Output Example**: \nFor a tree like:\n\n```\n{\n  'title': 'Root',\n  'children': [\n    {'title': 'Child 1', 'children': []},\n    {'title': 'Child 2', 'children': [{'title': 'Grandchild 1', 'children': []}]}\n  ]\n}\n```\n\nThe output of the `parse_tree` function would be:\n\n```\n{\n  0: ['Root'],\n  1: ['Child 1', 'Child 2'],\n  2: ['Grandchild 1']\n}\n```"
      ],
      "code_start_line": 10,
      "code_end_line": 18,
      "params": [
        "node",
        "current_depth",
        "levels"
      ],
      "have_return": true,
      "code_content": "def parse_tree(node, current_depth=0, levels=None):\n    if levels is None:\n        levels = defaultdict(list)\n    if 'title' in node:\n        levels[current_depth].append(node['title'])\n    if 'children' in node:\n        for child in node['children']:\n            parse_tree(child, current_depth + 1, levels)\n    return levels\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/tree_comparison.py/tree_similarity"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "text_similarity",
      "md_content": [
        "**text_similarity**: The function of text_similarity is to compute the similarity between two text inputs using natural language processing (NLP) techniques.\n\n**parameters**:\n· text1: A string containing the first text to be compared.\n· text2: A string containing the second text to be compared.\n\n**Code Description**:  \nThe `text_similarity` function calculates the similarity between two given text inputs (`text1` and `text2`). This is achieved by processing each text input using a natural language processing model (`nlp`), which converts the text into a document object. The `similarity` method of the `spacy` document object is then used to compute a similarity score, which ranges from 0 to 1, where 1 indicates identical content and 0 indicates no similarity. This function returns the similarity score between the two documents.\n\nIn the broader context of the project, the `text_similarity` function is invoked by the `level_similarity` function. The `level_similarity` function uses `text_similarity` to compute a similarity matrix between two sets of nodes (`nodes_A` and `nodes_B`). These nodes are presumably elements in some data structure, and `level_similarity` calculates the overall similarity between two groups of nodes by comparing each pairwise combination of nodes. The `text_similarity` function is called within nested loops iterating over these nodes, and its output is used to populate a similarity matrix. Once the matrix is constructed, the `linear_sum_assignment` function is applied to optimize the assignment of nodes based on the similarity values, and the final similarity score is computed by summing the similarities of the assigned pairs.\n\nThus, `text_similarity` serves as a key component for comparing individual nodes at the text level within the `level_similarity` function, which ultimately computes a similarity score between entire groups of nodes.\n\n**Note**:  \n- The `text_similarity` function assumes that the `nlp` model (likely a `spaCy` model) has already been initialized before being called.\n- Both `text1` and `text2` must be valid text strings. If either of them is empty or malformed, the function may not behave as expected.\n- The returned similarity score is a floating-point number between 0 and 1, with higher values indicating greater textual similarity.\n\n**Output Example**:  \nFor example, if `text1` is \"Hello, how are you?\" and `text2` is \"Hi, how are you doing?\", the function might return a similarity score like `0.89`, indicating that the two texts are highly similar, but not identical."
      ],
      "code_start_line": 20,
      "code_end_line": 23,
      "params": [
        "text1",
        "text2"
      ],
      "have_return": true,
      "code_content": "def text_similarity(text1, text2):\n    doc1 = nlp(text1)\n    doc2 = nlp(text2)\n    return doc1.similarity(doc2)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/tree_comparison.py/level_similarity"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "level_similarity",
      "md_content": [
        "**level_similarity**: The function of level_similarity is to compute the similarity score between two sets of nodes based on their textual content.\n\n**parameters**: \n· nodes_A: A list of nodes representing the first group of elements to be compared. Each node is expected to contain textual content.  \n· nodes_B: A list of nodes representing the second group of elements to be compared. Each node is also expected to contain textual content.\n\n**Code Description**: The `level_similarity` function calculates a similarity score between two groups of nodes, `nodes_A` and `nodes_B`, by utilizing a similarity matrix derived from the textual content of each node. Initially, the function checks if both input lists are empty; if so, it returns a similarity score of 1.0, indicating a perfect match due to the absence of nodes. \n\nNext, a similarity matrix is created with dimensions corresponding to the lengths of `nodes_A` and `nodes_B`. The function then iterates through each node in `nodes_A` and `nodes_B`, calculating the textual similarity between each pair of nodes using the `text_similarity` function. This function employs natural language processing techniques to derive a similarity score for each pair, which is stored in the similarity matrix.\n\nAfter populating the matrix, the function applies the `linear_sum_assignment` method from the SciPy library to find the optimal assignment of nodes that maximizes the total similarity score. The total similarity score is computed by summing the values in the similarity matrix corresponding to the optimal assignments. Finally, the function normalizes this score by dividing it by the maximum possible value, which is determined by the larger of the two input lists' lengths. If both lists are non-empty, the function returns the computed similarity score; otherwise, it returns 0.\n\nThe `level_similarity` function is called by the `tree_similarity` function, which is responsible for comparing two hierarchical tree structures. Within `tree_similarity`, the function first parses the trees into levels and then computes the similarity for each level using `level_similarity`. This integration allows `tree_similarity` to assess the overall structural and semantic similarity between two trees by evaluating their respective levels.\n\n**Note**: \n- The `text_similarity` function, which is called within `level_similarity`, must be properly initialized with a natural language processing model before use.\n- Both `nodes_A` and `nodes_B` should contain valid text nodes; otherwise, the function may not yield meaningful results.\n- The returned similarity score is a floating-point number between 0 and 1, where higher values indicate greater similarity between the two sets of nodes.\n\n**Output Example**: For instance, if `nodes_A` contains [\"Node A1\", \"Node A2\"] and `nodes_B` contains [\"Node B1\", \"Node B2\"], the function might return a similarity score of `0.75`, indicating a moderate level of similarity between the two groups of nodes."
      ],
      "code_start_line": 25,
      "code_end_line": 35,
      "params": [
        "nodes_A",
        "nodes_B"
      ],
      "have_return": true,
      "code_content": "def level_similarity(nodes_A, nodes_B):\n    if not nodes_A and not nodes_B:\n        return 1.0  # 双方无节点视为完全匹配\n    sim_matrix = np.zeros((len(nodes_A), len(nodes_B)))\n    for i, a in enumerate(nodes_A):\n        for j, b in enumerate(nodes_B):\n            sim_matrix[i][j] = text_similarity(a, b)\n    row_ind, col_ind = linear_sum_assignment(-sim_matrix)\n    total_sim = sim_matrix[row_ind, col_ind].sum()\n    max_possible = max(len(nodes_A), len(nodes_B))\n    return total_sim / max_possible if max_possible > 0 else 0\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/tree_comparison.py/tree_similarity"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/tree_comparison.py/text_similarity"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "tree_similarity",
      "md_content": [
        "**tree_similarity**: The function of tree_similarity is to calculate the semantic and structural similarity score between two hierarchical tree structures.\n\n**parameters**: The parameters of this Function.\n- **std_tree**: A dictionary representing the standard tree structure, which is compared against the student's tree structure.\n- **student_tree**: A dictionary representing the student's tree structure, which is compared to the standard tree structure.\n- **depth_decay**: A float (default is 0.8) that determines the decay factor for the weight assigned to each tree depth. Deeper levels of the tree are given higher weights.\n- **alpha**: A float (default is 0.3) used in the penalty calculation for the difference in the number of nodes between corresponding levels of the trees.\n- **beta**: A float (default is 0.02) that penalizes missing layers in the student's tree compared to the standard tree.\n- **gamma**: A float (default is 0.02) that penalizes redundant layers in the student's tree compared to the standard tree.\n\n**Code Description**: \nThe `tree_similarity` function computes the overall similarity between two tree structures, `std_tree` and `student_tree`. It does so by first parsing both trees into levels using the `parse_tree` function. This process groups the nodes in each tree by their depth level. Then, the function compares these corresponding levels of the two trees by computing their semantic similarity using the `level_similarity` function. It also calculates penalties based on the differences in the number of nodes at each level and the presence of missing or redundant levels in the student's tree.\n\nThe function proceeds through the following steps:\n\n1. **Tree Parsing**: The two input trees (`std_tree` and `student_tree`) are parsed into levels using the `parse_tree` function. This gives each tree a dictionary mapping depth levels to lists of nodes at those depths.\n\n2. **Depth Calculation**: The function determines the maximum depth among the two trees, which will be the range of depths that the function iterates over. The comparison will be done from depth 0 up to the maximum depth in either tree.\n\n3. **Iterative Comparison**: The function iterates over each depth level from 0 to the maximum depth. For each depth:\n   - It checks if the current depth exists in both trees. If a depth exists in one tree but not the other, it counts as either a missing or redundant layer, depending on which tree lacks the depth.\n   - The function calculates a weight for the depth based on the `depth_decay` factor. Deeper levels of the tree receive a higher weight.\n   - For corresponding depths that exist in both trees, the function calculates the semantic similarity between the nodes at that level using the `level_similarity` function.\n   - A penalty is applied for discrepancies in the number of nodes at each depth. If the number of nodes differs between the two trees, a penalty is computed based on the `alpha` factor, which reduces the structure's similarity score.\n\n4. **Final Score Calculation**: After comparing all depths:\n   - The function computes a weighted average of the similarity scores at each depth, adjusting for missing or redundant layers in the student's tree using the `beta` and `gamma` penalties.\n   - The final score is computed by applying the structure penalty to the weighted average score.\n\nThe output is a final similarity score between the two trees, rounded to two decimal places. This score reflects how closely the student's tree structure matches the standard tree both semantically and structurally.\n\nThe `tree_similarity` function is used in the `evaluate_breadth` method of the `ReportEvaluation` class, which evaluates the similarity between the standard and student trees. The standard tree is fetched from `self.report_benchmark.breadth_gt`, and the student's tree is generated using `self.examinees_outline_generation()`. The resulting similarity score is returned as part of the evaluation process.\n\n**Note**:\n- The `parse_tree` function, which is called by `tree_similarity`, is responsible for parsing the tree structure and organizing it by depth levels.\n- The `level_similarity` function, which is used to calculate semantic similarity at each level, relies on comparing the nodes' textual content to compute a similarity score.\n- The parameters `depth_decay`, `alpha`, `beta`, and `gamma` influence how heavily the function weighs different aspects of the tree comparison, including the depth of nodes and the penalties for missing or redundant layers.\n- The returned score is a floating-point value between 0 and 1, with higher values indicating a closer match between the two trees."
      ],
      "code_start_line": 37,
      "code_end_line": 93,
      "params": [
        "std_tree",
        "student_tree",
        "depth_decay",
        "alpha",
        "beta",
        "gamma"
      ],
      "have_return": true,
      "code_content": "def tree_similarity(std_tree, student_tree, depth_decay=0.8, alpha=0.3, beta=0.02, gamma=0.02):\n    # 解析层级\n    std_levels = parse_tree(std_tree)\n    student_levels = parse_tree(student_tree)\n    \n    max_std_depth = max(std_levels.keys(), default=0)\n    max_student_depth = max(student_levels.keys(), default=0)\n    max_depth = max(max_std_depth, max_student_depth)\n    \n    total_score = 0.0\n    total_weight = 0.0\n    missing_layers = 0\n    redundant_layers = 0\n    \n    # 遍历每个可能的层级（从0到最大深度）\n    for depth in range(max_depth + 1):\n        in_std = depth in std_levels\n        in_student = depth in student_levels\n        weight = 1.0 / (depth_decay ** depth)  # 深层权重更高\n        \n        # 层级存在性检查\n        if in_std and not in_student:\n            missing_layers += 1\n            continue  # 学生缺失该层，跳过权重累加\n        elif not in_std and in_student:\n            redundant_layers += 1\n            continue  # 学生多出该层，跳过权重累加\n        \n        # 获取该层节点\n        nodes_std = std_levels.get(depth, [])\n        nodes_student = student_levels.get(depth, [])\n        m, n = len(nodes_std), len(nodes_student)\n        \n        # 计算语义相似度\n        semantic_score = level_similarity(nodes_std, nodes_student)\n        \n        # 节点数量差异惩罚\n        if m == 0:\n            num_penalty = 0  # 标准无节点时不惩罚\n        else:\n            num_diff = abs(m - n)\n            num_penalty = alpha * (num_diff / m)\n        structure_coeff = max(0, 1 - num_penalty)\n        \n        # 层级得分\n        layer_score = semantic_score * structure_coeff\n        total_score += layer_score * weight\n        total_weight += weight\n    \n    # 计算加权平均得分\n    weighted_avg = total_score / total_weight if total_weight > 0 else 0\n    \n    # 总结构惩罚\n    structure_penalty = beta * missing_layers + gamma * redundant_layers\n    final_score = weighted_avg * max(0, 1 - structure_penalty)\n    \n    return round(final_score, 2)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_evaluation.py",
        "src/criticsearch/reportbench/report_evaluation.py/ReportEvaluation/evaluate_breadth"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/tree_comparison.py/parse_tree",
        "src/criticsearch/reportbench/tree_comparison.py/level_similarity"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "src/criticsearch/reportbench/read_wikiItem_hierachy.py": [
    {
      "type": "FunctionDef",
      "name": "traverse",
      "md_content": [
        "**traverse**: The function of traverse is to recursively traverse a nested structure of dictionaries and lists and print the \"title\" key from any dictionary encountered.\n\n**parameters**: \n· parameter1: data (Required) – This parameter is the data structure to be traversed, which can be a dictionary, list, or any other nested combination of these types.\n· parameter2: level (Optional) – An integer indicating the current level of recursion. It is used for indentation purposes to visually represent the depth of traversal.\n\n**Code Description**: \nThe traverse function is a recursive function designed to navigate through a nested data structure, which may consist of dictionaries, lists, or combinations of both. The function begins by checking the type of the `data` parameter to determine how to process it.\n\n1. If `data` is a dictionary, it first checks if the dictionary contains a key named \"title.\" If this key exists, its value is printed with indentation that corresponds to the current `level`. The indentation is achieved by multiplying four spaces (\"    \") by the `level` parameter.\n   \n2. After checking for the \"title\" key, the function then iterates over all the key-value pairs in the dictionary. If any of the values are another dictionary or a list, the function calls itself recursively with the value and an incremented `level` (i.e., `level + 1`).\n\n3. If `data` is a list, the function iterates over each item in the list and recursively calls itself on each item.\n\nThe recursive nature of this function allows it to handle arbitrarily nested dictionaries and lists. The indentation helps to visualize the depth of each level of recursion when printing the \"title\" values.\n\n**Note**: \n- The `level` parameter is optional and defaults to 0, representing the top level of the data structure. If provided, it should be an integer.\n- The function expects the input data to be a dictionary or list; other data types are not processed.\n- If dictionaries or lists are nested inside each other, the function will process them in a depth-first manner, meaning it explores each item fully before moving to the next.\n- The printed output will display the \"title\" value for each dictionary that contains it, indented according to its level in the structure."
      ],
      "code_start_line": 4,
      "code_end_line": 14,
      "params": [
        "data",
        "level"
      ],
      "have_return": false,
      "code_content": "def traverse(data, level=0):\n    indent = \"    \" * level\n    if isinstance(data, dict):\n        if \"title\" in data:\n            print(indent + data[\"title\"])\n        for key, value in data.items():\n            if isinstance(value, (dict, list)):\n                traverse(value, level + 1)\n    elif isinstance(data, list):\n        for item in data:\n            traverse(item, level)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/reportbench/report_evaluation.py": [
    {
      "type": "ClassDef",
      "name": "ReportEvaluation",
      "md_content": [
        "**ReportEvaluation**: The function of ReportEvaluation is to evaluate a student’s report against a predefined benchmark, using various assessment methods such as breadth, depth, and factual accuracy.\n\n**attributes**: The attributes of this Class.\n· report_benchmark: This attribute holds an instance of the `ReportBenchmark` class, which is responsible for providing the benchmark data used for comparison and evaluation of the student’s report.\n· student_report: This attribute stores the student's report in the form of a string, which will be evaluated based on the comparison with the `ReportBenchmark` data.\n\n**Code Description**: The `ReportEvaluation` class is designed to perform the evaluation of a student's report. It leverages the `ReportBenchmark` instance to obtain ground truth data and perform a series of assessment procedures.\n\n1. **`__init__(self, report_benchmark: ReportBenchmark, student_report: str)`**:\n   This is the constructor method for initializing the `ReportEvaluation` object. It accepts two parameters:\n   - `report_benchmark`: An instance of the `ReportBenchmark` class, which holds necessary information for comparison such as user queries, ground truth data, and the agent used for interaction.\n   - `student_report`: A string containing the student's report that will be evaluated.\n\n   The constructor method assigns these parameters to the corresponding attributes of the `ReportEvaluation` class.\n\n2. **`examinees_outline_generation(self)`**:\n   This method generates the student report outline using the `ReportBenchmark` agent. It loads a template file, \"outline_generation.txt\", and renders it using data from the `ReportBenchmark`, specifically the `user_query`. The method sends the rendered template as a prompt to the `ReportBenchmark` agent and returns the response.\n\n3. **`evaluate_breadth(self)`**:\n   This method evaluates the breadth of the student's report. It generates a student tree by calling the `examinees_outline_generation()` method. Then, it compares the generated tree with the benchmark's breadth ground truth (`breadth_gt`) using the `tree_similarity` function, which calculates a similarity score. The result is returned as the evaluation score for breadth.\n\n4. **`evaluate_factualqa(self)`**:\n   This method performs a factual QA evaluation using the student's report. It loads the \"factual_qa.txt\" template, which is used to assess the factual accuracy of the student's report. The method renders this template using both the benchmark's `user_query` and `breadth_gt`, along with the `student_report`. The rendered prompt is sent to the agent, and the response is returned as the evaluation of the factual accuracy.\n\n5. **`extract_student_tree_structure(self)`**:\n   This method extracts the logical structure of the student's report. It loads the \"student_tree_extraction.txt\" template, which is used to analyze the report's structure. After rendering the template with the `student_report`, the resulting response is parsed into a JSON format representing the student’s tree structure. This allows for further analysis of the report’s organization.\n\n6. **`evaluate_depth(self)`**:\n   This method is meant to evaluate the depth of the student's report. However, the method is not yet implemented. The description suggests that the depth evaluation would be based on factual QA results, using accuracy (ACC) of student report-based answers to compute a final score. The current implementation does not provide details on the exact logic.\n\n**Note**: \n- The methods in this class depend heavily on templates stored and rendered by the `ReportBenchmark` agent.\n- The class provides a flexible structure for evaluating different aspects of a student's report, including outline, breadth, and factual accuracy.\n- The `evaluate_depth` method has not been fully implemented, and thus depth evaluation cannot currently be performed.\n\n**Output Example**:\n- For `evaluate_breadth`, an output might look like:\n  ```json\n  0.85\n  ```\n  This score represents the similarity between the student’s tree structure and the benchmark’s breadth ground truth.\n\n- For `evaluate_factualqa`, an output could be:\n  ```json\n  {\n    \"accuracy\": 0.92,\n    \"feedback\": \"The factual accuracy of the student’s report is 92%.\"\n  }\n  ```\n  This result indicates the factual accuracy score of the student’s report, along with feedback."
      ],
      "code_start_line": 5,
      "code_end_line": 52,
      "params": [],
      "have_return": true,
      "code_content": "class ReportEvaluation:\n    def __init__(self, report_benchmark: ReportBenchmark, student_report: str):\n        # 使用 ReportBenchmark 实例获得 ground truths\n        self.report_benchmark = report_benchmark\n        # 新增的 StudentReport 字段\n        self.student_report = student_report\n\n    def examinees_outline_generation(self):\n        # 使用 ReportBenchmark 的 BaseAgent 调用，生成学生树（此前在 ReportBenchmark 中的 run_outline_generation）\n        template_str = self.report_benchmark.agent.load_template(\"outline_generation.txt\")\n        data = {\n            \"Query\": self.report_benchmark.user_query,\n        }\n        prompt = self.report_benchmark.agent.render_template(template_str, data)\n        response = self.report_benchmark.agent.common_chat(usr_prompt=prompt)\n        return response\n\n    def evaluate_breadth(self):\n        # 直接使用 examinees_outline_generation 生成学生树\n        student_tree_str = self.examinees_outline_generation()\n        student_tree = json.loads(student_tree_str)\n        score = tree_similarity(self.report_benchmark.breadth_gt, student_tree)\n        return score\n\n    def evaluate_factualqa(self):\n        # 基于传入的 StudentReport 执行 FactualQA 评估\n        template_str = self.report_benchmark.agent.load_template(\"factual_qa.txt\")\n        data = {\n            \"Query\": self.report_benchmark.user_query,\n            \"BreadthGT\": json.dumps(self.report_benchmark.breadth_gt),\n            \"DepthGT\": self.student_report,\n        }\n        prompt = self.report_benchmark.agent.render_template(template_str, data)\n        response = self.report_benchmark.agent.common_chat(usr_prompt=prompt)\n        return response\n\n    def extract_student_tree_structure(self):\n        # 新增函数：从 student_report 中抽取目录树逻辑结构\n        template_str = self.report_benchmark.agent.load_template(\"student_tree_extraction.txt\")\n        data = {\"StudentReport\": self.student_report}\n        prompt = self.report_benchmark.agent.render_template(template_str, data)\n        response = self.report_benchmark.agent.common_chat(usr_prompt=prompt)\n        return json.loads(response)\n    \n    def evaluate_depth(self):\n        # 深度评估逻辑暂不实现，基于factual QA结果评估ACC\n        # 这里实现抽取student report based answer 的acc来计算最后的分数\n        pass\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the ReportEvaluation class with a ReportBenchmark instance and a student report string.\n\n**parameters**: The parameters of this Function.\n· report_benchmark: An instance of the ReportBenchmark class, which is used to obtain ground truths for report evaluations.  \n· student_report: A string representing the student's report that is to be evaluated.\n\n**Code Description**: The __init__ function is the constructor for the ReportEvaluation class. It takes two parameters: report_benchmark and student_report. The report_benchmark parameter is expected to be an instance of the ReportBenchmark class, which is responsible for generating report evaluations by building ground truths and performing fact extraction. The student_report parameter is a string that contains the report of a student, which will be evaluated against the ground truths provided by the ReportBenchmark instance.\n\nUpon initialization, the constructor assigns the report_benchmark instance to the instance variable self.report_benchmark, allowing the ReportEvaluation class to access the methods and attributes of the ReportBenchmark class for further processing. Additionally, the student_report string is stored in the instance variable self.student_report, which will be used in the evaluation process.\n\nThe relationship between the ReportEvaluation class and the ReportBenchmark class is crucial, as the ReportEvaluation class relies on the functionalities provided by the ReportBenchmark instance to perform its evaluations. This constructor sets up the necessary context for the ReportEvaluation class to operate effectively, ensuring that it has access to the required ground truths and the specific report that needs to be evaluated.\n\n**Note**: When using the ReportEvaluation class, ensure that the report_benchmark instance is properly initialized with valid data, as it directly influences the evaluation process of the student report."
      ],
      "code_start_line": 6,
      "code_end_line": 10,
      "params": [
        "self",
        "report_benchmark",
        "student_report"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, report_benchmark: ReportBenchmark, student_report: str):\n        # 使用 ReportBenchmark 实例获得 ground truths\n        self.report_benchmark = report_benchmark\n        # 新增的 StudentReport 字段\n        self.student_report = student_report\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark"
      ],
      "special_reference_type": [
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "examinees_outline_generation",
      "md_content": [
        "**examinees_outline_generation**: The function of examinees_outline_generation is to generate a student outline tree by utilizing a template rendering system and agent communication from the ReportBenchmark.\n\n**parameters**: The parameters of this Function.\n· There are no parameters passed to this function directly. It relies on the internal attributes of the `report_benchmark` object.\n\n**Code Description**: \nThe `examinees_outline_generation` function is responsible for generating a structured outline of the student data, referred to as a \"student tree,\" using the `ReportBenchmark`'s `BaseAgent`. The function performs the following steps:\n\n1. It loads a template file named `outline_generation.txt` using the `load_template` method of the `agent` object from `report_benchmark`. This template likely contains predefined instructions or a structure for generating the student outline.\n\n2. Next, it creates a data dictionary containing the user's query, retrieved from `report_benchmark.user_query`. This user query represents the input or context that will be used in the generation process.\n\n3. The template string loaded in step 1 is then rendered with the data dictionary using the `render_template` method of the `agent`. This step dynamically fills the template with the provided data (in this case, the user query) to create a customized prompt.\n\n4. The function sends the generated prompt to the agent for further processing by calling `common_chat` on the agent, passing the generated prompt as the `usr_prompt`. This step likely involves communicating with an external system or using an internal service to process the prompt and generate a response.\n\n5. The response, which is expected to be the student outline tree in string format, is returned as the output of the function.\n\nThe function does not take any parameters directly; instead, it relies on the `report_benchmark` attribute, which is assumed to be an instance of a class that holds the necessary data, such as the `user_query` and the `agent` responsible for communication and template handling.\n\nIn the context of the project, this function is called by the `evaluate_breadth` function. The `evaluate_breadth` function calls `examinees_outline_generation` to generate the student tree, then parses the returned string into a JSON object, which is compared to a ground truth value (`breadth_gt`) to calculate a similarity score using the `tree_similarity` function. This indicates that `examinees_outline_generation` is a part of the process that evaluates the breadth of student performance or outlines.\n\n**Note**: \n- The function relies heavily on the `report_benchmark` object, and any changes to this object may affect the function's behavior.\n- The response returned by `examinees_outline_generation` is expected to be in a specific format (likely a structured string representing a tree). Any discrepancies in the format may lead to errors in downstream processing.\n- This function is expected to interact with an external agent (through `common_chat`), so the performance or response times may depend on the efficiency and reliability of that external system.\n\n**Output Example**: \nThe output is expected to be a string representing the generated student tree. An example response could look like:\n\n```\n{\n    \"student_id\": \"12345\",\n    \"name\": \"John Doe\",\n    \"performance\": {\n        \"subject_1\": \"A\",\n        \"subject_2\": \"B\",\n        \"subject_3\": \"A\"\n    },\n    \"remarks\": \"Excellent performance in all subjects\"\n}\n```"
      ],
      "code_start_line": 12,
      "code_end_line": 20,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def examinees_outline_generation(self):\n        # 使用 ReportBenchmark 的 BaseAgent 调用，生成学生树（此前在 ReportBenchmark 中的 run_outline_generation）\n        template_str = self.report_benchmark.agent.load_template(\"outline_generation.txt\")\n        data = {\n            \"Query\": self.report_benchmark.user_query,\n        }\n        prompt = self.report_benchmark.agent.render_template(template_str, data)\n        response = self.report_benchmark.agent.common_chat(usr_prompt=prompt)\n        return response\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_evaluation.py/ReportEvaluation/evaluate_breadth"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "evaluate_breadth",
      "md_content": [
        "**evaluate_breadth**: The function of evaluate_breadth is to evaluate the similarity score between a generated student tree and a predefined standard tree based on breadth.\n\n**parameters**: The parameters of this Function.\n· There are no parameters passed directly to this function.\n\n**Code Description**: The `evaluate_breadth` function is a method within the `ReportEvaluation` class that is responsible for assessing the breadth of a student's performance by comparing a generated student tree against a ground truth tree structure. The function operates as follows:\n\n1. **Student Tree Generation**: The function first calls the `examinees_outline_generation` method, which generates a structured outline of the student data, referred to as a \"student tree.\" This method utilizes a template rendering system and agent communication to create the student tree based on the user's query.\n\n2. **Parsing the Student Tree**: The output from `examinees_outline_generation` is expected to be a string representation of the student tree. The function then parses this string into a JSON object using `json.loads`, allowing for structured manipulation and comparison.\n\n3. **Similarity Calculation**: The function then calculates the similarity score between the generated student tree and a predefined standard tree (`self.report_benchmark.breadth_gt`) by invoking the `tree_similarity` function. This function computes the semantic and structural similarity score between the two hierarchical tree structures, taking into account various parameters that influence the comparison.\n\n4. **Returning the Score**: Finally, the computed similarity score is returned as the output of the `evaluate_breadth` function. This score reflects how closely the student's tree structure matches the standard tree both semantically and structurally.\n\nThe `evaluate_breadth` function is integral to the evaluation process within the `ReportEvaluation` class, as it provides a quantitative measure of the student's performance in relation to established benchmarks.\n\n**Note**: It is important to ensure that the output from `examinees_outline_generation` is in the expected format for successful parsing. Any discrepancies in the format may lead to errors in the similarity calculation. The `tree_similarity` function, which is called within this method, relies on the proper structure of both the standard and student trees to produce an accurate similarity score.\n\n**Output Example**: The output of the `evaluate_breadth` function is expected to be a floating-point value representing the similarity score. An example return value could be:\n\n```\n0.85\n``` \n\nThis score indicates a high level of similarity between the generated student tree and the standard tree."
      ],
      "code_start_line": 22,
      "code_end_line": 27,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def evaluate_breadth(self):\n        # 直接使用 examinees_outline_generation 生成学生树\n        student_tree_str = self.examinees_outline_generation()\n        student_tree = json.loads(student_tree_str)\n        score = tree_similarity(self.report_benchmark.breadth_gt, student_tree)\n        return score\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "src/criticsearch/reportbench/tree_comparison.py/tree_similarity",
        "src/criticsearch/reportbench/report_evaluation.py/ReportEvaluation/examinees_outline_generation"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "evaluate_factualqa",
      "md_content": [
        "**evaluate_factualqa**: The function of evaluate_factualqa is to perform a FactualQA evaluation based on the provided StudentReport.\n\n**parameters**: The parameters of this Function.\n· self: An instance of the class that contains the report_benchmark and student_report attributes.\n\n**Code Description**: The evaluate_factualqa function is designed to execute a FactualQA evaluation using the information contained within the StudentReport. The function begins by loading a template string from a file named \"factual_qa.txt\" using the load_template method of the agent associated with report_benchmark. This template serves as a basis for constructing a prompt that will be sent for evaluation.\n\nNext, the function constructs a data dictionary that includes:\n- \"Query\": This key holds the user query from the report_benchmark.\n- \"BreadthGT\": This key contains the ground truth for breadth, which is serialized into a JSON format from the breadth_gt attribute of report_benchmark.\n- \"DepthGT\": This key directly references the student_report, which is expected to contain the depth ground truth for the evaluation.\n\nThe prompt is then generated by rendering the template string with the data dictionary using the render_template method of the agent. Following this, the function calls the common_chat method of the agent, passing the constructed prompt as the usr_prompt argument. This method is responsible for processing the prompt and generating a response based on the evaluation.\n\nFinally, the function returns the response obtained from the common_chat method, which is expected to contain the results of the FactualQA evaluation.\n\n**Note**: It is important to ensure that the template file \"factual_qa.txt\" exists and is correctly formatted, as the function relies on this template for generating the evaluation prompt. Additionally, the attributes report_benchmark and student_report must be properly initialized within the class instance for the function to operate correctly.\n\n**Output Example**: A possible return value from the evaluate_factualqa function could be a JSON object containing the evaluation results, such as:\n{\n    \"evaluation_score\": 0.85,\n    \"feedback\": \"The answer is mostly correct but lacks depth in certain areas.\"\n}"
      ],
      "code_start_line": 29,
      "code_end_line": 39,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def evaluate_factualqa(self):\n        # 基于传入的 StudentReport 执行 FactualQA 评估\n        template_str = self.report_benchmark.agent.load_template(\"factual_qa.txt\")\n        data = {\n            \"Query\": self.report_benchmark.user_query,\n            \"BreadthGT\": json.dumps(self.report_benchmark.breadth_gt),\n            \"DepthGT\": self.student_report,\n        }\n        prompt = self.report_benchmark.agent.render_template(template_str, data)\n        response = self.report_benchmark.agent.common_chat(usr_prompt=prompt)\n        return response\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_student_tree_structure",
      "md_content": [
        "**extract_student_tree_structure**: The function of extract_student_tree_structure is to extract the directory tree structure from a given student report.\n\n**parameters**: \n· None\n\n**Code Description**:  \nThe **extract_student_tree_structure** function is responsible for extracting the directory tree structure from the student report. The function operates as follows:\n\n1. **Load Template**: It starts by loading a template file, named `\"student_tree_extraction.txt\"`, using the `load_template` method from the `report_benchmark.agent`. This template is assumed to contain the necessary structure or instructions for processing the student report.\n\n2. **Prepare Data**: The function then prepares a dictionary with the key `\"StudentReport\"` which holds the `self.student_report` value. This suggests that the `self.student_report` contains the actual data of the student report to be processed.\n\n3. **Render Template**: Next, the function renders the loaded template using the `render_template` method of the `report_benchmark.agent`. It passes the template string (`template_str`) and the data dictionary (`data`) as parameters to generate a prompt. The prompt generated will likely contain placeholders or specific instructions based on the template, filled with the relevant data from the student report.\n\n4. **Chat with Agent**: The generated prompt is then sent to the `common_chat` method of the `report_benchmark.agent`. This method interacts with a system (likely a model or an agent) that processes the prompt and returns a response. The interaction here is presumably for processing or extracting the tree structure from the student report based on the prompt.\n\n5. **Parse Response**: Finally, the response from the `common_chat` method is expected to be in a JSON format. The `json.loads(response)` function is used to parse this JSON string into a Python dictionary or object, which is then returned as the output of the function.\n\n**Note**: \n- The function assumes that the student report (`self.student_report`) is already available and properly formatted.\n- The template `\"student_tree_extraction.txt\"` should exist in the specified location and be structured correctly to work with the `render_template` method.\n- The `common_chat` method's response is expected to be a JSON string that can be parsed directly with `json.loads()`.\n\n**Output Example**:\nAssuming the student report contains data structured with sections, the output might look like the following:\n\n```json\n{\n  \"root\": {\n    \"name\": \"Student Report\",\n    \"children\": [\n      {\n        \"name\": \"Personal Information\",\n        \"children\": [\n          {\n            \"name\": \"Name\",\n            \"value\": \"John Doe\"\n          },\n          {\n            \"name\": \"ID\",\n            \"value\": \"12345\"\n          }\n        ]\n      },\n      {\n        \"name\": \"Grades\",\n        \"children\": [\n          {\n            \"name\": \"Math\",\n            \"value\": \"A\"\n          },\n          {\n            \"name\": \"Science\",\n            \"value\": \"B\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\nIn this output example, the directory tree structure of the student report is represented as a hierarchical JSON object, where each node contains a `name` and can have `children` representing nested sections or data."
      ],
      "code_start_line": 41,
      "code_end_line": 47,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def extract_student_tree_structure(self):\n        # 新增函数：从 student_report 中抽取目录树逻辑结构\n        template_str = self.report_benchmark.agent.load_template(\"student_tree_extraction.txt\")\n        data = {\"StudentReport\": self.student_report}\n        prompt = self.report_benchmark.agent.render_template(template_str, data)\n        response = self.report_benchmark.agent.common_chat(usr_prompt=prompt)\n        return json.loads(response)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "evaluate_depth",
      "md_content": [
        "**evaluate_depth**: The function of evaluate_depth is to evaluate the depth of a report based on factual QA results and compute the accuracy of student-generated responses.\n\n**parameters**: \nThis function does not take any parameters.\n\n**Code Description**: \nThe **evaluate_depth** function is currently a placeholder and does not contain any functional implementation. As indicated in the comment within the code, it is intended to implement logic for depth evaluation based on factual QA results. However, the logic is not yet realized. The comment suggests that the function is designed to calculate the accuracy (ACC) of student report-based answers, which will ultimately contribute to determining a final score. The current implementation of this function simply contains a `pass` statement, implying that no action is taken when the function is called.\n\n**Note**: \n- The function is incomplete, and no depth evaluation logic has been implemented yet.\n- The function might be extended in the future to include actual calculations or data processing for evaluating the depth of responses in student reports."
      ],
      "code_start_line": 49,
      "code_end_line": 52,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def evaluate_depth(self):\n        # 深度评估逻辑暂不实现，基于factual QA结果评估ACC\n        # 这里实现抽取student report based answer 的acc来计算最后的分数\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/reportbench/test_llm.py": [
    {
      "type": "FunctionDef",
      "name": "test_llm",
      "md_content": [
        "**test_llm**: The function of test_llm is to test the functionality of the common_chat method in the BaseAgent class.\n\n**parameters**: The parameters of this Function.\n· usr_prompt: A string that serves as the input prompt for the common_chat method.\n\n**Code Description**: The test_llm function initiates a test for the common_chat method of the BaseAgent class. It begins by defining a prompt in Chinese, which translates to \"Hello, is the common_chat call successful?\" This prompt is then passed to an instance of the BaseAgent class, which is created by invoking its constructor. The common_chat method is called on this instance with the defined prompt as an argument. The response from the common_chat method is captured in the variable 'answer'. Finally, the function prints the response to the console, prefixed by the string \"common_chat Response:\". This allows the developer to see the output generated by the common_chat method when provided with the specified prompt.\n\n**Note**: It is important to ensure that the BaseAgent class and its common_chat method are properly implemented and accessible in the context where test_llm is executed. Additionally, the prompt used in the test should be relevant to the expected functionality of the common_chat method to yield meaningful results."
      ],
      "code_start_line": 4,
      "code_end_line": 8,
      "params": [],
      "have_return": false,
      "code_content": "def test_llm():\n    prompt = \"你好，测试common_chat调用是否顺利？\"\n    agent = BaseAgent()\n    answer = agent.common_chat(usr_prompt=prompt)\n    print(\"common_chat Response:\", answer)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/reportbench/extract_ground_truth.py": [
    {
      "type": "FunctionDef",
      "name": "filter_node",
      "md_content": [
        "**filter_node**: The function of filter_node is to recursively filter a data structure, specifically a dictionary or list, by removing any entries that contain an 'id' with the letter \"s\" in it.\n\n**parameters**: The parameters of this Function.\n· node: The input data structure, which can be a dictionary, list, or any other data type.\n\n**Code Description**: The filter_node function is designed to traverse a given data structure, which may be a dictionary or a list, and filter out specific entries based on the presence of an 'id' key. If the 'id' key exists and contains the letter \"s\" (case insensitive), that entry is excluded from the output. \n\nWhen the input is a dictionary, the function checks each key-value pair. If a key-value pair meets the filtering criteria (i.e., contains an 'id' with \"s\"), it returns None for that entry. Otherwise, it recursively calls itself on the value to continue filtering deeper into the structure. The resulting filtered entries are collected into a new dictionary, which is returned at the end of the function.\n\nIf the input is a list, the function iterates through each item in the list, applying the same filtering logic. Filtered items that do not meet the exclusion criteria are appended to a new list, which is returned.\n\nIf the input is neither a dictionary nor a list, the function simply returns the input as is. This behavior ensures that the function can handle mixed data types gracefully.\n\nThe filter_node function is called within the extractDirectoryTree function, which reads a JSON file and constructs a tree structure from its contents. After loading the JSON data, extractDirectoryTree invokes filter_node to filter the data based on the specified criteria before proceeding to build a tree structure and validate it as valid JSON. This integration highlights the importance of filter_node in ensuring that only relevant data is processed and included in the final output.\n\n**Note**: It is important to ensure that the input to filter_node is either a dictionary or a list for the function to operate correctly. If the input does not conform to these types, the function will return the input unchanged.\n\n**Output Example**: Given an input like the following dictionary:\n{\n    \"id\": \"123\",\n    \"title\": \"Sample Title\",\n    \"children\": [\n        {\"id\": \"s456\", \"title\": \"Excluded Title\"},\n        {\"id\": \"789\", \"title\": \"Included Title\"}\n    ]\n}\nThe output of filter_node would be:\n{\n    \"id\": \"123\",\n    \"title\": \"Sample Title\",\n    \"children\": [\n        {\"id\": \"789\", \"title\": \"Included Title\"}\n    ]\n}"
      ],
      "code_start_line": 4,
      "code_end_line": 24,
      "params": [
        "node"
      ],
      "have_return": true,
      "code_content": "def filter_node(node):\n    # Recursively filter node: if a dict contains an 'id' with \"s\" in it, skip it.\n    if isinstance(node, dict):\n        if 'id' in node and ('s' in node['id'].lower()):\n            return None\n        new_dict = {}\n        for key, value in node.items():\n            filtered = filter_node(value)\n            # if filtering a list or dict returns a falsey (None), we still want to include keys like title/text\n            if filtered is not None:\n                new_dict[key] = filtered\n        return new_dict\n    elif isinstance(node, list):\n        new_list = []\n        for item in node:\n            filtered_item = filter_node(item)\n            if filtered_item is not None:\n                new_list.append(filtered_item)\n        return new_list\n    else:\n        return node\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/extract_ground_truth.py/extractDirectoryTree"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "build_tree",
      "md_content": [
        "**build_tree**: The function of build_tree is to recursively construct a tree structure from a given node, extracting only the \"title\" and optionally including \"children\" based on the content.\n\n**parameters**: \n- node: A dictionary or list representing the current node to be processed in the tree construction.\n\n**Code Description**:  \nThe `build_tree` function is designed to recursively transform a given node (which can be either a dictionary or a list) into a simplified tree structure. This tree only includes the \"title\" field and, when applicable, a list of \"children\" nodes, which are also processed recursively.\n\nThe function starts by checking the type of the `node`:\n1. **When the node is a dictionary**:  \n   - If the dictionary contains a \"title\" key, a new dictionary is created with just the \"title\" from the original node. \n   - It then checks for the presence of a \"content\" key, which, if it's a list, indicates that the node has child elements. The function then processes each child recursively using the same `build_tree` function.\n   - If child trees are found, they are added to the \"children\" key of the new node.\n   - If the node doesn’t contain a \"title\" but has \"content\", the function proceeds by building the children for the \"content\" and, if children are found, returns a dictionary containing just the \"children\".\n   \n2. **When the node is a list**:  \n   - The function iterates through each item in the list, calling `build_tree` recursively for each. It collects and returns all resulting trees in a list.\n   - If no valid trees are returned, it returns `None`.\n\n3. **When the node is neither a dictionary nor a list**, it returns `None`.\n\nThe function ensures that only the title and children (if any) are retained in the final tree structure, and this structure can be nested depending on the depth of the input data.\n\nFrom a functional perspective, this function is used by the `extractDirectoryTree` function to simplify and reformat a potentially complex JSON structure into a tree with only titles and hierarchical relationships. This tree structure can then be further validated or processed as needed.\n\n**Note**: \n- The input data passed into `build_tree` should be either a dictionary or a list of dictionaries that include \"title\" and optionally \"content\" keys.\n- If the \"content\" field contains a list, the function will process it recursively.\n- The function can return either a single node (with a title and children) or a list of nodes, depending on the structure of the input.\n\n**Output Example**:  \nIf the input data is as follows:\n```json\n{\n    \"title\": \"Root\",\n    \"content\": [\n        {\n            \"title\": \"Child 1\",\n            \"content\": []\n        },\n        {\n            \"title\": \"Child 2\",\n            \"content\": [\n                {\n                    \"title\": \"Grandchild 1\"\n                }\n            ]\n        }\n    ]\n}\n```\n\nThe output of the `build_tree` function would be:\n```json\n{\n    \"title\": \"Root\",\n    \"children\": [\n        {\n            \"title\": \"Child 1\"\n        },\n        {\n            \"title\": \"Child 2\",\n            \"children\": [\n                {\n                    \"title\": \"Grandchild 1\"\n                }\n            ]\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 26,
      "code_end_line": 66,
      "params": [
        "node"
      ],
      "have_return": true,
      "code_content": "def build_tree(node):\n    # Recursively build a tree with only \"title\" (and optional \"children\")\n    if isinstance(node, dict):\n        if \"title\" in node:\n            new_node = {\"title\": node[\"title\"]}\n            if \"content\" in node and isinstance(node[\"content\"], list):\n                children = []\n                for child in node[\"content\"]:\n                    child_tree = build_tree(child)\n                    if child_tree:\n                        if isinstance(child_tree, list):\n                            children.extend(child_tree)\n                        else:\n                            children.append(child_tree)\n                if children:\n                    new_node[\"children\"] = children\n            return new_node\n        else:\n            if \"content\" in node and isinstance(node[\"content\"], list):\n                children = []\n                for child in node[\"content\"]:\n                    child_tree = build_tree(child)\n                    if child_tree:\n                        if isinstance(child_tree, list):\n                            children.extend(child_tree)\n                        else:\n                            children.append(child_tree)\n                if children:\n                    return {\"children\": children}\n            return None\n    elif isinstance(node, list):\n        trees = []\n        for item in node:\n            tree = build_tree(item)\n            if tree:\n                if isinstance(tree, list):\n                    trees.extend(tree)\n                else:\n                    trees.append(tree)\n        return trees if trees else None\n    return None\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/extract_ground_truth.py/extractDirectoryTree"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "build_markdown",
      "md_content": [
        "**build_markdown**: The function of build_markdown is to recursively generate markdown text from a given JSON structure, extracting titles and sentences while ignoring references.\n\n**parameters**:\n· node: A dictionary or list representing the current level of the JSON content being processed.  \n· level: An integer representing the current level of recursion, defaulting to 1. \n\n**Code Description**:  \nThe `build_markdown` function is designed to recursively traverse a JSON structure, extracting specific content to generate markdown text. This function is primarily used to convert JSON data into a readable markdown format, which includes the titles and sentence text but excludes any reference data.\n\n1. **Input Structure**: The function accepts a `node`, which can be a dictionary or a list. If it is a dictionary, the function checks for the presence of certain keys, such as `title`, `sentences`, and `content`. The key `title` will be converted into a markdown header (using `#` characters based on the `level` argument), and `sentences` will be processed to extract the text to be included in the markdown output.\n\n2. **Markdown Construction**:\n   - If the node is a dictionary, the function checks if the node contains a `title`. If found, the title is prepended with `#` characters, based on the current level of recursion (`level`), followed by the title itself and a newline.\n   - Next, if the node has a `sentences` key containing a list, the function iterates through each sentence and appends the `text` of the sentence to the markdown.\n   - The function also recursively processes child nodes under the `content` key, if present, by calling `build_markdown` on each child node, increasing the `level` by 1 to indicate the deeper hierarchy in the markdown.\n\n3. **List Handling**: If the `node` is a list, the function processes each item in the list by calling `build_markdown` on each item. This ensures that the function can handle both list and dictionary-based JSON structures.\n\n4. **Return Value**: The function returns the accumulated markdown content as a string, which includes titles, sentences, and recursively processed content from nested structures.\n\nThis function plays a key role in processing structured data and converting it into a markdown format suitable for human-readable documentation. It is used by other parts of the project, such as in the `extractMarkdownContent` function, which loads a JSON file and passes the data to `build_markdown` to generate markdown content from the JSON structure.\n\n**Note**: This function only processes data associated with `title` and `sentences`, excluding references or other data that might be present in the JSON. Therefore, the output will consist only of text data that is relevant to the markdown generation process.\n\n**Output Example**:  \nFor a JSON structure like:\n```json\n{\n  \"title\": \"Main Title\",\n  \"sentences\": [\n    {\"text\": \"This is the first sentence.\"},\n    {\"text\": \"This is the second sentence.\"}\n  ],\n  \"content\": [\n    {\n      \"title\": \"Sub Title 1\",\n      \"sentences\": [\n        {\"text\": \"Subsentence 1.1\"}\n      ]\n    }\n  ]\n}\n```\n\nThe output markdown generated by `build_markdown` would be:\n```\n# Main Title\n\nThis is the first sentence.\n\nThis is the second sentence.\n\n## Sub Title 1\n\nSubsentence 1.1\n```"
      ],
      "code_start_line": 68,
      "code_end_line": 87,
      "params": [
        "node",
        "level"
      ],
      "have_return": true,
      "code_content": "def build_markdown(node, level=1):\n    \"\"\"\n    Recursively builds a markdown text from the JSON content.\n    Only includes 'title' and text from 'sentences', ignoring references.\n    \"\"\"\n    md = \"\"\n    if isinstance(node, dict):\n        if \"title\" in node:\n            md += (\"#\" * level) + \" \" + node[\"title\"] + \"\\n\\n\"\n        if \"sentences\" in node and isinstance(node[\"sentences\"], list):\n            for sentence in node[\"sentences\"]:\n                if \"text\" in sentence:\n                    md += sentence[\"text\"].strip() + \"\\n\\n\"\n        if \"content\" in node and isinstance(node[\"content\"], list):\n            for child in node[\"content\"]:\n                md += build_markdown(child, level+1)\n    elif isinstance(node, list):\n        for item in node:\n            md += build_markdown(item, level)\n    return md\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/extract_ground_truth.py/extractMarkdownContent"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "build_section_content_pairs",
      "md_content": [
        "**build_section_content_pairs**: The function of build_section_content_pairs is to recursively construct a tree-like structure of section-content pairs.\n\n**parameters**: The parameters of this Function.\n· parameter1: node - A dictionary representing a section of a document. This dictionary may contain a title, content, and other possible children sections.\n\n**Code Description**: The build_section_content_pairs function is designed to process a section of a document and construct a hierarchical structure of section-content pairs. This structure includes the section's title, the content within the section, references within the content, and any child sections that may exist.\n\n- The function begins by verifying that the input node is a dictionary. If the input is not a dictionary or does not contain a \"title\" key, it returns `None`, indicating that the node is not a valid section.\n- The function then initializes a `result` dictionary, which will hold the section's title.\n- A helper function, `process_sentences`, is defined and used to extract the text and references from the sentences contained within a section. It checks if the content has the \"sentences\" key and extracts the relevant information from each sentence. Text content is added to a list, and references are collected in a set to avoid duplicates.\n- The function checks if the node contains direct content that is not part of a child section. If such content exists, it processes these sentences and adds them to the result.\n- The function proceeds to gather any references associated with the text and adds them to the result. The references are sorted before being added.\n- The function then looks for child sections in the current node's content. If any child sections are found, the function recursively calls itself to process those children and add them to the \"children\" list of the result.\n- Finally, the function returns the `result` dictionary, which contains the title, content, references, and any children sections of the node, following the section-content pair structure.\n\nThis function is called by the `extractSectionContentPairs` function. The `extractSectionContentPairs` function reads a JSON file, loads the data, and calls `build_section_content_pairs` to construct the section-content pairs. After constructing the structure, it validates the structure by attempting to serialize it back into a JSON string and then deserializing it to ensure that the structure is valid. The validated section-content pairs are then returned.\n\n**Note**: \n- This function assumes that the input node is a valid section that may or may not contain child sections. \n- The text content in the section is extracted from sentences within the node and any items in the \"content\" list that are not child sections.\n- Only sections with a \"title\" are processed, while non-section content is treated as plain text and processed accordingly.\n- The function ensures that references are collected without duplication by using a set, which is later sorted before being returned.\n  \n**Output Example**: A possible return value could look like this:\n\n```json\n{\n    \"title\": \"Introduction\",\n    \"content\": \"This is the introductory section of the document.\",\n    \"references\": [\"ref1\", \"ref2\"],\n    \"children\": [\n        {\n            \"title\": \"Background\",\n            \"content\": \"This section provides the background information.\",\n            \"references\": [\"ref3\"],\n            \"children\": []\n        },\n        {\n            \"title\": \"Objectives\",\n            \"content\": \"This section outlines the objectives of the study.\",\n            \"references\": [\"ref4\", \"ref5\"],\n            \"children\": []\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 89,
      "code_end_line": 151,
      "params": [
        "node"
      ],
      "have_return": true,
      "code_content": "def build_section_content_pairs(node):\n    \"\"\"\n    递归构建section-content pairs格式的树状结构:\n    {\n        \"title\": str,            # section标题\n        \"content\": str,          # section直接包含的文本内容\n        \"references\": [str],     # 文本内容的引用\n        \"children\": [{...}]      # 子sections (可选)\n    }\n    \"\"\"\n    if not isinstance(node, dict):\n        return None\n\n    # 如果不是section节点(没有title)，返回None\n    if \"title\" not in node:\n        return None\n    \n    result = {\n        \"title\": node[\"title\"]\n    }\n    \n    # 收集当前节点的文本内容和引用\n    text_parts = []\n    references = set()\n\n    # 处理当前节点及其直接内容的sentences\n    def process_sentences(content):\n        if isinstance(content, dict) and \"sentences\" in content:\n            for sentence in content[\"sentences\"]:\n                if isinstance(sentence, dict):\n                    if \"text\" in sentence:\n                        text_parts.append(sentence[\"text\"].strip())\n                    if \"references\" in sentence and isinstance(sentence[\"references\"], list):\n                        references.update(sentence[\"references\"])\n\n    # 处理当前节点的直接sentences\n    process_sentences(node)\n\n    # 处理content列表中的直接sentences\n    if \"content\" in node and isinstance(node[\"content\"], list):\n        for item in node[\"content\"]:\n            if not isinstance(item, dict) or \"title\" not in item:  # 只处理非section的直接内容\n                process_sentences(item)\n\n    # 如果收集到了文本内容，添加到结果中\n    if text_parts:\n        result[\"content\"] = \" \".join(text_parts)\n        if references:\n            result[\"references\"] = sorted(list(references))\n\n    # 处理子sections\n    children = []\n    if \"content\" in node and isinstance(node[\"content\"], list):\n        for child in node[\"content\"]:\n            if isinstance(child, dict) and \"title\" in child:  # 只处理作为section的子节点\n                child_result = build_section_content_pairs(child)\n                if child_result:\n                    children.append(child_result)\n    \n    if children:\n        result[\"children\"] = children\n\n    return result\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/extract_ground_truth.py/extractSectionContentPairs"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_sentences",
      "md_content": [
        "**process_sentences**: The function of process_sentences is to extract and collect specific information from a structured content input, particularly focusing on sentence text and references.\n\n**parameters**: The parameters of this function.\n· content: A dictionary that may contain a list of sentences, each potentially with text and references.\n\n**Code Description**: The function `process_sentences` processes the input `content`, which is expected to be a dictionary. It first checks if the dictionary contains a key named \"sentences\". If this key exists, the function proceeds to iterate over each item in the \"sentences\" list. For each item in the list, the function performs further checks:\n1. It ensures that the item is a dictionary.\n2. If the dictionary contains a key \"text\", it appends the stripped text of the sentence to a global or previously defined list, `text_parts`.\n3. Additionally, if the dictionary contains a \"references\" key and the value associated with this key is a list, it updates a global or previously defined set, `references`, by adding all items from the references list.\n\nThe function is intended to process structured sentence data, extracting relevant text and references, which can later be used for further processing or analysis.\n\n**Note**: \n- The function does not return any value; it modifies external variables (`text_parts` and `references`).\n- The function assumes that the structure of `content` and each sentence is consistent with the described format. Any deviation (e.g., missing expected keys or non-list values where lists are expected) may lead to no processing or errors.\n- The variables `text_parts` and `references` should be initialized before the function is called to ensure that the function has a place to store the extracted data."
      ],
      "code_start_line": 115,
      "code_end_line": 122,
      "params": [
        "content"
      ],
      "have_return": false,
      "code_content": "    def process_sentences(content):\n        if isinstance(content, dict) and \"sentences\" in content:\n            for sentence in content[\"sentences\"]:\n                if isinstance(sentence, dict):\n                    if \"text\" in sentence:\n                        text_parts.append(sentence[\"text\"].strip())\n                    if \"references\" in sentence and isinstance(sentence[\"references\"], list):\n                        references.update(sentence[\"references\"])\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extractDirectoryTree",
      "md_content": [
        "**extractDirectoryTree**: The function of extractDirectoryTree is to read a JSON file, filter its contents based on specific criteria, build a hierarchical tree structure from the filtered data, and validate the resulting structure as valid JSON.\n\n**parameters**: \n· input_file_path: A string representing the path to the input JSON file that will be read and processed.\n\n**Code Description**: The extractDirectoryTree function begins by opening and reading a JSON file specified by the input_file_path parameter. It uses the json library to load the contents of the file into a Python data structure. The function then calls the filter_node function to filter the loaded data. This filtering process removes any entries that contain an 'id' with the letter \"s\" in it, ensuring that only relevant data is retained for further processing.\n\nAfter filtering, the function invokes the build_tree function, which constructs a simplified tree structure that includes only the \"title\" and its associated \"children\" nodes. This tree structure is essential for representing the hierarchical relationships within the data.\n\nTo ensure the integrity of the resulting tree structure, the function attempts to serialize the tree into a JSON string and then deserialize it back into a Python object. This step serves as a validation check to confirm that the tree structure is valid JSON. If the serialization or deserialization fails, a ValueError is raised, indicating that the structure is invalid.\n\nFinally, the function returns the validated tree structure, which can be utilized by other components of the application. The extractDirectoryTree function is called within the __init__ method of the ReportBenchmark class, where it is used to extract a breadth ground truth representation from the specified JSON input path. This integration highlights the function's role in preparing data for further analysis and reporting within the broader context of the application.\n\n**Note**: It is crucial that the input JSON file adheres to the expected structure, as the filtering and tree-building processes depend on the presence of specific keys such as \"id\" and \"title\". Any deviations from this structure may lead to unexpected results or errors during execution.\n\n**Output Example**: Given a valid input JSON file, the output of the extractDirectoryTree function might resemble the following structure:\n```json\n{\n    \"title\": \"Root Node\",\n    \"children\": [\n        {\n            \"title\": \"Child Node 1\"\n        },\n        {\n            \"title\": \"Child Node 2\",\n            \"children\": [\n                {\n                    \"title\": \"Grandchild Node\"\n                }\n            ]\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 153,
      "code_end_line": 172,
      "params": [
        "input_file_path"
      ],
      "have_return": true,
      "code_content": "def extractDirectoryTree(input_file_path):\n    # Read original JSON file\n    with open(input_file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    # Filter JSON tree based on node \"id\"\n    filtered_data = filter_node(data)\n\n    # Build tree containing only the layer structure and titles\n    tree_structure = build_tree(filtered_data)\n\n    # Validate that tree_structure is valid JSON by serializing and deserializing it\n    try:\n        s = json.dumps(tree_structure)\n        valid_tree = json.loads(s)\n    except Exception as e:\n        raise ValueError(\"Invalid JSON structure: \" + str(e))\n    \n    # Return the valid JSON structure\n    return valid_tree\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/__init__",
        "src/criticsearch/reportbench/reward_calculation.py"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/extract_ground_truth.py/filter_node",
        "src/criticsearch/reportbench/extract_ground_truth.py/build_tree"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "extractMarkdownContent",
      "md_content": [
        "**extractMarkdownContent**: The function of extractMarkdownContent is to read a JSON file and convert its content into a markdown format.\n\n**parameters**: The parameters of this Function.\n· input_file_path: A string representing the path to the input JSON file that contains the data to be processed.\n\n**Code Description**: The extractMarkdownContent function is responsible for reading a JSON file specified by the input_file_path parameter. It opens the file in read mode with UTF-8 encoding and loads its content into a Python dictionary using the json.load function. Once the JSON data is loaded, the function calls the build_markdown function, passing the loaded data as an argument. The build_markdown function processes the JSON structure recursively, extracting titles and sentences to generate a markdown representation of the content. After the markdown text is generated, extractMarkdownContent returns this text as a string instead of saving it to a file.\n\nThis function is utilized within the ReportBenchmark class's __init__ method, where it is called to extract markdown content from the JSON input path provided during the instantiation of the class. The markdown content generated by extractMarkdownContent is then used to create sections for the report, demonstrating its role in transforming structured JSON data into a human-readable markdown format.\n\n**Note**: It is important to ensure that the input JSON file is correctly formatted and contains the expected structure, as the function relies on the presence of specific keys to generate the markdown content accurately.\n\n**Output Example**: For a JSON structure like:\n```json\n{\n  \"title\": \"Sample Title\",\n  \"sentences\": [\n    {\"text\": \"This is a sample sentence.\"},\n    {\"text\": \"This is another sample sentence.\"}\n  ],\n  \"content\": [\n    {\n      \"title\": \"Subsection Title\",\n      \"sentences\": [\n        {\"text\": \"This is a sentence in the subsection.\"}\n      ]\n    }\n  ]\n}\n```\nThe output markdown generated by extractMarkdownContent would be:\n```\n# Sample Title\n\nThis is a sample sentence.\n\nThis is another sample sentence.\n\n## Subsection Title\n\nThis is a sentence in the subsection.\n```"
      ],
      "code_start_line": 174,
      "code_end_line": 183,
      "params": [
        "input_file_path"
      ],
      "have_return": true,
      "code_content": "def extractMarkdownContent(input_file_path):\n    # Read original JSON file\n    with open(input_file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    # Build markdown text from JSON structure\n    md_text = build_markdown(data)\n    \n    # Return markdown text instead of saving to a file\n    return md_text\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/__init__"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/extract_ground_truth.py/build_markdown"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "extractSectionContentPairs",
      "md_content": [
        "**extractSectionContentPairs**: The function of extractSectionContentPairs is to extract a structured set of section-content pairs from a JSON file and validate the structure.\n\n**parameters**:\n· parameter1: input_file_path (str) - The path to the JSON file that contains the data to be processed.\n\n**Code Description**: The function `extractSectionContentPairs` is designed to extract and return a structured set of section-content pairs from a given JSON file. It follows these main steps:\n\n1. **Reading the JSON File**: The function first opens and reads the JSON file specified by the `input_file_path` parameter. It uses Python's built-in `json.load()` method to parse the contents of the file into a Python dictionary.\n\n2. **Building Section-Content Pairs**: After loading the data, the function calls the `build_section_content_pairs` function, which is responsible for constructing the hierarchical structure of section-content pairs from the data. This function processes the JSON structure recursively to create a tree-like representation of the sections and their associated content.\n\n3. **Validating the Structure**: The function then attempts to validate the generated structure by serializing it into a JSON string using `json.dumps()` and deserializing it back into a Python object using `json.loads()`. This step ensures that the resulting data structure adheres to valid JSON formatting. If any error occurs during this process (e.g., invalid or malformed data), a `ValueError` is raised with a message indicating the specific issue.\n\n4. **Returning the Validated Pairs**: After validation, the function returns the validated section-content pairs structure, which is in the form of a dictionary containing titles, content, references, and child sections (if any).\n\nIn the broader context of the project, the `extractSectionContentPairs` function is invoked within the `ReportBenchmark` class's `__init__` method. Specifically, it is called to extract the section-content pairs from the input JSON file, which is then used as part of the initialization process for creating a report benchmark. The section-content pairs are essential for further processing and organizing the content into structured sections.\n\nThe function also relies on the `build_section_content_pairs` function to construct the section-content pairs. This helper function is responsible for processing each section recursively and ensuring that sections, their content, and any child sections are correctly represented in the final structure.\n\n**Note**: \n- The input file must be a valid JSON file containing a structure that can be processed into section-content pairs.\n- The validation step ensures that the resulting structure is compatible with standard JSON formatting, which prevents issues during further processing.\n- The `build_section_content_pairs` function should return a valid structure for each section in the input data to ensure proper functionality.\n\n**Output Example**: A possible return value from the `extractSectionContentPairs` function could look like this:\n\n```json\n{\n    \"title\": \"Introduction\",\n    \"content\": \"This is the introductory section of the document.\",\n    \"references\": [\"ref1\", \"ref2\"],\n    \"children\": [\n        {\n            \"title\": \"Background\",\n            \"content\": \"This section provides the background information.\",\n            \"references\": [\"ref3\"],\n            \"children\": []\n        },\n        {\n            \"title\": \"Objectives\",\n            \"content\": \"This section outlines the objectives of the study.\",\n            \"references\": [\"ref4\", \"ref5\"],\n            \"children\": []\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 185,
      "code_end_line": 203,
      "params": [
        "input_file_path"
      ],
      "have_return": true,
      "code_content": "def extractSectionContentPairs(input_file_path):\n    \"\"\"\n    从JSON文件中提取section-content pairs结构\n    \"\"\"\n    # 读取原始JSON文件\n    with open(input_file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    # 构建section-content pairs结构\n    pairs_structure = build_section_content_pairs(data)\n    \n    # 验证JSON结构\n    try:\n        s = json.dumps(pairs_structure)\n        valid_pairs = json.loads(s)\n    except Exception as e:\n        raise ValueError(\"Invalid JSON structure: \" + str(e))\n    \n    return valid_pairs\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/__init__"
      ],
      "reference_who": [
        "src/criticsearch/reportbench/extract_ground_truth.py/build_section_content_pairs"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "extract_markdown_sections",
      "md_content": [
        "**extract_markdown_sections**: The function of extract_markdown_sections is to extract markdown sections based on header lines.\n\n**parameters**:\n· md_text: A string representing the markdown text from which sections will be extracted.\n\n**Code Description**: The `extract_markdown_sections` function processes a given markdown text (`md_text`) and divides it into sections based on headers, which are lines that start with the `#` symbol. It iterates through each line of the markdown text, identifying these header lines and treating them as boundaries between different sections. \n\n- The function initializes two variables: `sections` (an empty list to store the resulting sections) and `current_section` (an empty list that temporarily holds the lines of the current section being processed).\n- It then loops through each line of the markdown text:\n  - If the line starts with `#` (indicating a header), the function checks whether there is any content in `current_section`. If so, it joins the lines of `current_section` into a single string, strips any leading or trailing whitespace, and appends it to the `sections` list.\n  - After handling the previous section, `current_section` is reset to an empty list, and the new section starts accumulating lines.\n  - If the line does not start with a header, it is simply added to the current section being processed.\n- After the loop finishes, the function checks if there is any remaining content in `current_section` and appends it to the `sections` list if necessary.\n- The function returns the `sections` list, where each element is a string representing a separate section of the original markdown content.\n\nThis function is primarily called within the `ReportBenchmark` class constructor in `src/criticsearch/reportbench/report_benchmark.py`. In that context, it is used to process the markdown content of a report, splitting the content into distinct sections, each corresponding to a markdown header. These sections are then stored in the `sections` attribute, which can be used later in the class for generating reports or performing other operations related to the markdown content.\n\n**Note**: \n- The function assumes that the markdown content is formatted correctly with headers starting with `#`.\n- It does not handle cases where headers are malformed or have no content between them.\n- The function processes the markdown text line by line, so it may not handle large files efficiently in cases of extreme text sizes.\n\n**Output Example**:\nGiven an input markdown text:\n\n```\n# Section 1\nThis is the first section.\n\n# Section 2\nThis is the second section.\n```\n\nThe function would return the following list:\n\n```\n[\n    \"# Section 1\\nThis is the first section.\",\n    \"# Section 2\\nThis is the second section.\"\n]\n```"
      ],
      "code_start_line": 205,
      "code_end_line": 221,
      "params": [
        "md_text"
      ],
      "have_return": true,
      "code_content": "def extract_markdown_sections(md_text):\n    \"\"\"\n    Extract markdown sections based on header lines.\n    遇到新的标题（以#开头）则开始新的 section，\n    返回一个包含各 section 的列表，每个 section 为一个字符串。\n    \"\"\"\n    sections = []\n    current_section = []\n    for line in md_text.splitlines():\n        if line.strip().startswith(\"#\"):\n            if current_section:\n                sections.append(\"\\n\".join(current_section).strip())\n                current_section = []\n        current_section.append(line)\n    if current_section:\n        sections.append(\"\\n\".join(current_section).strip())\n    return sections\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "src/criticsearch/reportbench/report_benchmark.py",
        "src/criticsearch/reportbench/report_benchmark.py/ReportBenchmark/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "src/criticsearch/reportbench/reward_calculation.py": [
    {
      "type": "ClassDef",
      "name": "RewardCalculator",
      "md_content": [
        "**RewardCalculator**: The function of RewardCalculator is to manage and store the details related to the current section's name, its ground truth, and the student's answer for evaluation.\n\n**attributes**: The attributes of this Class.\n- current_section_name: Holds the name of the current section being evaluated or processed.\n- current_section_ground_truth: Contains the ground truth data for the current section, which is used for comparison with the student’s answer.\n- current_section_student_answer: Stores the student's answer for the current section to be evaluated against the ground truth.\n\n**Code Description**:  \nThe `RewardCalculator` class is a simple container for tracking information related to a specific section within a larger evaluation framework. It holds three key attributes:\n1. `current_section_name`: This attribute is used to store the name of the section being worked on or evaluated. It could be a string representing the section's title or identifier, which the model uses to guide its operations.\n2. `current_section_ground_truth`: This attribute holds the correct or expected answer for the section, often referred to as the \"ground truth.\" It is likely to be used for comparison with the student’s answer to determine correctness or performance.\n3. `current_section_student_answer`: This stores the answer provided by the student for the current section. This answer is intended to be evaluated against the `current_section_ground_truth`.\n\nThe class does not have any methods or other functionality defined beyond these attributes. The primary role of this class appears to be to act as a data holder for the current section’s name, the correct answer, and the student's response, which can then be used in further processing, such as scoring or feedback generation.\n\n**Note**:  \n- The class is designed with simple attributes and currently lacks any methods for manipulating or processing the data stored within. It could be extended in the future to include methods for evaluating the student's answer or generating reports based on the ground truth.\n- The attributes are set to `None` by default, indicating that they are initially unassigned. It is expected that the attributes will be set to specific values during the execution of the larger program that utilizes this class."
      ],
      "code_start_line": 3,
      "code_end_line": 7,
      "params": [],
      "have_return": false,
      "code_content": "class RewardCalculator:\n    def __init__(self):\n        self.current_section_name = None  # Guide model to search/generate current section\n        self.current_section_ground_truth = None\n        self.current_section_student_answer = None\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the attributes of the object to their default values.\n\n**parameters**: The __init__ function does not accept any parameters apart from the default 'self' parameter, which refers to the instance of the object being created.\n\n**Code Description**:  \nThis function is the constructor for the class and is called automatically when a new instance of the class is created. It sets the initial values of three instance attributes:\n- `self.current_section_name`: Initialized to `None`. This attribute is intended to hold the name of the current section, which can be used by the guide model to search or generate the relevant section.\n- `self.current_section_ground_truth`: Also initialized to `None`. This attribute is meant to store the ground truth for the current section, possibly representing the correct or expected content for comparison or validation purposes.\n- `self.current_section_student_answer`: Set to `None` initially. This attribute is meant to store the student’s answer for the current section.\n\nThe default assignment of `None` allows these attributes to be dynamically updated later in the program's execution as the object interacts with data related to the current section being processed.\n\n**Note**: This constructor method is essential for initializing the object's state when it is instantiated. The attributes set to `None` allow for flexible handling of data later in the object’s lifecycle."
      ],
      "code_start_line": 4,
      "code_end_line": 7,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.current_section_name = None  # Guide model to search/generate current section\n        self.current_section_ground_truth = None\n        self.current_section_student_answer = None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ]
}