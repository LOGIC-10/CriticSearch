{
  "critic_agent/critic_agent.py": [
    {
      "type": "ClassDef",
      "name": "CriticAgent",
      "md_content": [
        "**CriticAgent**: The function of CriticAgent is to generate critiques based on user questions and agent responses.\n\n**attributes**: The attributes of this Class.\n· original_task: A string that holds the user's original question or task.\n· critic_prompt: A template used for generating critiques, retrieved from the environment.\n· agent_answer: A string that stores the answer provided by the agent.\n\n**Code Description**: The CriticAgent class inherits from the BaseAgent class and is designed to facilitate the generation of critiques for agent responses to user questions. Upon initialization, it sets up the original task as an empty string and retrieves a template for critiques from the environment, specifically from a file named 'critic_agent.txt'. \n\nThe class contains several methods:\n- The `__init__` method initializes the instance of the CriticAgent, setting up the original task and loading the critique template.\n- The `critic` method is responsible for generating a critique. It first gathers the necessary data by calling the `get_data_for_critic` method, which collects the user's question and the agent's answer. It then utilizes the `chat_with_template` method to interact with the critique template and obtain a response from the model. The response is expected to be in YAML format, which is validated and formatted. If the response contains invalid YAML, an error message is printed, and the method returns None.\n- The `receive_agent_answer` method allows the CriticAgent to store the agent's answer for later critique.\n- The `get_data_for_critic` method constructs and returns a dictionary containing the original user question and the agent's answer, which is essential for generating the critique.\n\n**Note**: It is important to ensure that the agent's answer is properly received before invoking the `critic` method, as the critique generation relies on this data. Additionally, users should handle potential YAML errors gracefully when utilizing the `critic` method.\n\n**Output Example**: A possible return value from the `critic` method could be a formatted YAML string such as:\n```yaml\ncritique:\n  - feedback: \"The response was clear and concise.\"\n  - suggestions:\n      - \"Consider providing more examples.\"\n      - \"Ensure to address all parts of the user's question.\"\n```"
      ],
      "code_start_line": 18,
      "code_end_line": 46,
      "params": [],
      "have_return": true,
      "code_content": "class CriticAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.original_task = ''\n        self.critic_prompt = self.env.get_template('critic_agent.txt')\n\n    def critic(self):\n        \"\"\"\n        生成评论。\n        \"\"\"\n        data = self.get_data_for_critic()\n        model_response = self.chat_with_template(data, self.critic_prompt)\n\n        try:\n            formatted_yaml = self.extract_and_validate_yaml(model_response)\n            return formatted_yaml\n        \n        except yaml.YAMLError as exc:\n            print(f\"Invalid YAML content: {exc}\")\n            return None\n    \n    def receive_agent_answer(self, agent_answer):\n        self.agent_answer = agent_answer\n\n    def get_data_for_critic(self):\n        return {\n            'user_question': self.original_task,\n            'agent_answer': self.agent_answer\n        }\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the CriticAgent class.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The __init__ function is a constructor method that is called when an instance of the CriticAgent class is created. It first invokes the constructor of its parent class using `super().__init__()`, ensuring that any initialization defined in the parent class is executed. Following this, it initializes an instance variable `original_task` to an empty string, which is likely intended to hold a description or identifier of the task that the agent will be working on. Additionally, it initializes another instance variable `critic_prompt` by calling `self.env.get_template('critic_agent.txt')`. This line suggests that the CriticAgent class is associated with an environment that can retrieve templates, and it specifically loads a template named 'critic_agent.txt'. This template may be used later in the class for generating prompts or responses related to the critic agent's functionality.\n\n**Note**: It is important to ensure that the environment (`self.env`) is properly set up before this constructor is called, as it relies on the `get_template` method to function correctly. Additionally, the `original_task` variable should be assigned a meaningful value before it is used in any operations to avoid issues with uninitialized data."
      ],
      "code_start_line": 19,
      "code_end_line": 22,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super().__init__()\n        self.original_task = ''\n        self.critic_prompt = self.env.get_template('critic_agent.txt')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "critic",
      "md_content": [
        "**critic**: The function of critic is to generate a review based on user input and agent responses.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The critic method is a member of the CriticAgent class, responsible for producing a critique based on the interaction between the user and the agent. It begins by invoking the get_data_for_critic method, which retrieves essential data in the form of a dictionary containing the user's question and the agent's answer. This data is then passed to the chat_with_template method along with a predefined prompt (self.critic_prompt) to generate a model response.\n\nThe expected output from chat_with_template is a string formatted in YAML. The critic method subsequently attempts to validate and format this YAML content using the extract_and_validate_yaml method. If the YAML content is valid, it is returned as the output of the critic method. However, if a YAMLError occurs during this validation process, an error message is printed to the console, and the method returns None.\n\nThe relationship between critic and its callees is crucial for its functionality. The get_data_for_critic method provides the necessary context for the critique by supplying the user question and agent answer. The chat_with_template method is responsible for generating the critique based on this context, while extract_and_validate_yaml ensures that the output is in a valid format. This structured flow ensures that the critique process is both systematic and reliable.\n\n**Note**: It is important to ensure that the instance variables self.original_task and self.agent_answer are properly initialized before calling this function to avoid returning None or causing errors in the subsequent processing.\n\n**Output Example**: A possible return value from the critic method could be a well-structured YAML string, such as:\n```yaml\nreview:\n  user_question: \"What is the capital of France?\"\n  agent_answer: \"The capital of France is Paris.\"\n  critique: \"The answer is accurate and concise.\"\n```"
      ],
      "code_start_line": 24,
      "code_end_line": 37,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def critic(self):\n        \"\"\"\n        生成评论。\n        \"\"\"\n        data = self.get_data_for_critic()\n        model_response = self.chat_with_template(data, self.critic_prompt)\n\n        try:\n            formatted_yaml = self.extract_and_validate_yaml(model_response)\n            return formatted_yaml\n        \n        except yaml.YAMLError as exc:\n            print(f\"Invalid YAML content: {exc}\")\n            return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "critic_agent/critic_agent.py/CriticAgent/get_data_for_critic"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "receive_agent_answer",
      "md_content": [
        "**receive_agent_answer**: The function of receive_agent_answer is to store the answer provided by an agent.\n\n**parameters**: The parameters of this Function.\n· agent_answer: This parameter represents the answer received from an agent, which is expected to be of any data type that can be assigned to the instance variable.\n\n**Code Description**: The receive_agent_answer function is a method defined within the CriticAgent class. Its primary purpose is to accept an input parameter named agent_answer and assign this value to an instance variable also named agent_answer. This method effectively allows the CriticAgent instance to store the response from an agent for later use or reference. The assignment operation is straightforward, ensuring that whatever value is passed to the function is directly saved as part of the object's state.\n\n**Note**: It is important to ensure that the agent_answer parameter is provided in the correct format expected by the application, as this method does not perform any validation or type checking on the input. The stored value can be accessed later through the instance variable, which may be used in further processing or decision-making within the CriticAgent class."
      ],
      "code_start_line": 39,
      "code_end_line": 40,
      "params": [
        "self",
        "agent_answer"
      ],
      "have_return": false,
      "code_content": "    def receive_agent_answer(self, agent_answer):\n        self.agent_answer = agent_answer\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_data_for_critic",
      "md_content": [
        "**get_data_for_critic**: The function of get_data_for_critic is to retrieve the original user question and the agent's answer in a structured format.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The get_data_for_critic function is a method within the CriticAgent class that returns a dictionary containing two key pieces of information: 'user_question' and 'agent_answer'. The value for 'user_question' is obtained from the instance variable self.original_task, which presumably holds the question posed by the user. The value for 'agent_answer' is derived from another instance variable, self.agent_answer, which likely contains the response generated by the agent to the user's question.\n\nThis function is called within the critic method of the same class. The critic method is responsible for generating a review or critique based on the interaction between the user and the agent. It first calls get_data_for_critic to gather the necessary data, which is then used to create a model response through the chat_with_template method. The output from chat_with_template is expected to be in a YAML format, which is subsequently validated and formatted. If the YAML content is invalid, an error is caught, and a message is printed.\n\nThus, get_data_for_critic plays a crucial role in providing the foundational data needed for the critique process, ensuring that the user question and agent's answer are readily accessible for further processing.\n\n**Note**: It is important to ensure that the instance variables self.original_task and self.agent_answer are properly initialized before calling this function to avoid returning None or causing errors in the subsequent processing.\n\n**Output Example**: An example of the return value from get_data_for_critic could look like this:\n{\n    'user_question': 'What is the capital of France?',\n    'agent_answer': 'The capital of France is Paris.'\n}"
      ],
      "code_start_line": 42,
      "code_end_line": 46,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_data_for_critic(self):\n        return {\n            'user_question': self.original_task,\n            'agent_answer': self.agent_answer\n        }\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "critic_agent/critic_agent.py/CriticAgent/critic"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "agent_factory/tools.py": [
    {
      "type": "ClassDef",
      "name": "GlobalContextNotebook",
      "md_content": [
        "**GlobalContextNotebook**: The function of GlobalContextNotebook is to manage and optimize the storage and retrieval of text embeddings and knowledge graphs for enhanced contextual understanding.\n\n**attributes**: The attributes of this Class.\n· text_embeddings: A dictionary that stores text embeddings, which are numerical representations of text data used for various natural language processing tasks.\n· knowledge_graph: A dictionary that maintains a knowledge graph, representing relationships between different entities or concepts.\n· optimized_subgraphs: A dictionary that contains optimized subgraphs, which are refined portions of the knowledge graph aimed at improving efficiency in data retrieval and processing.\n\n**Code Description**: The GlobalContextNotebook class is designed to facilitate the management of contextual information through the use of embeddings and knowledge graphs. Upon initialization, it creates three empty dictionaries: text_embeddings, knowledge_graph, and optimized_subgraphs. \n\nThe class includes three methods:\n\n1. **update(state, node)**: This method is intended to update the text embeddings and the knowledge graph based on the current state and a specific node. Although the implementation is not provided (indicated by the 'pass' statement), it is expected to modify the internal dictionaries to reflect new information or changes in the context.\n\n2. **find_related_paths(state)**: This method aims to identify and return optimized subgraphs that are related to the given state. The specifics of how the related paths are determined are not detailed in the code, but the method is crucial for efficiently navigating the knowledge graph.\n\n3. **prune()**: This method is responsible for removing ineffective workflows from the knowledge graph or the optimized subgraphs. The exact criteria for what constitutes an \"ineffective\" workflow are not specified, but this function is essential for maintaining the relevance and efficiency of the stored data.\n\n**Note**: Users of the GlobalContextNotebook class should be aware that the methods provided do not contain complete implementations. The update, find_related_paths, and prune methods require further development to fulfill their intended purposes effectively. Additionally, the management of the dictionaries should be handled carefully to ensure data integrity and relevance.\n\n**Output Example**: An example of the potential output from the find_related_paths method could be a list of optimized subgraphs represented as dictionaries, such as:\n{\n    \"subgraph_1\": {\"node_a\": \"related_info_1\", \"node_b\": \"related_info_2\"},\n    \"subgraph_2\": {\"node_c\": \"related_info_3\", \"node_d\": \"related_info_4\"}\n}"
      ],
      "code_start_line": 2,
      "code_end_line": 18,
      "params": [],
      "have_return": true,
      "code_content": "class GlobalContextNotebook:\n    def __init__(self):\n        self.text_embeddings = {}\n        self.knowledge_graph = {}\n        self.optimized_subgraphs = {}\n\n    def update(self, state, node):\n        # Update embeddings and knowledge graph\n        pass\n\n    def find_related_paths(self, state):\n        # Find and return related optimized subgraphs\n        pass\n\n    def prune(self):\n        # Remove ineffective workflows\n        pass\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the GlobalContextNotebook object by setting up its internal data structures.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The __init__ function is a constructor for the GlobalContextNotebook class. When an instance of this class is created, this function is automatically called to initialize the object. It sets up three internal attributes: \n- `self.text_embeddings`: This is initialized as an empty dictionary, which is intended to store text embeddings. Text embeddings are numerical representations of text that can be used in various machine learning and natural language processing tasks.\n- `self.knowledge_graph`: This is also initialized as an empty dictionary. It is likely intended to hold a knowledge graph, which is a structured representation of knowledge that can include entities and their relationships.\n- `self.optimized_subgraphs`: This is initialized as an empty dictionary as well. This attribute may be used to store optimized versions of subgraphs derived from the knowledge graph, potentially for improved performance in querying or analysis.\n\nOverall, the __init__ function establishes the foundational data structures necessary for the GlobalContextNotebook to function effectively in managing text embeddings, knowledge graphs, and their optimized representations.\n\n**Note**: It is important to ensure that the attributes initialized in this function are properly populated and managed throughout the lifecycle of the GlobalContextNotebook object to maintain the integrity and functionality of the class."
      ],
      "code_start_line": 3,
      "code_end_line": 6,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.text_embeddings = {}\n        self.knowledge_graph = {}\n        self.optimized_subgraphs = {}\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "update",
      "md_content": [
        "**update**: The function of update is to update embeddings and the knowledge graph based on the provided state and node.\n\n**parameters**: The parameters of this Function.\n· state: Represents the current state of the system or model that is being updated. It contains the necessary information required for the update process.\n· node: Refers to a specific node in the knowledge graph that is targeted for the update. This could represent an entity or a relationship that needs to be modified or enhanced.\n\n**Code Description**: The update function is designed to facilitate the process of updating embeddings and the knowledge graph. Although the function body currently contains a placeholder (pass), it is intended to implement the logic necessary to modify the embeddings and the knowledge graph based on the input parameters. The state parameter is crucial as it provides the context or conditions under which the update should occur. The node parameter specifies which part of the knowledge graph is to be updated, allowing for targeted modifications. This function is likely part of a larger system that manages embeddings and knowledge graphs, and its implementation will be essential for maintaining the accuracy and relevance of the data represented in these structures.\n\n**Note**: It is important to implement the logic within this function to ensure that the embeddings and knowledge graph are updated correctly. Proper handling of the state and node parameters is critical for the function to perform its intended purpose effectively."
      ],
      "code_start_line": 8,
      "code_end_line": 10,
      "params": [
        "self",
        "state",
        "node"
      ],
      "have_return": false,
      "code_content": "    def update(self, state, node):\n        # Update embeddings and knowledge graph\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "find_related_paths",
      "md_content": [
        "**find_related_paths**: The function of find_related_paths is to find and return related optimized subgraphs.\n\n**parameters**: The parameters of this Function.\n· state: This parameter represents the current state of the context in which the function is operating. It is expected to contain information necessary for identifying related paths.\n\n**Code Description**: The find_related_paths function is designed to identify and return subgraphs that are optimized and related to the current state provided as an argument. The function currently contains a placeholder implementation (indicated by the 'pass' statement), which means that the actual logic for finding these related paths has not yet been implemented. The intended functionality suggests that the function will analyze the provided state to determine which subgraphs are relevant and optimized for the given context. This could involve traversing a graph structure, applying optimization criteria, and collecting the relevant subgraphs for output.\n\n**Note**: It is important to implement the logic within this function to ensure it performs the intended operations. Additionally, the function should handle various states appropriately to avoid errors or unexpected behavior.\n\n**Output Example**: A possible return value of the function could be a list of optimized subgraphs represented in a specific format, such as:\n[\n    {'subgraph_id': 1, 'nodes': [1, 2, 3], 'edges': [(1, 2), (2, 3)]},\n    {'subgraph_id': 2, 'nodes': [4, 5], 'edges': [(4, 5)]}\n] \nThis example illustrates a scenario where two optimized subgraphs are returned, each with its own unique identifier, nodes, and edges."
      ],
      "code_start_line": 12,
      "code_end_line": 14,
      "params": [
        "self",
        "state"
      ],
      "have_return": true,
      "code_content": "    def find_related_paths(self, state):\n        # Find and return related optimized subgraphs\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "prune",
      "md_content": [
        "**prune**: The function of prune is to remove ineffective workflows.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The prune function is designed to eliminate workflows that are deemed ineffective within the context of the application. Currently, the function body contains a placeholder comment indicating its intended purpose, but it does not implement any logic or functionality at this time. The use of the `pass` statement signifies that the function is incomplete and serves as a stub for future development. When fully implemented, this function will likely include logic to assess the effectiveness of various workflows and subsequently remove those that do not meet certain criteria or performance metrics.\n\n**Note**: It is important to recognize that as it stands, the prune function does not perform any operations. Developers intending to utilize this function should ensure that it is properly implemented before calling it in their workflows. Additionally, any criteria for determining the effectiveness of workflows should be clearly defined and integrated into the function's logic to achieve the desired outcome."
      ],
      "code_start_line": 16,
      "code_end_line": 18,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def prune(self):\n        # Remove ineffective workflows\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "agent_factory/utils.py": [
    {
      "type": "FunctionDef",
      "name": "read_prompt_template",
      "md_content": [
        "**read_prompt_template**: The function of read_prompt_template is to read the contents of a specified file and return it as a string.\n\n**parameters**: The parameters of this Function.\n· file_path: A string representing the path to the file that contains the prompt template to be read.\n\n**Code Description**: The read_prompt_template function is designed to open a file located at the path specified by the file_path parameter. It uses a context manager (the 'with' statement) to ensure that the file is properly opened and closed after its contents are read. Inside the context manager, the file is opened in read mode ('r'), and the entire content of the file is read using the read() method. The contents are then stored in the variable prompt, which is subsequently returned as the output of the function. This function is useful for loading prompt templates or any text data stored in a file format.\n\n**Note**: It is important to ensure that the file specified by file_path exists and is accessible; otherwise, a FileNotFoundError will be raised. Additionally, the function assumes that the file contains text data and is encoded in a format compatible with the default encoding used by Python (usually UTF-8).\n\n**Output Example**: If the file located at the specified file_path contains the text \"Hello, this is a prompt template.\", the function will return the string \"Hello, this is a prompt template.\""
      ],
      "code_start_line": 7,
      "code_end_line": 10,
      "params": [
        "file_path"
      ],
      "have_return": true,
      "code_content": "def read_prompt_template(file_path):\n  with open(file_path, 'r') as file:\n    prompt = file.read()\n  return prompt\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "call_llm",
      "md_content": [
        "**call_llm**: The function of call_llm is to interact with a language model API to generate a response based on provided prompts.\n\n**parameters**: The parameters of this Function.\n· model: A string representing the name of the language model to be used for generating responses.\n· sys_prompt: A string containing the system-level prompt that sets the context for the model.\n· usr_prompt: A string that represents the user-level prompt, which is the specific query or input from the user.\n· config: A dictionary containing configuration settings such as API key, base URL, timeout, maximum retries, temperature, and maximum tokens for the model.\n\n**Code Description**: The call_llm function is designed to facilitate communication with a language model, specifically through the OpenAI API. It begins by initializing an OpenAI client using the provided model name and configuration settings. The configuration includes essential parameters such as the API key, base URL, timeout, and maximum retries, which are retrieved from the config dictionary.\n\nNext, the function constructs a list of messages that includes both the system prompt and the user prompt. This structured format is necessary for the chat completion request to the API. The function then calls the chat completion method of the OpenAI client, passing in the model name, the constructed messages, and additional parameters like temperature and maximum tokens.\n\nUpon receiving the response from the API, the function extracts the content of the first message choice from the response object and returns it. This content represents the model's generated reply based on the provided prompts.\n\nThe call_llm function is invoked within the breakdown_task method of the Manager class in the agent_factory/manager.py file. In this context, it is used to break down a larger task into smaller sub-tasks by rendering a prompt and sending it to the language model for processing. The response from call_llm is then returned as the output of the breakdown_task method, indicating its role in task decomposition and interaction with the language model.\n\n**Note**: When using this function, ensure that the configuration dictionary is properly populated with all necessary keys and values to avoid runtime errors. Additionally, be mindful of the API usage limits and the potential costs associated with calling the OpenAI API.\n\n**Output Example**: A possible return value from the call_llm function could be a string such as \"To break down the task, consider the following steps: 1. Analyze the requirements, 2. Identify key components, 3. Create sub-tasks for each component.\"",
        "**call_llm**: The function of call_llm is to make an API request to a language model service, sending a system and user prompt, and returning the model's response.\n\n**parameters**: The parameters of this function.\n· model: The model name or identifier used for making the request to the language model service.\n· sys_prompt: A string that serves as the system message for the model, providing instructions or context for the conversation.\n· usr_prompt: A string that serves as the user's message or input for the model, to which the model should respond.\n· config: A dictionary containing configuration settings required for the request, including API keys, timeouts, retry settings, temperature, and model-specific settings.\n\n**Code Description**:  \nThe `call_llm` function is responsible for interfacing with a language model API (presumably OpenAI's API). It does so by creating a client instance for the OpenAI service using the configuration values passed in the `config` parameter. The function then constructs a list of messages that includes the system message (`sys_prompt`) and the user message (`usr_prompt`). This list is sent as part of the request to the `chat.completions.create` method of the API client.\n\nThe function retrieves the response from the API and extracts the model's reply from the `choices` list in the response. Specifically, it fetches the content of the message from the first choice in the list, which is assumed to be the relevant response from the model.\n\nKey steps in the process:\n1. An instance of the OpenAI client is created using the provided configuration values, including the API key, base URL, timeout, and retry settings.\n2. A list of messages is constructed, where the system message provides context or instructions, and the user message contains the query or input.\n3. The request is sent to the API, specifying the model, the messages, and other parameters like temperature.\n4. The model's response is extracted from the API's response and returned as the result.\n\nThe `call_llm` function is called within the `chat` method of the `BaseAgent` class in the `agent_factory/agent.py` module. In this context, it is used to send a dynamically rendered prompt to the language model based on the input data. The system and user prompts are provided as part of this interaction, and the function returns the model's response to be processed further or sent back to the user.\n\n**Note**:  \n- The `config` parameter must include valid API keys and any necessary configuration settings like `timeout`, `max_retries`, and `temperature` for the API request to be successful.\n- The function assumes the model will return a response in the form of a list of choices, with the actual message located in `response.choices[0].message`.\n- While the `max_tokens` parameter is mentioned in the code as a potential setting, it is commented out, implying it is either optional or controlled elsewhere in the codebase.\n\n**Output Example**:  \nThe return value of `call_llm` would typically be a string representing the content of the model's response. For example:\n\n```\n\"Sure, here's the information you requested: ... \"\n```",
        "**call_llm**: The function of call_llm is to interact with a language model API by sending a system prompt and a user prompt, and returning the model's response.\n\n**parameters**: The parameters of this Function.\n· model: A string representing the identifier of the language model to be used for generating responses.\n· sys_prompt: A string that serves as the system prompt, providing context or instructions to the language model.\n· usr_prompt: A string that contains the user-specific prompt, which is dynamically generated based on user input.\n· config: A dictionary containing configuration settings, including API key, base URL, timeout, maximum retries, and temperature for the model.\n\n**Code Description**: The call_llm function is designed to facilitate communication with a language model by making an API request. It begins by initializing an OpenAI client using the provided configuration settings. The API key and base URL are extracted from the config dictionary based on the specified model, ensuring that the correct credentials and endpoint are used for the API call.\n\nThe function constructs a list of messages that includes both the system prompt and the user prompt. These messages are formatted as dictionaries, where each dictionary specifies the role (either \"system\" or \"user\") and the corresponding content. This structured format is essential for the language model to understand the context of the conversation.\n\nNext, the function calls the chat completion method of the OpenAI client, passing in the model identifier, the constructed messages, and additional parameters such as temperature. The temperature setting influences the randomness of the model's responses, allowing for more creative or focused outputs depending on the desired outcome.\n\nUpon receiving the response from the API, the function extracts the content of the first message choice returned by the model. This content represents the model's reply to the user prompt and is returned as the output of the call_llm function.\n\nThe call_llm function is invoked by the chat method of the BaseAgent class, which is responsible for generating user-specific prompts based on input data. The chat method renders a prompt template using the provided data and then calls call_llm with the necessary parameters. This relationship highlights the role of call_llm as a backend service that processes user interactions and generates responses from the language model.\n\n**Note**: It is crucial to ensure that the configuration settings provided to the call_llm function are complete and valid to avoid errors during the API interaction. Additionally, the model specified must be supported by the OpenAI client to ensure successful communication.\n\n**Output Example**: A possible return value from the call_llm function could be a string such as \"Here is the information you requested based on your input: ...\"",
        "**call_llm**: The function of call_llm is to interact with a language model API to generate a response based on provided system and user prompts.\n\n**parameters**: The parameters of this Function.\n· model: A string representing the name of the language model to be used for generating the response.\n· sys_prompt: A string that serves as the system message, providing context or instructions to the model.\n· usr_prompt: A string that represents the user's input or question directed to the model.\n· config: A dictionary containing configuration settings, including API keys, base URLs, timeout settings, and other parameters necessary for the API call.\n\n**Code Description**: The call_llm function initiates a connection to an OpenAI language model using the provided configuration settings. It retrieves the API key and base URL specific to the model from the config dictionary. The function sets up a client instance of the OpenAI API with specified timeout and retry settings.\n\nNext, it constructs a list of messages, where the first message is the system prompt and the second is the user prompt. These messages are formatted as dictionaries containing the role (either \"system\" or \"user\") and the corresponding content.\n\nThe function then calls the chat completion endpoint of the OpenAI client, passing the model name and the constructed messages. It also includes a temperature setting from the config, which controls the randomness of the model's responses. The function is designed to handle a maximum token limit, although this is currently commented out in the code.\n\nAfter receiving the response from the API, the function extracts the content of the first message in the response choices and returns it. This content represents the model's generated reply based on the inputs provided.\n\n**Note**: When using this function, ensure that the config dictionary is properly populated with the necessary keys and values, including the model-specific API key and base URL. Additionally, be aware of the potential for rate limits or errors from the API, which may require handling in a production environment.\n\n**Output Example**: A possible return value from the function could be a string such as \"The weather today is sunny with a high of 75 degrees.\" This represents the model's generated response based on the provided prompts."
      ],
      "code_start_line": 13,
      "code_end_line": 36,
      "params": [
        "model",
        "sys_prompt",
        "usr_prompt",
        "config"
      ],
      "have_return": true,
      "code_content": "def call_llm(model, sys_prompt, usr_prompt, config):\n\n    client = OpenAI(\n        api_key=config.get(\"models\").get(model).get(\"api_key\"),\n        base_url=config.get(\"models\").get(model).get(\"base_url\"),\n        timeout=config.get(\"timeout\"),\n        max_retries=config.get(\"max_retries\"),\n    )\n\n    messages = [\n        {\"role\": \"system\", \"content\": sys_prompt},\n        {\"role\": \"user\", \"content\": usr_prompt},\n    ]\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=config.get(\"temperature\"),\n        # max_tokens=config.get(\"models\").get(model).get(\"max_tokens\",\"8192\"),\n    )\n\n    response_message = response.choices[0].message\n\n    return response_message.content",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "agent_factory/lmm_node.py": [
    {
      "type": "ClassDef",
      "name": "LLMNode",
      "md_content": [
        "**LLMNode**: The function of LLMNode is to serve as a base class representing an LLM Node in the execution graph, handling execution and optimization of prompts using LLMs and TextGrad.\n\n**attributes**: The attributes of this Class.\n· name: Unique identifier for the node.  \n· llm_model: Identifier for the LLM model to be used.  \n· prompt_template: Template string for the LLM prompt.  \n· config: Configuration dictionary for LLM settings.  \n· router: Optional function to determine the next node based on state.  \n· input_schema: Optional schema to validate input data.  \n· output_schema: Optional schema to validate output data.  \n· optimized: Boolean flag indicating if the node has been optimized.  \n· frozen: Boolean flag indicating if the node is frozen and cannot be modified.  \n· optimizer: Placeholder for the optimizer, to be initialized during optimization.  \n\n**Code Description**: The LLMNode class is designed to facilitate the execution of tasks using large language models (LLMs) within an execution graph. It provides methods for executing tasks, optimizing prompts, and validating input and output data against specified schemas. The constructor initializes the node with essential parameters such as its name, the LLM model to be used, the prompt template, and configuration settings. The optional parameters allow for dynamic routing and validation of input and output data.\n\nThe `execute` method is responsible for running the node's task. It validates the input data, generates a prompt using the provided template, and then calls the LLM to obtain a response. If the node is frozen, execution is skipped, and a warning is logged. The method also handles exceptions, logging errors and returning an error message if execution fails.\n\nThe `optimize` method enhances the prompt template using TextGrad, a tool for gradient-based optimization. It initializes necessary variables, sets up the model, and iteratively adjusts the prompt based on performance evaluations until a stopping criterion is met. After optimization, the prompt template is updated, and the node is marked as frozen to prevent further modifications.\n\nThe `validate_input` and `validate_output` methods ensure that the data conforms to the specified schemas, raising exceptions if validation fails. The `adjust_prompt` method allows for modifications to the prompt template unless the node is frozen. Finally, the `route` method determines the next node to execute based on the current state, utilizing an optional routing function.\n\nThe LLMNode class serves as a foundational component for its subclasses, such as ConstrainedDecodingNode and SocraticRoutingNode. These subclasses inherit from LLMNode and extend its functionality by enforcing strict input/output types or facilitating dynamic routing based on context, respectively. This hierarchical structure allows for specialized behavior while maintaining the core execution and optimization capabilities provided by LLMNode.\n\n**Note**: When using the LLMNode class, ensure that the input and output schemas are correctly defined to avoid validation errors. Additionally, be aware that once the node is optimized, the prompt template cannot be adjusted unless the node is unfrozen.\n\n**Output Example**: A possible return value from the `execute` method could be:\n```json\n{\n  \"response\": \"This is the generated response from the LLM based on the input data.\"\n}\n```"
      ],
      "code_start_line": 15,
      "code_end_line": 238,
      "params": [],
      "have_return": true,
      "code_content": "class LLMNode:\n    \"\"\"\n    Base class representing an LLM Node in the execution graph.\n    Handles execution and optimization of prompts using LLMs and TextGrad.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        llm_model: str,\n        prompt_template: str,\n        config: Dict[str, Any],\n        router: Optional[Callable[[Dict[str, Any]], str]] = None,\n        input_schema: Optional[Dict[str, Any]] = None,\n        output_schema: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"\n        Initializes the LLMNode.\n\n        :param name: Unique identifier for the node.\n        :param llm_model: Identifier for the LLM model to be used.\n        :param prompt_template: Template string for the LLM prompt.\n        :param config: Configuration dictionary for LLM settings.\n        :param router: Optional function to determine the next node based on state.\n        :param input_schema: Optional schema to validate input data.\n        :param output_schema: Optional schema to validate output data.\n        \"\"\"\n        self.name = name\n        self.llm_model = llm_model\n        self.prompt_template = prompt_template\n        self.config = config\n        self.router = router\n        self.input_schema = input_schema\n        self.output_schema = output_schema\n        self.optimized = False\n        self.frozen = False\n        self.optimizer = None  # To be initialized during optimization\n\n        logger.info(f\"Initialized LLMNode: {self.name}\")\n\n    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Executes the node's task using the LLM.\n\n        :param input_data: Dictionary containing input data for the node.\n        :return: Dictionary containing the output data.\n        \"\"\"\n        if self.frozen:\n            logger.warning(f\"Node {self.name} is frozen. Skipping execution.\")\n            return {}\n\n        try:\n            # Validate input data\n            if self.input_schema:\n                self.validate_input(input_data)\n\n            # Generate the prompt\n            prompt = self.prompt_template.format(**input_data)\n            logger.debug(f\"Node {self.name} prompt: {prompt}\")\n\n            # Call the LLM using the helper function\n            llm_response = call_llm(\n                model=self.llm_model,\n                sys_prompt=self.config.get(\"system_prompt\", \"\"),\n                usr_prompt=prompt,\n                config=self.config,\n            )\n            logger.debug(f\"Node {self.name} LLM response: {llm_response}\")\n\n            # Validate output data\n            output_data = {\"response\": llm_response}\n            if self.output_schema:\n                self.validate_output(output_data)\n\n            # Log successful execution\n            logger.info(f\"Node {self.name} executed successfully.\")\n            return output_data\n\n        except Exception as e:\n            logger.error(f\"Error executing node {self.name}: {e}\")\n            return {\"error\": str(e)}\n\n    def optimize(self, sample_data: Dict[str, Any], evaluator: 'Evaluator') -> None:\n        \"\"\"\n        Optimizes the node's prompt using TextGrad.\n\n        :param sample_data: Dictionary containing sample input and expected output.\n        :param evaluator: Instance of the Evaluator class for assessing performance.\n        \"\"\"\n        if self.optimized or self.frozen:\n            logger.info(f\"Node {self.name} is already optimized or frozen.\")\n            return\n\n        try:\n            logger.info(f\"Starting optimization for node {self.name}.\")\n\n            # Initialize TextGrad variables\n            tg.set_backward_engine(tg.get_engine(self.config.get(\"backward_engine\", \"gpt-4o\")))\n            prompt_variable = tg.Variable(\n                self.prompt_template,\n                requires_grad=True,\n                role_description=\"prompt template for the LLM\",\n            )\n\n            model = tg.BlackboxLLM(\n                engine=self.llm_model,\n                system_prompt=self.config.get(\"system_prompt\", \"\"),\n                prompt_variable=prompt_variable,\n            )\n\n            optimizer = tg.TGD(parameters=[prompt_variable])\n            loss_instruction = tg.Variable(\n                self.config.get(\"loss_instruction\", \"Evaluate the correctness of the response.\"),\n                requires_grad=False,\n                role_description=\"loss function instruction\",\n            )\n\n            loss_fn = tg.TextLoss(loss_instruction)\n\n            for step in range(MAX_OPTIMIZATION_STEPS):\n                # Generate prediction\n                prediction = model(sample_data['input'])\n                logger.debug(f\"Optimization step {step}: Prediction: {prediction}\")\n\n                # Compute loss\n                loss = loss_fn(prediction)\n                logger.debug(f\"Optimization step {step}: Loss: {loss}\")\n\n                # Backward pass and optimizer step\n                loss.backward()\n                optimizer.step()\n\n                # Evaluate performance\n                accuracy = evaluator.evaluate(prediction, sample_data['expected_output'])\n                logger.info(f\"Optimization step {step}: Accuracy: {accuracy}\")\n\n                # Check if optimization should stop\n                if evaluator.should_stop_optimization(accuracy):\n                    logger.info(f\"Stopping optimization for node {self.name} at step {step}.\")\n                    break\n\n            # Update the prompt template with the optimized prompt\n            self.prompt_template = prompt_variable.value\n            self.optimized = True\n            self.frozen = True  # Mark as frozen after optimization\n\n            logger.info(f\"Optimization completed for node {self.name}.\")\n\n        except Exception as e:\n            logger.error(f\"Error optimizing node {self.name}: {e}\")\n\n    def validate_input(self, input_data: Dict[str, Any]) -> None:\n        \"\"\"\n        Validates the input data against the input schema.\n\n        :param input_data: Dictionary containing input data.\n        :raises: ValueError if validation fails.\n        \"\"\"\n        try:\n            # Placeholder for schema validation logic\n            # Implement actual validation based on self.input_schema\n            json_schema = self.input_schema\n            if json_schema:\n                # For example, using jsonschema library\n                from jsonschema import validate, ValidationError\n\n                validate(instance=input_data, schema=json_schema)\n                logger.debug(f\"Input data for node {self.name} is valid.\")\n\n        except ValidationError as ve:\n            logger.error(f\"Input validation error in node {self.name}: {ve}\")\n            raise ValueError(f\"Input validation error: {ve}\")\n\n    def validate_output(self, output_data: Dict[str, Any]) -> None:\n        \"\"\"\n        Validates the output data against the output schema.\n\n        :param output_data: Dictionary containing output data.\n        :raises: ValueError if validation fails.\n        \"\"\"\n        try:\n            # Placeholder for schema validation logic\n            # Implement actual validation based on self.output_schema\n            json_schema = self.output_schema\n            if json_schema:\n                from jsonschema import validate, ValidationError\n\n                validate(instance=output_data, schema=json_schema)\n                logger.debug(f\"Output data for node {self.name} is valid.\")\n\n        except ValidationError as ve:\n            logger.error(f\"Output validation error in node {self.name}: {ve}\")\n            raise ValueError(f\"Output validation error: {ve}\")\n\n    def adjust_prompt(self, new_prompt: str) -> None:\n        \"\"\"\n        Adjusts the prompt template based on optimization feedback.\n\n        :param new_prompt: The new prompt template.\n        \"\"\"\n        if not self.frozen:\n            self.prompt_template = new_prompt\n            logger.info(f\"Prompt for node {self.name} adjusted.\")\n        else:\n            logger.warning(f\"Cannot adjust prompt for node {self.name} as it is frozen.\")\n\n    def route(self, state: Dict[str, Any]) -> Optional[str]:\n        \"\"\"\n        Determines the next node to execute based on the current state.\n\n        :param state: Current state dictionary.\n        :return: Name of the next node or None.\n        \"\"\"\n        if self.router:\n            try:\n                next_node = self.router(state)\n                logger.debug(f\"Node {self.name} routing to {next_node}.\")\n                return next_node\n            except Exception as e:\n                logger.error(f\"Routing error in node {self.name}: {e}\")\n                return None\n        else:\n            logger.debug(f\"Node {self.name} has no router. No routing performed.\")\n            return None\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/lmm_node.py/ConstrainedDecodingNode",
        "agent_factory/lmm_node.py/SocraticRoutingNode"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the LLMNode class with specified parameters.\n\n**parameters**: The parameters of this Function.\n· name: Unique identifier for the node.  \n· llm_model: Identifier for the LLM model to be used.  \n· prompt_template: Template string for the LLM prompt.  \n· config: Configuration dictionary for LLM settings.  \n· router: Optional function to determine the next node based on state.  \n· input_schema: Optional schema to validate input data.  \n· output_schema: Optional schema to validate output data.  \n\n**Code Description**: The __init__ function is a constructor for the LLMNode class, which is responsible for setting up the initial state of an LLMNode object. It takes several parameters that define the characteristics and behavior of the node. The 'name' parameter serves as a unique identifier, allowing for easy reference to the node within a larger system. The 'llm_model' parameter specifies which language model will be utilized, while 'prompt_template' provides a string template that will be used to generate prompts for the model. The 'config' parameter is a dictionary that contains various settings related to the LLM's configuration, allowing for customization of its behavior.\n\nAdditionally, the constructor accepts optional parameters: 'router', which is a callable function that can determine the next node based on the current state; 'input_schema', which is a dictionary that defines the expected structure of input data; and 'output_schema', which outlines the expected structure of output data. These optional parameters enhance the flexibility and robustness of the LLMNode by allowing for validation and routing based on specific criteria.\n\nThe constructor also initializes several attributes, including 'optimized', 'frozen', and 'optimizer', which are set to default values. The 'optimized' and 'frozen' attributes are initialized to False, indicating that the node is not yet optimized or frozen in its current state. The 'optimizer' attribute is set to None, indicating that it will be initialized later during the optimization process.\n\nFinally, the constructor logs an informational message indicating that the LLMNode has been successfully initialized, which can be useful for debugging and tracking the state of the application.\n\n**Note**: It is important to ensure that the parameters passed to the __init__ function are valid and conform to the expected types, as this will affect the behavior of the LLMNode instance. Proper validation of input and output schemas is recommended to maintain data integrity."
      ],
      "code_start_line": 21,
      "code_end_line": 53,
      "params": [
        "self",
        "name",
        "llm_model",
        "prompt_template",
        "config",
        "router",
        "input_schema",
        "output_schema"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        name: str,\n        llm_model: str,\n        prompt_template: str,\n        config: Dict[str, Any],\n        router: Optional[Callable[[Dict[str, Any]], str]] = None,\n        input_schema: Optional[Dict[str, Any]] = None,\n        output_schema: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"\n        Initializes the LLMNode.\n\n        :param name: Unique identifier for the node.\n        :param llm_model: Identifier for the LLM model to be used.\n        :param prompt_template: Template string for the LLM prompt.\n        :param config: Configuration dictionary for LLM settings.\n        :param router: Optional function to determine the next node based on state.\n        :param input_schema: Optional schema to validate input data.\n        :param output_schema: Optional schema to validate output data.\n        \"\"\"\n        self.name = name\n        self.llm_model = llm_model\n        self.prompt_template = prompt_template\n        self.config = config\n        self.router = router\n        self.input_schema = input_schema\n        self.output_schema = output_schema\n        self.optimized = False\n        self.frozen = False\n        self.optimizer = None  # To be initialized during optimization\n\n        logger.info(f\"Initialized LLMNode: {self.name}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "execute",
      "md_content": [
        "**execute**: The function of execute is to perform the node's task using the LLM (Large Language Model) and return the output data.\n\n**parameters**: The parameters of this Function.\n· input_data: A dictionary containing input data for the node.\n\n**Code Description**: The execute function is a critical method within the LLMNode class that orchestrates the execution of a task using a specified LLM. It begins by checking if the node is in a \"frozen\" state, which prevents any further execution. If the node is frozen, a warning is logged, and an empty dictionary is returned, indicating that no execution took place.\n\nIf the node is not frozen, the function proceeds to validate the input data against a predefined schema, if such a schema exists. This validation is performed by calling the validate_input method, which ensures that the input_data adheres to the expected format and constraints. This step is essential for maintaining data integrity and preventing errors during the execution process.\n\nNext, the function generates a prompt for the LLM by formatting a prompt template with the provided input_data. This prompt is crucial as it guides the LLM in generating a relevant response. The generated prompt is logged for debugging purposes.\n\nThe function then calls an external helper function, call_llm, passing the necessary parameters such as the model, system prompt, user prompt, and configuration. This function is responsible for interacting with the LLM and obtaining a response based on the generated prompt. The response from the LLM is logged for further analysis.\n\nAfter receiving the LLM's response, the function prepares the output data in a dictionary format. It then validates this output data against a predefined output schema, if available, by invoking the validate_output method. This validation ensures that the output conforms to the expected structure and constraints.\n\nFinally, if the execution is successful, an informational log is generated, and the output data is returned. In the event of any exceptions during the execution process, an error message is logged, and a dictionary containing the error details is returned. This robust error handling mechanism ensures that any issues encountered during execution are clearly communicated.\n\nThe execute function is integral to the overall functionality of the LLMNode class, as it encapsulates the entire process of validating input, generating prompts, interacting with the LLM, and validating output, thereby ensuring a seamless execution flow.\n\n**Note**: It is important to ensure that the input_data provided to the execute function is structured as a dictionary and that both input and output schemas are properly defined in the LLMNode instance. Failure to do so may result in validation errors, which will be logged and raised as exceptions.\n\n**Output Example**: A possible return value from the execute function could be:\n```json\n{\n    \"response\": \"This is the generated response from the LLM based on the input data.\"\n}\n```\nIn the case of an error, the return value might look like:\n```json\n{\n    \"error\": \"Input validation error: <details of the validation issue>\"\n}\n```"
      ],
      "code_start_line": 55,
      "code_end_line": 95,
      "params": [
        "self",
        "input_data"
      ],
      "have_return": true,
      "code_content": "    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Executes the node's task using the LLM.\n\n        :param input_data: Dictionary containing input data for the node.\n        :return: Dictionary containing the output data.\n        \"\"\"\n        if self.frozen:\n            logger.warning(f\"Node {self.name} is frozen. Skipping execution.\")\n            return {}\n\n        try:\n            # Validate input data\n            if self.input_schema:\n                self.validate_input(input_data)\n\n            # Generate the prompt\n            prompt = self.prompt_template.format(**input_data)\n            logger.debug(f\"Node {self.name} prompt: {prompt}\")\n\n            # Call the LLM using the helper function\n            llm_response = call_llm(\n                model=self.llm_model,\n                sys_prompt=self.config.get(\"system_prompt\", \"\"),\n                usr_prompt=prompt,\n                config=self.config,\n            )\n            logger.debug(f\"Node {self.name} LLM response: {llm_response}\")\n\n            # Validate output data\n            output_data = {\"response\": llm_response}\n            if self.output_schema:\n                self.validate_output(output_data)\n\n            # Log successful execution\n            logger.info(f\"Node {self.name} executed successfully.\")\n            return output_data\n\n        except Exception as e:\n            logger.error(f\"Error executing node {self.name}: {e}\")\n            return {\"error\": str(e)}\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "agent_factory/lmm_node.py/LLMNode/validate_input",
        "agent_factory/lmm_node.py/LLMNode/validate_output"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "optimize",
      "md_content": [
        "**optimize**: The function of optimize is to enhance the prompt of the node using the TextGrad optimization technique.\n\n**parameters**: The parameters of this Function.\n· sample_data: A dictionary containing sample input and expected output for the optimization process.  \n· evaluator: An instance of the Evaluator class that is responsible for assessing the performance of the model during optimization.\n\n**Code Description**: The optimize function is designed to improve the prompt template of a node by utilizing the TextGrad framework. The function begins by checking if the node has already been optimized or if it is in a frozen state, in which case it logs an informational message and exits early. If the node is eligible for optimization, it proceeds to initialize the necessary components for the optimization process.\n\nThe function sets the backward engine for TextGrad based on the configuration provided. It creates a prompt variable that is marked to require gradients, indicating that it will be adjusted during the optimization. A model instance of BlackboxLLM is created using the specified language model and system prompt. The optimizer is initialized with the prompt variable, and a loss function is defined based on a loss instruction from the configuration.\n\nThe optimization process is executed in a loop for a maximum number of steps defined by MAX_OPTIMIZATION_STEPS. In each iteration, the model generates a prediction based on the input data provided in sample_data. The loss is computed using the loss function, and a backward pass is performed to calculate gradients. The optimizer then updates the prompt variable based on the computed gradients.\n\nAfter each optimization step, the accuracy of the prediction is evaluated against the expected output using the evaluator. If the evaluator determines that optimization should stop based on the accuracy, the process is terminated early. Once the optimization is complete, the prompt template is updated with the optimized prompt, and the node is marked as optimized and frozen to prevent further modifications.\n\nThroughout the process, logging statements are used to provide insights into the optimization steps, including predictions, loss values, and accuracy metrics. In the event of an error during the optimization, an error message is logged to indicate the failure.\n\n**Note**: It is important to ensure that the node is not already optimized or frozen before calling this function. Proper configuration of the backward engine and loss instruction is necessary for effective optimization.\n\n**Output Example**: The optimized prompt template might appear as follows: \"What is the correct response to the input: [input data]?\""
      ],
      "code_start_line": 97,
      "code_end_line": 164,
      "params": [
        "self",
        "sample_data",
        "evaluator"
      ],
      "have_return": true,
      "code_content": "    def optimize(self, sample_data: Dict[str, Any], evaluator: 'Evaluator') -> None:\n        \"\"\"\n        Optimizes the node's prompt using TextGrad.\n\n        :param sample_data: Dictionary containing sample input and expected output.\n        :param evaluator: Instance of the Evaluator class for assessing performance.\n        \"\"\"\n        if self.optimized or self.frozen:\n            logger.info(f\"Node {self.name} is already optimized or frozen.\")\n            return\n\n        try:\n            logger.info(f\"Starting optimization for node {self.name}.\")\n\n            # Initialize TextGrad variables\n            tg.set_backward_engine(tg.get_engine(self.config.get(\"backward_engine\", \"gpt-4o\")))\n            prompt_variable = tg.Variable(\n                self.prompt_template,\n                requires_grad=True,\n                role_description=\"prompt template for the LLM\",\n            )\n\n            model = tg.BlackboxLLM(\n                engine=self.llm_model,\n                system_prompt=self.config.get(\"system_prompt\", \"\"),\n                prompt_variable=prompt_variable,\n            )\n\n            optimizer = tg.TGD(parameters=[prompt_variable])\n            loss_instruction = tg.Variable(\n                self.config.get(\"loss_instruction\", \"Evaluate the correctness of the response.\"),\n                requires_grad=False,\n                role_description=\"loss function instruction\",\n            )\n\n            loss_fn = tg.TextLoss(loss_instruction)\n\n            for step in range(MAX_OPTIMIZATION_STEPS):\n                # Generate prediction\n                prediction = model(sample_data['input'])\n                logger.debug(f\"Optimization step {step}: Prediction: {prediction}\")\n\n                # Compute loss\n                loss = loss_fn(prediction)\n                logger.debug(f\"Optimization step {step}: Loss: {loss}\")\n\n                # Backward pass and optimizer step\n                loss.backward()\n                optimizer.step()\n\n                # Evaluate performance\n                accuracy = evaluator.evaluate(prediction, sample_data['expected_output'])\n                logger.info(f\"Optimization step {step}: Accuracy: {accuracy}\")\n\n                # Check if optimization should stop\n                if evaluator.should_stop_optimization(accuracy):\n                    logger.info(f\"Stopping optimization for node {self.name} at step {step}.\")\n                    break\n\n            # Update the prompt template with the optimized prompt\n            self.prompt_template = prompt_variable.value\n            self.optimized = True\n            self.frozen = True  # Mark as frozen after optimization\n\n            logger.info(f\"Optimization completed for node {self.name}.\")\n\n        except Exception as e:\n            logger.error(f\"Error optimizing node {self.name}: {e}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "validate_input",
      "md_content": [
        "**validate_input**: The function of validate_input is to validate the input data against a predefined input schema.\n\n**parameters**: The parameters of this Function.\n· input_data: A dictionary containing the input data that needs to be validated.\n\n**Code Description**: The validate_input function is designed to ensure that the input data provided to the LLMNode instance adheres to a specified schema, which is stored in the instance variable self.input_schema. This validation process is crucial for maintaining data integrity and ensuring that the input data meets the expected format and constraints before any further processing occurs.\n\nThe function begins by attempting to validate the input_data against the json_schema, which is derived from self.input_schema. If a schema is defined, the function utilizes the jsonschema library to perform the validation. The validate function checks if the input_data conforms to the json_schema. If the validation is successful, a debug log is generated to indicate that the input data is valid.\n\nHowever, if the input_data does not conform to the schema, a ValidationError is raised. This error is caught by the function, which logs an error message detailing the validation issue and raises a ValueError to signal that the input validation has failed. This mechanism ensures that any issues with the input data are clearly communicated and handled appropriately.\n\nThe validate_input function is called within the execute method of the LLMNode class. Before executing the main task of the node, the execute method checks if the node is frozen and if an input schema is defined. If both conditions are met, it calls validate_input to validate the input_data. This ensures that only valid input data is processed further, thereby preventing potential errors during execution and maintaining the robustness of the node's operations.\n\n**Note**: It is important to ensure that the input_data provided to the validate_input function is structured as a dictionary and that the input schema is properly defined in self.input_schema. Failure to do so may result in validation errors, which will be logged and raised as exceptions."
      ],
      "code_start_line": 166,
      "code_end_line": 186,
      "params": [
        "self",
        "input_data"
      ],
      "have_return": false,
      "code_content": "    def validate_input(self, input_data: Dict[str, Any]) -> None:\n        \"\"\"\n        Validates the input data against the input schema.\n\n        :param input_data: Dictionary containing input data.\n        :raises: ValueError if validation fails.\n        \"\"\"\n        try:\n            # Placeholder for schema validation logic\n            # Implement actual validation based on self.input_schema\n            json_schema = self.input_schema\n            if json_schema:\n                # For example, using jsonschema library\n                from jsonschema import validate, ValidationError\n\n                validate(instance=input_data, schema=json_schema)\n                logger.debug(f\"Input data for node {self.name} is valid.\")\n\n        except ValidationError as ve:\n            logger.error(f\"Input validation error in node {self.name}: {ve}\")\n            raise ValueError(f\"Input validation error: {ve}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/lmm_node.py/LLMNode/execute"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "validate_output",
      "md_content": [
        "**validate_output**: The function of validate_output is to validate the output data against a predefined output schema.\n\n**parameters**: The parameters of this Function.\n· output_data: Dictionary containing output data.\n\n**Code Description**: The validate_output function is responsible for ensuring that the output data generated by the node adheres to a specified schema, which is defined in the self.output_schema attribute. This validation process is crucial for maintaining data integrity and ensuring that the output meets the expected format and constraints.\n\nWhen the function is called, it attempts to validate the provided output_data dictionary using the jsonschema library. If an output schema is defined (i.e., self.output_schema is not None), the function imports the validate and ValidationError classes from the jsonschema module. It then calls the validate function, passing the output_data and the schema for validation.\n\nIf the output_data does not conform to the schema, a ValidationError is raised. The function catches this exception, logs an error message that includes the name of the node and the details of the validation error, and subsequently raises a ValueError to indicate that the output validation has failed.\n\nThis function is called within the execute method of the LLMNode class. After generating the output data from the LLM response, the execute method checks if an output schema is defined. If so, it invokes validate_output to ensure that the output data is valid before proceeding. This relationship highlights the importance of validate_output in the overall execution flow, as it acts as a safeguard to ensure that only valid data is returned from the node's execution.\n\n**Note**: It is important to ensure that the output schema is properly defined before calling this function, as the validation process relies on it. Additionally, users should handle the ValueError exception that may be raised if the output data fails validation, to ensure robust error handling in their applications."
      ],
      "code_start_line": 188,
      "code_end_line": 207,
      "params": [
        "self",
        "output_data"
      ],
      "have_return": false,
      "code_content": "    def validate_output(self, output_data: Dict[str, Any]) -> None:\n        \"\"\"\n        Validates the output data against the output schema.\n\n        :param output_data: Dictionary containing output data.\n        :raises: ValueError if validation fails.\n        \"\"\"\n        try:\n            # Placeholder for schema validation logic\n            # Implement actual validation based on self.output_schema\n            json_schema = self.output_schema\n            if json_schema:\n                from jsonschema import validate, ValidationError\n\n                validate(instance=output_data, schema=json_schema)\n                logger.debug(f\"Output data for node {self.name} is valid.\")\n\n        except ValidationError as ve:\n            logger.error(f\"Output validation error in node {self.name}: {ve}\")\n            raise ValueError(f\"Output validation error: {ve}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/lmm_node.py/LLMNode/execute"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "adjust_prompt",
      "md_content": [
        "**adjust_prompt**: The function of adjust_prompt is to modify the prompt template based on new input while considering the state of the object.\n\n**parameters**: The parameters of this Function.\n· new_prompt: A string representing the new prompt template that will replace the current one.\n\n**Code Description**: The adjust_prompt function is designed to update the prompt template of an instance of the class it belongs to. It accepts a single parameter, new_prompt, which is expected to be a string. The function first checks if the instance is not in a \"frozen\" state, indicated by the self.frozen attribute. If the instance is not frozen, it assigns the new_prompt value to the prompt_template attribute of the instance and logs an informational message indicating that the prompt has been successfully adjusted for the specific node identified by self.name. If the instance is frozen, it logs a warning message stating that the prompt cannot be adjusted while in this state.\n\n**Note**: It is important to ensure that the instance is not frozen before attempting to adjust the prompt. If the instance is frozen, any attempt to change the prompt will result in a warning, and the prompt will remain unchanged. This mechanism is crucial for maintaining the integrity of the prompt in scenarios where it should not be modified."
      ],
      "code_start_line": 209,
      "code_end_line": 219,
      "params": [
        "self",
        "new_prompt"
      ],
      "have_return": false,
      "code_content": "    def adjust_prompt(self, new_prompt: str) -> None:\n        \"\"\"\n        Adjusts the prompt template based on optimization feedback.\n\n        :param new_prompt: The new prompt template.\n        \"\"\"\n        if not self.frozen:\n            self.prompt_template = new_prompt\n            logger.info(f\"Prompt for node {self.name} adjusted.\")\n        else:\n            logger.warning(f\"Cannot adjust prompt for node {self.name} as it is frozen.\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "route",
      "md_content": [
        "**route**: The function of route is to determine the next node to execute based on the current state.\n\n**parameters**: The parameters of this Function.\n· state: A dictionary representing the current state of the system.\n\n**Code Description**: The route function is responsible for determining the next node in a routing process based on the provided current state. It first checks if a router function is assigned to the instance. If a router is present, it attempts to call this router function with the current state as an argument. If the router successfully returns a value, this value represents the name of the next node to which the process should transition. The function logs a debug message indicating the routing action taken. In the event of an exception during the routing process, an error message is logged, and the function returns None, indicating that routing could not be performed. If no router is assigned, a debug message is logged stating that no routing action will be taken, and the function also returns None.\n\n**Note**: It is important to ensure that the router function is properly defined and can handle the state input to avoid exceptions during execution. Additionally, the logging mechanism should be configured to capture debug and error messages for troubleshooting purposes.\n\n**Output Example**: If the current state is processed correctly and the router function returns a valid next node, the output could be a string such as \"next_node_1\". If there is an error or no router is defined, the output will be None."
      ],
      "code_start_line": 221,
      "code_end_line": 238,
      "params": [
        "self",
        "state"
      ],
      "have_return": true,
      "code_content": "    def route(self, state: Dict[str, Any]) -> Optional[str]:\n        \"\"\"\n        Determines the next node to execute based on the current state.\n\n        :param state: Current state dictionary.\n        :return: Name of the next node or None.\n        \"\"\"\n        if self.router:\n            try:\n                next_node = self.router(state)\n                logger.debug(f\"Node {self.name} routing to {next_node}.\")\n                return next_node\n            except Exception as e:\n                logger.error(f\"Routing error in node {self.name}: {e}\")\n                return None\n        else:\n            logger.debug(f\"Node {self.name} has no router. No routing performed.\")\n            return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "ConstrainedDecodingNode",
      "md_content": [
        "**ConstrainedDecodingNode**: The function of ConstrainedDecodingNode is to enforce strict input and output types for tasks requiring precise outputs.\n\n**attributes**: The attributes of this Class.\n· name: Unique identifier for the node.  \n· llm_model: Identifier for the LLM model to be used.  \n· prompt_template: Template string for the LLM prompt.  \n· config: Configuration dictionary for LLM settings.  \n· input_schema: Schema to validate input data.  \n· output_schema: Schema to validate output data.  \n\n**Code Description**: The ConstrainedDecodingNode class is a specialized subclass of the LLMNode class, designed to ensure that both input and output data adhere to predefined schemas. This class is particularly useful in scenarios where the accuracy and format of the data are critical, such as in applications involving natural language processing tasks that require strict compliance with expected data structures.\n\nUpon initialization, the ConstrainedDecodingNode inherits attributes and methods from its parent class, LLMNode. The constructor of ConstrainedDecodingNode accepts parameters such as name, llm_model, prompt_template, config, input_schema, and output_schema. These parameters are essential for defining the node's identity, the model it utilizes, the prompt it generates, and the validation schemas for input and output data.\n\nThe relationship with its parent class, LLMNode, is significant as it inherits the core functionalities that allow for the execution of tasks using large language models (LLMs). The LLMNode class provides methods for executing tasks, optimizing prompts, and validating data against schemas. The ConstrainedDecodingNode extends this functionality by enforcing strict validation rules through the input_schema and output_schema parameters. This ensures that any data processed by the node meets the specified criteria, thereby reducing the likelihood of errors during execution.\n\nThe ConstrainedDecodingNode is particularly beneficial in environments where data integrity is paramount. By utilizing this class, developers can create nodes that not only execute tasks but also guarantee that the data being processed conforms to expected formats, thus enhancing the reliability of the overall system.\n\n**Note**: When using the ConstrainedDecodingNode, it is crucial to define the input and output schemas accurately to prevent validation errors. This class is intended for scenarios where strict adherence to data formats is necessary, and any deviations may lead to execution failures."
      ],
      "code_start_line": 240,
      "code_end_line": 272,
      "params": [],
      "have_return": false,
      "code_content": "class ConstrainedDecodingNode(LLMNode):\n    \"\"\"\n    LLMNode subclass that enforces strict input and output types.\n    Used for tasks requiring precise outputs.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        llm_model: str,\n        prompt_template: str,\n        config: Dict[str, Any],\n        input_schema: Dict[str, Any],\n        output_schema: Dict[str, Any],\n    ):\n        \"\"\"\n        Initializes the ConstrainedDecodingNode.\n\n        :param name: Unique identifier for the node.\n        :param llm_model: Identifier for the LLM model to be used.\n        :param prompt_template: Template string for the LLM prompt.\n        :param config: Configuration dictionary for LLM settings.\n        :param input_schema: Schema to validate input data.\n        :param output_schema: Schema to validate output data.\n        \"\"\"\n        super().__init__(\n            name=name,\n            llm_model=llm_model,\n            prompt_template=prompt_template,\n            config=config,\n            input_schema=input_schema,\n            output_schema=output_schema,\n        )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "agent_factory/lmm_node.py/LLMNode"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the ConstrainedDecodingNode with specified parameters.\n\n**parameters**: The parameters of this Function.\n· name: Unique identifier for the node.  \n· llm_model: Identifier for the LLM model to be used.  \n· prompt_template: Template string for the LLM prompt.  \n· config: Configuration dictionary for LLM settings.  \n· input_schema: Schema to validate input data.  \n· output_schema: Schema to validate output data.  \n\n**Code Description**: The __init__ function is a constructor for the ConstrainedDecodingNode class. It is responsible for initializing an instance of the class with specific attributes that define its behavior and configuration. The function takes six parameters: `name`, `llm_model`, `prompt_template`, `config`, `input_schema`, and `output_schema`. Each of these parameters serves a distinct purpose in setting up the node. The `name` parameter provides a unique identifier for the node, which is essential for distinguishing it from other nodes in a larger system. The `llm_model` parameter specifies which language model will be utilized, allowing for flexibility in model selection. The `prompt_template` parameter is a string that serves as a template for generating prompts for the language model, ensuring that the input to the model is formatted correctly. The `config` parameter is a dictionary that contains various settings related to the language model, enabling customization of its behavior. The `input_schema` and `output_schema` parameters are dictionaries that define the structure and validation rules for the input and output data, respectively. This ensures that the data processed by the node adheres to expected formats, enhancing robustness and reliability.\n\nThe constructor also calls the superclass's __init__ method, passing along all parameters to ensure that any inherited properties or behaviors are properly initialized. This is crucial for maintaining the integrity of the object-oriented design and ensuring that the ConstrainedDecodingNode inherits any necessary functionality from its parent class.\n\n**Note**: When using this constructor, it is important to ensure that all parameters are provided with valid values to avoid runtime errors. The schemas for input and output should be carefully defined to match the expected data structures, as this will affect the node's ability to process data correctly."
      ],
      "code_start_line": 246,
      "code_end_line": 272,
      "params": [
        "self",
        "name",
        "llm_model",
        "prompt_template",
        "config",
        "input_schema",
        "output_schema"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        name: str,\n        llm_model: str,\n        prompt_template: str,\n        config: Dict[str, Any],\n        input_schema: Dict[str, Any],\n        output_schema: Dict[str, Any],\n    ):\n        \"\"\"\n        Initializes the ConstrainedDecodingNode.\n\n        :param name: Unique identifier for the node.\n        :param llm_model: Identifier for the LLM model to be used.\n        :param prompt_template: Template string for the LLM prompt.\n        :param config: Configuration dictionary for LLM settings.\n        :param input_schema: Schema to validate input data.\n        :param output_schema: Schema to validate output data.\n        \"\"\"\n        super().__init__(\n            name=name,\n            llm_model=llm_model,\n            prompt_template=prompt_template,\n            config=config,\n            input_schema=input_schema,\n            output_schema=output_schema,\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "SocraticRoutingNode",
      "md_content": [
        "**SocraticRoutingNode**: The function of SocraticRoutingNode is to facilitate reasoning and dynamic routing by deciding the next node(s) based on the current context.\n\n**attributes**: The attributes of this Class.\n· name: Unique identifier for the node.  \n· llm_model: Identifier for the LLM model to be used.  \n· prompt_template: Template string for the LLM prompt.  \n· config: Configuration dictionary for LLM settings.  \n· router: Function to determine the next node based on state.  \n· input_schema: Optional schema to validate input data.  \n· output_schema: Optional schema to validate output data.  \n· optimized: Boolean flag indicating if the node has been optimized.  \n· frozen: Boolean flag indicating if the node is frozen and cannot be modified.  \n· optimizer: Placeholder for the optimizer, to be initialized during optimization.  \n\n**Code Description**: The SocraticRoutingNode class is a specialized subclass of the LLMNode class, designed to enhance the functionality of LLM nodes by incorporating reasoning capabilities and dynamic routing based on contextual information. It inherits all attributes and methods from the LLMNode class, which serves as a foundational component for executing tasks using large language models (LLMs).\n\nThe constructor of SocraticRoutingNode initializes the node with essential parameters such as its name, the LLM model to be used, the prompt template, and configuration settings. Additionally, it accepts a router function that determines the next node to execute based on the current state. This dynamic routing capability allows for more flexible and context-aware execution flows within an execution graph.\n\nBy leveraging the existing methods of the LLMNode class, the SocraticRoutingNode can execute tasks, validate input and output data, and optimize its prompt template. The routing functionality provided by the router parameter enables the node to adapt its behavior based on the context, making it suitable for applications that require reasoning and decision-making.\n\nThe SocraticRoutingNode does not introduce new methods but extends the capabilities of its parent class, allowing it to participate in a more complex execution graph where the flow of execution can change dynamically based on the context provided by the router function.\n\n**Note**: When utilizing the SocraticRoutingNode, it is essential to ensure that the router function is correctly implemented to facilitate accurate routing decisions. Additionally, the input and output schemas should be defined appropriately to avoid validation errors during execution."
      ],
      "code_start_line": 274,
      "code_end_line": 305,
      "params": [],
      "have_return": false,
      "code_content": "class SocraticRoutingNode(LLMNode):\n    \"\"\"\n    LLMNode subclass that facilitates reasoning and dynamic routing.\n    Can decide the next node(s) based on the current context.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        llm_model: str,\n        prompt_template: str,\n        config: Dict[str, Any],\n        router: Callable[[Dict[str, Any]], str],\n    ):\n        \"\"\"\n        Initializes the SocraticRoutingNode.\n\n        :param name: Unique identifier for the node.\n        :param llm_model: Identifier for the LLM model to be used.\n        :param prompt_template: Template string for the LLM prompt.\n        :param config: Configuration dictionary for LLM settings.\n        :param router: Function to determine the next node based on state.\n        \"\"\"\n        super().__init__(\n            name=name,\n            llm_model=llm_model,\n            prompt_template=prompt_template,\n            config=config,\n            router=router,\n            input_schema=None,\n            output_schema=None,\n        )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "agent_factory/lmm_node.py/LLMNode"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the SocraticRoutingNode with specified parameters.\n\n**parameters**: The parameters of this Function.\n· name: Unique identifier for the node.  \n· llm_model: Identifier for the LLM model to be used.  \n· prompt_template: Template string for the LLM prompt.  \n· config: Configuration dictionary for LLM settings.  \n· router: Function to determine the next node based on state.  \n\n**Code Description**: The __init__ function is a constructor for the SocraticRoutingNode class. It is responsible for initializing an instance of the class with the provided parameters. The function takes five parameters: `name`, `llm_model`, `prompt_template`, `config`, and `router`. Each parameter serves a specific purpose in configuring the node's behavior and functionality. \n\n- The `name` parameter is a string that serves as a unique identifier for the node, allowing it to be referenced distinctly within a larger system. \n- The `llm_model` parameter specifies which language model (LLM) will be utilized by the node, ensuring that the correct model is employed for processing inputs.\n- The `prompt_template` parameter is a string that defines the format of the prompt that will be sent to the LLM, allowing for customization of the input based on the context or requirements of the task.\n- The `config` parameter is a dictionary that contains various settings and configurations related to the LLM, enabling fine-tuning of its behavior and performance.\n- The `router` parameter is a callable function that takes a dictionary as input and returns a string. This function is crucial for determining the next node to which the process should route based on the current state, facilitating dynamic decision-making within the node network.\n\nThe constructor also calls the superclass's __init__ method, passing along the parameters along with `input_schema` and `output_schema`, which are set to None in this case. This ensures that the base class is properly initialized with the relevant attributes.\n\n**Note**: It is important to ensure that the parameters passed to the __init__ function are correctly formatted and valid, as they directly influence the behavior of the SocraticRoutingNode. Proper configuration of the router function is essential for the effective routing of tasks within the node network."
      ],
      "code_start_line": 280,
      "code_end_line": 305,
      "params": [
        "self",
        "name",
        "llm_model",
        "prompt_template",
        "config",
        "router"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        name: str,\n        llm_model: str,\n        prompt_template: str,\n        config: Dict[str, Any],\n        router: Callable[[Dict[str, Any]], str],\n    ):\n        \"\"\"\n        Initializes the SocraticRoutingNode.\n\n        :param name: Unique identifier for the node.\n        :param llm_model: Identifier for the LLM model to be used.\n        :param prompt_template: Template string for the LLM prompt.\n        :param config: Configuration dictionary for LLM settings.\n        :param router: Function to determine the next node based on state.\n        \"\"\"\n        super().__init__(\n            name=name,\n            llm_model=llm_model,\n            prompt_template=prompt_template,\n            config=config,\n            router=router,\n            input_schema=None,\n            output_schema=None,\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "agent_factory/agent.py": [
    {
      "type": "ClassDef",
      "name": "BaseAgent",
      "md_content": [
        "**BaseAgent**: The function of BaseAgent is to serve as a foundational class for agents that interact with a language model for chat-based functionalities.\n\n**attributes**: The attributes of this Class.\n· config: A configuration object that is read from a configuration file, containing various settings for the agent's operation.  \n· model: A string that specifies the default model to be used for generating responses, defaulting to \"gpt-4o-mini\".  \n· env: An instance of the Environment class used for loading templates from the file system, specifically for prompt management.  \n· sys_prompt: A string that holds the system prompt, initialized as an empty string, which can be used to set the context for the chat.  \n· repeat_turns: An integer that defines the number of times to repeat the task or interaction, defaulting to 10.\n\n**Code Description**: The BaseAgent class is designed to provide a common interface and functionality for agents that require interaction with a language model. Upon initialization, the class reads configuration settings from an external file, which includes the model to be used and the path to the prompt templates. The class sets up an environment for loading these templates, allowing for dynamic prompt generation based on the data provided during chat interactions.\n\nThe `chat` method is a core function of the BaseAgent class, which facilitates communication with the language model. It takes in a data dictionary and a prompt template, rendering the prompt with the provided data. The rendered prompt is then sent to a function called `call_llm`, which is responsible for interacting with the language model and generating a response based on the system prompt and user prompt.\n\nThe BaseAgent class is utilized by the Manager class, which inherits from it. The Manager class leverages the chat functionality of BaseAgent to break down tasks into sub-tasks. It initializes its own attributes while also inheriting the configuration and model settings from BaseAgent. This relationship allows the Manager to effectively utilize the chat method to interact with the language model for task management purposes.\n\n**Note**: It is essential to ensure that the configuration file is correctly set up and that the necessary template files are available in the specified prompt folder path. The `call_llm` function must also be properly defined elsewhere in the codebase to handle the interaction with the language model.\n\n**Output Example**: An example output from the `chat` method might look like this:\n```\n{\n    \"response\": \"Here are the steps to break down your task: Research the topic, Draft an outline, Write the introduction, Gather references.\",\n    \"status\": \"success\",\n    \"message\": \"Response generated successfully.\"\n}\n```",
        "**BaseAgent**: The function of BaseAgent is to serve as a foundational class for agents that interact with a language model, facilitating chat functionalities and task reception.\n\n**attributes**: The attributes of this Class.\n· config: A configuration object that holds various settings and parameters for the agent, loaded from a configuration file.  \n· model: A string representing the default model to be used for language processing, initialized to \"gpt-4o-mini\" unless specified otherwise in the configuration.  \n· env: An Environment object that manages the loading of templates from the specified prompt folder path in the configuration.  \n· sys_prompt: A string that holds the system prompt, initialized as an empty string.  \n· repeat_turns: An integer that defines the number of turns to repeat in the chat, initialized to 10.\n\n**Code Description**: The BaseAgent class is designed to provide essential functionalities for agents that require interaction with a language model. Upon initialization, it reads configuration settings, including the default model and the path for prompt templates. The class utilizes the Jinja2 templating engine to load prompts from the specified folder, allowing for dynamic prompt generation based on the provided data.\n\nThe primary method of interest in the BaseAgent class is `chat`, which takes in a data dictionary and a prompt template. This method renders the prompt using the provided data, enabling the customization of the prompt based on the specific context of the conversation. It then calls the `call_llm` function, passing the rendered prompt along with the system prompt and model configuration to obtain a response from the language model. This interaction is crucial for generating contextually relevant replies in a chat setting.\n\nAnother significant method is `receive_task`, which allows the agent to accept and store an original task. This method is essential for setting the context for subsequent operations, particularly in scenarios where the agent is expected to perform tasks based on the received input.\n\nThe BaseAgent class is inherited by the Manager class, which extends its functionalities to include task management capabilities. The Manager class utilizes the `receive_task` method to store the original task and employs the `chat` method to break down the task into sub-tasks. This relationship illustrates how the Manager class leverages the capabilities of the BaseAgent class to perform its task management functions effectively.\n\n**Note**: It is important to ensure that the configuration file is correctly set up with the necessary parameters, including the model and prompt folder path. Additionally, the interaction with the language model through the `chat` method relies on the correct implementation of the `call_llm` function, which must be defined elsewhere in the codebase.\n\n**Output Example**: A possible output from the `chat` method might look like this:\n```\n{\n    \"response\": \"Hello! How can I assist you today?\",\n    \"status\": \"success\",\n    \"message\": \"Chat initiated successfully.\"\n}\n```",
        "**BaseAgent**: The function of BaseAgent is to serve as a foundational class for handling chat interactions and processing tasks using a specified language model.\n\n**attributes**: The attributes of this Class.\n· config: A configuration object that holds various settings for the agent, including model specifications and prompt folder paths.  \n· model: The name of the default language model to be used for chat interactions, initialized to \"gpt-4o-mini\".  \n· env: An instance of the Environment class, which is configured to load templates from a specified folder path.  \n· sys_prompt: A string that holds the system prompt for the chat interactions, initialized as an empty string.  \n· repeat_turns: An integer that defines the number of turns to repeat in the chat, initialized to 10.  \n· original_task: A variable to store the original task received by the agent.\n\n**Code Description**: The BaseAgent class is designed to facilitate interactions with a language model through chat functionalities. Upon initialization, it reads the configuration settings, including the default model and the path for prompt templates. The class provides several methods to interact with the model and process tasks.\n\nThe `common_chat` method takes a user query as input and calls the language model using the specified system prompt and user prompt. This method serves as the primary interface for generating responses based on user input.\n\nThe `chat_with_template` method allows for dynamic prompt generation by rendering a template with provided data. It utilizes the Jinja2 templating engine to create a customized prompt before passing it to the `common_chat` method for processing.\n\nThe `receive_task` method is used to accept and store an original task, which can be further processed or utilized in chat interactions.\n\nThe `extract_and_validate_yaml` method is responsible for extracting YAML content from a model's response. It uses regular expressions to find content wrapped in ```yaml``` tags and attempts to parse it using the PyYAML library. If successful, it returns the YAML content in a standardized format; otherwise, it handles errors gracefully by returning None.\n\n**Note**: When using the BaseAgent class, ensure that the configuration file is correctly set up with the necessary parameters, including the prompt folder path and the default model. Additionally, proper error handling should be implemented when dealing with YAML content to avoid runtime exceptions.\n\n**Output Example**: A possible return value from the `common_chat` method could be a string response from the language model, such as: \"Hello! How can I assist you today?\" If the `extract_and_validate_yaml` method is called with a valid model response containing YAML, it might return a formatted YAML string like:\n```yaml\nkey1: value1\nkey2: value2\n```"
      ],
      "code_start_line": 36,
      "code_end_line": 80,
      "params": [],
      "have_return": true,
      "code_content": "class BaseAgent:\n    def __init__(self):\n        self.config = read_config()\n        self.model = self.config.get('default_model', \"gpt-4o-mini\")\n        self.env = Environment(loader=FileSystemLoader(self.config.get('prompt_folder_path')))\n        self.sys_prompt = ''\n        self.repeat_turns = 10\n\n    def common_chat(self, query):\n        return call_llm(model=self.model, sys_prompt=self.sys_prompt, usr_prompt=query, config=self.config)\n    \n    def chat_with_template(self, data, prompt_template):\n        \"\"\"\n        通用的聊天方法，根据传入的data字典适配不同的prompt。\n        \"\"\"\n        rendered_prompt = prompt_template.render(**data)\n        # print(rendered_prompt)\n        response_message = self.common_chat(query=rendered_prompt)\n        return response_message\n    \n\n    def receive_task(self, task):\n        \"\"\"\n        接收原始任务。\n        \"\"\"\n        self.original_task = task\n        \n    def extract_and_validate_yaml(self, model_response):\n        # 正则表达式匹配包裹在```yaml```之间的内容\n        import re\n        match = re.search(r'```yaml\\n([\\s\\S]*?)\\n```', model_response, re.DOTALL)\n        \n        if not match:\n            return None  # 如果没有找到匹配的内容，返回None\n        \n        model_response = match.group(1).strip()\n        \n        try:\n            # 尝试解析YAML内容\n            parsed_yaml = yaml.safe_load(model_response)\n            return yaml.dump(parsed_yaml, default_flow_style=False)\n\n        except yaml.YAMLError as exc:\n            print(f\"Invalid YAML content: {exc}\")\n            return None",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the BaseAgent class by setting up its configuration and environment.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The __init__ method is a constructor for the BaseAgent class. It is called when an instance of the class is created. The method performs several key operations:\n\n1. It invokes the read_config function to load configuration settings from a YAML file. This function returns a dictionary containing various configuration parameters, which is stored in the instance variable self.config.\n\n2. The method retrieves the value associated with the key 'default_model' from the configuration dictionary. If this key is not present, it defaults to the string \"gpt-4o-mini\". This value is stored in the instance variable self.model, which likely represents the model that the agent will use for its operations.\n\n3. The method initializes an Environment object from the Jinja2 templating library. This object is configured with a loader that points to the directory specified by the 'prompt_folder_path' key in the configuration dictionary. This allows the agent to dynamically load prompt templates from the specified folder.\n\n4. The instance variable self.sys_prompt is initialized as an empty string, which may be used later to store system prompts or messages.\n\n5. Finally, the method sets self.repeat_turns to 10, which could indicate the number of times the agent is allowed to repeat certain actions or interactions during its operation.\n\nOverall, the __init__ method establishes the foundational settings and components necessary for the BaseAgent to function effectively. It relies on the read_config function to ensure that the agent is configured according to the specified settings in the YAML file, thus enabling flexibility and customization in its behavior.\n\n**Note**: It is crucial to ensure that the YAML configuration file is accessible and correctly formatted. Any issues with file access or parsing may result in runtime errors, preventing the BaseAgent from initializing properly."
      ],
      "code_start_line": 37,
      "code_end_line": 42,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.config = read_config()\n        self.model = self.config.get('default_model', \"gpt-4o-mini\")\n        self.env = Environment(loader=FileSystemLoader(self.config.get('prompt_folder_path')))\n        self.sys_prompt = ''\n        self.repeat_turns = 10\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "common_chat",
      "md_content": [
        "**common_chat**: The function of common_chat is to facilitate communication with a language model by sending a user-defined query along with system prompts and configuration settings.\n\n**parameters**: The parameters of this Function.\n· query: A string that represents the user input or question that will be sent to the language model.\n\n**Code Description**: The common_chat function is designed to interact with a language model (LLM) by calling the function `call_llm`. It takes a single parameter, `query`, which is expected to be a string. This string is typically the user's input that needs to be processed by the language model. The function constructs a call to `call_llm`, passing it several arguments: `model`, `sys_prompt`, `usr_prompt`, and `config`. Here, `model` refers to the specific language model being utilized, `sys_prompt` is a predefined system prompt that sets the context for the conversation, `usr_prompt` is the user query (in this case, the `query` parameter), and `config` contains additional configuration settings that may influence the behavior of the language model.\n\nThe common_chat function is called within the `chat_with_template` method of the BaseAgent class. In this context, `chat_with_template` prepares a prompt by rendering it with data provided in the `data` dictionary using a specified `prompt_template`. Once the prompt is rendered, it invokes `common_chat`, passing the rendered prompt as the query. The response from `common_chat` is then returned as the output of `chat_with_template`. This establishes a clear relationship where `chat_with_template` relies on `common_chat` to handle the actual communication with the language model after preparing the appropriate prompt.\n\n**Note**: It is important to ensure that the `query` passed to common_chat is properly formatted and relevant to the context established by the system prompt to achieve meaningful responses from the language model.\n\n**Output Example**: A possible return value from the common_chat function could be a string such as \"Sure, I can help you with that! What specific information are you looking for?\" This response would depend on the input query and the configuration of the language model."
      ],
      "code_start_line": 44,
      "code_end_line": 45,
      "params": [
        "self",
        "query"
      ],
      "have_return": true,
      "code_content": "    def common_chat(self, query):\n        return call_llm(model=self.model, sys_prompt=self.sys_prompt, usr_prompt=query, config=self.config)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/agent.py/BaseAgent/chat_with_template"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "chat_with_template",
      "md_content": [
        "**chat_with_template**: The function of chat_with_template is to facilitate a conversation by rendering a prompt template with provided data and then communicating with a language model.\n\n**parameters**: The parameters of this Function.\n· data: A dictionary containing key-value pairs that will be used to populate the prompt template.\n· prompt_template: An object that defines the structure of the prompt to be rendered, which will be filled with the values from the data dictionary.\n\n**Code Description**: The chat_with_template method is designed to generate a dynamic prompt for a conversation based on the input data. It takes two parameters: `data`, which is a dictionary containing the necessary information to fill in the prompt template, and `prompt_template`, which is an object that specifies how the prompt should be structured. \n\nThe method begins by rendering the prompt using the `render` method of the `prompt_template`, passing the unpacked `data` dictionary as keyword arguments. This results in a `rendered_prompt`, which is a string that represents the final prompt to be sent to the language model.\n\nFollowing the rendering process, the method calls `common_chat`, passing the `rendered_prompt` as the `query` parameter. The `common_chat` function is responsible for sending this query to a language model, allowing for interaction based on the generated prompt. The response from `common_chat` is then returned as the output of the `chat_with_template` method.\n\nThis establishes a clear functional relationship where `chat_with_template` serves as a preparatory step that formats the input data into a suitable prompt, which is then processed by `common_chat` to obtain a response from the language model.\n\n**Note**: It is essential to ensure that the `data` provided is complete and correctly structured to match the expectations of the `prompt_template`. This will ensure that the rendered prompt is coherent and relevant, leading to meaningful interactions with the language model.\n\n**Output Example**: A possible return value from the chat_with_template function could be a string such as \"Hello! How can I assist you today?\" This response will depend on the specific data provided and the structure of the prompt template used."
      ],
      "code_start_line": 47,
      "code_end_line": 54,
      "params": [
        "self",
        "data",
        "prompt_template"
      ],
      "have_return": true,
      "code_content": "    def chat_with_template(self, data, prompt_template):\n        \"\"\"\n        通用的聊天方法，根据传入的data字典适配不同的prompt。\n        \"\"\"\n        rendered_prompt = prompt_template.render(**data)\n        # print(rendered_prompt)\n        response_message = self.common_chat(query=rendered_prompt)\n        return response_message\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "agent_factory/agent.py/BaseAgent/common_chat"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "receive_task",
      "md_content": [
        "**receive_task**: The function of receive_task is to accept and store an original task.\n\n**parameters**: The parameters of this Function.\n· task: This parameter represents the raw task that is being received and stored within the object.\n\n**Code Description**: The receive_task function is designed to accept a task as input and assign it to the instance variable original_task. This function is a straightforward setter that allows the BaseAgent class to store the task it receives for further processing or management. The simplicity of this function ensures that any task passed to it is directly associated with the agent instance, facilitating task management within the broader context of the application.\n\nIn the context of the project, this function can be called by other components, such as the manager module (agent_factory/manager.py). Although there is no specific documentation or raw code provided for the manager module, it can be inferred that the manager likely interacts with instances of BaseAgent to assign tasks. When the manager calls receive_task, it provides a task that the agent will then store for its operations. This relationship indicates that the manager plays a crucial role in task distribution, while the BaseAgent is responsible for maintaining the state of the tasks assigned to it.\n\n**Note**: It is important to ensure that the task being passed to receive_task is in the expected format and contains all necessary information for the agent to process it effectively. Proper validation of the task before calling this function may be necessary to avoid errors in task handling later in the workflow.",
        "**receive_task**: The function of receive_task is to accept and store the original task provided to the agent.\n\n**parameters**: The parameters of this Function.\n· task: This parameter represents the original task that is being received by the agent. It is expected to be of any data type that encapsulates the task details.\n\n**Code Description**: The receive_task function is designed to accept a task as input and assign it to the instance variable original_task. This function serves as a method for the BaseAgent class, allowing it to receive tasks that it will process or manage later. When the function is called, it takes the provided task and directly assigns it to the instance variable self.original_task. This action effectively stores the task within the agent's context, making it accessible for further operations or processing within the agent's lifecycle.\n\n**Note**: It is important to ensure that the task being passed to this function is properly formatted and contains all necessary information required for the agent to perform its intended operations. Additionally, this function does not perform any validation or processing on the task; it simply stores it. Therefore, any necessary checks or transformations should be handled before invoking this method."
      ],
      "code_start_line": 57,
      "code_end_line": 61,
      "params": [
        "self",
        "task"
      ],
      "have_return": false,
      "code_content": "    def receive_task(self, task):\n        \"\"\"\n        接收原始任务。\n        \"\"\"\n        self.original_task = task\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "extract_and_validate_yaml",
      "md_content": [
        "**extract_and_validate_yaml**: The function of extract_and_validate_yaml is to extract YAML content from a given string and validate its syntax.\n\n**parameters**: The parameters of this Function.\n· model_response: A string input that potentially contains YAML content wrapped in ```yaml``` markers.\n\n**Code Description**: The extract_and_validate_yaml function begins by importing the regular expression module (re) to facilitate pattern matching. It uses a regular expression to search for content that is enclosed between ```yaml``` markers in the provided model_response string. The pattern `r'```yaml\\n([\\s\\S]*?)\\n```'` is designed to capture everything between the opening and closing markers, including newlines and whitespace.\n\nIf the search does not find a match, the function returns None, indicating that no valid YAML content was found. If a match is found, the captured content is stripped of leading and trailing whitespace. The function then attempts to parse this YAML content using the yaml.safe_load method from the PyYAML library. If the parsing is successful, it returns the YAML content formatted as a string using yaml.dump, with default_flow_style set to False for a more human-readable format.\n\nIn the event of a parsing error, the function catches the yaml.YAMLError exception, prints an error message indicating that the YAML content is invalid, and returns None.\n\n**Note**: It is important to ensure that the input string contains valid YAML syntax wrapped in the specified markers. If the input does not conform to this structure, the function will return None without raising an error.\n\n**Output Example**: If the input model_response is:\n```\nHere is some configuration:\n```yaml\nkey: value\nlist:\n  - item1\n  - item2\n```\n```\nThe function would return:\n```\nkey:\n  value\nlist:\n- item1\n- item2\n```"
      ],
      "code_start_line": 63,
      "code_end_line": 80,
      "params": [
        "self",
        "model_response"
      ],
      "have_return": true,
      "code_content": "    def extract_and_validate_yaml(self, model_response):\n        # 正则表达式匹配包裹在```yaml```之间的内容\n        import re\n        match = re.search(r'```yaml\\n([\\s\\S]*?)\\n```', model_response, re.DOTALL)\n        \n        if not match:\n            return None  # 如果没有找到匹配的内容，返回None\n        \n        model_response = match.group(1).strip()\n        \n        try:\n            # 尝试解析YAML内容\n            parsed_yaml = yaml.safe_load(model_response)\n            return yaml.dump(parsed_yaml, default_flow_style=False)\n\n        except yaml.YAMLError as exc:\n            print(f\"Invalid YAML content: {exc}\")\n            return None",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "agent_factory/evaluator.py": [
    {
      "type": "ClassDef",
      "name": "Evaluator",
      "md_content": [
        "**Evaluator**: The function of Evaluator is to assess the performance of a model based on specified criteria and determine when to halt optimization processes.\n\n**attributes**: The attributes of this Class.\n· evaluation_criteria: This parameter defines the criteria used to evaluate the outputs of the model against expected results. It is essential for determining the effectiveness of the model's predictions.\n· cutoff_step_size: This parameter sets the threshold for determining whether the improvement in accuracy is significant enough to continue the optimization process. It helps in identifying diminishing returns during optimization.\n\n**Code Description**: The Evaluator class is designed to facilitate the evaluation of model outputs and the optimization process. It includes methods to compare actual outputs against expected outputs, assess whether optimization should continue based on historical accuracy data, and evaluate the expected reward of specific actions within a given state. \n\nThe `__init__` method initializes the Evaluator instance with the specified evaluation criteria and cutoff step size. The `evaluate` method is intended to compare the node's output to the expected output based on the defined evaluation criteria, although its implementation is currently a placeholder. The `should_stop_optimization` method analyzes the accuracy history to determine if the optimization process should be halted, specifically checking if the change in accuracy between the last two recorded values is less than the cutoff step size. Lastly, the `evaluate_action` method is designed to assess the expected reward of a given action in a specific state, but like `evaluate`, it is also a placeholder for future implementation.\n\n**Note**: It is important to implement the logic within the `evaluate` and `evaluate_action` methods to ensure that the Evaluator class functions as intended. Additionally, users should ensure that the accuracy history passed to `should_stop_optimization` contains sufficient data points to make a valid determination.\n\n**Output Example**: \nAssuming the `evaluate` method is implemented, a possible return value when comparing node_output and expected_output could be a score or a boolean indicating whether the output meets the evaluation criteria. For instance, it might return a float value representing the accuracy percentage or a simple True/False indicating success or failure in meeting the expected criteria."
      ],
      "code_start_line": 3,
      "code_end_line": 20,
      "params": [],
      "have_return": true,
      "code_content": "class Evaluator:\n    def __init__(self, evaluation_criteria, cutoff_step_size):\n        self.evaluation_criteria = evaluation_criteria\n        self.cutoff_step_size = cutoff_step_size\n\n    def evaluate(self, node_output, expected_output):\n        # Compare node_output to expected_output based on criteria\n        pass\n\n    def should_stop_optimization(self, accuracy_history):\n        # Determine if optimization should stop based on diminishing returns\n        if len(accuracy_history) < 2:\n            return False\n        return (accuracy_history[-1] - accuracy_history[-2]) < self.cutoff_step_size\n\n    def evaluate_action(self, state, action):\n        # Evaluate the action's expected reward\n        pass",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the Evaluator class with specified evaluation criteria and cutoff step size.\n\n**parameters**: The parameters of this Function.\n· evaluation_criteria: This parameter defines the criteria used to evaluate the performance or effectiveness of a certain process or model. It is expected to be provided in a format that the Evaluator can utilize for its evaluation tasks.\n\n· cutoff_step_size: This parameter specifies the step size at which evaluations will be cut off or limited. It is crucial for controlling the granularity of the evaluation process, allowing for more efficient performance assessments.\n\n**Code Description**: The __init__ function serves as the constructor for the Evaluator class. When an instance of the Evaluator is created, this function is invoked to set up the initial state of the object. It takes two parameters: evaluation_criteria and cutoff_step_size. The evaluation_criteria parameter is stored as an instance variable, allowing the Evaluator to access it later during its evaluation processes. Similarly, the cutoff_step_size parameter is also stored as an instance variable, which will be used to determine how evaluations are conducted in terms of step size. This initialization process is essential for ensuring that the Evaluator has the necessary information to perform its intended functions effectively.\n\n**Note**: It is important to ensure that the evaluation_criteria provided is valid and compatible with the evaluation methods that the Evaluator class will implement. Additionally, the cutoff_step_size should be a positive value to avoid any potential errors during the evaluation process."
      ],
      "code_start_line": 4,
      "code_end_line": 6,
      "params": [
        "self",
        "evaluation_criteria",
        "cutoff_step_size"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, evaluation_criteria, cutoff_step_size):\n        self.evaluation_criteria = evaluation_criteria\n        self.cutoff_step_size = cutoff_step_size\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "evaluate",
      "md_content": [
        "**evaluate**: The function of evaluate is to compare the output of a node against the expected output based on specified criteria.\n\n**parameters**: The parameters of this Function.\n· parameter1: node_output - This parameter represents the actual output generated by the node that is being evaluated. It is expected to be in a format that can be compared to the expected output.\n· parameter2: expected_output - This parameter represents the output that is anticipated from the node. It serves as the benchmark against which the node_output will be assessed.\n\n**Code Description**: The evaluate function is designed to facilitate the comparison between the actual output produced by a node (node_output) and the output that is expected (expected_output). The function currently contains a placeholder comment indicating that the comparison will be made based on certain criteria. However, the implementation details of how this comparison will be executed are not provided within the function body. This suggests that the function is intended to be further developed to include specific logic for evaluating the accuracy or correctness of the node_output in relation to the expected_output.\n\nThe function is likely part of a larger evaluation framework where outputs from various nodes are assessed to ensure they meet predefined standards or criteria. The absence of a concrete implementation indicates that the function may be a stub, awaiting further development to define the criteria for comparison and the method of evaluation.\n\n**Note**: It is important to implement the comparison logic within this function to ensure that it fulfills its intended purpose. Additionally, developers should consider what criteria will be used for comparison and how the results of the evaluation will be utilized in the broader context of the application."
      ],
      "code_start_line": 8,
      "code_end_line": 10,
      "params": [
        "self",
        "node_output",
        "expected_output"
      ],
      "have_return": false,
      "code_content": "    def evaluate(self, node_output, expected_output):\n        # Compare node_output to expected_output based on criteria\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "should_stop_optimization",
      "md_content": [
        "**should_stop_optimization**: The function of should_stop_optimization is to determine whether the optimization process should be halted based on the trend of accuracy improvements.\n\n**parameters**: The parameters of this Function.\n· accuracy_history: A list of accuracy values recorded during the optimization process.\n\n**Code Description**: The should_stop_optimization function evaluates the history of accuracy values to decide if the optimization should continue. It first checks if the length of the accuracy_history list is less than 2. If it is, the function returns False, indicating that there is not enough data to make a decision. If there are at least two accuracy values, the function compares the most recent accuracy value (accuracy_history[-1]) with the one before it (accuracy_history[-2]). If the difference between these two values is less than a predefined threshold, self.cutoff_step_size, it suggests that the improvements in accuracy are diminishing, and the function returns True, indicating that optimization should stop. Otherwise, it returns False, suggesting that further optimization may still yield better results.\n\n**Note**: It is important to ensure that the accuracy_history list contains at least two entries before calling this function to avoid unintended behavior. The cutoff_step_size should be set appropriately based on the specific optimization context to ensure meaningful comparisons.\n\n**Output Example**: \n- If accuracy_history = [0.8, 0.81], and self.cutoff_step_size = 0.01, the function will return False, indicating that optimization should continue.\n- If accuracy_history = [0.8, 0.81, 0.81], and self.cutoff_step_size = 0.01, the function will return True, indicating that optimization should stop."
      ],
      "code_start_line": 12,
      "code_end_line": 16,
      "params": [
        "self",
        "accuracy_history"
      ],
      "have_return": true,
      "code_content": "    def should_stop_optimization(self, accuracy_history):\n        # Determine if optimization should stop based on diminishing returns\n        if len(accuracy_history) < 2:\n            return False\n        return (accuracy_history[-1] - accuracy_history[-2]) < self.cutoff_step_size\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "evaluate_action",
      "md_content": [
        "**evaluate_action**: The function of evaluate_action is to evaluate the expected reward of a given action in a specific state.\n\n**parameters**: The parameters of this Function.\n· parameter1: state - This represents the current state of the environment or system in which the action is being evaluated. It contains all relevant information necessary to assess the outcome of the action.\n· parameter2: action - This denotes the specific action that is being evaluated for its expected reward. It can be any operation or decision that can be taken in the given state.\n\n**Code Description**: The evaluate_action function is designed to assess the expected reward associated with a particular action taken in a specified state. Although the function body is currently a placeholder (indicated by the 'pass' statement), the intended functionality would involve implementing logic to calculate or retrieve the expected reward based on the provided state and action. This could involve various methods, such as querying a reward model, applying a reward function, or utilizing reinforcement learning principles to derive the expected outcome. The function is likely part of a larger framework where actions are evaluated to inform decision-making processes, such as in an agent-based system or a reinforcement learning environment.\n\n**Note**: It is important to implement the logic for calculating the expected reward within this function to ensure it serves its intended purpose. Additionally, the function should handle different types of states and actions appropriately to provide accurate evaluations."
      ],
      "code_start_line": 18,
      "code_end_line": 20,
      "params": [
        "self",
        "state",
        "action"
      ],
      "have_return": false,
      "code_content": "    def evaluate_action(self, state, action):\n        # Evaluate the action's expected reward\n        pass",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "agent_factory/manager.py": [
    {
      "type": "ClassDef",
      "name": "Manager",
      "md_content": [
        "**Manager**: The function of Manager is to manage tasks by receiving an original task and breaking it down into sub-tasks for further processing.\n\n**attributes**: The attributes of this Class.\n· original_task: A string that holds the original task received by the manager.  \n· sub_tasks: A list that stores the sub-tasks generated from the original task.  \n· config: A configuration object that is read from a configuration file.  \n· model: A string that specifies the default model to be used, defaulting to \"gpt-4o-mini\".  \n· env: An instance of the Environment class used for loading templates from the file system.  \n· sys_prompt: A string that holds the system prompt, initialized as an empty string.  \n· breakdown_prompt: A template loaded from a file named 'manager_break_down.txt' used for breaking down tasks.  \n· reflection_prompt: A placeholder for a reflection prompt, initialized as None.  \n· repeat_turns: An integer that defines the number of times to repeat the task breakdown, defaulting to 10.\n\n**Code Description**: The Manager class is designed to facilitate the management of tasks by allowing the user to input an original task and subsequently breaking it down into smaller, manageable sub-tasks. Upon initialization, the class sets up several attributes, including the original task, a list for sub-tasks, and configuration settings read from an external source. The class utilizes a templating engine to render prompts that guide the task breakdown process. \n\nThe `__init__` method initializes the Manager instance, setting up the necessary attributes and loading the configuration settings. The `receive_task` method allows the user to input an original task, which is stored in the `original_task` attribute. The `breakdown_task` method is responsible for taking the original task and rendering a prompt using the `breakdown_prompt` template. This rendered prompt is then passed to a function called `call_llm`, which presumably interacts with a language model to generate a response based on the task breakdown.\n\n**Note**: It is important to ensure that the configuration file is correctly set up and that the necessary template files are available in the specified prompt folder path. The `call_llm` function must also be properly defined elsewhere in the codebase to handle the interaction with the language model.\n\n**Output Example**: An example output from the `breakdown_task` method might look like this:\n```\n{\n    \"sub_tasks\": [\n        \"Research the topic\",\n        \"Draft an outline\",\n        \"Write the introduction\",\n        \"Gather references\"\n    ],\n    \"status\": \"success\",\n    \"message\": \"Task has been successfully broken down.\"\n}\n```",
        "**Manager**: The function of Manager is to handle task management by receiving original tasks and breaking them down into sub-tasks.\n\n**attributes**: The attributes of this Class.\n· original_task: A string that stores the original task received by the Manager.  \n· sub_tasks: A list that holds the sub-tasks generated from the breakdown of the original task.  \n· breakdown_prompt: A template used for generating prompts related to breaking down tasks, retrieved from the environment.  \n· reflection_prompt: A variable that is initialized as None, intended for future use related to task reflection.\n\n**Code Description**: The Manager class extends the BaseAgent class, inheriting its functionalities and attributes while adding specific capabilities for task management. Upon initialization, the Manager class sets up its own attributes, including `original_task`, which is initialized as an empty string, and `sub_tasks`, which is initialized as an empty list. The `breakdown_prompt` is obtained from the environment's template system, specifically designed for breaking down tasks.\n\nThe primary method of interest in the Manager class is `receive_task`, which accepts a task as an argument and assigns it to the `original_task` attribute. This method is crucial for setting the context for subsequent operations. \n\nAnother significant method is `breakdown_task`, which is responsible for decomposing the original task into smaller, manageable sub-tasks. This method calls `get_data_for_breakdown` to prepare the necessary data, which includes the original task, and then utilizes the inherited `chat` method from the BaseAgent class. The `chat` method interacts with a language model to generate a response based on the breakdown prompt and the provided data.\n\nThe `get_data_for_breakdown` method constructs a dictionary containing the `original_task`, which is then used in the `chat` method to facilitate the breakdown process. This relationship illustrates how the Manager class leverages the capabilities of the BaseAgent class to perform its task management functions effectively.\n\n**Note**: It is important to ensure that the environment is properly set up with the necessary templates for the breakdown prompt. Additionally, the interaction with the language model through the `chat` method relies on the correct implementation of the `call_llm` function, which must be defined elsewhere in the codebase.\n\n**Output Example**: A possible output from the `breakdown_task` method might look like this:\n```\n{\n    \"response\": \"To complete your task, consider the following steps: Define the scope, Research relevant information, Create an outline, Draft the content.\",\n    \"status\": \"success\",\n    \"message\": \"Task breakdown completed successfully.\"\n}\n```",
        "**Manager**: The function of Manager is to manage and break down tasks into sub-tasks for further processing.\n\n**attributes**: The attributes of this Class.\n· original_task: A string that holds the original task received by the Manager.  \n· sub_tasks: A list that stores the sub-tasks generated from the breakdown of the original task.  \n· breakdown_prompt: A template used for generating prompts related to task breakdown, loaded from a specified template file.  \n· reflection_prompt: A variable that can hold a prompt for reflection, currently initialized to None.\n\n**Code Description**: The Manager class extends the BaseAgent class, inheriting its functionalities while adding specific capabilities for task management. Upon initialization, the Manager class calls the constructor of the BaseAgent, ensuring that all foundational attributes and methods are available. It initializes its own attributes, including `original_task`, which is set to an empty string, and `sub_tasks`, which is initialized as an empty list. The `breakdown_prompt` is loaded from a template file named 'manager_break_down.txt' using the environment object from the BaseAgent class, allowing for dynamic prompt generation based on the original task.\n\nThe primary method of the Manager class is `breakdown_task`, which is responsible for decomposing the original task into smaller, manageable sub-tasks. This method first retrieves the necessary data for breakdown by calling `get_data_for_breakdown`, which constructs a dictionary containing the `original_task`. The method then utilizes the `chat` method inherited from BaseAgent, passing the data and the `breakdown_prompt` to generate a response that outlines the sub-tasks.\n\nThe `get_data_for_breakdown` method serves as a utility function that prepares the data structure required for the breakdown process. It returns a dictionary with the key 'task' mapped to the `original_task`, ensuring that the prompt can be rendered with the correct context.\n\nIn summary, the Manager class leverages the capabilities of the BaseAgent class to facilitate task management, specifically focusing on breaking down complex tasks into simpler components that can be handled more effectively.\n\n**Note**: It is essential to ensure that the template file 'manager_break_down.txt' is correctly formatted and accessible within the specified prompt folder path. The interaction with the language model through the `chat` method relies on the proper implementation of the `call_llm` function, which must be defined in the BaseAgent class or elsewhere in the codebase.\n\n**Output Example**: A possible output from the `breakdown_task` method might look like this:\n```\n{\n    \"sub_tasks\": [\n        \"Define the main objectives of the task.\",\n        \"Identify the resources required.\",\n        \"Establish a timeline for completion.\"\n    ],\n    \"status\": \"success\",\n    \"message\": \"Task has been successfully broken down.\"\n}\n```",
        "**Manager**: The function of Manager is to facilitate the breakdown of tasks into sub-tasks for better management and execution.\n\n**attributes**: The attributes of this Class.\n· original_task: A string that holds the main task to be broken down.\n· sub_tasks: A list that stores the sub-tasks generated from the breakdown of the original task.\n· breakdown_prompt: A template used for generating prompts related to task breakdown, retrieved from the environment.\n· reflection_prompt: A variable that is intended to hold a prompt for reflection, currently initialized to None.\n\n**Code Description**: The Manager class inherits from the BaseAgent class and is designed to manage tasks by breaking them down into smaller, manageable sub-tasks. Upon initialization, the class sets up the original task as an empty string and initializes an empty list for sub-tasks. It also retrieves a template for task breakdown prompts from the environment, which will be used in the breakdown process. The class contains two primary methods: `breakdown_task` and `get_data_for_breakdown`.\n\nThe `breakdown_task` method is responsible for breaking down the original task into sub-tasks. It first calls the `get_data_for_breakdown` method to retrieve the necessary data, which includes the original task. This data is then passed to the `chat_with_template` method along with the breakdown prompt to generate the sub-tasks.\n\nThe `get_data_for_breakdown` method constructs and returns a dictionary containing the original task. This method serves as a helper function to provide the required data format for the breakdown process.\n\n**Note**: It is important to ensure that the original_task attribute is set before calling the breakdown_task method, as this will directly influence the output of the task breakdown process.\n\n**Output Example**: If the original_task is set to \"Prepare a project report\", the output of the breakdown_task method might resemble the following structure:\n- Sub-task 1: \"Gather data and statistics\"\n- Sub-task 2: \"Draft the report outline\"\n- Sub-task 3: \"Write the introduction section\"\n- Sub-task 4: \"Compile the final document\""
      ],
      "code_start_line": 11,
      "code_end_line": 29,
      "params": [],
      "have_return": true,
      "code_content": "class Manager(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.original_task = ''\n        self.sub_tasks = []\n        self.breakdown_prompt = self.env.get_template('manager_break_down.txt')\n        self.reflection_prompt = None\n\n    def breakdown_task(self):\n        \"\"\"\n        将任务拆解成子任务。\n        \"\"\"\n        data = self.get_data_for_breakdown()\n        return self.chat_with_template(data, self.breakdown_prompt)\n\n    def get_data_for_breakdown(self):\n        return {\n            'task': self.original_task\n        }\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize a new instance of the Manager class, setting up its attributes with default values and loading configuration settings.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this __init__ method.\n\n**Code Description**: The __init__ method is a constructor for the Manager class, which is responsible for initializing the instance attributes when a new Manager object is created. Upon instantiation, the method performs the following actions:\n\n1. It initializes the `original_task` attribute as an empty string. This attribute is likely intended to hold the main task that the Manager will handle.\n2. The `sub_tasks` attribute is initialized as an empty list, which may be used to store any sub-tasks related to the original task.\n3. The `config` attribute is populated by calling the `read_config` function, which reads a configuration file and returns its contents as a dictionary. This function is defined in the agent_factory/config.py file and is crucial for loading the necessary settings for the Manager's operation.\n4. The `model` attribute is set by retrieving the value associated with the key 'default_model' from the `config` dictionary. If this key does not exist, it defaults to the string \"gpt-4o-mini\".\n5. The `env` attribute is initialized as an instance of the Environment class from the Jinja2 library, using a FileSystemLoader that points to the directory specified by the 'prompt_folder_path' key in the `config` dictionary. This setup allows the Manager to load templates for generating prompts.\n6. The `sys_prompt` attribute is initialized as an empty string, which may be used to store a system prompt for the Manager's operations.\n7. The `breakdown_prompt` attribute is assigned a template loaded from the file 'manager_break_down.txt' using the `env` object. This template is likely used for breaking down tasks or generating specific prompts.\n8. The `reflection_prompt` attribute is initialized as None, indicating that it may be set later in the process.\n9. Finally, the `repeat_turns` attribute is set to 10, which may define the number of iterations or turns the Manager will perform in certain operations.\n\nOverall, the __init__ method establishes the foundational state of the Manager object, ensuring that all necessary attributes are initialized and that configuration settings are loaded for subsequent use. The relationship with the `read_config` function is particularly important, as it provides the configuration data that influences the behavior of the Manager.\n\n**Note**: It is essential to ensure that the configuration file specified in the `read_config` function exists and is correctly formatted in YAML. Any issues with the configuration file may lead to errors during the initialization of the Manager instance.",
        "**__init__**: The function of __init__ is to initialize an instance of the Manager class.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The __init__ function is a constructor method that is automatically called when an instance of the Manager class is created. It begins by invoking the constructor of its parent class using `super().__init__()`, ensuring that any initialization defined in the parent class is also executed. \n\nFollowing this, the function initializes several instance variables:\n- `self.original_task` is set to an empty string, which is likely intended to hold the main task that the Manager will handle.\n- `self.sub_tasks` is initialized as an empty list, suggesting that this Manager instance can manage multiple sub-tasks related to the original task.\n- `self.breakdown_prompt` is assigned a template retrieved from the environment using `self.env.get_template('manager_break_down.txt')`. This indicates that the Manager class is likely designed to generate or manipulate prompts based on a predefined template, which could be used for task breakdowns.\n- `self.reflection_prompt` is initialized to `None`, indicating that it may be set later in the code, possibly to hold a prompt related to reflection or review of tasks.\n\nOverall, this constructor sets up the necessary attributes for the Manager class, preparing it for further operations related to task management.\n\n**Note**: It is important to ensure that the parent class's constructor is called to maintain the integrity of the class hierarchy. Additionally, the template file 'manager_break_down.txt' should be present in the expected directory for the code to function correctly."
      ],
      "code_start_line": 12,
      "code_end_line": 17,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super().__init__()\n        self.original_task = ''\n        self.sub_tasks = []\n        self.breakdown_prompt = self.env.get_template('manager_break_down.txt')\n        self.reflection_prompt = None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "breakdown_task",
      "md_content": [
        "**breakdown_task**: The function of breakdown_task is to decompose a larger task into smaller sub-tasks.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The breakdown_task method is a member of the Manager class and is responsible for breaking down an original task into sub-tasks. It begins by creating a dictionary named `data`, which contains the key 'task' associated with the value of `self.original_task`. This dictionary is then used to render a prompt through the `self.breakdown_prompt.render(**data)` method call. The rendered prompt is a structured input that will guide the language model in generating a relevant response.\n\nFollowing the prompt rendering, the method invokes the `call_llm` function, passing in several parameters: `self.model`, `self.sys_prompt`, and the `rendered_prompt`. The `call_llm` function is designed to interact with a language model API, specifically to generate a response based on the provided prompts. It initializes an OpenAI client using the model name and configuration settings, constructs a list of messages that includes both the system prompt and the user prompt, and then sends this data to the language model for processing.\n\nThe response from the `call_llm` function is captured in the variable `response_message`, which is then returned as the output of the breakdown_task method. This indicates that the breakdown_task method not only facilitates the decomposition of tasks but also serves as a bridge to the language model, allowing for intelligent processing and generation of sub-tasks based on the original task.\n\n**Note**: Ensure that the `self.original_task` and `self.breakdown_prompt` are properly initialized before calling this method to avoid runtime errors. Additionally, be aware of the API usage limits and the potential costs associated with calling the language model API.\n\n**Output Example**: A possible return value from the breakdown_task method could be a string such as \"The task can be broken down into the following sub-tasks: 1. Research the topic, 2. Draft an outline, 3. Write the introduction.\"",
        "**breakdown_task**: The function of breakdown_task is to decompose a task into subtasks.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The breakdown_task method is a member of the Manager class and is responsible for breaking down a larger task into smaller, manageable subtasks. This method first calls the get_data_for_breakdown method to retrieve the original task data, which is essential for the decomposition process. The data returned is structured as a dictionary, where the key 'task' holds the value of the original task that needs to be broken down.\n\nOnce the data is obtained, the breakdown_task method proceeds to invoke the chat method from the BaseAgent class. This method is designed to interact with a language model by generating a prompt based on the input data. In this case, the breakdown_task method passes the retrieved data and a predefined breakdown prompt to the chat method. The chat method then renders the prompt using the provided data and communicates with the language model to obtain a response, which is expected to contain the subtasks derived from the original task.\n\nThe relationship between breakdown_task and its callees is crucial for the overall functionality of task decomposition. The breakdown_task method relies on get_data_for_breakdown to ensure it has the correct context for the task at hand, and it utilizes the chat method to facilitate the interaction with the language model, effectively bridging the gap between the original task and its decomposition into subtasks.\n\n**Note**: It is important to ensure that the original task is properly initialized within the Manager class before invoking breakdown_task to avoid any runtime errors. Additionally, the breakdown prompt used in the chat method should be appropriately defined to elicit meaningful responses from the language model.\n\n**Output Example**: A possible return value from the breakdown_task method could be a list of subtasks such as [\"Research climate change effects\", \"Draft an outline for the report\", \"Write the introduction section\"].",
        "**breakdown_task**: The function of breakdown_task is to decompose a task into sub-tasks.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The breakdown_task method is a member of the Manager class. Its primary purpose is to facilitate the decomposition of a larger task into smaller, manageable sub-tasks. This method achieves this by first invoking the get_data_for_breakdown method, which retrieves the original task data necessary for the breakdown process. The data returned from get_data_for_breakdown is then passed to the chat_with_template method along with a predefined prompt, referred to as self.breakdown_prompt.\n\nThe get_data_for_breakdown method is crucial to the functionality of breakdown_task, as it ensures that the current state of the original task is accurately captured and made available for further processing. The output of get_data_for_breakdown is a dictionary containing the original task, which serves as the foundation for generating the sub-tasks.\n\nThe chat_with_template method, which is called within breakdown_task, is responsible for interacting with a language model or template to produce the desired output based on the provided data and prompt. This indicates that breakdown_task not only retrieves the necessary data but also processes it to yield actionable sub-tasks.\n\nOverall, breakdown_task plays a vital role in the task management workflow by ensuring that tasks can be effectively broken down into smaller components, thereby enhancing the manageability and clarity of the overall task structure.\n\n**Note**: It is important to ensure that the instance variable self.original_task is properly initialized before invoking this method to avoid any runtime errors.\n\n**Output Example**: A possible return value from the breakdown_task method could be a structured response detailing the sub-tasks derived from the original task, such as: {'sub_tasks': ['Research climate change impacts', 'Draft report outline', 'Write introduction section']}."
      ],
      "code_start_line": 19,
      "code_end_line": 24,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def breakdown_task(self):\n        \"\"\"\n        将任务拆解成子任务。\n        \"\"\"\n        data = self.get_data_for_breakdown()\n        return self.chat_with_template(data, self.breakdown_prompt)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "agent_factory/manager.py/Manager/get_data_for_breakdown"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_data_for_breakdown",
      "md_content": [
        "**get_data_for_breakdown**: The function of get_data_for_breakdown is to retrieve the original task data for further processing.\n\n**parameters**: The parameters of this Function.\n· None\n\n**Code Description**: The get_data_for_breakdown method is a member of the Manager class. Its primary role is to return a dictionary containing the original task associated with the instance of the Manager class. Specifically, it constructs a dictionary with a single key-value pair, where the key is 'task' and the value is obtained from the instance variable self.original_task. \n\nThis method is called by the breakdown_task method within the same Manager class. The breakdown_task method utilizes get_data_for_breakdown to gather the necessary data before proceeding to render a prompt for further processing. By calling get_data_for_breakdown, breakdown_task ensures that it has access to the current state of the original task, which is essential for generating meaningful sub-tasks.\n\nThe output of get_data_for_breakdown is directly integrated into the breakdown_task method, which then uses this data to create a structured prompt for a language model. This relationship highlights the importance of get_data_for_breakdown in the overall functionality of task decomposition, as it provides the foundational data needed for subsequent operations.\n\n**Note**: It is crucial to ensure that self.original_task is properly initialized before invoking this method to prevent any runtime errors.\n\n**Output Example**: A possible return value from the get_data_for_breakdown method could be a dictionary such as {'task': 'Write a report on climate change.'}."
      ],
      "code_start_line": 26,
      "code_end_line": 29,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_data_for_breakdown(self):\n        return {\n            'task': self.original_task\n        }\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/manager.py/Manager/breakdown_task"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "agent_factory/planner.py": [
    {
      "type": "ClassDef",
      "name": "Planner",
      "md_content": [
        "**Planner**: The function of Planner is to create and execute a plan to achieve a specified goal using a large language model (LLM) and a defined set of available nodes.\n\n**attributes**: The attributes of this Class.\n· llm_model: An instance of a large language model used to generate actions based on the current state.\n· goal: The target state that the planner aims to achieve.\n· available_nodes: A collection of nodes that can be utilized in the planning process.\n· complexity: A measure of the complexity of the planning task.\n· global_context: Contextual information that may influence the planning process.\n· evaluator: An object responsible for evaluating the effectiveness of actions taken.\n\n**Code Description**: The Planner class is designed to facilitate the planning process in a structured manner. It utilizes a priority queue to manage states that need to be explored, implementing a variant of the Q* algorithm. The planning process begins with an initial state, which is added to the open set. The algorithm iteratively explores states, checking if the current state meets the goal. If the goal is reached, the path taken to achieve it is reconstructed and returned. \n\nThe class contains several key methods:\n- `plan(initial_state)`: This method initiates the planning process. It maintains an open set of states to explore, tracks the cost of reaching each state, and determines the best path to the goal based on the actions generated by the LLM and the rewards evaluated by the evaluator.\n- `get_actions(state)`: This method generates a prompt based on the current state and uses the LLM to produce a list of possible actions. The actions are then parsed into a usable format.\n- `transition(state, action)`: This method is intended to apply a given action to the current state to derive the next state. The implementation details for this method are not provided in the code.\n- `is_goal(state)`: This method checks if the current state satisfies the goal condition.\n- `heuristic(state)`: This method is meant to estimate the cost to reach the goal from the current state, although its implementation is not provided.\n- `reconstruct_path(came_from, current_state)`: This method reconstructs the path taken from the initial state to the goal state based on the actions taken.\n- `generate_action_prompt(state)`: This method generates a prompt for the LLM to decide on actions based on the current state.\n- `parse_actions(action_descriptions)`: This method converts the output from the LLM into actionable items.\n\n**Note**: It is important to ensure that the LLM model and evaluator are properly configured before using the Planner class. Additionally, the transition and heuristic methods require implementation for the class to function fully.\n\n**Output Example**: A possible output of the `plan` method could be a sequence of actions leading from the initial state to the goal state, such as:\n```\n[Action1, Action2, Action3]\n``` \nThis output indicates the steps taken to achieve the specified goal, based on the planning process executed by the Planner class."
      ],
      "code_start_line": 4,
      "code_end_line": 71,
      "params": [],
      "have_return": true,
      "code_content": "class Planner:\n    def __init__(self, llm_model, goal, available_nodes, complexity, global_context, evaluator):\n        self.llm_model = llm_model\n        self.goal = goal\n        self.available_nodes = available_nodes\n        self.complexity = complexity\n        self.global_context = global_context\n        self.evaluator = evaluator\n\n    def plan(self, initial_state):\n        # Implement Q* algorithm with LLM-based get_actions\n        open_set = PriorityQueue()\n        open_set.put((0, initial_state))\n        came_from = {}\n        cost_so_far = {initial_state: 0}\n\n        while not open_set.empty():\n            _, current_state = open_set.get()\n\n            if self.is_goal(current_state):\n                return self.reconstruct_path(came_from, current_state)\n\n            actions = self.get_actions(current_state)\n            for action in actions:\n\n                # use the transition agent to get the next action node\n                next_state = self.transition(current_state, action)\n                # generate a reward function based off of optimization of that path\n                reward = self.evaluator.evaluate_action(current_state, action)\n                new_cost = cost_so_far[current_state] - reward  # Negative reward as cost\n\n                if next_state not in cost_so_far or new_cost < cost_so_far[next_state]:\n                    cost_so_far[next_state] = new_cost\n                    priority = new_cost + self.heuristic(next_state)\n                    open_set.put((priority, next_state))\n                    came_from[next_state] = (current_state, action)\n        return None  # No valid plan found\n\n    def get_actions(self, state):\n        # Use the LLM to decide among tools and nodes\n        prompt = self.generate_action_prompt(state)\n        action_descriptions = self.llm_model.generate(prompt)\n        actions = self.parse_actions(action_descriptions)\n        return actions\n\n    def transition(self, state, action):\n        # Apply action to state to get next state\n        pass\n\n    def is_goal(self, state):\n        # Determine if the goal has been achieved\n        return state.meets_goal(self.goal)\n\n    def heuristic(self, state):\n        # Estimate cost to reach goal from current state\n        pass\n\n    def reconstruct_path(self, came_from, current_state):\n        # Reconstruct the path from start to goal\n        pass\n\n    def generate_action_prompt(self, state):\n        # Generate prompt for the LLM to decide actions\n        pass\n\n    def parse_actions(self, action_descriptions):\n        # Parse LLM output into actionable items\n        pass\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the Planner class with specific parameters.\n\n**parameters**: The parameters of this Function.\n· llm_model: This parameter represents the language model that will be used for processing and generating language-related tasks.  \n· goal: This parameter defines the objective or target that the planner aims to achieve.  \n· available_nodes: This parameter lists the nodes that are accessible for the planner to utilize in its operations.  \n· complexity: This parameter indicates the level of complexity involved in the planning process.  \n· global_context: This parameter provides the broader context within which the planning is taking place, influencing decision-making.  \n· evaluator: This parameter is responsible for assessing the outcomes of the planning process, ensuring that the goals are met effectively.  \n\n**Code Description**: The __init__ function serves as the constructor for the Planner class. It initializes an instance of the class by assigning the provided parameters to instance variables. Each parameter plays a crucial role in defining the behavior and capabilities of the Planner. The llm_model is essential for any language processing tasks, while the goal sets the direction for the planning activities. The available_nodes parameter allows the planner to know which nodes it can work with, facilitating the planning process. The complexity parameter helps in understanding the intricacies involved in achieving the goal, while the global_context provides necessary background information that can influence the planning decisions. Finally, the evaluator is critical for measuring the success of the planning efforts, ensuring that the outcomes align with the intended goals.\n\n**Note**: It is important to ensure that all parameters are provided with appropriate values when creating an instance of the Planner class, as they directly affect the functionality and effectiveness of the planning process."
      ],
      "code_start_line": 5,
      "code_end_line": 11,
      "params": [
        "self",
        "llm_model",
        "goal",
        "available_nodes",
        "complexity",
        "global_context",
        "evaluator"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, llm_model, goal, available_nodes, complexity, global_context, evaluator):\n        self.llm_model = llm_model\n        self.goal = goal\n        self.available_nodes = available_nodes\n        self.complexity = complexity\n        self.global_context = global_context\n        self.evaluator = evaluator\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "plan",
      "md_content": [
        "**plan**: The function of plan is to implement the Q* algorithm for pathfinding from an initial state to a goal state.\n\n**parameters**: The parameters of this Function.\n· initial_state: The starting state from which the planning process begins, representing the current configuration of the system.\n\n**Code Description**: The plan function is a core component of the Planner class, designed to find an optimal path from a given initial state to a goal state using a variant of the Q* algorithm. The function begins by initializing a priority queue called open_set, which is used to explore states based on their cost. The initial state is added to this queue with a priority of zero, indicating that it is the starting point of the search.\n\nThe function maintains two dictionaries: came_from, which tracks the state transitions, and cost_so_far, which records the cumulative cost to reach each state. The algorithm operates in a loop that continues until there are no more states to explore in the open_set. Within this loop, the current state is retrieved from the priority queue, and the function checks if this state meets the goal criteria by invoking the is_goal method. If the goal is reached, the function calls reconstruct_path to backtrack and return the sequence of states leading to the goal.\n\nIf the current state is not the goal, the function retrieves possible actions for the current state using the get_actions method. For each action, it computes the next state by applying the transition function. The cost associated with this action is evaluated using the evaluator's evaluate_action method, which generates a reward. The new cost is calculated by subtracting this reward from the current state's cost, effectively treating the reward as a negative cost.\n\nThe function then checks if the next state has been encountered before or if the new cost is lower than any previously recorded cost for that state. If either condition is satisfied, the cost for the next state is updated, and its priority is calculated by adding the new cost to the heuristic estimate of the cost to reach the goal from the next state. The next state is then added to the open_set for further exploration, and the came_from dictionary is updated to reflect the transition from the current state to the next state.\n\nIf the open_set is exhausted without finding a valid plan, the function returns None, indicating that no path to the goal could be found. The plan function is thus integral to the overall planning process, coordinating the exploration of states, action generation, cost evaluation, and path reconstruction.\n\n**Note**: It is essential that the methods is_goal, get_actions, transition, evaluator.evaluate_action, and heuristic are correctly implemented for the plan function to operate effectively. The performance and accuracy of the pathfinding algorithm depend heavily on these components working together seamlessly.\n\n**Output Example**: An example of the return value from the plan function could be a list of states representing the path taken from the initial state to the goal state, such as: [state1, state2, state3, ..., goal_state]. If no valid plan is found, the output would be None."
      ],
      "code_start_line": 13,
      "code_end_line": 40,
      "params": [
        "self",
        "initial_state"
      ],
      "have_return": true,
      "code_content": "    def plan(self, initial_state):\n        # Implement Q* algorithm with LLM-based get_actions\n        open_set = PriorityQueue()\n        open_set.put((0, initial_state))\n        came_from = {}\n        cost_so_far = {initial_state: 0}\n\n        while not open_set.empty():\n            _, current_state = open_set.get()\n\n            if self.is_goal(current_state):\n                return self.reconstruct_path(came_from, current_state)\n\n            actions = self.get_actions(current_state)\n            for action in actions:\n\n                # use the transition agent to get the next action node\n                next_state = self.transition(current_state, action)\n                # generate a reward function based off of optimization of that path\n                reward = self.evaluator.evaluate_action(current_state, action)\n                new_cost = cost_so_far[current_state] - reward  # Negative reward as cost\n\n                if next_state not in cost_so_far or new_cost < cost_so_far[next_state]:\n                    cost_so_far[next_state] = new_cost\n                    priority = new_cost + self.heuristic(next_state)\n                    open_set.put((priority, next_state))\n                    came_from[next_state] = (current_state, action)\n        return None  # No valid plan found\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "agent_factory/planner.py/Planner/get_actions",
        "agent_factory/planner.py/Planner/transition",
        "agent_factory/planner.py/Planner/is_goal",
        "agent_factory/planner.py/Planner/heuristic",
        "agent_factory/planner.py/Planner/reconstruct_path"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_actions",
      "md_content": [
        "**get_actions**: The function of get_actions is to determine a list of actions based on the current state using a Large Language Model (LLM).\n\n**parameters**: The parameters of this Function.\n· state: This parameter represents the current state of the system or environment that is used to generate the actions.\n\n**Code Description**: The get_actions function is responsible for generating a list of actionable items based on the current state of the system. It accomplishes this by first creating a prompt that encapsulates the relevant details of the state through the generate_action_prompt method. This prompt is then passed to the LLM model, which generates descriptions of potential actions. The output from the LLM, referred to as action_descriptions, is subsequently processed by the parse_actions method to convert these descriptions into a structured format suitable for execution.\n\nThe get_actions function is a critical component within the Planner class, as it serves as the intermediary between the state of the system and the action generation process facilitated by the LLM. It relies on the generate_action_prompt function to accurately reflect the current context, ensuring that the LLM can provide relevant action suggestions. The relationship with parse_actions is equally important, as this function transforms the raw output from the LLM into actionable items that can be utilized by other parts of the application.\n\nThe get_actions function is invoked by the plan method of the Planner class. Within the plan method, get_actions is called repeatedly as the algorithm explores different states in search of a goal. The actions returned by get_actions are used to determine the next state in the planning process, making it an essential part of the overall functionality of the planning algorithm.\n\n**Note**: It is crucial to ensure that both the generate_action_prompt and parse_actions functions are properly implemented. The effectiveness of get_actions hinges on the quality of the prompt generated and the accuracy of the parsing logic, as these elements directly influence the relevance and usability of the actions produced.\n\n**Output Example**: An example of the return value from get_actions could be a list of actions such as: [\"move_forward\", \"turn_left\", \"pick_up_object\"]."
      ],
      "code_start_line": 42,
      "code_end_line": 47,
      "params": [
        "self",
        "state"
      ],
      "have_return": true,
      "code_content": "    def get_actions(self, state):\n        # Use the LLM to decide among tools and nodes\n        prompt = self.generate_action_prompt(state)\n        action_descriptions = self.llm_model.generate(prompt)\n        actions = self.parse_actions(action_descriptions)\n        return actions\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/planner.py/Planner/plan"
      ],
      "reference_who": [
        "agent_factory/planner.py/Planner/generate_action_prompt",
        "agent_factory/planner.py/Planner/parse_actions"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "transition",
      "md_content": [
        "**transition**: The function of transition is to apply an action to a given state to determine the next state.\n\n**parameters**: The parameters of this Function.\n· parameter1: state - The current state from which the action will be applied.  \n· parameter2: action - The action to be applied to the current state.\n\n**Code Description**: The transition function is designed to take a current state and an action as inputs, and it is expected to compute the resulting next state after the action is applied. However, the actual implementation of this function is currently a placeholder (indicated by the `pass` statement), meaning that it does not perform any operations or return any values at this time.\n\nThis function is called within the plan method of the Planner class. The plan method implements a pathfinding algorithm (specifically a variant of the Q* algorithm) that seeks to find an optimal path from an initial state to a goal state. During its execution, the plan method retrieves possible actions for the current state and utilizes the transition function to compute the next state that results from applying each action. The next state is then evaluated to determine the cost associated with that action, which is crucial for the algorithm's decision-making process.\n\nThe relationship between the transition function and the plan method is integral, as the transition function is a key component in determining the state space exploration of the algorithm. Without a proper implementation of the transition function, the plan method would be unable to progress through the state space effectively, rendering the pathfinding process incomplete.\n\n**Note**: It is important to implement the transition function to ensure that it accurately reflects the dynamics of the system being modeled. The correctness of the pathfinding algorithm heavily relies on the proper functioning of this transition mechanism."
      ],
      "code_start_line": 49,
      "code_end_line": 51,
      "params": [
        "self",
        "state",
        "action"
      ],
      "have_return": false,
      "code_content": "    def transition(self, state, action):\n        # Apply action to state to get next state\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/planner.py/Planner/plan"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "is_goal",
      "md_content": [
        "**is_goal**: The function of is_goal is to determine if the specified goal has been achieved based on the current state.\n\n**parameters**: The parameters of this Function.\n· state: An object representing the current state of the system, which includes information necessary to evaluate whether the goal has been met.\n\n**Code Description**: The is_goal function checks if the current state meets the predefined goal criteria. It does this by calling the meets_goal method on the state object, passing the goal attribute of the Planner instance as an argument. This function is crucial in the planning process, as it serves as a condition to identify when a valid plan has been successfully executed. \n\nThe is_goal function is called within the plan method of the Planner class. In the plan method, a priority queue is utilized to explore possible states starting from an initial state. As the algorithm iterates through the states, it checks each current state against the goal using the is_goal function. If the current state meets the goal, the plan method will reconstruct and return the path taken to reach that state. If no valid plan is found, the plan method will return None. Thus, the is_goal function plays a pivotal role in the decision-making process of the planning algorithm, directly influencing the flow of state exploration and the eventual outcome of the planning operation.\n\n**Note**: It is important to ensure that the state object passed to the is_goal function has a properly defined meets_goal method that can accurately assess the achievement of the goal.\n\n**Output Example**: If the current state meets the goal, the function will return True; otherwise, it will return False. For instance, if the state object has a meets_goal method that evaluates to True when the goal conditions are satisfied, the output would be True."
      ],
      "code_start_line": 53,
      "code_end_line": 55,
      "params": [
        "self",
        "state"
      ],
      "have_return": true,
      "code_content": "    def is_goal(self, state):\n        # Determine if the goal has been achieved\n        return state.meets_goal(self.goal)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/planner.py/Planner/plan"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "heuristic",
      "md_content": [
        "**heuristic**: The function of heuristic is to estimate the cost to reach the goal from the current state.\n\n**parameters**: The parameters of this Function.\n· state: The current state from which the cost to the goal is estimated.\n\n**Code Description**: The heuristic function is designed to provide an estimation of the cost required to reach a goal state from a given current state. This estimation is crucial for guiding the search algorithm towards the goal efficiently. The function currently contains a placeholder implementation (indicated by the `pass` statement), which means it does not perform any calculations or return any values at this time.\n\nThe heuristic function is called within the `plan` method of the Planner class. In the context of the `plan` method, the heuristic is used to calculate the priority of the next state in the priority queue. Specifically, after determining the new cost of reaching a potential next state (factoring in the reward from the evaluator), the heuristic function is invoked to add an estimated cost to the goal. This combined value (new cost plus heuristic estimate) is then used to prioritize states in the open set, which is a priority queue that manages the states to be explored. The effectiveness of the heuristic function directly impacts the efficiency of the planning process, as it influences the order in which states are explored.\n\n**Note**: It is important to implement the heuristic function with a suitable estimation strategy that reflects the specific problem domain. A well-designed heuristic can significantly improve the performance of the planning algorithm by reducing the number of states that need to be explored."
      ],
      "code_start_line": 57,
      "code_end_line": 59,
      "params": [
        "self",
        "state"
      ],
      "have_return": false,
      "code_content": "    def heuristic(self, state):\n        # Estimate cost to reach goal from current state\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/planner.py/Planner/plan"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "reconstruct_path",
      "md_content": [
        "**reconstruct_path**: The function of reconstruct_path is to reconstruct the path from the start state to the goal state based on the information about where each state came from.\n\n**parameters**: The parameters of this Function.\n· parameter1: came_from - A dictionary that maps each state to the state it came from, which is used to trace back the path taken.\n· parameter2: current_state - The state at which the path reconstruction should end, typically the goal state.\n\n**Code Description**: The reconstruct_path function is designed to take two parameters: came_from and current_state. The came_from parameter is a dictionary that records the relationship between states, specifically which state led to which other state during the planning process. The current_state parameter represents the final state that has been reached, usually the goal state. \n\nAlthough the function body is currently a placeholder (pass), its intended purpose is to utilize the came_from dictionary to backtrack from the current_state to the initial state, effectively reconstructing the sequence of states that were traversed to reach the goal. This is crucial in pathfinding algorithms, where understanding the route taken is as important as finding the route itself.\n\nThe reconstruct_path function is called within the plan method of the Planner class. The plan method implements a pathfinding algorithm (specifically a variant of the Q* algorithm) that explores possible actions from an initial state until it finds a goal state. When the goal state is identified, the plan method invokes reconstruct_path, passing the came_from dictionary and the current_state (goal state) to retrieve the complete path taken from the initial state to the goal. This relationship highlights the importance of reconstruct_path in providing a meaningful output after the planning process has concluded.\n\n**Note**: As the reconstruct_path function is currently not implemented, developers should ensure that the logic for backtracking through the came_from dictionary is correctly defined to achieve the intended path reconstruction. Proper implementation will be essential for the functionality of the planning algorithm."
      ],
      "code_start_line": 61,
      "code_end_line": 63,
      "params": [
        "self",
        "came_from",
        "current_state"
      ],
      "have_return": false,
      "code_content": "    def reconstruct_path(self, came_from, current_state):\n        # Reconstruct the path from start to goal\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/planner.py/Planner/plan"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "generate_action_prompt",
      "md_content": [
        "**generate_action_prompt**: The function of generate_action_prompt is to generate a prompt for the LLM (Large Language Model) to decide actions based on the current state.\n\n**parameters**: The parameters of this Function.\n· state: This parameter represents the current state of the system or environment that is used to generate the action prompt.\n\n**Code Description**: The generate_action_prompt function is designed to create a prompt that will be utilized by a Large Language Model (LLM) to determine the appropriate actions to take in a given situation. The function takes a single parameter, `state`, which encapsulates the current context or conditions that the LLM needs to consider when generating its response. \n\nAlthough the function itself currently contains a placeholder (pass statement), its intended purpose is crucial for the operation of the calling function, get_actions. The get_actions function relies on generate_action_prompt to produce a prompt that is then passed to the LLM model for action generation. Specifically, get_actions invokes generate_action_prompt with the current state, retrieves the generated prompt, and subsequently uses the LLM to generate action descriptions based on that prompt. These descriptions are then parsed into actionable items.\n\nThis relationship highlights the importance of generate_action_prompt in the overall workflow of action decision-making within the system. Without a properly implemented generate_action_prompt, the get_actions function would not be able to effectively communicate the necessary context to the LLM, potentially leading to suboptimal or irrelevant action suggestions.\n\n**Note**: It is important to implement the generate_action_prompt function to ensure that it accurately reflects the necessary details of the state to facilitate effective action generation by the LLM. Proper handling of the state parameter is essential for the success of the action decision-making process."
      ],
      "code_start_line": 65,
      "code_end_line": 67,
      "params": [
        "self",
        "state"
      ],
      "have_return": false,
      "code_content": "    def generate_action_prompt(self, state):\n        # Generate prompt for the LLM to decide actions\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/planner.py/Planner/get_actions"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "parse_actions",
      "md_content": [
        "**parse_actions**: The function of parse_actions is to parse LLM output into actionable items.\n\n**parameters**: The parameters of this Function.\n· action_descriptions: This parameter represents the output generated by the LLM (Language Model), which contains descriptions of potential actions that need to be parsed into a format suitable for execution.\n\n**Code Description**: The parse_actions function is designed to take the output from a language model, specifically the action_descriptions parameter, and convert it into actionable items. The function currently contains a placeholder (pass statement), indicating that the implementation is yet to be completed. \n\nThis function is called within the get_actions method of the Planner class. The get_actions method is responsible for generating a prompt based on the current state and using the LLM to generate action descriptions. Once the action descriptions are obtained, they are passed to the parse_actions function for further processing. The output of parse_actions is then returned as the final list of actions that can be executed based on the LLM's suggestions.\n\nThe relationship between parse_actions and get_actions is crucial, as parse_actions serves as a processing step that transforms raw output from the LLM into a structured format that can be utilized by the rest of the application. This indicates that the functionality of parse_actions is essential for the overall action generation process within the Planner class.\n\n**Note**: It is important to implement the logic within parse_actions to ensure that the action descriptions are correctly interpreted and converted into actionable items. Without this implementation, the get_actions method will not be able to provide meaningful actions based on the LLM's output."
      ],
      "code_start_line": 69,
      "code_end_line": 71,
      "params": [
        "self",
        "action_descriptions"
      ],
      "have_return": false,
      "code_content": "    def parse_actions(self, action_descriptions):\n        # Parse LLM output into actionable items\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "agent_factory/planner.py/Planner/get_actions"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "agent_factory/config.py": [
    {
      "type": "FunctionDef",
      "name": "read_config",
      "md_content": [
        "**read_config**: The function of read_config is to read a configuration file in YAML format and return its contents as a Python dictionary.\n\n**parameters**: The parameters of this Function.\n· file_path: A string representing the path to the configuration file. The default value is '../config/config.yaml'.\n\n**Code Description**: The read_config function opens a specified YAML configuration file in read mode and utilizes the `yaml.safe_load` method to parse the contents of the file into a Python dictionary. This function is designed to facilitate the loading of configuration settings that can be used throughout the application. The default file path points to a configuration file located in a 'config' directory, which is one level up from the current directory.\n\nIn the context of its usage, the read_config function is called within the __init__ method of the Manager class located in the agent_factory/manager.py file. During the initialization of a Manager object, the read_config function is invoked without any arguments, which means it will use the default file path to load the configuration settings. The resulting dictionary is stored in the `self.config` attribute of the Manager instance. This configuration dictionary is then used to retrieve various settings, such as the default model and the path to the prompt folder, which are essential for the operation of the Manager class.\n\n**Note**: It is important to ensure that the specified configuration file exists at the given path and is formatted correctly in YAML. Failure to do so will result in an error when attempting to open or parse the file.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```python\n{\n    'default_model': 'gpt-4o-mini',\n    'prompt_folder_path': '/path/to/prompts',\n    'other_setting': 'value'\n}\n```",
        "**read_config**: The function of read_config is to load configuration settings from a specified YAML file.\n\n**parameters**: The parameters of this Function.\n· file_path: A string representing the path to the YAML configuration file. The default value is '../config/config.yaml'.\n\n**Code Description**: The read_config function is designed to read configuration data from a YAML file. It takes a single parameter, file_path, which specifies the location of the configuration file. If no path is provided, it defaults to '../config/config.yaml'. The function opens the specified file in read mode and uses the yaml.safe_load method to parse the contents of the file into a Python dictionary. This dictionary, which contains the configuration settings, is then returned to the caller.\n\nIn the context of the project, the read_config function is called within the __init__ method of the BaseAgent class located in agent_factory/agent.py. When an instance of BaseAgent is created, the read_config function is invoked to load the configuration settings. The resulting configuration dictionary is stored in the instance variable self.config. Subsequently, specific configuration values are accessed, such as the default model name and the path to the prompt folder, which are used to initialize other components of the agent.\n\n**Note**: It is important to ensure that the specified YAML file exists and is correctly formatted, as any issues with file access or parsing could lead to runtime errors.\n\n**Output Example**: A possible appearance of the code's return value could be:\n```yaml\n{\n  'default_model': 'gpt-4o-mini',\n  'prompt_folder_path': '/path/to/prompts',\n  ...\n}\n```",
        "**read_config**: The function of read_config is to read configuration data from a YAML file and return it as a Python dictionary.\n\n**parameters**: The parameters of this Function.\n· file_path: A string representing the path to the YAML configuration file. The default value is '../config/config.yaml'.\n\n**Code Description**: The read_config function is designed to load configuration settings from a specified YAML file. It takes one optional parameter, file_path, which defaults to '../config/config.yaml' if not provided. The function opens the specified file in read mode ('r') and utilizes the yaml.safe_load method to parse the contents of the file. This method safely loads the YAML data into a Python dictionary, which is then returned by the function. The use of safe_load is important as it prevents the execution of arbitrary code that could be present in the YAML file, thereby enhancing security.\n\n**Note**: It is essential to ensure that the specified YAML file exists at the given path; otherwise, a FileNotFoundError will be raised. Additionally, the function requires the PyYAML library to be installed in the environment to function correctly.\n\n**Output Example**: A possible appearance of the code's return value could be:\n{\n  'database': {\n    'host': 'localhost',\n    'port': 5432,\n    'user': 'admin',\n    'password': 'secret'\n  },\n  'logging': {\n    'level': 'debug',\n    'file': 'app.log'\n  }\n}"
      ],
      "code_start_line": 3,
      "code_end_line": 6,
      "params": [
        "file_path"
      ],
      "have_return": true,
      "code_content": "def read_config(file_path='../config/config.yaml'):\n  with open(file_path, 'r') as file:\n    config = yaml.safe_load(file)\n  return config\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ]
}