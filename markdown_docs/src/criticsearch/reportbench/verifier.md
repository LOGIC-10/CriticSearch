## ClassDef ReportVerifier
# ReportVerifier Class Documentation

## Overview
The `ReportVerifier` class is designed to perform factual verification of extracted data based on a provided context. It verifies whether the answers extracted from a report or document match the expected answers to specific questions. The class utilizes the Rouge-L scoring metric for partial matches and supports multi-threaded verification of multiple questions for efficiency.

### Key Features:
- **Factual Question Verification**: Verifies the correctness of answers to factual questions.
- **Answer Comparison**: Compares model-generated answers with ground truth using exact matches and Rouge-L score for partial matches.
- **Parallel Processing**: Uses a thread pool to process multiple questions concurrently, improving efficiency.
- **Answer Normalization**: Standardizes answers by removing non-alphanumeric characters and converting text to lowercase.

## Constructor: `__init__(self, agent)`
### Parameters:
- `agent`: The agent used to interact with the external source (e.g., a model or API) for verifying answers.

### Description:
The constructor initializes the `ReportVerifier` with an agent for interacting with external systems and a `RougeScorer` to calculate Rouge-L scores for partial answer matches. 

---

## Methods

### 1. `verify_section(self, context: str, extracted_facts: List[Dict]) -> float`
#### Parameters:
- `context`: A string representing the context (e.g., document or passage) that is used to verify the extracted facts.
- `extracted_facts`: A list of dictionaries, where each dictionary contains a question (`"question"`), the expected answer (`"answer"`), and the format in which the answer should be.

#### Returns:
- A float representing the final accuracy score for the verification process.

#### Description:
This method verifies a list of factual questions using the provided context. Each question is matched against the extracted answers. For each question, it sends the question to an agent for verification, compares the model's answer with the ground truth, and computes the accuracy. The verification process is executed in parallel using a thread pool for efficiency.

- For each extracted fact, the `verify_single_question` helper function is used to verify the question by interacting with the agent.
- After all questions are verified, the method calculates the accuracy by considering exact matches and Rouge-L scores for partial matches.
- The final accuracy is a weighted score (70% exact match and 30% Rouge-L score).

---

### 2. `_normalize_text(self, text: str) -> str`
#### Parameters:
- `text`: The string to be normalized.

#### Returns:
- A normalized string containing only alphanumeric characters, converted to lowercase, with spaces removed.

#### Description:
This method standardizes the input text by:
- Removing all non-alphanumeric characters.
- Converting the text to lowercase.
- This is particularly useful for comparing answers while ignoring irrelevant formatting differences.

---

### 3. `_check_answer(self, model_answer: str, ground_truth: str) -> tuple
#### Parameters:
- `model_answer`: The answer generated by the model or system.
- `ground_truth`: The expected correct answer.

#### Returns:
- A tuple `(is_correct, rouge_score)`, where:
  - `is_correct` is a boolean indicating whether the answer is an exact match.
  - `rouge_score` is the Rouge-L score for the answer, if the match is partial.

#### Description:
This method compares the model’s answer to the ground truth:
- It extracts the boxed answers from both the model and the ground truth, if present.
- The answers are normalized (non-alphanumeric characters are removed, and text is converted to lowercase) for comparison.
- The method checks if the answers are an exact match. If not, it computes the Rouge-L score for partial matches.
- The method also prints detailed information about the comparison, showing both the original and normalized answers, and indicating whether the match was exact or partial.

---

### 4. `_calculate_score(self, results: List, total: int) -> float`
#### Parameters:
- `results`: A list of tuples containing the verification results, where each tuple consists of a boolean indicating if the answer was correct and the Rouge-L score for partial matches.
- `total`: The total number of questions verified.

#### Returns:
- A float representing the final weighted accuracy score.

#### Description:
This method calculates the final accuracy score:
- It counts the exact matches and computes the average Rouge-L score for partial matches.
- The final accuracy is a weighted sum of the exact matches (70%) and the average Rouge-L score (30%).
- The method prints a summary of the verification results, including the number of exact matches, average Rouge-L score for partial matches, and the final weighted score.

---

## Example Usage

```python
# Example of initializing the ReportVerifier class and verifying a section
common_agent = BaseAgent()  # Assuming BaseAgent is defined elsewhere
verifier = ReportVerifier(common_agent)

context = "The context of the document or report."
extracted_facts = [
    {"question": "What is the capital of France?", "answer": "Paris", "format": "text"},
    {"question": "Who wrote Hamlet?", "answer": "Shakespeare", "format": "text"}
]

accuracy = verifier.verify_section(context, extracted_facts)
print(f"Final Verification Accuracy: {accuracy:.2%}")
```

---

## Notes:
- The `verify_section` method performs concurrent verification of multiple facts using a thread pool with a maximum of 20 workers.
- The Rouge-L score is used to assess partial matches between the model's answer and the ground truth. This score measures the overlap of the longest common subsequences between the answers.
- The method prints detailed logs during the verification process, including the comparison results for each question.
### FunctionDef __init__(self, agent)
**__init__**: The function of __init__ is to initialize the ReportVerifier object with a given agent and configure a RougeScorer instance for evaluating text similarity.

**parameters**: The parameters of this Function.
- agent: An object that is assigned to the `agent` attribute of the ReportVerifier class.

**Code Description**: 
The `__init__` function is the constructor method for the `ReportVerifier` class. When an instance of `ReportVerifier` is created, the method takes in a parameter `agent` and assigns it to the instance's `agent` attribute. This allows the `ReportVerifier` object to interact with or use the agent throughout its lifecycle.

In addition to initializing the `agent`, the method also initializes a `rouge_scorer.RougeScorer` object. The `RougeScorer` is configured to evaluate the Rouge-L metric, which is used to assess the quality of text summaries by comparing them with reference texts. The `use_stemmer=True` argument is passed to the `RougeScorer` to ensure that stemming is applied during the comparison process. Stemming reduces words to their base or root form, enhancing the robustness of the text comparison by ignoring minor variations in word forms.

**Note**: The `agent` parameter must be an object that is compatible with the functionality intended in the `ReportVerifier` class. The `RougeScorer` instance will always be configured with the Rouge-L metric and stemming enabled, which is crucial for ensuring consistent evaluation results when performing text similarity comparisons.
***
### FunctionDef verify_section(self, context, extracted_facts)
## Function Documentation: `verify_section`

### Description:
The `verify_section` function is responsible for verifying a series of factual questions based on a provided context and extracted facts. It evaluates the accuracy of answers for each question by comparing the response generated by an agent with the expected answer. The function utilizes parallel processing to verify multiple questions simultaneously and calculates an overall accuracy score based on the results.

### Parameters:
- **context** (str): A string representing the context in which the questions are being asked. This context is used by the agent to generate responses relevant to each question.
  
- **extracted_facts** (List[Dict]): A list of dictionaries, where each dictionary represents a question and its associated metadata. Each dictionary must contain the following keys:
  - `"question"` (str): The question to be verified.
  - `"format"` (str): The expected format of the answer.
  - `"answer"` (str): The expected answer to the question.

### Return Value:
- **float**: The function returns a floating-point number representing the overall accuracy score of the verification process. This score is calculated by evaluating exact matches and partial matches using the ROUGE-L metric.

### Functionality:
1. **Initialization**: The function begins by printing a summary of the number of questions to be verified.
  
2. **Verification Process**:
   - A nested function `verify_single_question` is defined to verify each question individually. This function creates a `qa_data` dictionary containing the context, user question, and expected answer format.
   - For each fact in `extracted_facts`, the function invokes the agent's `chat_with_template` method, passing the `qa_data` to generate a response based on the context.
   - The agent's response is then compared with the expected answer using the `_check_answer` method to determine if the answer is correct.
   
3. **Parallel Processing**:
   - The function utilizes `ThreadPoolExecutor` to concurrently verify multiple questions, optimizing the process and reducing the overall verification time.
   - A list of future tasks is created, each representing a call to `verify_single_question`. These tasks are executed in parallel, and their results are collected as they complete.

4. **Results Compilation**:
   - After all questions have been verified, the results (whether each answer was correct or not, and the corresponding ROUGE-L score) are collected.

5. **Accuracy Calculation**:
   - The function calls the `_calculate_score` method to compute a final accuracy score based on the results of the verification. The score is calculated as a weighted average of exact matches and ROUGE-L scores.

6. **Final Output**:
   - The accuracy score is returned to indicate the overall correctness of the factual question-answer verification process.

### Example:
```python
context = "The context of the document or passage to verify."
extracted_facts = [
    {"question": "What is the capital of France?", "format": "text", "answer": "Paris"},
    {"question": "Who wrote '1984'?", "format": "text", "answer": "George Orwell"}
]
accuracy = verifier.verify_section(context, extracted_facts)
print(f"Verification Accuracy: {accuracy}")
```

### Notes:
- The function is designed to handle large numbers of questions efficiently by utilizing multithreading.
- The accuracy score returned provides a combined metric of both exact matches and partial matches, ensuring a comprehensive evaluation of the verification process.
#### FunctionDef verify_single_question(fact)
**verify_single_question**: The function of verify_single_question is to verify a single question by interacting with a model, comparing its answer to the expected answer, and checking the accuracy.

**parameters**: The parameters of this Function.
· fact: A dictionary containing information about the question to be verified, including the question itself, the format of the question, and the expected answer.

**Code Description**: The verify_single_question function is responsible for verifying a specific question and its associated answer by interacting with a conversational model. It accepts a single parameter, fact, which is a dictionary containing key information about the question, its format, and the expected answer.

The function begins by constructing a dictionary, `qa_data`, which stores the relevant details of the question and its format. The dictionary contains the following keys:
- "context": context (assumed to be defined elsewhere in the class or script)
- "user_question": the question to be verified, which is retrieved from the fact dictionary under the key "question"
- "constrained_format": the format of the question, which is retrieved from the fact dictionary under the key "format"

Next, the function prints the question and its format to the console for logging or tracking purposes.

The function then interacts with a model by calling the `chat_with_template` method from the `self.agent` object. This method is used to communicate with the model, utilizing a specific template file, "factQA_verifier.txt". The `qa_data` dictionary is passed as input to the template, allowing the model to generate a response that is contextually relevant to the provided question and format.

After receiving the model's response, the function calls another method, `_check_answer`, to evaluate the model's answer. The response from the model is compared to the expected answer stored in the fact dictionary under the key "answer". The `_check_answer` method checks if the model's answer matches the expected answer and provides a score (typically ROUGE-L) if there is a partial match. This result is returned by the `verify_single_question` function.

The function relies on the `chat_with_template` method from the `BaseAgent` class to facilitate communication with the model, and the `_check_answer` method to determine the accuracy of the response.

**Note**: The `verify_single_question` function assumes that the necessary context is available when it is called, as the "context" field is part of the `qa_data` but is not explicitly provided in the function itself. Additionally, the correct formatting of the "fact" dictionary is essential for the function to perform its task successfully.

**Output Example**: The output of the function is the result of the `_check_answer` method, which typically returns a tuple indicating whether the model's answer is correct and the ROUGE-L score (if applicable). A possible return value could be:
```
(is_correct=True, rouge_score=0.0)
```
***
***
### FunctionDef _normalize_text(self, text)
**_normalize_text**: The function of _normalize_text is to standardize the input text by retaining only alphanumeric characters, converting the text to lowercase, and removing any spaces.

**parameters**: The parameters of this Function.
· text: The input string that needs to be standardized.

**Code Description**: 
The _normalize_text function is responsible for normalizing an input string by performing the following operations:
1. It removes any non-alphanumeric characters using a regular expression (`re.sub(r'[^a-zA-Z0-9]', '', text)`), which ensures that only letters (both uppercase and lowercase) and numbers remain in the string.
2. It converts the resulting string to lowercase using the `lower()` method, ensuring case uniformity.
3. It then returns the transformed string.

This function is primarily used within the context of answer verification. For example, the function is called by `_check_answer`, where it plays a key role in standardizing both the model's predicted answer and the ground truth answer. After extracting specific parts of the answers (using a regular expression to find content inside `\boxed{}`), `_normalize_text` is applied to both the model's and the ground truth answers before performing any further comparisons. This standardization ensures that the comparison is made in a consistent format, eliminating discrepancies due to case differences, spacing, or punctuation.

**Note**: 
- The function only retains alphanumeric characters, so punctuation and spaces are completely discarded. Ensure that the input text is appropriate for this kind of transformation.
- This function may be useful in scenarios where exact matches are required and formatting
***
### FunctionDef _check_answer(self, model_answer, ground_truth)
**_check_answer**: The function of _check_answer is to evaluate the correctness of a model's answer against a ground truth answer by performing normalization and comparison, and to calculate a ROUGE-L score if the answers do not match exactly.

**parameters**: The parameters of this Function.
· model_answer: A string representing the answer generated by the model that needs to be evaluated.
· ground_truth: A string representing the correct answer against which the model's answer is compared.

**Code Description**: The _check_answer function is designed to assess the accuracy of a model's answer by comparing it with the ground truth answer. It begins by defining a regular expression pattern to extract content enclosed within `\boxed{}` from both the model's answer and the ground truth. The function utilizes the `re.findall` method to retrieve these boxed answers.

Once the boxed answers are extracted, the function initializes two variables: `is_correct`, which is set to False, and `rouge_score`, initialized to 0.0. If both the model's boxed answer and the ground truth boxed answer are present, the function proceeds to normalize these answers using the _normalize_text method. This normalization process standardizes the text by removing non-alphanumeric characters, converting it to lowercase, and eliminating spaces.

After normalization, the function checks for an exact match between the normalized model answer and the normalized ground truth. If they match, `is_correct` is set to True. If they do not match, the function calculates the ROUGE-L score using the scorer's score method, which provides a measure of similarity between the two answers based on their longest common subsequence.

The function then prints a detailed output, indicating whether the answers matched exactly or partially, along with the original and normalized versions of both answers. Finally, it returns a tuple containing the correctness of the model's answer (as a boolean) and the calculated ROUGE-L score.

This function is called by the verify_single_question function, which is responsible for verifying individual questions by preparing the necessary data and invoking the _check_answer function to perform the evaluation. The verify_single_question function collects the question and its expected answer, interacts with an agent to obtain the model's response, and then utilizes _check_answer to determine the accuracy of that response.

**Note**: It is important to ensure that the model_answer and ground_truth inputs are formatted correctly to contain boxed answers for the function to operate effectively. The function's output will provide insights into the accuracy of the model's response, which is crucial for evaluating the performance of the model in generating answers.

**Output Example**: 
(is_correct=True, rouge_score=0.0)
***
### FunctionDef _calculate_score(self, results, total)
**_calculate_score**: The function of _calculate_score is to compute a final accuracy score based on exact matches and ROUGE-L scores.

**parameters**:
· results: A list of tuples where each tuple consists of a boolean indicating whether the answer was correct and the ROUGE-L score for incorrect answers.  
· total: An integer representing the total number of questions being evaluated.

**Code Description**:  
The _calculate_score function calculates a weighted accuracy score for a factual question-answer verification task. The function takes in two arguments: `results` and `total`.

1. **exact_matches**: This is calculated using a generator expression that iterates through the `results` list. For each tuple, it checks if the first element (a boolean) is `True` and counts the number of such occurrences. This represents the number of exact matches between the expected answer and the actual answer.

2. **rouge_scores**: A list comprehension is used to extract the second element (ROUGE-L score) from the tuples in the `results` list where the first element is `False`. This is done to track the ROUGE-L scores for answers that were not an exact match. ROUGE-L is a metric used to evaluate the quality of partial matches in text.

3. **avg_rouge**: The average ROUGE-L score is computed by summing up the values in the `rouge_scores` list and dividing by the `total`. If there are no partial matches (i.e., `rouge_scores` is empty), it defaults to 0.

4. **final_accuracy**: This is the weighted average of the exact match rate and the average ROUGE-L score. The final score is calculated as 70% of the exact match ratio and 30% of the average ROUGE-L score. This weighted score provides a combined metric of both exact and partial matches.

5. **Print Statements**: The function outputs a summary of the verification process, including the total number of questions, the number of exact matches, the average ROUGE-L score for partial matches, and the final weighted accuracy score.

6. **Return Value**: The function returns the final accuracy score as a floating-point value.

The _calculate_score function is called by the `verify_section` function, which is responsible for verifying a set of factual questions. The results from the verification process (a list of tuples containing exact match statuses and ROUGE-L scores) are passed into _calculate_score to determine the final accuracy of the verification process.

**Note**:  
- The accuracy score returned by _calculate_score is crucial for evaluating the performance of the factual question-answer verification task.  
- The exact match percentage has a higher weight (70%) compared to the partial match score (30%).  
- Ensure that the `results` list contains the appropriate structure, i.e., a tuple with a boolean and a score, for this function to work correctly.

**Output Example**:  
The following is an example of the output printed by the function:
```
=== Verification Results Summary ===
Total Questions: 100
Exact Matches: 85/100 (85.00%)
Average ROUGE-L for Partial Matches: 25.00%
Final Weighted Score: 66.50%
========================================
```
This indicates that out of 100 questions, 85 exact matches were found, and the average ROUGE-L score for partial matches is 25%. The final weighted score is 66.50%.
***
